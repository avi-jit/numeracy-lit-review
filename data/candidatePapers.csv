"id","url","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"5be5619fc22300ef356ec4ef729d567ce7116c57","https://www.semanticscholar.org/paper/5be5619fc22300ef356ec4ef729d567ce7116c57",10,"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data","A hierarchical taxonomy for numerical reasoning skills with more than ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning is proposed.","arXiv.org",2023,"Mubashara Akhtar,Abhilash Shankarampeta,Vivek Gupta,Arpit Patil,O. Cocarascu,Elena Simperl",0,62,0
"28a5a53dafacebad8a7c47773079caeffb9a5baa","https://www.semanticscholar.org/paper/28a5a53dafacebad8a7c47773079caeffb9a5baa",9,"Representing Numbers in NLP: a Survey and a Vision","This work synthesizes best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.","North American Chapter of the Association for Computational Linguistics",2021,"Avijit Thawani,J. Pujara,Pedro A. Szekely,Filip Ilievski",73,70,4
"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc",7,"Reasoning with Language Model Prompting: A Survey","This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","Annual Meeting of the Association for Computational Linguistics",2022,"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen",78,218,3
"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","https://www.semanticscholar.org/paper/b1fe7bdcfef4e12febe7e8bed8826e66689d60ed",7,"Auto-Regressive Next-Token Predictors are Universal Learners","This work presents a theoretical framework for studying auto-regressive next-token predictors, and introduces a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and examines the interplay between length complexity and other notions of complexity.","arXiv.org",2023,"Eran Malach",2,48,0
"8424082e3bf4792462eb112d7ebcecf5b0dc3613","https://www.semanticscholar.org/paper/8424082e3bf4792462eb112d7ebcecf5b0dc3613",7,"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning","This survey paper discusses the performance of transformers on diﬀerent reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning.","Conference on Automated Knowledge Base Construction",2021,"Chadi Helwe,C. Clavel,Fabian M. Suchanek",32,95,3
"4d17732d90440682b0500f4e209c6cc4fac20e0e","https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e",6,"Teaching Algorithmic Reasoning via In-context Learning","This work shows that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which it is referred to as algorithmic prompting, and evaluates the approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrates significant boosts in performance.","arXiv.org",2022,"Hattie Zhou,Azade Nova,H. Larochelle,Aaron C. Courville,Behnam Neyshabur,Hanie Sedghi",56,68,12
"8da9b1436212b233fc49c7daf1ba15c22874ff5a","https://www.semanticscholar.org/paper/8da9b1436212b233fc49c7daf1ba15c22874ff5a",5,"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models","The proposed CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization, disentangles abstract tool creation and concrete decision execution, resulting in improved performance.","",2023,"Cheng Qian,Chi Han,Y. Fung,Yujia Qin,Zhiyuan Liu,Heng Ji",7,35,1
"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","https://www.semanticscholar.org/paper/dd18782960f9ee4c66b79e1518b342ad3f8d19e7",5,"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct","WizardMath is presented, which enhances the mathematical reasoning abilities of Llama-2, by applying the proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math.","arXiv.org",2023,"Haipeng Luo,Qingfeng Sun,Can Xu,Pu Zhao,Jian-Guang Lou,Chongyang Tao,Xiubo Geng,Qingwei Lin,Shifeng Chen,Dongmei Zhang",29,107,6
"2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e","https://www.semanticscholar.org/paper/2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e",5,"Design of Chain-of-Thought in Math Problem Solving","Through extensive experiments on GSM8K, MATHQA, and SVAMP, it is found that program CoTs often have superior effectiveness in math problem solving and Python is a better choice of language than Wolfram for program CoT.","arXiv.org",2023,"Zhanming Jie,Trung Quoc Luong,Xinbo Zhang,Xiaoran Jin,Hang Li",1,42,1
"b9e8b62bcc019f47a0a015568f70039b3b7c1196","https://www.semanticscholar.org/paper/b9e8b62bcc019f47a0a015568f70039b3b7c1196",5,"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model","This paper introduces Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-s solving (CoS) approach, and results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities.","arXiv.org",2023,"Cheng Qian,Chenyan Xiong,Zhenghao Liu,Zhiyuan Liu",0,39,0
"f176d0d466d7c778a6435fe9a8d7e49508cb9059","https://www.semanticscholar.org/paper/f176d0d466d7c778a6435fe9a8d7e49508cb9059",5,"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts","This work proposes XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts by allowing method switching, which provides a fresh perspective on the collaborative integration of diverse reasoning thinking in a unified framework.","arXiv.org",2023,"Tengxiao Liu,Qipeng Guo,Yuqing Yang,Xiangkun Hu,Yue Zhang,Xipeng Qiu,Zheng Zhang",0,41,0
"3693683c4e0405819fae7115ad680f769eb83534","https://www.semanticscholar.org/paper/3693683c4e0405819fae7115ad680f769eb83534",5,"Neural Comprehension: Language Models with Compiled Neural Networks","The method, which call ""Neural Comprehension"", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning.","arXiv.org",2023,"Yixuan Weng,Minjun Zhu,Fei Xia,Bin Li,Shizhu He,Kang Liu,Jun Zhao",4,66,0
"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",5,"PAL: Program-aided Language Models","This paper presents Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.","International Conference on Machine Learning",2022,"Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,Yiming Yang,Jamie Callan,Graham Neubig",201,62,34
"db4ab91d5675c37795e719e997a2827d3d83cd45","https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45",5,"Towards Reasoning in Large Language Models: A Survey","A comprehensive overview of the current state of knowledge on reasoning in large language models, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions are provided.","Annual Meeting of the Association for Computational Linguistics",2022,"Jie Huang,K. Chang",126,130,4
"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","https://www.semanticscholar.org/paper/1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f",5,"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning","A tight linkage between the scaling of a network weights’ standard deviation and its effective length scale on a sinusoidal regression problem is demonstrated, suggesting simple modifications to control the length scale of the function learned by a neural network and, thus, its smoothness.","Trans. Mach. Learn. Res.",2023,"David A. Klindt",0,74,0
"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","https://www.semanticscholar.org/paper/0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",5,"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension","This study is the largest survey of the deep learning models in NLP field to date, providing an overview of the various formats and domains of the current resources, and highlighting the current lacunae for future work.","ACM Computing Surveys",2021,"Anna Rogers,Matt Gardner,Isabelle Augenstein",103,347,8
"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","https://www.semanticscholar.org/paper/2e86c8cab0135ad7e363bb64aacef4eb8c639e6a",5,"Benchmarks for Automated Commonsense Reasoning: A Survey","A number of recommendations for future development of commonsense AI benchmarks are made, including that the creators of benchmarks invest the work needed to ensure that benchmark examples are consistently high quality.","ACM Computing Surveys",2023,"E. Davis",14,189,1
"681cee58cf7e54199191cf9e0baf6851d8356704","https://www.semanticscholar.org/paper/681cee58cf7e54199191cf9e0baf6851d8356704",5,"Complex QA and language models hybrid architectures, Survey","This paper reviews the state-of-the-art of language models architectures and strategies for ""complex""question-answering (QA, C QA, CPS) with a focus on hybridization, and discusses some challenges associated with complex QA.","",2023,"Xavier Daull,P. Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco",6,390,0
"12d16f426edc6ab248fb476007bd1646282d4d68","https://www.semanticscholar.org/paper/12d16f426edc6ab248fb476007bd1646282d4d68",5,"Language Model Behavior: A Comprehensive Survey","Over 250 recent studies of English language model behavior before task-specific fine-tuning are discussed, synthesizing recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.","arXiv.org",2023,"Tyler A. Chang,B. Bergen",15,392,0
"5dc15ac1c92ab7492f121471823fb13a95d273ba","https://www.semanticscholar.org/paper/5dc15ac1c92ab7492f121471823fb13a95d273ba",4,"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis","A mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework is presented and results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism.","",2023,"Alessandro Stolfo,Yonatan Belinkov,Mrinmaya Sachan",1,51,1
"fae57797d357bfa3b39b220336d1a2e8deba5318","https://www.semanticscholar.org/paper/fae57797d357bfa3b39b220336d1a2e8deba5318",4,"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving","The idea is to maintain a moderate-sized model and employ the cross-task knowledge sharing to improve the model capacity in a multi-task setting, and construct a Mixture-of-Experts (MoE) architecture for modeling mathematical text, to capture the common mathematical knowledge across tasks.","Knowledge Discovery and Data Mining",2023,"Wayne Xin Zhao,Kun Zhou,Beichen Zhang,Zheng Gong,Zhipeng Chen,Yuanhang Zhou,Ji-rong Wen,Jing Sha,Shijin Wang,Cong Liu,Guoping Hu",0,59,0
"4993258852711c4e3d0011325ac3db680eae84f4","https://www.semanticscholar.org/paper/4993258852711c4e3d0011325ac3db680eae84f4",4,"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models","","arXiv.org",2023,"Xiaoxuan Wang,Ziniu Hu,Pan Lu,Yanqiao Zhu,Jieyu Zhang,Satyen Subramaniam,Arjun R. Loomba,Shichang Zhang,Yizhou Sun,Wei Wang",12,48,1
"3e565c544a8639cc9c7568833e484d7610f5e5d4","https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4",4,"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning","A novel approach is proposed, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example, which verifies its effectiveness in selecting in- context examples.","International Conference on Learning Representations",2022,"Pan Lu,Liang Qiu,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Tanmay Rajpurohit,Peter Clark,A. Kalyan",57,57,6
"170c97c7215f42edfb20c2248f954879e91ef86e","https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e",4,"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","This paper presents Chameleon, an AI system that mitigates LLM limitations by augmenting LLMs with plug-and-play modules for compositional reasoning, and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","arXiv.org",2023,"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao",82,68,12
"a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","https://www.semanticscholar.org/paper/a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9",4,"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning","The MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%, and underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.","arXiv.org",2023,"Xiang Yue,Xingwei Qu,Ge Zhang,Yao Fu,Wenhao Huang,Huan Sun,Yu Su,Wenhu Chen",13,77,0
"765225535151b5908d99a96a509729795c2eb840","https://www.semanticscholar.org/paper/765225535151b5908d99a96a509729795c2eb840",4,"ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving","This paper proposes ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools, thereby amalgamating the analytical prowess of language and the computational efficiency of tools.","arXiv.org",2023,"Zhibin Gou,Zhihong Shao,Yeyun Gong,Yelong Shen,Yujiu Yang,Minlie Huang,Nan Duan,Weizhu Chen",2,64,0
"5076bbbf831a92174c9cc1b347bd0584560435fc","https://www.semanticscholar.org/paper/5076bbbf831a92174c9cc1b347bd0584560435fc",4,"Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning","Experimental results from complex reasoning tasks reveal that the prompting method adaptation and decomposition granularity adaptation enhance performance across all tasks, and the model adaptation approach significantly reduces API costs while maintaining superior performance.","arXiv.org",2023,"Jianpeng Zhou,Wanjun Zhong,Yanlin Wang,Jiahai Wang",0,47,0
"b6667ba4f586489f12587446c6daaa3f09cfc539","https://www.semanticscholar.org/paper/b6667ba4f586489f12587446c6daaa3f09cfc539",4,"Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset","This work proposes Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education that provides a high-quality formal representation, the reasoning steps, and the final solution, and experiments show that existing large language models exhibit weak performance on complex reasoning.","",2023,"Haoyi Wu,Wenyang Hui,Yezeng Chen,Weiqi Wu,Kewei Tu,Yi Zhou",0,39,0
"965e409a3e7b5670d609837fac9823b160d6639c","https://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c",4,"Logical Tasks for Measuring Extrapolation and Rule Comprehension","This work defines and characterize logical tasks and discusses system requirements for their solution, and discusses the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias.","arXiv.org",2022,"Ippei Fujisawa,R. Kanai",4,61,0
"8f1b0c247171510fc68da27a16a377456376a5a7","https://www.semanticscholar.org/paper/8f1b0c247171510fc68da27a16a377456376a5a7",4,"Assessing GPT-3.5 and GPT-4 in Generating International Classification of Diseases Billing Codes","While the models appear to exhibit a general conceptual understanding of the codes and their descriptions, they have a propensity for hallucinating key details, suggesting underlying technological limitations of the base LLMs.","medRxiv",2023,"A. Soroush,B. Glicksberg,E. Zimlichman,Y. Barash,R. Freeman,A. Charney,G. Nadkarni,E. Klang",0,31,0
"9160673f89ecb326db8c64f8d09ab30f35a74e9d","https://www.semanticscholar.org/paper/9160673f89ecb326db8c64f8d09ab30f35a74e9d",4,"Arithmetic with Language Models: from Memorization to Computation","These findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.","arXiv.org",2023,"D. Maltoni,M. Ferrara",0,11,0
"1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b","https://www.semanticscholar.org/paper/1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b",4,"What Algorithms can Transformers Learn? A Study in Length Generalization","This work proposes a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task and provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.","arXiv.org",2023,"Hattie Zhou,Arwen Bradley,Etai Littwin,Noam Razin,O. Saremi,Josh Susskind,Samy Bengio,Preetum Nakkiran",0,55,0
"0f2199296f01694ee46b6059879260fb80a84fa6","https://www.semanticscholar.org/paper/0f2199296f01694ee46b6059879260fb80a84fa6",4,"Teaching Autoregressive Language Models Complex Tasks By Demonstration","The results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.","arXiv.org",2021,"Gabriel Recchia",18,46,1
"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","https://www.semanticscholar.org/paper/d331de3b6bebb0f9af1fddf1b730ec057a7026d4",4,"Relational World Knowledge Representation in Contextual Language Models: A Review","This work proposes to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision, and provides a high-level, extensible taxonomy for knowledge representation in L Ms.","Conference on Empirical Methods in Natural Language Processing",2021,"Tara Safavi,Danai Koutra",32,132,1
"338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c","https://www.semanticscholar.org/paper/338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c",4,"ReTAG: Reasoning Aware Table to Analytic Text Generation","ReTAG is the first model that can controllably use multiple reasoning methods within a structure-aware sequence to sequence model to surpass state of the art performance in multiple table to text tasks.","",2023,"Deepanway Ghosal,Preksha Nema,A. Raghuveer",0,56,0
"11a4284e335ba39330b59d9f42ca3272a6166991","https://www.semanticscholar.org/paper/11a4284e335ba39330b59d9f42ca3272a6166991",4,"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future","A thorough survey of the current research according to the taxonomies of methods within the domain of chain-of-thought reasoning, and describes XoT with frontier applications, covering planning, tool use, and distillation.","arXiv.org",2023,"Zheng Chu,Jingchang Chen,Qianglong Chen,Weijiang Yu,Tao He,Haotian Wang,Weihua Peng,Ming Liu,Bing Qin,Ting Liu",2,211,0
"d69521c1521bfa3fefb32b2106da38b1821e975d","https://www.semanticscholar.org/paper/d69521c1521bfa3fefb32b2106da38b1821e975d",3,"NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering","This paper proposes a non-autoregressive program generation framework, which independently generates complete program tuples containing both operators and operands, which can address the error propagation issue while significantly boosting the speed of program generation.","Natural Language Processing and Chinese Computing",2022,"Tengxun Zhang,Hongfei Xu,Josef van Genabith,Deyi Xiong,Hongying Zan",2,63,0
"a3bbe282f1666900a6604991f96e1637c6946253","https://www.semanticscholar.org/paper/a3bbe282f1666900a6604991f96e1637c6946253",3,"MATHVISTA: EVALUATING MATHEMATICAL REASON-","A benchmark designed to amalgamate challenges from diverse mathematical and visual tasks, MATHVISTA fuels future research in the development of generalpurpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks.","",2023,"Pan Lu,Hritik Bansal,Tony Xia,Jiacheng Liu,Chun-yue Li,Hannaneh Hajishirzi,Hao Cheng,Kai-Wei Chang,Michel Galley,Jianfeng Gao",0,88,0
"0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","https://www.semanticscholar.org/paper/0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0",3,"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs","This work proposes a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements of the given question and document with Semantic-oriented hierarchical Graph structures.","arXiv.org",2023,"Fengbin Zhu,Chao Wang,Fuli Feng,Zifeng Ren,Moxin Li,Tat-seng Chua",0,34,0
"e61a96cf602ebff6683929aaf916e25614a475bc","https://www.semanticscholar.org/paper/e61a96cf602ebff6683929aaf916e25614a475bc",3,"UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities","The UPAR prompting framework is proposed, designed to emulate the structure of human cognition within LLMs, enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection and offering an epistemological foundation for existing prompting techniques.","arXiv.org",2023,"Hejia Geng,Boxun Xu,Peng Li",0,87,0
"c22cc4e2eed78b4b31e50d94ea35da0405aabb87","https://www.semanticscholar.org/paper/c22cc4e2eed78b4b31e50d94ea35da0405aabb87",3,"Multi-Operational Mathematical Derivations in Latent Space","This paper investigates how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation.","arXiv.org",2023,"Marco Valentino,Jordan Meadows,Lan Zhang,Andr'e Freitas",0,38,0
"3f2cb353c7528efafb847309ab1e1e95245740a4","https://www.semanticscholar.org/paper/3f2cb353c7528efafb847309ab1e1e95245740a4",3,"Mathematical Capabilities of ChatGPT","It is found that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a Mathematical search engine and knowledge base interface, and GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty.","arXiv.org",2023,"Simon Frieder,Luca Pinchetti,Ryan-Rhys Griffiths,Tommaso Salvatori,Thomas Lukasiewicz,Philipp Christian Petersen,Alexis Chevalier,J. Berner",156,78,4
"95ca67ba607d7859ee8eec457f4b59b115d69bf5","https://www.semanticscholar.org/paper/95ca67ba607d7859ee8eec457f4b59b115d69bf5",3,"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models","This work proposes ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs that can effectively leverage the multi-turn conversation ability of chat- based LLMs, and integrate the thought chain following and tools manipulation in a unified way.","arXiv.org",2023,"Z. Chen,Kun Zhou,Beichen Zhang,Zheng Gong,Wayne Xin Zhao,Ji-rong Wen",7,31,0
"87875a07976c26f82705de1fc70041169e5d652b","https://www.semanticscholar.org/paper/87875a07976c26f82705de1fc70041169e5d652b",3,"LeanDojo: Theorem Proving with Retrieval-Augmented Language Models","This paper introduces LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks, and develops ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library.","arXiv.org",2023,"Kaiyu Yang,Aidan M. Swope,Alex Gu,Rahul Chalamala,Peiyang Song,Shixing Yu,Saad Godil,R. Prenger,Anima Anandkumar",14,107,2
"888728745dbb769e29ed475d4f7661eebe1a71cf","https://www.semanticscholar.org/paper/888728745dbb769e29ed475d4f7661eebe1a71cf",3,"A Survey on Evaluation of Large Language Models","An overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas, is provided.","arXiv.org",2023,"Yu-Chu Chang,Xu Wang,Jindong Wang,Yuanyi Wu,Kaijie Zhu,Hao Chen,Linyi Yang,Xiaoyuan Yi,Cunxiang Wang,Yidong Wang,Weirong Ye,Yue Zhang,Yi Chang,Philip S. Yu,Qian Yang,Xingxu Xie",79,280,1
"537335d9aad0ddbaef93e7f88b0db096671ef6ec","https://www.semanticscholar.org/paper/537335d9aad0ddbaef93e7f88b0db096671ef6ec",3,"No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function","A method is proposed that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning in large language models without requiring additional fine-tuning or reinforcement learning with human feedback alignment.","arXiv.org",2023,"Haotian Xu",1,45,0
"e892a225c417fbac7545c3e31b45d1c42dc9c933","https://www.semanticscholar.org/paper/e892a225c417fbac7545c3e31b45d1c42dc9c933",3,"Chain-of-Thought Reasoning is a Policy Improvement Operator","The central hypothesis is that chain-of-thought reasoning can act as a policy improvement operator, analogously to how Monte-Carlo Tree Search is used in AlphaZero.","arXiv.org",2023,"Hugh Zhang,David C. Parkes",2,59,1
"8d806a91e5f2166ee6823eb7e6e8e56826b6776d","https://www.semanticscholar.org/paper/8d806a91e5f2166ee6823eb7e6e8e56826b6776d",3,"NLPBench: Evaluating Large Language Models on Solving NLP Problems","This study presents a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams, and reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance.","arXiv.org",2023,"Linxin Song,Jieyu Zhang,Lechao Cheng,Pengyuan Zhou,Tianyi Zhou,Irene Li",0,41,0
"b16c7d45183b9d595ab64301be019741b1528860","https://www.semanticscholar.org/paper/b16c7d45183b9d595ab64301be019741b1528860",3,"Llemma: An Open Language Model For Mathematics","Llemma is a large language model for mathematics that outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis, and is capable of tool use and formal theorem proving without any further finetuning.","arXiv.org",2023,"Zhangir Azerbayev,Hailey Schoelkopf,Keiran Paster,Marco Dos Santos,Stephen McAleer,Albert Q. Jiang,Jia Deng,Stella Rose Biderman,S. Welleck",1,92,0
"44b506d9619b5f957dc2b5588801138f343c0308","https://www.semanticscholar.org/paper/44b506d9619b5f957dc2b5588801138f343c0308",3,"Let's reward step by step: Step-Level reward model as the Navigators for Reasoning","This work proposes a heuristic greedy search algorithm that employs the step-level feedback from PRM to optimize the reasoning pathways explored by LLMs and demonstrates enhanced results compared to the Chain of Thought on mathematical benchmarks like GSM8K and MATH.","arXiv.org",2023,"Qianli Ma,Haotian Zhou,Tingkai Liu,Jianbo Yuan,Pengfei Liu,Yang You,Hongxia Yang",0,54,0
"9c6cecf409ca6257e717291b47c2cf8b75cf4a58","https://www.semanticscholar.org/paper/9c6cecf409ca6257e717291b47c2cf8b75cf4a58",3,"Learning Multi-step Reasoning from Arithmetic Task","This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning by continually pre-training LMs on a synthetic dataset M S AT, which stands for M ulti- s tep A rithmetic T ask.","",2023,"Tianduo Wang,Wei Lu",1,28,0
"5938abafd61881f6b23a2ba318d2d3d0327402c0","https://www.semanticscholar.org/paper/5938abafd61881f6b23a2ba318d2d3d0327402c0",3,"A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram","This work converts diagrams into basic textual clauses to describe diagram features effectively, and proposes a new neural solver called PGPSNet to fuse multi-modal information efficiently and promote geometric understanding and reasoning.","International Joint Conference on Artificial Intelligence",2023,"Ming-Liang Zhang,Fei Yin,Cheng-Lin Liu",1,46,0
"1d29334cfbe9a1a943082058876f0c22d44c62fd","https://www.semanticscholar.org/paper/1d29334cfbe9a1a943082058876f0c22d44c62fd",3,"A Survey of Large Language Models","A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.","arXiv.org",2023,"Wayne Xin Zhao,Kun Zhou,Junyi Li,Tianyi Tang,Xiaolei Wang,Yupeng Hou,Yingqian Min,Beichen Zhang,Junjie Zhang,Zican Dong,Yifan Du,Chen Yang,Yushuo Chen,Z. Chen,Jinhao Jiang,Ruiyang Ren,Yifan Li,Xinyu Tang,Zikang Liu,Peiyu Liu,J. Nie,Ji-rong Wen",524,417,40
"bebede105aa69a81045bf79682272ee4cfb61475","https://www.semanticscholar.org/paper/bebede105aa69a81045bf79682272ee4cfb61475",3,"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks","By incorporating CoNN modules into the LM, the framework effectively tackles rule-intensive challenges and achieves flawless execution on symbolic operations tasks, highlighting the potential of the method in enabling LMs to possess true symbolic comprehension capabilities.","",2023,"Yixuan Weng,Minjun Zhu,Fei Xia,Bin Li,Shizhu He,Kang Liu,Jun Zhao",0,63,0
"261549439aebdda72b648ecc462448fd24857ac1","https://www.semanticscholar.org/paper/261549439aebdda72b648ecc462448fd24857ac1",3,"Progressive-Hint Prompting Improves Reasoning in Large Language Models","This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers.","arXiv.org",2023,"Chuanyang Zheng,Zhengying Liu,Enze Xie,Zhenguo Li,Yu Li",43,66,4
"62176de125738e3b95850d1227bac81fd646b78e","https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e",3,"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models","The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem.","Annual Meeting of the Association for Computational Linguistics",2023,"Lei Wang,Wanyu Xu,Yihuai Lan,Zhiqiang Hu,Yunshi Lan,R. Lee,Ee-Peng Lim",49,53,4
"a39f960b28a627d554712a457efa5d3e3b7f8406","https://www.semanticscholar.org/paper/a39f960b28a627d554712a457efa5d3e3b7f8406",3,"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning","Three pretraining tasks that operate at both the whole program and sub-program level are proposed: Variable Integrity Ranking, which guides the model to focus on useful variables; Variable Operator Prediction, which decomposes the supervision into fine-grained single operator prediction; and Variable Keyphrase Masking, which encourages the modelto identify key evidence that sub- programs are derived from.","arXiv.org",2023,"Qianying Liu,Dongsheng Yang,Wenjie Zhong,Fei Cheng,S. Kurohashi",0,41,0
"a52dd1e900200e0733eea927edc7d6c27aeba187","https://www.semanticscholar.org/paper/a52dd1e900200e0733eea927edc7d6c27aeba187",3,"TheoremQA: A Theorem-driven Question Answering dataset","This paper introduces TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems and finds that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting.","arXiv.org",2023,"Wenhu Chen,Ming Yin,Max Ku,Yixin Wan,Xueguang Ma,Jianyu Xu,Tony Xia,Xinyi Wang,Pan Lu",16,57,5
"4313b009cd30a469400d36b9fb20267620293d04","https://www.semanticscholar.org/paper/4313b009cd30a469400d36b9fb20267620293d04",3,"A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers","A data generation method for producing intricate mathematical derivations, and systematically perturb them with respect to syntax, structure, and semantics, employing symbolic algebra for scalable data production and augmentation.","arXiv.org",2023,"Jordan Meadows,Marco Valentino,Damien Teney,André Freitas",2,49,0
"8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","https://www.semanticscholar.org/paper/8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c",3,"An Empirical Study on Challenging Math Problem Solving with GPT-4","This work explores the frontier of using GPT-4 for solving more complex and challenging math problems, and considers MathChat, a conversational problem-solving framework newly proposed in this work.","arXiv.org",2023,"Yiran Wu,Feiran Jia,Shaokun Zhang,Han-Tai Li,Erkang Zhu,Yue Wang,Y. Lee,Richard Peng,Qingyun Wu,Chi Wang",8,38,0
"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","https://www.semanticscholar.org/paper/0b47c8eae44c8c3c5f474d1b9869ba90a5199eae",3,"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements","This paper proposes a framework for MWP solvers based on the generation of linguistic variants of the problem text and shows that training on linguistic variant of problem statements and voting on candidate predictions improve the mathematical reasoning and robustness of the model.","Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",2023,"Syed Rifat Raiyan,Md. Nafis Faiyaz,S. Kabir,Mohsinul Kabir,H. Mahmud,Md. Kamrul Hasan",0,105,0
"ca31b8584b6c022ef15ddfe994fe361e002b7729","https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729",3,"A Comprehensive Overview of Large Language Models","This review article is intended to provide a systematic survey, but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research direction.","arXiv.org",2023,"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,N. Barnes,A. Mian",12,443,0
"5bb1b0fadd773f6b4de0d37130c668ecef84d7b2","https://www.semanticscholar.org/paper/5bb1b0fadd773f6b4de0d37130c668ecef84d7b2",3,"When Do Program-of-Thoughts Work for Reasoning?","This work proposes complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities, and designs an auto-synthesizing and stratifying algorithm that applies to instruction generation for mathematical reasoning and code data filtering for code generation tasks.","arXiv.org",2023,"Zhen Bi,Ningyu Zhang,Yinuo Jiang,Shumin Deng,Guozhou Zheng,Huajun Chen",2,53,0
"8db1dcae055842f43ccac04182957b20d15bbe6b","https://www.semanticscholar.org/paper/8db1dcae055842f43ccac04182957b20d15bbe6b",3,"Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems","This paper formally defines the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith, and proposes three novel techniques that improve performance.","arXiv.org",2023,"Aniruddha Deb,Neeva Oza,Sarthak Singla,Dinesh Khandelwal,Dinesh Garg,Parag Singla",0,37,0
"cddb552f6c3464a54a02b0b64b2d1af56c086606","https://www.semanticscholar.org/paper/cddb552f6c3464a54a02b0b64b2d1af56c086606",3,"MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning","This paper proposes a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities, and yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems.","arXiv.org",2023,"Ke Wang,Houxing Ren,Aojun Zhou,Zimu Lu,Sichun Luo,Weikang Shi,Renrui Zhang,Linqi Song,Mingjie Zhan,Hongsheng Li",0,53,0
"c27f3904490b8e5f9f39fe2b36722090c189e916","https://www.semanticscholar.org/paper/c27f3904490b8e5f9f39fe2b36722090c189e916",3,"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF","This work describes the participation of the team in the ARQMath 2022 Lab, where two highly complementary methods for effective math answer and formula retrieval are applied, using a lexical sparse retriever and a fine-tuned bi-encoder dense retriever to capture contextual similarity and semantic matching.","Conference and Labs of the Evaluation Forum",2022,"Wei Zhong,Yuqing Xie,Jimmy J. Lin",4,58,0
"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","https://www.semanticscholar.org/paper/1f036b092a74f0c2f7eef37daa16eeb0f5954d9b",3,"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)","A comparison between human-developed machine learning model and models sampled through Neural Architecture Search (NAS) determine an efficient approach to solve the problem of addition using embedded hexadecimal digits.","arXiv.org",2022,"Victor Robila,Kexin Pei,Junfeng Yang",0,20,0
"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","https://www.semanticscholar.org/paper/0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62",3,"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering","This model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers, which outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery.","Annual Meeting of the Association for Computational Linguistics",2023,"Vaishali Pal,Andrew Yates,E. Kanoulas,M. de Rijke",0,40,0
"d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","https://www.semanticscholar.org/paper/d40dbe668d5b68419e934dfa4c5851ffa1c24aa2",3,"Exposing Attention Glitches with Flip-Flop Language Modeling","This work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning, and hypothesizes that attention glitches account for some of the closed-domain hallucinations in natural LLMs.","arXiv.org",2023,"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang",6,106,0
"edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a","https://www.semanticscholar.org/paper/edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a",3,"It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models","It is discovered that the strong ID generalization stems from structured representations, while behind the unsatisfying OOD performance, the models still exhibit clear learned algebraic structures.","arXiv.org",2023,"Xingcheng Xu,Zihao Pan,Haipeng Zhang,Yanqing Yang",0,28,0
"cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6","https://www.semanticscholar.org/paper/cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6",3,"Can transformers learn the greatest common divisor?",,"arXiv.org",2023,"Franccois Charton",0,37,0
"0b778079946764292de3771a489d5ce9e1868a8b","https://www.semanticscholar.org/paper/0b778079946764292de3771a489d5ce9e1868a8b",3,"The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models","Recent works that employ LLMs for human-centric tasks are discussed and a case study showing that popular LLMs tokenize temporal data incorrectly is presented, highlighting potential solutions that can help bridge this ""modality gap"".","arXiv.org",2023,"Dimitris Spathis,F. Kawsar",0,39,0
"34ca51ce10e8d1d6c950ba519329714a0184d004","https://www.semanticscholar.org/paper/34ca51ce10e8d1d6c950ba519329714a0184d004",3,"KwaiYiiMath: Technical Report","The KwaiyiiMath is introduced, which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks.","arXiv.org",2023,"Jia-Yi Fu,Lei Lin,Xiaoyang Gao,Pengli Liu,Zhengzong Chen,Zhirui Yang,Shengnan Zhang,Xue Zheng,Yan Li,Yuliang Liu,Xucheng Ye,Yiqiao Liao,Chao Liao,Bin Chen,Chengru Song,Junchen Wan,Zijia Lin,Fuzheng Zhang,Zhongyuan Wang,Di Zhang,Kun Gai",0,123,0
"b09420b30fc093d63fb2ee1aac26c71da81da437","https://www.semanticscholar.org/paper/b09420b30fc093d63fb2ee1aac26c71da81da437",3,"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks","Language-Interfaced Fine-Tuning is proposed and found that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks.","Neural Information Processing Systems",2022,"Tuan Dinh,Yuchen Zeng,Ruisu Zhang,Ziqian Lin,Shashank Rajput,Michael Gira,Jy-yong Sohn,Dimitris Papailiopoulos,Kangwook Lee",32,181,3
"e82e3f4347674b75c432cb80604d38ee630d4bf6","https://www.semanticscholar.org/paper/e82e3f4347674b75c432cb80604d38ee630d4bf6",3,"Transformers Learn Shortcuts to Automata","It is found that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics.","International Conference on Learning Representations",2022,"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang",43,107,1
"3214670b9da8462905cf7a5ced17eb9e5d4fbc52","https://www.semanticscholar.org/paper/3214670b9da8462905cf7a5ced17eb9e5d4fbc52",3,"LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models","LM-Infinite is computationally efficient with $O(n)$ time and space, and demonstrates consistent text generation fluency and quality to as long as 128k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup.","arXiv.org",2023,"Chi Han,Qifan Wang,Wenhan Xiong,Yu Chen,Heng Ji,Sinong Wang",7,49,2
"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","https://www.semanticscholar.org/paper/4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80",3,"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence","A novel intermediate training task, names meaning-matching, designed to directly learn a meaning-text correspondence, is proposed that enables PLMs to learn lexical semantic information and is found to be a safe intermediate task that guarantees a similar or better performance of downstream tasks.","NAACL-HLT",2022,"Myeongjun Jang,Frank Mtumbuka,Thomas Lukasiewicz",5,53,1
"0cf5c61f367e3937dc07be634a43952a50b589a4","https://www.semanticscholar.org/paper/0cf5c61f367e3937dc07be634a43952a50b589a4",3,"A Simple, Yet Effective Approach to Finding Biases in Code Generation","This work introduces an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test, and demonstrates how the framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.","Annual Meeting of the Association for Computational Linguistics",2022,"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski",4,50,0
"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","https://www.semanticscholar.org/paper/6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249",3,"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data","DyRRen is proposed, an extended retriever-reranker-generator framework where each generation step is enhanced by a dynamic reranking of retrieved sentences that outperforms existing baselines on the FinQA dataset.","AAAI Conference on Artificial Intelligence",2022,"Xiao Li,Yin Zhu,Sichen Liu,Jiangzhou Ju,Yuzhong Qu,Gong Cheng",4,45,0
"be8f769f135b0ffcf829594b9421788781f38008","https://www.semanticscholar.org/paper/be8f769f135b0ffcf829594b9421788781f38008",3,"KitchenScale: Learning to predict ingredient quantities from recipe contexts","This work introduces KitchenScale, a fine-tuned Pre- trained Pre-trained Language Model (PLM) that predicts a target ingredient's quantity and measurement unit given its recipe context and adopted the Discrete Latent Exponent method to cope with high variance of numerical scales in recipe corpora.","Expert systems with applications",2023,"Donghee Choi,Mogan Gim,Samy Badreddine,Hajung Kim,Donghyeon Park,Jaewoo Kang",0,42,0
"856c342606aca05434e48f2e53cdbd6f6b886802","https://www.semanticscholar.org/paper/856c342606aca05434e48f2e53cdbd6f6b886802",3,"Pre‐trained language models: What do they know?","The behavior of pre‐trained language models (PLMs) in some inference tasks they were not initially trained for are studied, focusing on very recent research works related to the inference capabilities of PLMs in some selected tasks such as factual probing and common‐sense reasoning.","WIREs Data Mining and Knowledge Discovery",2023,"Nuno Guimarães,Ricardo Campos,Alípio Jorge",0,19,0
"884c0b6db564208d99cadf2548f0aa96dee5f859","https://www.semanticscholar.org/paper/884c0b6db564208d99cadf2548f0aa96dee5f859",3,"Commonsense Reasoning with Implicit Knowledge in Natural Language","This work takes a middle ground where it uses smaller language models together with a relatively smaller but targeted natural language text corpora to develop unstructured commonsense knowledge sources and explores three strategies for knowledge incorporation.","Conference on Automated Knowledge Base Construction",2021,"Pratyay Banerjee,Swaroop Mishra",2,71,0
"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","https://www.semanticscholar.org/paper/f8a36a6c10b4f598f7a936f09c58de31c9e10bcd",3,"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning","Investigation of the underlying hypothesis in the most simple conceivable scenario – the addition of real numbers finds that two layer neural networks fail to learn the structure of this task and that growing the network’s width leads to a complex division of input space.","",2022,"",0,25,0
"706c6b3781374b0b11f98f204a4ddd05b26ed009","https://www.semanticscholar.org/paper/706c6b3781374b0b11f98f204a4ddd05b26ed009",3,"Knowledge Infused Decoding","Knowledge Infused Decoding (KID) -- a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding, which maintains a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning.","International Conference on Learning Representations",2022,"Ruibo Liu,Guoqing Zheng,Shashank Gupta,Radhika Gaonkar,Chongyang Gao,Soroush Vosoughi,Milad Shokouhi,A. Awadallah",11,93,3
"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","https://www.semanticscholar.org/paper/9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3",3,"20Q: Overlap-Free World Knowledge Benchmark for Language Models","20Q is introduced, a novel benchmark using the Twenty Questions game to evaluate world knowledge and common sense of language models and shows that in-context learning is inefficient for evaluating language models’ world knowledge — fine-tuning is necessary to show their true capabilities.","IEEE Games Entertainment Media Conference",2022,"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans",1,36,0
"8b0b5b998d65add5c036b45c36afa3f9df96f4b0","https://www.semanticscholar.org/paper/8b0b5b998d65add5c036b45c36afa3f9df96f4b0",3,"Toward Building a Language Model for Understanding Temporal Commonsense","This paper focuses on the development of language models for temporal commonsense inference over several pre-trained language models, and relies on multi-step fine-tuning using multiple corpora, and masked language modeling to predict masked temporal indicators that are crucial for temporal Commonsense reasoning.","AACL",2022,"Mayuko Kimura,L. Pereira,Ichiro Kobayashi",0,24,0
"d3a1e7f060bfe7d97c2e430da9eb6967bbbe358a","https://www.semanticscholar.org/paper/d3a1e7f060bfe7d97c2e430da9eb6967bbbe358a",3,"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks","This work presents an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which is called KG-to-task match and demonstrates that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHow-based KGs are the bestmatch for PIQA across all 3 analysis phases.","Conference of the European Chapter of the Association for Computational Linguistics",2021,"Lisa Bauer",13,43,1
"6597d61bdb531051678c773526758a6dc113b9ce","https://www.semanticscholar.org/paper/6597d61bdb531051678c773526758a6dc113b9ce",3,"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences","This work introduces a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs, and proposes a pairwise accuracy metric to reliably measure an agent's ability to perform Commonsense reasoning over a given situation.","Findings",2021,"Shikhar Singh,Nuan Wen,Yu Hou,Pegah Alipoormolabashi,Te-Lin Wu,Xuezhe Ma,Nanyun Peng",27,50,8
"7e5ca499cd9b932921bda84db98f75087d0b0683","https://www.semanticscholar.org/paper/7e5ca499cd9b932921bda84db98f75087d0b0683",3,"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey","A survey of commonsense knowledge acquisition and reasoning tasks, the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions are presented.","AAAI Conference on Artificial Intelligence",2022,"Prajjwal Bhargava,Vincent Ng",25,84,0
"ee153a2c91d36b034dc86c945aee9859b79da812","https://www.semanticscholar.org/paper/ee153a2c91d36b034dc86c945aee9859b79da812",3,"Test of Time: Instilling Video-Language Models with a Sense of Time","This work proposes a temporal adaptation recipe on top of one video-language model, VideoCLIp, based on post-pretraining on a small amount of video-text data and conducts a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness.","Computer Vision and Pattern Recognition",2023,"Piyush Bagad,Makarand Tapaswi,Cees G. M. Snoek",6,139,0
"60b4b7514ac3a3d9fbebf10f935dd89267c93621","https://www.semanticscholar.org/paper/60b4b7514ac3a3d9fbebf10f935dd89267c93621",3,"SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning","This work introduces a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor and formalizes this challenging task as SituatedGen.","arXiv.org",2023,"Yunxiang Zhang,Xiaojun Wan",0,62,0
"dfeca881c80ce2cb222e619e2e8f27a00c0e7b6f","https://www.semanticscholar.org/paper/dfeca881c80ce2cb222e619e2e8f27a00c0e7b6f",3,"An Overview Of Temporal Commonsense Reasoning and Acquisition","The need for careful interpretation of research is emphasized to guard against overpromising evaluation results in light of the shallow reasoning present in transformers, and the need for appropriately preparing datasets and suitable evaluation metrics is emphasized.","arXiv.org",2023,"Georg Wenzel,A. Jatowt",0,104,0
"23cc6b2ed88872fcd3767cf054100e8eddcdb0a1","https://www.semanticscholar.org/paper/23cc6b2ed88872fcd3767cf054100e8eddcdb0a1",3,"CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks","A significant performance gap is found when NLP systems are evaluated on CRoW compared to humans, showcasing that commonsense reasoning is far from being solved in real-world task settings.","arXiv.org",2023,"Mete Ismayilzada,Debjit Paul,Syrielle Montariol,Mor Geva,Antoine Bosselut",0,75,0
"ec5f1c1bc361ca4c53932e3a3a14d6f3f814e268","https://www.semanticscholar.org/paper/ec5f1c1bc361ca4c53932e3a3a14d6f3f814e268",3,"MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document","This work proposes MTGER, a novel Multi-view Temporal Graph Enhanced Temporal Reasoning framework for temporal reasoning over time-involved documents, which explicitly models the temporal relationships among facts by multi-view temporal graphs.","",2023,"Zheng Chu,Zekun Wang,Jiafeng Liang,Ming Liu,Bing Qin",0,45,0
"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","https://www.semanticscholar.org/paper/b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea",3,"Self-Attention Networks Can Process Bounded Hierarchical Languages","It is proved that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language.","Annual Meeting of the Association for Computational Linguistics",2021,"Shunyu Yao,Binghui Peng,C. Papadimitriou,Karthik Narasimhan",38,57,4
"9ca329408813d209b1dcb36936f7f9cba82506bd","https://www.semanticscholar.org/paper/9ca329408813d209b1dcb36936f7f9cba82506bd",3,"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation","This work shows that extrapolation can be enabled by simply changing the position representation method, though it finds that current methods do not allow for efficient extrapolation, and introduces a simpler and more efficient position method, Attention with Linear Biases (ALiBi).","International Conference on Learning Representations",2021,"Ofir Press,Noah A. Smith,M. Lewis",205,48,43
"dc48bc1a4d81e0f37603013fd2a95644dc233bd0","https://www.semanticscholar.org/paper/dc48bc1a4d81e0f37603013fd2a95644dc233bd0",3,"Functional Interpolation for Relative Positions Improves Long Context Transformers","It is theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple, and empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.","arXiv.org",2023,"Shanda Li,Chong You,Guru Guruganesh,J. Ainslie,Santiago Ontanon,M. Zaheer,Sumit K. Sanghai,Yiming Yang,Sanjiv Kumar,Srinadh Bhojanapalli",1,69,0
"42b877f7423fc727bff5b6e173ad336da33079a9","https://www.semanticscholar.org/paper/42b877f7423fc727bff5b6e173ad336da33079a9",3,"Uncovering hidden geometry in Transformers via disentangling position and context","This decomposition of hidden states (or embeddings) of trained transformers into interpretable components offers structural insights about input formats in in-context learning (especially for induction heads) and in arithmetic tasks.","arXiv.org",2023,"Jiajun Song,Yiqiao Zhong",0,49,0
"200fcd964f41efe0c35a3f888a520ede08a3269c","https://www.semanticscholar.org/paper/200fcd964f41efe0c35a3f888a520ede08a3269c",3,"From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers","This paper investigates the inherent capabilities of transformer models in learning arithmetic algorithms and introduces Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which are linked to mechanisms in relative position encoding.","arXiv.org",2023,"Shaoxiong Duan,Yining Shi",0,25,0
"3ec464696db25acc2c39a6d967ec3df09e06f633","https://www.semanticscholar.org/paper/3ec464696db25acc2c39a6d967ec3df09e06f633",2,"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models","A tool-interacting divide-and-conquer strategy enabling large language models (LLMs) to answer complex multimodal multi-hop questions, demonstrating the efficacy and generality of this approach.","arXiv.org",2023,"Hossein Rajabzadeh,Suyuchen Wang,Hyock Ju Kwon,Bang Liu",0,33,0
"c16c05ca0a3d24519405849fd24604fc1ce47751","https://www.semanticscholar.org/paper/c16c05ca0a3d24519405849fd24604fc1ce47751",2,"Towards Best Practices of Activation Patching in Language Models: Metrics and Methods","In several settings of localization and circuit discovery in language models, it is found that varying these hyperparameters could lead to disparate interpretability results.","arXiv.org",2023,"Fred Zhang,Neel Nanda",0,66,0
"69bfa665e507fcee4a8d003933998eb89f336c9f","https://www.semanticscholar.org/paper/69bfa665e507fcee4a8d003933998eb89f336c9f",2,"Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning","This study investigates the impact of CoT on the ability of transformers to in-context learn a simple to study, yet general family of compositional functions: multi-layer perceptrons (MLPs).","",,"Yingcong Li,Kartik K. Sreenivasan,Angeliki Giannou,Dimitris Papailiopoulos,Samet Oymak",0,45,0
"123acfbccca0460171b6b06a4012dbb991cde55b","https://www.semanticscholar.org/paper/123acfbccca0460171b6b06a4012dbb991cde55b",2,"Large Language Models Are Zero-Shot Time Series Forecasters","This work proposes procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values and shows how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions.","arXiv.org",2023,"Nate Gruver,Marc Finzi,Shikai Qiu,Andrew Gordon Wilson",3,77,0
"4ef578869c957f5fb969fa9c164dca0433f48042","https://www.semanticscholar.org/paper/4ef578869c957f5fb969fa9c164dca0433f48042",2,"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining","This paper proposes FORTAP, the first exploration to leverage spreadsheet formulas for table pretraining with tree attention, and proposes two novel self-supervised pretraining objectives derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP).","Annual Meeting of the Association for Computational Linguistics",2021,"Zhoujun Cheng,Haoyu Dong,Fan Cheng,Ran Jia,Pengfei Wu,Shi Han,Dongmei Zhang",19,45,2
"cbccb1201eee6432020276762a44ebfbf8f981a0","https://www.semanticscholar.org/paper/cbccb1201eee6432020276762a44ebfbf8f981a0",2,"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge","A novel unsupervised methodology leveraging external knowledge and contextualized word embeddings from ClinicalBERT for numerical reasoning in a variety of phenotypic contexts is presented.","arXiv.org",2022,"Ashwani Tanwar,Jingqing Zhang,Julia Ive,Vibhor Gupta,Yike Guo",3,40,0
"efcd9c1438559d908efd702333232078fd251a0f","https://www.semanticscholar.org/paper/efcd9c1438559d908efd702333232078fd251a0f",2,"Phenotyping in clinical text with unsupervised numerical reasoning for patient stratification","This article proposes a novel unsupervised method that leverages external clinical knowledge and contextualized word embeddings by ClinicalBERT for numerical reasoning in different phenotypic contexts and finds that these phenotypes from clinical text can be used to impute the missing values in structured data, which enrich and improve data quality.","Experimental biology and medicine",2022,"Ashwani Tanwar,Jingqing Zhang,Julia Ive,Vibhor Gupta,Yike Guo",1,39,0
"35922cd0d6b17e45320917338e9f98cb5c1a4f6f","https://www.semanticscholar.org/paper/35922cd0d6b17e45320917338e9f98cb5c1a4f6f",2,"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters","It is shown that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference.","Annual Meeting of the Association for Computational Linguistics",2022,"Boshi Wang,Sewon Min,Xiang Deng,Jiaming Shen,You Wu,Luke Zettlemoyer,Huan Sun",53,40,7
"27f32dc1aab7919eb7039deca067c3fbdc719c2a","https://www.semanticscholar.org/paper/27f32dc1aab7919eb7039deca067c3fbdc719c2a",2,"Koala: An Index for Quantifying Overlaps with Pre-training Corpora","Koala, a searchable index over large pre-training corpora using compressed suffix arrays with highly efficient compression rate and search support, is launched, providing a framework to do forensic analysis on the current and future benchmarks as well as to assess the degree of memorization in the output from the LLMs.","arXiv.org",2023,"Thuy-Trang Vu,Xuanli He,Gholamreza Haffari,Ehsan Shareghi",2,42,0
"015f7ef29bebe85b705efead4c737e252baa3da7","https://www.semanticscholar.org/paper/015f7ef29bebe85b705efead4c737e252baa3da7",2,"Large Language Models Are Not Strong Abstract Reasoners","A new benchmark for evaluating language models beyond memorization on abstract reasoning tasks is introduced and extensive evaluations of state-of-the-art LLMs are performed, showing that they currently achieve very limited performance in contrast with other natural language tasks.","",2023,"Gaël Gendron,Qiming Bao,M. Witbrock,G. Dobbie",3,66,1
"6a3c67d35f3a9c10c8fde6c325a0535c03876068","https://www.semanticscholar.org/paper/6a3c67d35f3a9c10c8fde6c325a0535c03876068",2,"SEER: A Knapsack approach to Exemplar Selection for In-Context HybridQA","This work presents Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse and formulates exemplar selection as a Knapsack Integer Linear Program.","arXiv.org",2023,"Jonathan Tonglet,Manon Reusens,Philipp Borchert,Bart Baesens",0,51,0
"7e4f5589327b6b574cc950a03fd1d6236e9e6128","https://www.semanticscholar.org/paper/7e4f5589327b6b574cc950a03fd1d6236e9e6128",2,"Answering Questions by Meta-Reasoning over Multiple Chains of Thought","This work introduces Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers.","arXiv.org",2023,"Ori Yoran,Tomer Wolfson,Ben Bogin,Uri Katz,Daniel Deutch,Jonathan Berant",23,68,1
"5ffd2ded3d828a34aed5c0076834e14c634256a2","https://www.semanticscholar.org/paper/5ffd2ded3d828a34aed5c0076834e14c634256a2",2,"Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs","","",2023,"Xiang Zhang,Senyu Li,B. Hauer,Ning Shi,Grzegorz Kondrak",2,32,0
"692902404c282de936249aef68ce9f974815128c","https://www.semanticscholar.org/paper/692902404c282de936249aef68ce9f974815128c",2,"CLEVA: Chinese Language Models EVAluation Platform","CLEVA is a user-friendly platform crafted to holistically evaluate Chinese LLMs, employing a standardized workflow to assess LLMs' performance across various dimensions, regularly updating a competitive leaderboard.","arXiv.org",2023,"Yanyang Li,Jianqiao Zhao,Duo Zheng,Zi-Yuan Hu,Zhi Chen,Xiaohui Su,Yongfeng Huang,Shijia Huang,Dahua Lin,Michael R. Lyu,Liwei Wang",1,95,0
"287a30043ad1c3e349095af7e3e42d3be3b6c0c9","https://www.semanticscholar.org/paper/287a30043ad1c3e349095af7e3e42d3be3b6c0c9",2,"SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training","SNIP, a Symbolic-Numeric Integrated Pre-training, is introduced, which employs joint contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the pre-trained embeddings, and provides cross-domain insights into the representations.","arXiv.org",2023,"Kazem Meidani,Parshin Shojaee,Chandan K. Reddy,A. Farimani",0,55,0
"8d2709ed1788a67e64425fb410bb49f3ee49e088","https://www.semanticscholar.org/paper/8d2709ed1788a67e64425fb410bb49f3ee49e088",2,"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review","This review offers an extensive analysis on the transformative potential of LLMs in modern medicine and highlights the pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.","arXiv.org",2023,"Mingze Yuan,Peng Bao,J. Yuan,Yunhao Shen,Zi Chen,Yi Xie,Jie Zhao,Yang Chen,Li Zhang,Lin Shen,Bin Dong",0,186,0
"cc8417aa578016203cb52efc63592bba64b08bb3","https://www.semanticscholar.org/paper/cc8417aa578016203cb52efc63592bba64b08bb3",2,"FinMath: Injecting a Tree-structured Solver for Question Answering over Financial Reports","A new framework named FinMath is proposed, which improves the model’s numerical reasoning capacity by injecting a tree-structured neural model to perform multi-step numerical reasoning.","International Conference on Language Resources and Evaluation",2022,"Chenying Li,Wenbo Ye,Yilun Zhao",7,35,2
"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e",2,"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.","BIGSCIENCE",2022,"Sid Black,Stella Rose Biderman,Eric Hallahan,Quentin G. Anthony,Leo Gao,Laurence Golding,Horace He,Connor Leahy,Kyle McDonell,Jason Phang,M. Pieler,USVSN Sai Prashanth,Shivanshu Purohit,Laria Reynolds,J. Tow,Benqi Wang,Samuel Weinbach",349,142,38
"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a",2,"Exploring Length Generalization in Large Language Models","This paper establishes that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale, and shows that combining pretrained large language models' in-context learning abilities with scratchpad prompting results in a dramatic improvement in lengthgeneralization.","Neural Information Processing Systems",2022,"Cem Anil,Yuhuai Wu,Anders Andreassen,Aitor Lewkowycz,Vedant Misra,V. Ramasesh,Ambrose Slone,Guy Gur-Ari,Ethan Dyer,Behnam Neyshabur",70,33,16
"7d645a3fd276918374fd9483fd675c28e46506d1","https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1",2,"Galactica: A Large Language Model for Science","Galactica is introduced: a large language model that can store, combine and reason about scientific knowledge, and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%.","arXiv.org",2022,"Ross Taylor,Marcin Kardas,Guillem Cucurull,Thomas Scialom,A. Hartshorn,Elvis Saravia,Andrew Poulton,Viktor Kerkez,Robert Stojnic",261,107,47
"7d5175db1b99552491063d2d9581b0b51e1d2932","https://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932",2,"Despite ""super-human"" performance, current LLMs are unsuited for decisions about ethics and safety","This work provides a simple new prompting strategy that leads to yet another supposedly ""super-human"" result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset).","arXiv.org",2022,"Joshua Albrecht,Ellie Kitanidis,Abraham J. Fetterman",8,43,0
"033275ccc2c7c5c38592ae893da0b5923cf90717","https://www.semanticscholar.org/paper/033275ccc2c7c5c38592ae893da0b5923cf90717",2,"Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases","A holistic survey of CLQA is provided with a detailed taxonomy studying the field from multiple angles, including graph types (modality, reasoning domain, background semantics), modeling aspects, modeling aspects (encoder, processor, decoder), supported queries, datasets, evaluation metrics, and applications.","arXiv.org",2023,"Hongyu Ren,Mikhail Galkin,Michael Cochez,Zhaocheng Zhu,J. Leskovec",15,171,1
"8236010c2ecc94d826be6010ff187fdc000e7df6","https://www.semanticscholar.org/paper/8236010c2ecc94d826be6010ff187fdc000e7df6",2,"Deductive Verification of Chain-of-Thought Reasoning","This work proposes Natural Program, a natural language-based deductive reasoning format that enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps and significantly enhances the rigor and trustfulness of generated reasoning steps.","arXiv.org",2023,"Z. Ling,Yunhao Fang,Xuanlin Li,Zhiao Huang,Mingu Lee,R. Memisevic,Hao Su",12,71,2
"50f9f33b284b7363fbd9b9d2da4939b989a1c7cd","https://www.semanticscholar.org/paper/50f9f33b284b7363fbd9b9d2da4939b989a1c7cd",2,"Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models","This perspective paper comprehensively review existing evaluations of Large Language Models using both standardized tests and ability-oriented benchmarks to highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting.","arXiv.org",2023,"Yuxi Ma,Chi Zhang,Song-Chun Zhu",4,65,0
"1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc","https://www.semanticscholar.org/paper/1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",2,"Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification","The effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter is explored, and a novel and effective prompting method, explicit \uline{c}ode-based \ULine{s}elf-\uline {v}erification~(CSV), is proposed to further boost the mathematical reasoning potential of GPN.","arXiv.org",2023,"Aojun Zhou,Ke Wang,Zimu Lu,Weikang Shi,Sichun Luo,Zipeng Qin,Shaoqing Lu,Anya Jia,Linqi Song,Mingjie Zhan,Hongsheng Li",15,29,0
"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","https://www.semanticscholar.org/paper/894ed1aba8e42a4ec27ba53ecde383b14c5128ca",2,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models","This survey paper provides an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility, and aims to equip researchers, practitioners, and enthusiasts with a thorough understanding of these models.","arXiv.org",2023,"Kaiyuan Gao,Su He,Zhenyu He,Jiacheng Lin,Qizhi Pei,Jie Shao,Wei Zhang",0,178,0
"62b4e06f5249d22e4a153ec4a2dc934c6a014372","https://www.semanticscholar.org/paper/62b4e06f5249d22e4a153ec4a2dc934c6a014372",2,"OWL: A Large Language Model for IT Operations","The OWL is introduced, a large language model trained on the authors' collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks.","arXiv.org",2023,"Hongcheng Guo,Jian Yang,Jiaheng Liu,Liqun Yang,Linzheng Chai,Jiaqi Bai,Junran Peng,Xiaorong Hu,Chao Chen,Dongfeng Zhang,Xu Shi,Tieqiao Zheng,Liangfan Zheng,Bo Zhang,Ke Xu,Zhoujun Li",1,73,0
"12b233752c7097ea6525622bed238ae2d2193c5a","https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",2,"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback","MINT is introduced, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback, and repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making into a compact subset for efficient evaluation.","arXiv.org",2023,"Xingyao Wang,Zihan Wang,Jiateng Liu,Yangyi Chen,Lifan Yuan,Hao Peng,Heng Ji",13,69,1
"b099104d1a065cbc1432af22e6085b1a44dbc839","https://www.semanticscholar.org/paper/b099104d1a065cbc1432af22e6085b1a44dbc839",2,"LPML: LLM-Prompting Markup Language for Mathematical Reasoning","This paper proposed a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL) and discovered that by prompting LLMs to generate structured text in XML-like markup language, it could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs.","arXiv.org",2023,"Ryutaro Yamauchi,Sho Sonoda,Akiyoshi Sannai,Wataru Kumagai",0,20,0
"77b1f1c6d1658d120456b9046667cf009ceb39ce","https://www.semanticscholar.org/paper/77b1f1c6d1658d120456b9046667cf009ceb39ce",2,"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models","Experimental results on two popular benchmarks for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin.","arXiv.org",2023,"L. Yu,Weisen Jiang,Han Shi,Jincheng Yu,Zhengying Liu,Yu Zhang,James T. Kwok,Zheng Li,Adrian Weller,Weiyang Liu",11,74,1
"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","https://www.semanticscholar.org/paper/5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",2,"Qwen Technical Report","Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts, and includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques.","arXiv.org",2023,"Jinze Bai,Shuai Bai,Yunfei Chu,Zeyu Cui,Kai Dang,Xiaodong Deng,Yang Fan,Wenhang Ge,Yu Han,Fei Huang,Binyuan Hui,Luo Ji,Mei Li,Junyang Lin,Runji Lin,Dayiheng Liu,Gao Liu,Chengqiang Lu,K. Lu,Jianxin Ma,Rui Men,Xingzhang Ren,Xuancheng Ren,Chuanqi Tan,Sinan Tan,Jianhong Tu,Peng Wang,Shijie Wang,Wei Wang,Shengguang Wu,Benfeng Xu,Jin Xu,An Yang,Hao Yang,Jian Yang,Jian Yang,Shusheng Yang,Yang Yao,Bowen Yu,Yu Bowen,Hongyi Yuan,Zheng Yuan,Jianwei Zhang,Xing Zhang,Yichang Zhang,Zhenru Zhang,Chang Zhou,Jingren Zhou,Xiaohuan Zhou,Tianhang Zhu",5,164,1
"a1426b13b74dbad17b34606d25aabe1d61f6e11a","https://www.semanticscholar.org/paper/a1426b13b74dbad17b34606d25aabe1d61f6e11a",2,"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets","CRAFT is designed to be flexible and offers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning, and achieves substantial improvements compared to strong baselines.","arXiv.org",2023,"Lifan Yuan,Yangyi Chen,Xingyao Wang,Y. Fung,Hao Peng,Heng Ji",1,61,0
"06eb6cec99c2abcd2f275bb203b28eb9820db421","https://www.semanticscholar.org/paper/06eb6cec99c2abcd2f275bb203b28eb9820db421",2,"Large Language Models as Analogical Reasoners","Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, this approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem.","arXiv.org",2023,"Michihiro Yasunaga,Xinyun Chen,Yujia Li,Panupong Pasupat,J. Leskovec,Percy Liang,Ed H. Chi,Denny Zhou",1,90,0
"b29134737a0c81c13d31fc0263b3c4d4f05ccb78","https://www.semanticscholar.org/paper/b29134737a0c81c13d31fc0263b3c4d4f05ccb78",2,"Guiding Language Model Reasoning with Planning Tokens","'planning tokens' are introduced at the start of each reasoning step, serving as a guide for the model, and these token embeddings are then fine-tuned along with the rest of the model parameters.","arXiv.org",2023,"Xinyi Wang,Lucas Caccia,O. Ostapenko,Xingdi Yuan,Alessandro Sordoni",1,59,1
"e93562137240873bf1262e769dd9d73c2dcba858","https://www.semanticscholar.org/paper/e93562137240873bf1262e769dd9d73c2dcba858",2,"Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization","An investigation for data augmentation in math reasoning with large language models finds that MuggleMath is weak in out-of-domain math reasoning generalization to MATH, which suggest that augmentation on a single benchmark could not help with overall math reasoning performance.","arXiv.org",2023,"Chengpeng Li,Zheng Yuan,Guanting Dong,Keming Lu,Jiancan Wu,Chuanqi Tan,Xiang Wang,Chang Zhou",1,31,0
"8147cec9245d34d13732a08e915c920a1a499bb5","https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",2,"Lemur: Harmonizing Natural Language and Code for Language Agents","Lemur and Lemur-Chat are introduced, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either.","arXiv.org",2023,"Yiheng Xu,Hongjin Su,Chen Xing,Boyu Mi,Qian Liu,Weijia Shi,Binyuan Hui,Fan Zhou,Yitao Liu,Tianbao Xie,Zhoujun Cheng,Siheng Zhao,Lingpeng Kong,Bailin Wang,Caiming Xiong,Tao Yu",2,95,0
"3b918b15178bcc84fd22af5094fe1efbcd388e72","https://www.semanticscholar.org/paper/3b918b15178bcc84fd22af5094fe1efbcd388e72",2,"Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration","This work proposes Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs), built upon representative domain use cases.","arXiv.org",2023,"Fanqi Wan,Xinting Huang,Tao Yang,Xiaojun Quan,Wei Bi,Shuming Shi",0,40,0
"8868a6d452b06bf4ad33237d0f3952d895ca20e7","https://www.semanticscholar.org/paper/8868a6d452b06bf4ad33237d0f3952d895ca20e7",2,"Improving Large Language Model Fine-tuning for Solving Math Problems","A fine-tuning recipe is designed that yields an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting and a multi-task sequential fine- Tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance.","arXiv.org",2023,"Yixin Liu,Avi Singh,C. D. Freeman,John D. Co-Reyes,Peter J. Liu",0,28,0
"aac8cdd40b2bfd1b967f0e5ea6c01e93385169e7","https://www.semanticscholar.org/paper/aac8cdd40b2bfd1b967f0e5ea6c01e93385169e7",2,"SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving","A novel framework called SEGO is proposed to enhance LLMs' ability to solve mathematical problems by establishing a connection between the subgoal breakdown process and the probability of solving problems, and SEGO aims to identify better subgoals with theoretical guarantees.","arXiv.org",2023,"Xueliang Zhao,Xinting Huang,Wei Bi,Lingpeng Kong",0,44,0
"4411e6b32865933cab87696c2738cb7a204e4240","https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",2,"Implicit Chain of Thought Reasoning via Knowledge Distillation","This work explores an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, the language model's internal hidden states are used to perform implicit reasoning, which enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain- of-thought.","arXiv.org",2023,"Yuntian Deng,Kiran Prasad,Roland Fernandez,P. Smolensky,Vishrav Chaudhary,Stuart Shieber",0,47,0
"4befd752d21a6231a9d930b1946177bd4cba30cb","https://www.semanticscholar.org/paper/4befd752d21a6231a9d930b1946177bd4cba30cb",2,"Mathematical Word Problem Solution Evaluation via Data Preprocessing Approach","An overview of methods for processing mathematical text problems and correspondent domain datasets is described and a new approach to estimate MWP solutions is proposed that could use more context around the problem.","International Conference ""Information Technology and Interactions""",2021,"Andrii D. Nikolaiev,A. Anisimov",0,21,0
"064422a3713b96acc173d6bbcdfc6b6b15e3f5b7","https://www.semanticscholar.org/paper/064422a3713b96acc173d6bbcdfc6b6b15e3f5b7",2,": Leveraging Collective Human Intelligence to Study Large Language Models","L EET P ROMPT automatically evaluates human-LLM interactions to provide insights about both LLMs as well as human-interaction behavior, and finds that people use more diverse instruction strategies than these auto-* mechanisms.","",2023,"Sebastin Santy,Ayana Bharadwaj,Sahith N. Dambekodi,Alex Albert,Cathy Yuan,Ranjay Krishna",0,80,0
"1b6e810ce0afd0dd093f789d2b2742d047e316d5","https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",2,"Chain of Thought Prompting Elicits Reasoning in Large Language Models","Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.","Neural Information Processing Systems",2022,"Jason Wei,Xuezhi Wang,Dale Schuurmans,Maarten Bosma,E. Chi,F. Xia,Quoc Le,Denny Zhou",2213,117,378
"c9f48406851954cb098911eccb4124ea5f966675","https://www.semanticscholar.org/paper/c9f48406851954cb098911eccb4124ea5f966675",2,"A Survey on Multi-hop Question Answering and Generation","A general and formal definition of MHQA task is provided, the existing attempts to this highly interesting, yet quite challenging task are organized and summarized, and the best methods to createMHQA datasets are outlined.","arXiv.org",2022,"Vaibhav Mavi,Anubhav Jangra,A. Jatowt",20,182,0
"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","https://www.semanticscholar.org/paper/d9055c52b9454d91be09c197b11ed3f60ee7bb0a",2,"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers","This paper critically examine the various models that have been developed for solving word problems, their pros and cons and the challenges ahead, and endeavours to provide a road-map for future math word problem research.","arXiv.org",2022,"S. S. Sundaram,Sairam Gurajada,M. Fisichella,Deepak P,Savitha Sam Abraham",9,81,1
"3ce8c07349d91bb3f022a211be36e98eef0e1046","https://www.semanticscholar.org/paper/3ce8c07349d91bb3f022a211be36e98eef0e1046",2,"MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems","A novel model MMTM that leverages multi-tasking and multi-decoder during pre-training that achieves better mathematical reasoning ability and generalisability, which is demonstrated by outperforming the best state of the art baseline models from Seq2Seq, GTS, and Graph2Tree with a relative improvement on an adversarial challenge dataset SVAMP.","arXiv.org",2022,"Keyur Faldu,Amit P. Sheth,Prashant Kikani,Darshan Patel",0,42,0
"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db",2,"Language models show human-like content effects on reasoning","This work hypothesized that language models would show human-like content effects on abstract reasoning problems, and finds that state of the art large language models reflect many of the same patterns observed in humans across these tasks.","arXiv.org",2022,"Ishita Dasgupta,Andrew Kyle Lampinen,Stephanie C. Y. Chan,Antonia Creswell,D. Kumaran,James L. McClelland,Felix Hill",75,113,6
"2fe6060ced80c1c245a718e6188b6516207bf0a8","https://www.semanticscholar.org/paper/2fe6060ced80c1c245a718e6188b6516207bf0a8",2,"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions","An augmented intelligence system for simplifying and enhancing the modeling experience for operations research is described that receives a suggested formulation of an optimization problem based on its description and enables the users to validate and edit the suggestions.","Conference on Empirical Methods in Natural Language Processing",2022,"Rindranirina Ramamonjison,Haley Li,Timothy T. Yu,Shiqi He,Vishnu Rengan,Amin Banitalebi-Dehkordi,Zirui Zhou,Yong Zhang",11,46,3
"2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8",2,"Mind's Eye: Grounded Language Model Reasoning through Simulation","Mind's Eye is presented, a paradigm to ground language model reasoning in the physical world by using a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then using the simulation results as part of the input, which enables language models to perform reasoning.","International Conference on Learning Representations",2022,"Ruibo Liu,Jason Wei,S. Gu,Te-Yen Wu,Soroush Vosoughi,Claire Cui,Denny Zhou,Andrew M. Dai",41,82,3
"b23a8493f384a52adf22d3c70c5827fd1a6ca42d","https://www.semanticscholar.org/paper/b23a8493f384a52adf22d3c70c5827fd1a6ca42d",2,"Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem","Experiments show the proposed multi-view consistent contrastive learning approach significantly outperforms the existing baselines, especially on complex problems, and can absorb the merits of both views and generate more diverse results consistent with the mathematical laws.","Conference on Empirical Methods in Natural Language Processing",2022,"Wenqi Zhang,Yongliang Shen,Yanna Ma,Xiaoxia Cheng,Zeqi Tan,Qingpeng Nong,Weiming Lu",3,51,1
"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",2,"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks","Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoTA performance on all math problem datasets and near-SoTAperformance on financial datasets.","arXiv.org",2022,"Wenhu Chen,Xueguang Ma,Xinyi Wang,William W. Cohen",174,49,23
"e6745fb621481ccb0ed53c267a37292e499c1b42","https://www.semanticscholar.org/paper/e6745fb621481ccb0ed53c267a37292e499c1b42",2,"Automatic Generation of Socratic Subquestions for Teaching Math Word Problems","This work explores the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving and proposes various guided question generation schemes based on input conditioning and reinforcement learning.","Conference on Empirical Methods in Natural Language Processing",2022,"K. Shridhar,Jakub Macina,Mennatallah El-Assady,Tanmay Sinha,Manu Kapur,Mrinmaya Sachan",11,65,1
"8fd462f6248d5e3f1b6602697c09489086b5655f","https://www.semanticscholar.org/paper/8fd462f6248d5e3f1b6602697c09489086b5655f",2,"Distilling Reasoning Capabilities into Smaller Language Models","This work proposes an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps, and uses this to train a combination of two small distilled models.","Annual Meeting of the Association for Computational Linguistics",2022,"K. Shridhar,Alessandro Stolfo,Mrinmaya Sachan",15,43,1
"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","https://www.semanticscholar.org/paper/a9e3e5dd7b30890553b7ae1c41f932e99192bb44",2,"Large Language Models Are Reasoning Teachers","This paper uses very large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude, and proposes Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tunes smaller models.","Annual Meeting of the Association for Computational Linguistics",2022,"Namgyu Ho,Laura Schmid,Se-Young Yun",76,74,12
"27f0ce04403158b61328716ae4aaab5840c0d123","https://www.semanticscholar.org/paper/27f0ce04403158b61328716ae4aaab5840c0d123",2,"Batch Prompting: Efficient Inference with Large Language Model APIs","Batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time, is proposed, which reduces both token and time costs while retaining downstream performance.","arXiv.org",2023,"Zhoujun Cheng,Jungo Kasai,Tao Yu",12,80,0
"3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e","https://www.semanticscholar.org/paper/3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",2,"Large Language Models Can Be Easily Distracted by Irrelevant Context","This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description.","International Conference on Machine Learning",2023,"Freda Shi,Xinyun Chen,Kanishka Misra,Nathan Scales,David Dohan,E. Chi,Nathanael Scharli,Denny Zhou",82,63,5
"b115c1e1e9e51f8ad7d47b745bc04e29a654b84d","https://www.semanticscholar.org/paper/b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",2,"Faithful Chain-of-Thought Reasoning","Faithful CoT, a reasoning framework involving two stages: Translation and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively, guarantees that the reasoning chain provides a faithful explanation of the final answer.","arXiv.org",2023,"Qing Lyu,Shreya Havaldar,Adam Stein,Li Zhang,D. Rao,Eric Wong,Marianna Apidianaki,Chris Callison-Burch",45,42,2
"69619a2a47faee7a29ec596db13172e2a42ff921","https://www.semanticscholar.org/paper/69619a2a47faee7a29ec596db13172e2a42ff921",2,"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models","Synthetic prompting is introduced, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning.","International Conference on Machine Learning",2023,"Zhihong Shao,Yeyun Gong,Yelong Shen,Minlie Huang,Nan Duan,Weizhu Chen",28,40,3
"cd0988714ea326642d2b1bb18753e187fec71e42","https://www.semanticscholar.org/paper/cd0988714ea326642d2b1bb18753e187fec71e42",2,"A Categorical Archive of ChatGPT Failures","","arXiv.org",2023,"A. Borji",170,72,13
"40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a","https://www.semanticscholar.org/paper/40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a",2,"Techniques to Improve Neural Math Word Problem Solvers","A new encoder-decoder architecture that fully leverages the question text and preserves step-wise commutative law is proposed and outperforms state-of-the-art neural MWP solvers, showing the effectiveness of the techniques.","arXiv.org",2023,"Youyuan Zhang",0,36,0
"873a581320d928249609d3c07229d5af182a379c","https://www.semanticscholar.org/paper/873a581320d928249609d3c07229d5af182a379c",2,"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?","It is found that ChatGPT performs well on many tasks favoring reasoning capabilities while it still faces challenges when solving specific tasks such as sequence tagging, and with extensive empirical studies, both the effectiveness and limitations of the current version of ChatG PT are demonstrated.","arXiv.org",2023,"Chengwei Qin,Aston Zhang,Zhuosheng Zhang,Jiaao Chen,Michihiro Yasunaga,Diyi Yang",251,88,17
"0d42221038c05cee8443c5b5af838505ee137dc3","https://www.semanticscholar.org/paper/0d42221038c05cee8443c5b5af838505ee137dc3",2,"ART: Automatic multi-step reasoning and tool-use for large language models","Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program, achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks.","arXiv.org",2023,"Bhargavi Paranjape,Scott M. Lundberg,Sameer Singh,Hannaneh Hajishirzi,Luke Zettlemoyer,Marco Tulio Ribeiro",43,40,5
"00a9a469bb019bf33eeee438c110f704b71cda73","https://www.semanticscholar.org/paper/00a9a469bb019bf33eeee438c110f704b71cda73",2,"Retrieving Multimodal Information for Augmented Generation: A Survey","This survey reviews methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio, and offers a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness.","arXiv.org",2023,"Ruochen Zhao,Hailin Chen,Weishi Wang,Fangkai Jiao,Xuan Long Do,Chengwei Qin,Bosheng Ding,Xiaobao Guo,Minzhi Li,Xingxuan Li,Shafiq R. Joty",9,249,0
"dca6c3927ade6481a1ae080f5c24decbfeced1be","https://www.semanticscholar.org/paper/dca6c3927ade6481a1ae080f5c24decbfeced1be",2,"Boosted Prompt Ensembles for Large Language Models","A prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble'' that outperforms single-prompt output-space ensembles and bagged prompt-spaceEnsemble on the GSM8k and AQuA datasets, among others.","arXiv.org",2023,"Silviu Pitis,Michael Ruogu Zhang,Andrew Wang,Jimmy Ba",13,91,2
"fce42753155280051ac64817404b4e1d3be6ebaa","https://www.semanticscholar.org/paper/fce42753155280051ac64817404b4e1d3be6ebaa",2,"MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks","This paper aims to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks, and showcases the possibility of extending the capability of LLM to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks.","arXiv.org",2023,"Lei Zhang,Yuge Zhang,Kan Ren,Dongsheng Li,Yuqing Yang",5,86,0
"0061d6c1aa6c6032120974e8939ee11f5eed8813","https://www.semanticscholar.org/paper/0061d6c1aa6c6032120974e8939ee11f5eed8813",2,"Self-Evaluation Guided Beam Search for Reasoning","A stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs through stochastic beam search and a decoding algorithm integrating the self- evaluation guidance via stochastically beam search are proposed.","",2023,"Yuxi Xie,Kenji Kawaguchi,Yiran Zhao,Xu Zhao,MingSung Kan,Junxian He,Qizhe Xie",22,52,1
"3922365b7b40a447ecc57e027530cc90131e171e","https://www.semanticscholar.org/paper/3922365b7b40a447ecc57e027530cc90131e171e",2,"NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports","This work presents a novel resource to advance research on NLI for reasoning on CTRs, a corpus of 2400 statements and CTRs annotated for two main tasks, and is the first to design a task that covers the interpretation of full CTRs.","arXiv.org",2023,"Mael Jullien,Marco Valentino,H. Frost,Paul O'Regan,Dónal Landers,Andre Freitas",0,64,0
"9ccb2beaec722232a84e9a7682c72dcf7de667df","https://www.semanticscholar.org/paper/9ccb2beaec722232a84e9a7682c72dcf7de667df",2,"Non-Autoregressive Math Word Problem Solver with Unified Tree Structure","A novel non-autoregressive solver is proposed, named \textit{MWP-NAS}, to parse the problem and deduce the solution expression based on the unified tree, where the elements are permutable and identical for all the expression variants.","arXiv.org",2023,"Yi Bin,Meng Han,Wenhao Shi,Lei Wang,Yang Yang,Heng Tao Shen",3,55,1
"bcdaf6c98ddbd6809cf6241aa77200d7394db163","https://www.semanticscholar.org/paper/bcdaf6c98ddbd6809cf6241aa77200d7394db163",2,"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing","A framework called CRITIC is introduced that allows LLMs, which are essentially""black boxes"" to validate and progressively amend their own outputs in a manner similar to human interaction with tools.","arXiv.org",2023,"Zhibin Gou,Zhihong Shao,Yeyun Gong,Yelong Shen,Yujiu Yang,Nan Duan,Weizhu Chen",38,144,2
"22d5459d1f47341b355feeb1becc37208d6ec365","https://www.semanticscholar.org/paper/22d5459d1f47341b355feeb1becc37208d6ec365",2,"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought","This work proposes RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions and finds that manually written fine-grained feedback can dramatically improve LLM's reasoning abilities.","arXiv.org",2023,"Tianci Xue,Ziqi Wang,Zhenhailong Wang,Chi Han,Pengfei Yu,Heng Ji",9,51,1
"6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3","https://www.semanticscholar.org/paper/6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3",2,"Automatic Model Selection with Large Language Models for Reasoning","This work demonstrates that it is possible to combine the best of both worlds by using different models for different problems, employing a large language model (LLM) to perform model selection.","arXiv.org",2023,"Xu Zhao,Yuxi Xie,Kenji Kawaguchi,Junxian He,Qizhe Xie",11,33,0
"2591071728bba9e021c399013314139cb00cd7fd","https://www.semanticscholar.org/paper/2591071728bba9e021c399013314139cb00cd7fd",2,"Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems","Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning chains, is created and is suitable for teaching language models to offload computations to a symbolic system.","",2023,"Marek Kadlcík,Michal Štefánik",0,21,0
"643e4e9beed8b14656eadfa214b8106858dbb27c","https://www.semanticscholar.org/paper/643e4e9beed8b14656eadfa214b8106858dbb27c",2,"Getting MoRE out of Mixture of Language Model Reasoning Experts","The key insight is to leverage agreement among the specialized experts to select the best answer for each question, or to abstain from answering, which gives MoRE higher accuracy than any single specialized model on a collection of 12 QA datasets from four reasoning types.","",2023,"Chenglei Si,Weijia Shi,Chen Zhao,Luke Zettlemoyer,Jordan L. Boyd-Graber",2,56,0
"0875651b68e6602d45ae08bee67cf63c02faa512","https://www.semanticscholar.org/paper/0875651b68e6602d45ae08bee67cf63c02faa512",2,"Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models","This work explores code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps, and performs extensive ablation studies and error analyses and identifies several exclusive advantages of using symbolic promptings compared to natural language.","arXiv.org",2023,"Y. Hu,Haotong Yang,Zhouchen Lin,Muhan Zhang",5,42,0
"d75d11d2c89c01cd284383546ae057cb827dc272","https://www.semanticscholar.org/paper/d75d11d2c89c01cd284383546ae057cb827dc272",2,"Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning","The results suggest that leveraging a large amount of training data can improve the generalization ability of prompts and boost the performance of fine-tuned small models in MWP solving.","Annual Meeting of the Association for Computational Linguistics",2023,"Zhanming Jie,Wei Lu",3,39,0
"30ea884a5b8e4cffe18ebfef6c033e5c37b79eeb","https://www.semanticscholar.org/paper/30ea884a5b8e4cffe18ebfef6c033e5c37b79eeb",2,"World Models for Math Story Problems","This paper consolidates previous work on categorizing and representing math story problems and develops MathWorld, which is a graph-based semantic formalism specific for the domain of math story Problems, which represents the situations and actions introduced in the text and their mathematical relationships.","Annual Meeting of the Association for Computational Linguistics",2023,"Andreas Opedal,Niklas Stoehr,Abulhair Saparov,Mrinmaya Sachan",1,64,0
"c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36","https://www.semanticscholar.org/paper/c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36",2,"MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning","This work introduces a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles, and uniquely considers the various annotation formats as different""views"" and leverages them in training the model.","arXiv.org",2023,"Zhenwen Liang,Dian Yu,Xiaoman Pan,Wenlin Yao,Qingkai Zeng,Xiangliang Zhang,Dong Yu",1,45,0
"c29dbfbc17fa190b787a2662d49f08a38c8bd166","https://www.semanticscholar.org/paper/c29dbfbc17fa190b787a2662d49f08a38c8bd166",2,"ARB: Advanced Reasoning Benchmark for Large Language Models","This work introduces ARB, a novel benchmark composed of advanced reasoning problems in multiple fields, featuring problems in mathematics, physics, biology, chemistry, and law, and introduces a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps.","arXiv.org",2023,"Tomohiro Sawada,Daniel Paleka,Alex Havrilla,Pranav Tadepalli,Paula Vidas,Alexander Kranias,John J. Nay,Kshitij Gupta,Aran Komatsuzaki",11,68,2
"91206346edbe28abb606d7b3425cd455d4019d4f","https://www.semanticscholar.org/paper/91206346edbe28abb606d7b3425cd455d4019d4f",2,"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models","It is found that pre-training loss is a better indicator of the model's performance than the model’s parameter count and that with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs.","arXiv.org",2023,"Zheng Yuan,Hongyi Yuan,Cheng Li,Guanting Dong,Chuanqi Tan,Chang Zhou",22,54,7
"506ad0e58f5d6b99bbcc5e8efb519ae6fa34d307","https://www.semanticscholar.org/paper/506ad0e58f5d6b99bbcc5e8efb519ae6fa34d307",2,"Guiding Mathematical Reasoning via Mastering Commonsense Formula Knowledge","A novel Formula-mastered Solver (FOMAS) is established with two systems drawing insight from the dual process theory, including a Knowledge System and a Reasoning System, to learn and apply formula knowledge, respectively to improve both reasoning accuracy and interpretability.","Knowledge Discovery and Data Mining",2023,"Jia-Yin Liu,Zhenya Huang,Zhiyuan Ma,Qi Liu,Enhong Chen,Tianhuang Su,Haifeng Liu",0,57,0
"3886f3bd2a0af9e75bf9fa5b7db4224969dbf346","https://www.semanticscholar.org/paper/3886f3bd2a0af9e75bf9fa5b7db4224969dbf346",2,"MathAttack: Attacking Large Language Models Towards Math Solving Ability","This work proposes a MathAttack model to attack MWP samples which are closer to the essence of security in solving math problems, and proposes a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability.","arXiv.org",2023,"Zihao Zhou,Qiufeng Wang,Mingyu Jin,Jie Yao,Jianan Ye,Wei Liu,Wei Wang,Xiaowei Huang,Kaizhu Huang",0,32,0
"e716e6e0b3dd5124268780dc9bed521a07f371b8","https://www.semanticscholar.org/paper/e716e6e0b3dd5124268780dc9bed521a07f371b8",2,"Contrastive Decoding Improves Reasoning in Large Language Models","Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models.","arXiv.org",2023,"Sean O'Brien,Mike Lewis",1,35,0
"7fe071ea76e49bc3e573beb53f07721630954247","https://www.semanticscholar.org/paper/7fe071ea76e49bc3e573beb53f07721630954247",2,"Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution","Promptbreeder is a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain and outperforms state-of-the-art prompt strategies such as Chain- of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks.","arXiv.org",2023,"Chrisantha Fernando,D. Banarse,Henryk Michalewski,Simon Osindero,Tim Rocktäschel",4,73,0
"12db3efff4cc9e16822dd64bb1cad66f3f034f3b","https://www.semanticscholar.org/paper/12db3efff4cc9e16822dd64bb1cad66f3f034f3b",2,"L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models","This work presents L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance.","arXiv.org",2023,"Ansong Ni,Pengcheng Yin,Yilun Zhao,Martin Riddell,Troy Feng,Rui Shen,Stephen Yin,Ye Liu,Semih Yavuz,Caiming Xiong,Shafiq R. Joty,Yingbo Zhou,Dragomir R. Radev,Arman Cohan",0,81,0
"bfeda6c7aa7899a80adb01894555b09d24756a59","https://www.semanticscholar.org/paper/bfeda6c7aa7899a80adb01894555b09d24756a59",2,"Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration","This paper introduces Corex, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving, and demonstrates the cost-effectiveness of the method, facilitating collaboration among different LLMs and promoting annotation efficiency.","arXiv.org",2023,"Qiushi Sun,Zhangyue Yin,Xiang Li,Zhiyong Wu,Xipeng Qiu,Lingpeng Kong",0,113,0
"2179a13e78195fa78be5ccca2b5dcf9fad783ffc","https://www.semanticscholar.org/paper/2179a13e78195fa78be5ccca2b5dcf9fad783ffc",2,"An Expression Tree Decoding Strategy for Mathematical Equation Generation","This work integrates tree structure into the expression-level generation and advocates an expression tree decoding strategy, and shows that this method outperforms other baselines, especially for these equations with complex structures.","arXiv.org",2023,"Wenqi Zhang,Yongliang Shen,Qingpeng Nong,Zeqi Tan Yanna Ma,Weiming Lu",0,65,0
"a419e1d74cfc2b5ff400963476bda5c6ae66e172","https://www.semanticscholar.org/paper/a419e1d74cfc2b5ff400963476bda5c6ae66e172",2,"TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models","This work proposes TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proofs but also evaluates a generative LM's reasoning ability on formulas and its capability to manipulate, group, and factor number terms.","arXiv.org",2023,"Jing Xiong,Jianhao Shen,Ye Yuan,Haiming Wang,Yichun Yin,Zhengying Liu,Lin Li,Zhijiang Guo,Qingxing Cao,Yinya Huang,Chuanyang Zheng,Xiaodan Liang,Ming Zhang,Qun Liu",0,57,0
"3a47a5e95812c54cbbbf68389014d4fd74c7e881","https://www.semanticscholar.org/paper/3a47a5e95812c54cbbbf68389014d4fd74c7e881",2,"Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking","This work proposes IEP, a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non-linearly and observes that integrating IEP and CoT further improves the LLMs' performance on certain tasks, highlighting the necessity of equipping LLMs with mixed logic processes.","arXiv.org",2023,"Yongqi Tong,Yifan Wang,Dawei Li,Sizhe Wang,Zi Lin,Simeng Han,Jingbo Shang",0,49,0
"5402c22369d0190d0a002b7a1222d403edae010a","https://www.semanticscholar.org/paper/5402c22369d0190d0a002b7a1222d403edae010a",2,"Defining a New NLP Playground","This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research directions, covering theoretical analysis, new and challenging problems, learning paradigms, and interdisciplinary applications.","arXiv.org",2023,"Sha Li,Chi Han,Pengfei Yu,Carl N. Edwards,Manling Li,Xingyao Wang,Y. Fung,Charles Yu,Joel R. Tetreault,Eduard H. Hovy,Heng Ji",0,185,0
"027cd21f94a9c4bb7842c857e4641df463a55aa7","https://www.semanticscholar.org/paper/027cd21f94a9c4bb7842c857e4641df463a55aa7",2,"Perturbation-based Active Learning for Question Answering","This work proposes a perturbation-based active learning acquisition strategy and demonstrates it is more effective than existing commonly used strategies.","arXiv.org",2023,"Fan Luo,M. Surdeanu",0,207,0
"33149f835f391119d287cc2c6b009464e7d14fe4","https://www.semanticscholar.org/paper/33149f835f391119d287cc2c6b009464e7d14fe4",2,"On the Abilities of Mathematical Extrapolation with Implicit Models","This paper compares the robustness of implicitly-defined and classical deep learning models on a series of mathematical extrapolation tasks, where the models are tested with out-of-distribution samples during inference time to showcase implicit models’ unique advantages for mathematics extrapolation thanks to their flexible and selective framework.","",2022,"Juliette Decugis,Max Emerling,Ashwin Ganesh,Alicia Y. Tsai,L. Ghaoui",0,20,0
"346accd49d1359aa3985c7b298bc2057ae642271","https://www.semanticscholar.org/paper/346accd49d1359aa3985c7b298bc2057ae642271",2,"One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development","This study demonstrated that a domain-agnostic pretrained transformer model is able to effectively extract quantitative clinical measurements from diagnostic reports with a relatively small number of gold-standard annotations.","",2022,"Pulkit Singh,Julian S Haimovich,C. Reeder,S. Khurshid,S. Emily,Lau,Jonathan W Cunningham,A. Philippakis,C. D. Anderson,J. Ho,S. Lubitz,P. Batra",0,37,0
"58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","https://www.semanticscholar.org/paper/58d5e4cbb5f3a944bae0bcc2bff93f06696f8196",2,"JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection","","",2022,"Shunsuke Onuma,Kazuma Kadowaki",2,17,1
"4f451ba06c4c9effd6c4ac0bae222495501a6200","https://www.semanticscholar.org/paper/4f451ba06c4c9effd6c4ac0bae222495501a6200",2,"Innovations in Neural Data-to-text Generation","This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella.","arXiv.org",2022,"Mandar Sharma,Ajay K. Gogineni,Naren Ramakrishnan",4,340,0
"6c84fc8c5ec823342f34a020f8b92b7064e96ca2","https://www.semanticscholar.org/paper/6c84fc8c5ec823342f34a020f8b92b7064e96ca2",2,"Towards Efficient and Robust Out-of-Distribution Deep Learning with Implicit Models","This paper shows how to decrease implicit model training time by harnessing the state-driven implicit modeling framework to safely eliminate features while maintaining model accuracy.","",2023,"Ashwin Ganesh",0,49,0
"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","https://www.semanticscholar.org/paper/a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99",2,"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning","This research finds that the PLMs can easily generalize when the distribution is the same, however, it is still difficult for them to generalize out of the distribution.","Natural Language Processing and Chinese Computing",2021,"Cunxiang Wang,Boyuan Zheng,Y. Niu,Yue Zhang",10,35,0
"aead4418733b998792deb9cbf198a834449e00d2","https://www.semanticscholar.org/paper/aead4418733b998792deb9cbf198a834449e00d2",2,"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics","A methodology for evaluating generalization that takes advantage of the problem domain's structure and access to a verifier is developed, and the problem of symbolic mathematical integration is considered, as it requires generalizing systematically beyond the training set.","AAAI Conference on Artificial Intelligence",2021,"S. Welleck,Peter West,Jize Cao,Yejin Choi",17,36,1
"b8b813111c411ae61881ab9cd25707d9de6444ec","https://www.semanticscholar.org/paper/b8b813111c411ae61881ab9cd25707d9de6444ec",2,"Compositional Attention: Disentangling Search and Retrieval","This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.","arXiv.org",2021,"Sarthak Mittal,Sharath Chandra Raparthy,I. Rish,Yoshua Bengio,Guillaume Lajoie",11,52,1