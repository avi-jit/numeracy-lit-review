"id","url","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"cd854d3d7a1231b1dfbbfa09a697ac064026be51","https://www.semanticscholar.org/paper/cd854d3d7a1231b1dfbbfa09a697ac064026be51",10,"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models","This survey critically examines the recent literature, concluding that even state-of-the-art architectures often fall short when probed with relatively simple tasks designed to test basic numerical and arithmetic knowledge.","arXiv.org",2023,"Alberto Testolin",1,108,0
"2dbec38fe353ab0e495ad09263389dbc9260824d","https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d",9,"A Survey of Deep Learning for Mathematical Reasoning","This survey paper reviews the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade, and evaluates existing benchmarks and methods and discusses future research directions in this domain.","Annual Meeting of the Association for Computational Linguistics",2022,"Pan Lu,Liang Qiu,Wenhao Yu,S. Welleck,Kai-Wei Chang",23,233,2
"9b45af10429681249fafb07c3b6012ea4ce63ffe","https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe",7,"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models","This work proposes a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution, and applies this framework on a test bed of math word problems.","Annual Meeting of the Association for Computational Linguistics",2022,"Alessandro Stolfo,Zhijing Jin,K. Shridhar,B. Scholkopf,Mrinmaya Sachan",15,77,0
"8424082e3bf4792462eb112d7ebcecf5b0dc3613","https://www.semanticscholar.org/paper/8424082e3bf4792462eb112d7ebcecf5b0dc3613",7,"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning","This survey paper discusses the performance of transformers on diﬀerent reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning.","Conference on Automated Knowledge Base Construction",2021,"Chadi Helwe,C. Clavel,Fabian M. Suchanek",28,95,4
"4d17732d90440682b0500f4e209c6cc4fac20e0e","https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e",6,"Teaching Algorithmic Reasoning via In-context Learning","This work shows that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which it is referred to as algorithmic prompting, and evaluates the approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrates significant boosts in performance.","arXiv.org",2022,"Hattie Zhou,Azade Nova,H. Larochelle,Aaron C. Courville,Behnam Neyshabur,Hanie Sedghi",40,68,11
"5df55d94ab5026ff84ec01871592108fbadbddbe","https://www.semanticscholar.org/paper/5df55d94ab5026ff84ec01871592108fbadbddbe",6,"Measurement Extraction with Natural Language Processing: A Review","In this review, an overview of prior work on measurement extraction is presented and different approaches to measurement extraction are described and the challenges posed by this task are outlined.","Conference on Empirical Methods in Natural Language Processing",2022,"Jan Göpfert,Patrick Kuckertz,J. Weinand,Leander Kotzur,D. Stolten",1,175,0
"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc",5,"Reasoning with Language Model Prompting: A Survey","This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","Annual Meeting of the Association for Computational Linguistics",2022,"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen",55,217,2
"817e52b815560f95171d8fa60f78dd965e885a65","https://www.semanticscholar.org/paper/817e52b815560f95171d8fa60f78dd965e885a65",5,"How well do Large Language Models perform in Arithmetic tasks?","This work proposes an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatG PT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provides a detailed analysis of the ability of largelanguage models.","arXiv.org",2023,"Zheng Yuan,Hongyi Yuan,Chuanqi Tan,Wei Wang,Songfang Huang",21,35,0
"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","https://www.semanticscholar.org/paper/8c7846c9805834dbe2fb0c8f48253b8d65b79d6a",5,"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks","Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks, is introduced and an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks into a series of learnable tasks by leveraging basic arithmetic principles is proposed.","arXiv.org",2023,"Tiedong Liu,K. H. Low",9,36,1
"db4ab91d5675c37795e719e997a2827d3d83cd45","https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45",5,"Towards Reasoning in Large Language Models: A Survey","A comprehensive overview of the current state of knowledge on reasoning in large language models, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions are provided.","Annual Meeting of the Association for Computational Linguistics",2022,"Jie Huang,K. Chang",87,130,3
"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","https://www.semanticscholar.org/paper/1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f",5,"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning","A tight linkage between the scaling of a network weights’ standard deviation and its effective length scale on a sinusoidal regression problem is demonstrated, suggesting simple modifications to control the length scale of the function learned by a neural network and, thus, its smoothness.","Trans. Mach. Learn. Res.",2023,"David A. Klindt",0,74,0
"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","https://www.semanticscholar.org/paper/0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",5,"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension","This study is the largest survey of the deep learning models in NLP field to date, providing an overview of the various formats and domains of the current resources, and highlighting the current lacunae for future work.","ACM Computing Surveys",2021,"Anna Rogers,Matt Gardner,Isabelle Augenstein",96,348,9
"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","https://www.semanticscholar.org/paper/2e86c8cab0135ad7e363bb64aacef4eb8c639e6a",5,"Benchmarks for Automated Commonsense Reasoning: A Survey","A number of recommendations for future development of commonsense AI benchmarks are made, including that the creators of benchmarks invest the work needed to ensure that benchmark examples are consistently high quality.","ACM Computing Surveys",2023,"E. Davis",11,189,1
"681cee58cf7e54199191cf9e0baf6851d8356704","https://www.semanticscholar.org/paper/681cee58cf7e54199191cf9e0baf6851d8356704",5,"Complex QA and language models hybrid architectures, Survey","This paper reviews the state-of-the-art of language models architectures and strategies for ""complex""question-answering (QA, C QA, CPS) with a focus on hybridization, and discusses some challenges associated with complex QA.","",2023,"Xavier Daull,P. Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco",4,391,0
"13ac3b7f1ace63c76c1e081898369f8e0505411c","https://www.semanticscholar.org/paper/13ac3b7f1ace63c76c1e081898369f8e0505411c",4,"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning","This paper presents a method called Boosting Numerical Reason\textbfing by Decomposing the Generation of Equations (Bridge), which can improve the accuracy of LLMs in generating equations as IMRs by reducing the tendency of generating constant expressions and programs.","arXiv.org",2023,"Dingzirui Wang,Longxu Dou,Wenbin Zhang,Junyu Zeng,Wanxiang Che",0,42,0
"ba704774f194938b04b1e2be40b1d111a4ca08e1","https://www.semanticscholar.org/paper/ba704774f194938b04b1e2be40b1d111a4ca08e1",4,"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation",,"arXiv.org",2023,"Cheng Qian,Chi Han,Y. Fung,Yujia Qin,Zhiyuan Liu,Heng Ji",7,32,1
"587f22e4e04d77ba0750deea69192fbfb73d7435","https://www.semanticscholar.org/paper/587f22e4e04d77ba0750deea69192fbfb73d7435",4,"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning","Experimental results on CARP and six other datasets show that the proposed DELI mostly outperforms competitive baselines, and can further boost the performance of existing CoT methods.","arXiv.org",2023,"Beichen Zhang,Kun Zhou,Xilin Wei,Wayne Xin Zhao,Jing Sha,Shijin Wang,Ji-rong Wen",1,36,0
"8de36f4d1b0cf9193f6be19d1a1d50bd8a8da1e2","https://www.semanticscholar.org/paper/8de36f4d1b0cf9193f6be19d1a1d50bd8a8da1e2",4,"Design of Chain-of-Thought in Math Problem Solving","Through extensive experiments on GSM8K, MATHQA, and SVAMP, it is found that program CoTs often have superior effectiveness in math problem solving and Python is a better choice of language than Wolfram for program CoT.","",2023,"Zhanming Jie,Trung Quoc Luong,Xinbo Zhang,Xiaoran Jin,Hang Li",0,40,0
"3693683c4e0405819fae7115ad680f769eb83534","https://www.semanticscholar.org/paper/3693683c4e0405819fae7115ad680f769eb83534",4,"Neural Comprehension: Language Models with Compiled Neural Networks","The method, which call ""Neural Comprehension"", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning.","arXiv.org",2023,"Yixuan Weng,Minjun Zhu,Fei Xia,Bin Li,Shizhu He,Kang Liu,Jun Zhao",4,66,0
"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",4,"PAL: Program-aided Language Models","This paper presents Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.","International Conference on Machine Learning",2022,"Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,Yiming Yang,Jamie Callan,Graham Neubig",151,62,30
"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","https://www.semanticscholar.org/paper/ee7b871213e1deafadea4b7752467b5c5ab1b9fb",4,"GPT Can Solve Mathematical Problems Without a Calculator","The MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.","arXiv.org",2023,"Z. Yang,Ming Ding,Qingsong Lv,Zhihuan Jiang,Zehai He,Yuyi Guo,Jinfeng Bai,Jie Tang",1,45,0
"be8f769f135b0ffcf829594b9421788781f38008","https://www.semanticscholar.org/paper/be8f769f135b0ffcf829594b9421788781f38008",4,"KitchenScale: Learning to predict ingredient quantities from recipe contexts","This work introduces KitchenScale, a fine-tuned Pre- trained Pre-trained Language Model (PLM) that predicts a target ingredient's quantity and measurement unit given its recipe context and adopted the Discrete Latent Exponent method to cope with high variance of numerical scales in recipe corpora.","Expert systems with applications",2023,"Donghee Choi,Mogan Gim,Samy Badreddine,Hajung Kim,Donghyeon Park,Jaewoo Kang",0,42,0
"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","https://www.semanticscholar.org/paper/d331de3b6bebb0f9af1fddf1b730ec057a7026d4",4,"Relational World Knowledge Representation in Contextual Language Models: A Review","This work proposes to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision, and provides a high-level, extensible taxonomy for knowledge representation in L Ms.","Conference on Empirical Methods in Natural Language Processing",2021,"Tara Safavi,Danai Koutra",27,132,1
"338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c","https://www.semanticscholar.org/paper/338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c",4,"STOAT: Structured Data to Analytical Text With Controls","STOAT model is proposed, which is table and reasoning aware, with vector-quantization to infuse the given reasoning categories in the output and generates 15.3% more faithful and analytical descriptions as compared to the baseline models in human evaluation.","arXiv.org",2023,"Deepanway Ghosal,Preksha Nema,A. Raghuveer",0,51,0
"12d16f426edc6ab248fb476007bd1646282d4d68","https://www.semanticscholar.org/paper/12d16f426edc6ab248fb476007bd1646282d4d68",4,"Language Model Behavior: A Comprehensive Survey","Over 250 recent studies of English language model behavior before task-specific fine-tuning are discussed, synthesizing recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.","arXiv.org",2023,"Tyler A. Chang,B. Bergen",8,392,0
"002cfed5d4d9bf2fdaddb11d32f14751f2250e0c","https://www.semanticscholar.org/paper/002cfed5d4d9bf2fdaddb11d32f14751f2250e0c",4,"Teaching Arithmetic to Small Transformers","This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective.","arXiv.org",2023,"Nayoung Lee,Kartik K. Sreenivasan,Jason D. Lee,Kangwook Lee,Dimitris Papailiopoulos",4,68,1
"3e565c544a8639cc9c7568833e484d7610f5e5d4","https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4",3,"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning","A novel approach is proposed, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example, which verifies its effectiveness in selecting in- context examples.","International Conference on Learning Representations",2022,"Pan Lu,Liang Qiu,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Tanmay Rajpurohit,Peter Clark,A. Kalyan",36,57,4
"bebede105aa69a81045bf79682272ee4cfb61475","https://www.semanticscholar.org/paper/bebede105aa69a81045bf79682272ee4cfb61475",3,"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks","By incorporating CoNN modules into the LM, the framework effectively tackles rule-intensive challenges and achieves flawless execution on symbolic operations tasks, highlighting the potential of the method in enabling LMs to possess true symbolic comprehension capabilities.","",2023,"Yixuan Weng,Minjun Zhu,Fei Xia,Bin Li,Shizhu He,Kang Liu,Jun Zhao",0,63,0
"170c97c7215f42edfb20c2248f954879e91ef86e","https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e",3,"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","This paper presents Chameleon, an AI system that mitigates LLM limitations by augmenting LLMs with plug-and-play modules for compositional reasoning, and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","arXiv.org",2023,"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao",52,65,9
"fce42753155280051ac64817404b4e1d3be6ebaa","https://www.semanticscholar.org/paper/fce42753155280051ac64817404b4e1d3be6ebaa",3,"MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks","This paper aims to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks, and showcases the possibility of extending the capability of LLM to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks.","arXiv.org",2023,"Lei Zhang,Yuge Zhang,Kan Ren,Dongsheng Li,Yuqing Yang",3,86,0
"a39f960b28a627d554712a457efa5d3e3b7f8406","https://www.semanticscholar.org/paper/a39f960b28a627d554712a457efa5d3e3b7f8406",3,"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning","Three pretraining tasks that operate at both the whole program and sub-program level are proposed: Variable Integrity Ranking, which guides the model to focus on useful variables; Variable Operator Prediction, which decomposes the supervision into fine-grained single operator prediction; and Variable Keyphrase Masking, which encourages the modelto identify key evidence that sub- programs are derived from.","arXiv.org",2023,"Qianying Liu,Dongsheng Yang,Wenjie Zhong,Fei Cheng,S. Kurohashi",0,41,0
"261549439aebdda72b648ecc462448fd24857ac1","https://www.semanticscholar.org/paper/261549439aebdda72b648ecc462448fd24857ac1",3,"Progressive-Hint Prompting Improves Reasoning in Large Language Models","This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers.","arXiv.org",2023,"Chuanyang Zheng,Zhengying Liu,Enze Xie,Zhenguo Li,Yu Li",23,66,3
"62176de125738e3b95850d1227bac81fd646b78e","https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e",3,"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models","The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem.","Annual Meeting of the Association for Computational Linguistics",2023,"Lei Wang,Wanyu Xu,Yihuai Lan,Zhiqiang Hu,Yunshi Lan,R. Lee,Ee-Peng Lim",27,53,2
"a52dd1e900200e0733eea927edc7d6c27aeba187","https://www.semanticscholar.org/paper/a52dd1e900200e0733eea927edc7d6c27aeba187",3,"TheoremQA: A Theorem-driven Question Answering dataset","This paper introduces TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems and finds that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting.","arXiv.org",2023,"Wenhu Chen,Ming Yin,Max Ku,Yixin Wan,Xueguang Ma,Jianyu Xu,Tony Xia,Xinyi Wang,Pan Lu",8,57,3
"d8244cf970b9019cfb7f813275eba10d17c47166","https://www.semanticscholar.org/paper/d8244cf970b9019cfb7f813275eba10d17c47166",3,"Learning Multi-Step Reasoning by Solving Arithmetic Tasks","This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning by continually pre-training LMs on a synthetic dataset MsAT which is composed of Multi-step Arithmetic Tasks.","Annual Meeting of the Association for Computational Linguistics",2023,"Tianduo Wang,Wei Lu",2,40,0
"8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","https://www.semanticscholar.org/paper/8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c",3,"An Empirical Study on Challenging Math Problem Solving with GPT-4","This work explores the frontier of using GPT-4 for solving more complex and challenging math problems, and considers MathChat, a conversational problem-solving framework newly proposed in this work.","arXiv.org",2023,"Yiran Wu,Feiran Jia,Shaokun Zhang,Han-Tai Li,Erkang Zhu,Yue Wang,Y. Lee,Richard Peng,Qingyun Wu,Chi Wang",5,38,0
"fae57797d357bfa3b39b220336d1a2e8deba5318","https://www.semanticscholar.org/paper/fae57797d357bfa3b39b220336d1a2e8deba5318",3,"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving","The idea is to maintain a moderate-sized model and employ the cross-task knowledge sharing to improve the model capacity in a multi-task setting, and construct a Mixture-of-Experts (MoE) architecture for modeling mathematical text, to capture the common mathematical knowledge across tasks.","Knowledge Discovery and Data Mining",2023,"Wayne Xin Zhao,Kun Zhou,Beichen Zhang,Zheng Gong,Zhipeng Chen,Yuanhang Zhou,Ji-rong Wen,Jing Sha,Shijin Wang,Cong Liu,Guoping Hu",0,59,0
"4993258852711c4e3d0011325ac3db680eae84f4","https://www.semanticscholar.org/paper/4993258852711c4e3d0011325ac3db680eae84f4",3,"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models","","arXiv.org",2023,"Xiaoxuan Wang,Ziniu Hu,Pan Lu,Yanqiao Zhu,Jieyu Zhang,Satyen Subramaniam,Arjun R. Loomba,Shichang Zhang,Yizhou Sun,Wei Wang",7,48,1
"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","https://www.semanticscholar.org/paper/dd18782960f9ee4c66b79e1518b342ad3f8d19e7",3,"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct","WizardMath is presented, which enhances the mathematical reasoning abilities of Llama-2, by applying the proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math.","arXiv.org",2023,"Haipeng Luo,Qingfeng Sun,Can Xu,Pu Zhao,Jian-Guang Lou,Chongyang Tao,Xiubo Geng,Qingwei Lin,Shifeng Chen,Dongmei Zhang",5,107,2
"a1f95df5d38665bbc3753d4152f31c3d82bfc3a5","https://www.semanticscholar.org/paper/a1f95df5d38665bbc3753d4152f31c3d82bfc3a5",3,"When Do Program-of-Thoughts Work for Reasoning?","This work proposes complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities, and designs an auto-synthesizing and stratifying algorithm that applies to instruction generation for mathematical reasoning and code data filtering for code generation tasks.","arXiv.org",2023,"Zhen Bi,Ningyu Zhang,Yinuo Jiang,Shumin Deng,Guozhou Zheng,Huajun Chen",1,55,0
"b132a2eb43528645fc86c8dcf83ca0139821f740","https://www.semanticscholar.org/paper/b132a2eb43528645fc86c8dcf83ca0139821f740",3,"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning","The MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 13% and 29%, and underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.","arXiv.org",2023,"Xiang Yue,Xingwei Qu,Ge Zhang,Yao Fu,Wenhao Huang,Huan Sun,Yu Su,Wenhu Chen",0,73,0
"965e409a3e7b5670d609837fac9823b160d6639c","https://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c",3,"Logical Tasks for Measuring Extrapolation and Rule Comprehension","This work defines and characterize logical tasks and discusses system requirements for their solution, and discusses the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias.","arXiv.org",2022,"Ippei Fujisawa,R. Kanai",4,61,0
"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","https://www.semanticscholar.org/paper/b1fe7bdcfef4e12febe7e8bed8826e66689d60ed",3,"Auto-Regressive Next-Token Predictors are Universal Learners","This work presents a theoretical framework for studying auto-regressive next-token predictors, and introduces a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and examines the interplay between length complexity and other notions of complexity.","arXiv.org",2023,"Eran Malach",0,48,0
"763f47b73d0273a93a52a8ee8a63cf19009df6e1","https://www.semanticscholar.org/paper/763f47b73d0273a93a52a8ee8a63cf19009df6e1",3,"Injecting the BM25 Score as Text Improves BERT-Based Re-rankers",,"European Conference on Information Retrieval",2023,"Arian Askari,Amin Abolghasemi,G. Pasi,Wessel Kraaij,S. Verberne",6,68,0
"1b6e810ce0afd0dd093f789d2b2742d047e316d5","https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",3,"Chain of Thought Prompting Elicits Reasoning in Large Language Models","Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.","Neural Information Processing Systems",2022,"Jason Wei,Xuezhi Wang,Dale Schuurmans,Maarten Bosma,E. Chi,F. Xia,Quoc Le,Denny Zhou",1680,117,335
"0f2199296f01694ee46b6059879260fb80a84fa6","https://www.semanticscholar.org/paper/0f2199296f01694ee46b6059879260fb80a84fa6",3,"Teaching Autoregressive Language Models Complex Tasks By Demonstration","The results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.","arXiv.org",2021,"Gabriel Recchia",17,46,1
"81a8e7d6c69772471e71c84dbe5b7e3f78c576db","https://www.semanticscholar.org/paper/81a8e7d6c69772471e71c84dbe5b7e3f78c576db",3,"LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models",,"arXiv.org",2023,"Chi Han,Qifan Wang,Wenhan Xiong,Yu Chen,Heng Ji,Sinong Wang",2,48,0
"884c0b6db564208d99cadf2548f0aa96dee5f859","https://www.semanticscholar.org/paper/884c0b6db564208d99cadf2548f0aa96dee5f859",3,"Commonsense Reasoning with Implicit Knowledge in Natural Language","This work takes a middle ground where it uses smaller language models together with a relatively smaller but targeted natural language text corpora to develop unstructured commonsense knowledge sources and explores three strategies for knowledge incorporation.","Conference on Automated Knowledge Base Construction",2021,"Pratyay Banerjee,Swaroop Mishra",2,71,0
"4578717d5593b88e1c10555ce67a14be312b84b2","https://www.semanticscholar.org/paper/4578717d5593b88e1c10555ce67a14be312b84b2",3,"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning",,"Conference on Empirical Methods in Natural Language Processing",2022,"Yasaman Razeghi,IV RobertL.Logan,Matt Gardner,Sameer Singh",23,73,3
"0cf5c61f367e3937dc07be634a43952a50b589a4","https://www.semanticscholar.org/paper/0cf5c61f367e3937dc07be634a43952a50b589a4",3,"A Simple, Yet Effective Approach to Finding Biases in Code Generation","This work introduces an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test, and demonstrates how the framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.","Annual Meeting of the Association for Computational Linguistics",2022,"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski",4,50,0
"6681f0a0cc6ddaa70cdea109b941c47538caaa27","https://www.semanticscholar.org/paper/6681f0a0cc6ddaa70cdea109b941c47538caaa27",3,"Judicial knowledge-enhanced magnitude-aware reasoning for numerical legal judgment prediction","","Artificial Intelligence and Law",2022,"Sheng Bi,Zhiyao Zhou,Lu Pan,G. Qi",1,27,0
"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","https://www.semanticscholar.org/paper/6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249",3,"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data","DyRRen is proposed, an extended retriever-reranker-generator framework where each generation step is enhanced by a dynamic reranking of retrieved sentences that outperforms existing baselines on the FinQA dataset.","AAAI Conference on Artificial Intelligence",2022,"X. Li,Yin Zhu,Sichen Liu,Jiangzhou Ju,Yuzhong Qu,Gong Cheng",2,45,0
"0b315e6d4d800e04caf2f587312ce163e748d10c","https://www.semanticscholar.org/paper/0b315e6d4d800e04caf2f587312ce163e748d10c",3,"Numeric Magnitude Comparison Effects in Large Language Models","","Annual Meeting of the Association for Computational Linguistics",2023,"Raj Sanjay Shah,Vijay Marupudi,Reba Koenen,Khushi Bhardwaj,S. Varma",0,47,0
"c36277b67814e0a522e786a38e1768612b0e63f2","https://www.semanticscholar.org/paper/c36277b67814e0a522e786a38e1768612b0e63f2",3,"Exploring the Learning Mechanisms of Neural Division Modules","It is shown that robustly learning division in a systematic manner remains a challenge even at the simplest level of dividing two numbers, and a novel approach to division is proposed which is called the Neural Reciprocal Unit (NRU) and the Neural Multiplicative Reciproc Unit (NMRU).","Trans. Mach. Learn. Res.",2022,"Bhumika Mistry,K. Farrahi,Jonathon S. Hare",0,23,0
"def7c67c3688bc459fbef3ce50a466a0cbf411a9","https://www.semanticscholar.org/paper/def7c67c3688bc459fbef3ce50a466a0cbf411a9",3,"A Primer for Neural Arithmetic Logic Modules","Focusing on the shortcomings of the NALU, an in-depth analysis is provided to reason about design choices of recent modules, and a benchmark is created which compares all existing arithmetic NALMs.","Journal of machine learning research",2021,"Bhumika Mistry,K. Farrahi,Jonathon S. Hare",5,49,0
"0ce6a798f8222ed8f221326ca566311c648cb4dc","https://www.semanticscholar.org/paper/0ce6a798f8222ed8f221326ca566311c648cb4dc",3,"Learning Division with Neural Arithmetic Logic Modules","It is shown that robustly learning division in a systematic manner remains a challenge even at the simplest level of dividing two numbers, and two novel approaches for division are proposed, one of which is the Neural Reciprocal Unit and the NMRU.","arXiv.org",2021,"Bhumika Mistry,K. Farrahi,Jonathon S. Hare",0,14,0
"706c6b3781374b0b11f98f204a4ddd05b26ed009","https://www.semanticscholar.org/paper/706c6b3781374b0b11f98f204a4ddd05b26ed009",3,"Knowledge Infused Decoding","Knowledge Infused Decoding (KID) -- a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding, which maintains a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning.","International Conference on Learning Representations",2022,"Ruibo Liu,Guoqing Zheng,Shashank Gupta,Radhika Gaonkar,Chongyang Gao,Soroush Vosoughi,Milad Shokouhi,A. Awadallah",11,93,3
"9a9e68d400069f023f7dc9b982226c95159a509d","https://www.semanticscholar.org/paper/9a9e68d400069f023f7dc9b982226c95159a509d",3,"Dissociating language and thought in large language models: a cognitive perspective","","arXiv.org",2023,"Kyle Mahowald,Anna A. Ivanova,I. Blank,N. Kanwisher,J. Tenenbaum,Evelina Fedorenko",82,423,9
"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","https://www.semanticscholar.org/paper/9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3",3,"20Q: Overlap-Free World Knowledge Benchmark for Language Models","20Q is introduced, a novel benchmark using the Twenty Questions game to evaluate world knowledge and common sense of language models and shows that in-context learning is inefficient for evaluating language models’ world knowledge — fine-tuning is necessary to show their true capabilities.","IEEE Games Entertainment Media Conference",2022,"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans",0,36,0
"8b0b5b998d65add5c036b45c36afa3f9df96f4b0","https://www.semanticscholar.org/paper/8b0b5b998d65add5c036b45c36afa3f9df96f4b0",3,"Toward Building a Language Model for Understanding Temporal Commonsense","This paper focuses on the development of language models for temporal commonsense inference over several pre-trained language models, and relies on multi-step fine-tuning using multiple corpora, and masked language modeling to predict masked temporal indicators that are crucial for temporal Commonsense reasoning.","AACL",2022,"Mayuko Kimura,L. Pereira,Ichiro Kobayashi",0,24,0
"a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b","https://www.semanticscholar.org/paper/a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b",3,"Good Night at 4 pm?! Time Expressions in Different Cultures","3 language-agnostic methods are proposed for mapping from expressions such as “morning” in English or “Manhã’ in Portuguese to specific hours in the day and one achieves promising results on gold standard annotations that is collected for a small number of languages.","Findings",2022,"V. Shwartz",5,46,0
"ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb","https://www.semanticscholar.org/paper/ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb",3,"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks","This work presents an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which is called KG-to-task match and demonstrates that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHow-based KGs are the bestmatch for PIQA across all 3 analysis phases.","Conference of the European Chapter of the Association for Computational Linguistics",2021,"Lisa Bauer",12,43,1
"6597d61bdb531051678c773526758a6dc113b9ce","https://www.semanticscholar.org/paper/6597d61bdb531051678c773526758a6dc113b9ce",3,"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences","This work introduces a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs, and proposes a pairwise accuracy metric to reliably measure an agent's ability to perform Commonsense reasoning over a given situation.","Findings",2021,"Shikhar Singh,Nuan Wen,Yu Hou,Pegah Alipoormolabashi,Te-Lin Wu,Xuezhe Ma,Nanyun Peng",23,50,6
"7e5ca499cd9b932921bda84db98f75087d0b0683","https://www.semanticscholar.org/paper/7e5ca499cd9b932921bda84db98f75087d0b0683",3,"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey","A survey of commonsense knowledge acquisition and reasoning tasks, the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions are presented.","AAAI Conference on Artificial Intelligence",2022,"Prajjwal Bhargava,Vincent Ng",20,84,0
"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","https://www.semanticscholar.org/paper/9a3f1a51ab2b3816655e4c4f58a022421ea7b34b",3,"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility","This work introduces FeasibilityQA, a question-answering dataset involving binary classification (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility, and shows that even state-of-the-art models such as GPT-3, G PT-2, and T5 struggle to answer the feasibility questions correctly.","Conference of the European Chapter of the Association for Computational Linguistics",2022,"Himanshu Gupta,Neeraj Varshney,Swaroop Mishra,Kuntal Kumar Pal,Saurabh Arjun Sawant,Kevin Scaria,Siddharth Goyal,Chitta Baral",5,46,0
"ee153a2c91d36b034dc86c945aee9859b79da812","https://www.semanticscholar.org/paper/ee153a2c91d36b034dc86c945aee9859b79da812",3,"Test of Time: Instilling Video-Language Models with a Sense of Time","This work proposes a temporal adaptation recipe on top of one video-language model, VideoCLIp, based on post-pretraining on a small amount of video-text data and conducts a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness.","Computer Vision and Pattern Recognition",2023,"Piyush Bagad,Makarand Tapaswi,Cees G. M. Snoek",3,139,0
"24a285b1e7ee9acfec9a82f7123563b62f532203","https://www.semanticscholar.org/paper/24a285b1e7ee9acfec9a82f7123563b62f532203",3,"FERMAT: An Alternative to Accuracy for Numerical Reasoning","This work introduces a multi-view evaluation set for numerical reasoning in English, called FERMAT, which evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency.","Annual Meeting of the Association for Computational Linguistics",2023,"Jasivan Sivakumar,N. Moosavi",0,48,0
"f0a7872fed856fe4f10f8fcdca60f5aa8ecfae12","https://www.semanticscholar.org/paper/f0a7872fed856fe4f10f8fcdca60f5aa8ecfae12",3,"SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning","This work introduces a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor and formalizes this challenging task as SituatedGen.","arXiv.org",2023,"Yunxiang Zhang,Xiaojun Wan",0,52,0
"8d848ce7ab6ccde4854d3c658a169215ae483029","https://www.semanticscholar.org/paper/8d848ce7ab6ccde4854d3c658a169215ae483029",3,"An Overview Of Temporal Commonsense Reasoning and Acquisition","The need for careful interpretation of research is emphasized to guard against overpromising evaluation results in light of the shallow reasoning present in transformers, and the need for appropriately preparing datasets and suitable evaluation metrics is emphasized.","arXiv.org",2023,"Georg Wenzel,A. Jatowt",0,92,0
"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","https://www.semanticscholar.org/paper/b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea",3,"Self-Attention Networks Can Process Bounded Hierarchical Languages","It is proved that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language.","Annual Meeting of the Association for Computational Linguistics",2021,"Shunyu Yao,Binghui Peng,C. Papadimitriou,Karthik Narasimhan",26,57,2
"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","https://www.semanticscholar.org/paper/fc1ffc7df07cc9b665deca4a94b871732e1f0b4d",3,"Length Generalization in Arithmetic Transformers","It is shown that priming allows models trained on $5-digit $\times$ $3-digit multiplications to generalize to $35\times 3$ examples, and that the priming sample size scales as the logarithm of the training set size.","arXiv.org",2023,"Samy Jelassi,Stéphane d'Ascoli,Carles Domingo-Enrich,Yuhuai Wu,Yuan-Fang Li,Franccois Charton",3,54,1
"4533f5afa772b0b25703a7258993a7c28a85f9a3","https://www.semanticscholar.org/paper/4533f5afa772b0b25703a7258993a7c28a85f9a3",2,"NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering","This paper proposes a non-autoregressive program generation framework, which independently generates complete program tuples containing both operators and operands, which can significantly boost the speed of program generation while addressing the error accumulation issue.","arXiv.org",2022,"Tengxun Zhang,Hongfei Xu,Josef van Genabith,Deyi Xiong,Hongying Zan",1,47,0
"4ef578869c957f5fb969fa9c164dca0433f48042","https://www.semanticscholar.org/paper/4ef578869c957f5fb969fa9c164dca0433f48042",2,"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining","This paper proposes FORTAP, the first exploration to leverage spreadsheet formulas for table pretraining with tree attention, and proposes two novel self-supervised pretraining objectives derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP).","Annual Meeting of the Association for Computational Linguistics",2021,"Zhoujun Cheng,Haoyu Dong,Fan Cheng,Ran Jia,Pengfei Wu,Shi Han,Dongmei Zhang",16,45,2
"cbccb1201eee6432020276762a44ebfbf8f981a0","https://www.semanticscholar.org/paper/cbccb1201eee6432020276762a44ebfbf8f981a0",2,"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge","A novel unsupervised methodology leveraging external knowledge and contextualized word embeddings from ClinicalBERT for numerical reasoning in a variety of phenotypic contexts is presented.","arXiv.org",2022,"Ashwani Tanwar,Jingqing Zhang,Julia Ive,Vibhor Gupta,Yike Guo",3,40,0
"efcd9c1438559d908efd702333232078fd251a0f","https://www.semanticscholar.org/paper/efcd9c1438559d908efd702333232078fd251a0f",2,"Phenotyping in clinical text with unsupervised numerical reasoning for patient stratification","This article proposes a novel unsupervised method that leverages external clinical knowledge and contextualized word embeddings by ClinicalBERT for numerical reasoning in different phenotypic contexts and finds that these phenotypes from clinical text can be used to impute the missing values in structured data, which enrich and improve data quality.","Experimental biology and medicine",2022,"Ashwani Tanwar,Jingqing Zhang,Julia Ive,Vibhor Gupta,Yike Guo",1,39,0
"35922cd0d6b17e45320917338e9f98cb5c1a4f6f","https://www.semanticscholar.org/paper/35922cd0d6b17e45320917338e9f98cb5c1a4f6f",2,"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters","It is shown that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference.","Annual Meeting of the Association for Computational Linguistics",2022,"Boshi Wang,Sewon Min,Xiang Deng,Jiaming Shen,You Wu,Luke Zettlemoyer,Huan Sun",42,40,6
"27f32dc1aab7919eb7039deca067c3fbdc719c2a","https://www.semanticscholar.org/paper/27f32dc1aab7919eb7039deca067c3fbdc719c2a",2,"Koala: An Index for Quantifying Overlaps with Pre-training Corpora","Koala, a searchable index over large pre-training corpora using compressed suffix arrays with highly efficient compression rate and search support, is launched, providing a framework to do forensic analysis on the current and future benchmarks as well as to assess the degree of memorization in the output from the LLMs.","arXiv.org",2023,"Thuy-Trang Vu,Xuanli He,Gholamreza Haffari,Ehsan Shareghi",1,42,0
"ecf0cb0725de18659f9ba25a8cf65a1085564006","https://www.semanticscholar.org/paper/ecf0cb0725de18659f9ba25a8cf65a1085564006",2,"Understanding Arithmetic Reasoning in Language Models using Causal Mediation Analysis","A mechanistic interpretation of LLMs for arithmetic-based questions using a causal mediation analysis framework, which identifies the subset of parameters responsible for specific predictions and investigates the role of the attention mechanism.","arXiv.org",2023,"Alessandro Stolfo,Yonatan Belinkov,Mrinmaya Sachan",0,44,0
"0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","https://www.semanticscholar.org/paper/0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0",2,"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs","This work proposes a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements of the given question and document with Semantic-oriented hierarchical Graph structures.","arXiv.org",2023,"Fengbin Zhu,Chao Wang,Fuli Feng,Zifeng Ren,Moxin Li,Tat-Seng Chua",0,34,0
"87a31b729295a3357949683276a2625288fdd0f0","https://www.semanticscholar.org/paper/87a31b729295a3357949683276a2625288fdd0f0",2,"PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance","A novel method is proposed, namely UniPCQA, to adapt a hybrid format of input and output content in PCQA into the Seq2Seq problem, including the reformulation of the numerical reasoning process as code generation.","Conference on Empirical Methods in Natural Language Processing",2022,"Yang Deng,Wenqiang Lei,Wenxuan Zhang,W. Lam,Tat-Seng Chua",11,61,1
"1f84b6075ffa2d4ed685f31668ea1324bfba4fe5","https://www.semanticscholar.org/paper/1f84b6075ffa2d4ed685f31668ea1324bfba4fe5",2,"A Survey on Medical Document Summarization","This paper gives a comprehensive survey of the current techniques and trends in medical summarization.","arXiv.org",2022,"Raghav Jain,Anubhav Jangra,S. Saha,A. Jatowt",7,132,0
"9e1ba67d5f443a8bd42a8b856534f50c429baf11","https://www.semanticscholar.org/paper/9e1ba67d5f443a8bd42a8b856534f50c429baf11",2,"Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency","","Annual Meeting of the Association for Computational Linguistics",2023,"Mandar Sharma,N. Muralidhar,Naren Ramakrishnan",0,44,0
"1364e3c277e5833299f73b618c021b38510207cd","https://www.semanticscholar.org/paper/1364e3c277e5833299f73b618c021b38510207cd",2,"ML-LJP: Multi-Law Aware Legal Judgment Prediction","Following the actual legal process, the law article prediction is expanded as a multi-label classification task that includes both the charge-related law articles and term-relatedlaw articles and a novel multi-law aware LJP (ML-LJP) method is proposed to improve the performance of LJP.","Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",2023,"Yifei Liu,Yiquan Wu,Yating Zhang,Changlong Sun,Weiming Lu,Fei Wu,Kun Kuang",0,67,0
"064422a3713b96acc173d6bbcdfc6b6b15e3f5b7","https://www.semanticscholar.org/paper/064422a3713b96acc173d6bbcdfc6b6b15e3f5b7",2,": Leveraging Collective Human Intelligence to Study Large Language Models","L EET P ROMPT automatically evaluates human-LLM interactions to provide insights about both LLMs as well as human-interaction behavior, and finds that people use more diverse instruction strategies than these auto-* mechanisms.","",2023,"Sebastin Santy,Ayana Bharadwaj,Sahith N. Dambekodi,Alex Albert,Cathy Yuan,Ranjay Krishna",0,80,0
"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","https://www.semanticscholar.org/paper/d9055c52b9454d91be09c197b11ed3f60ee7bb0a",2,"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers","This paper critically examine the various models that have been developed for solving word problems, their pros and cons and the challenges ahead, and endeavours to provide a road-map for future math word problem research.","arXiv.org",2022,"S. S. Sundaram,Sairam Gurajada,M. Fisichella,Deepak P,Savitha Sam Abraham",9,81,1
"3ce8c07349d91bb3f022a211be36e98eef0e1046","https://www.semanticscholar.org/paper/3ce8c07349d91bb3f022a211be36e98eef0e1046",2,"MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems","A novel model MMTM that leverages multi-tasking and multi-decoder during pre-training that achieves better mathematical reasoning ability and generalisability, which is demonstrated by outperforming the best state of the art baseline models from Seq2Seq, GTS, and Graph2Tree with a relative improvement on an adversarial challenge dataset SVAMP.","arXiv.org",2022,"Keyur Faldu,Amit P. Sheth,Prashant Kikani,Darshan Patel",0,42,0
"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db",2,"Language models show human-like content effects on reasoning","This work hypothesized that language models would show human-like content effects on abstract reasoning problems, and finds that state of the art large language models reflect many of the same patterns observed in humans across these tasks.","arXiv.org",2022,"I. Dasgupta,Andrew Kyle Lampinen,Stephanie C. Y. Chan,Antonia Creswell,D. Kumaran,James L. McClelland,Felix Hill",62,113,5
"2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8",2,"Mind's Eye: Grounded Language Model Reasoning through Simulation","Mind's Eye is presented, a paradigm to ground language model reasoning in the physical world by using a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then using the simulation results as part of the input, which enables language models to perform reasoning.","International Conference on Learning Representations",2022,"Ruibo Liu,Jason Wei,S. Gu,Te-Yen Wu,Soroush Vosoughi,Claire Cui,Denny Zhou,Andrew M. Dai",27,82,3
"b23a8493f384a52adf22d3c70c5827fd1a6ca42d","https://www.semanticscholar.org/paper/b23a8493f384a52adf22d3c70c5827fd1a6ca42d",2,"Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem","Experiments show the proposed multi-view consistent contrastive learning approach significantly outperforms the existing baselines, especially on complex problems, and can absorb the merits of both views and generate more diverse results consistent with the mathematical laws.","Conference on Empirical Methods in Natural Language Processing",2022,"Wenqi Zhang,Yongliang Shen,Yanna Ma,Xiaoxia Cheng,Zeqi Tan,Qingpeng Nong,Weiming Lu",1,50,1
"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","https://www.semanticscholar.org/paper/a9e3e5dd7b30890553b7ae1c41f932e99192bb44",2,"Large Language Models Are Reasoning Teachers","This paper uses very large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude, and proposes Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tunes smaller models.","Annual Meeting of the Association for Computational Linguistics",2022,"Namgyu Ho,Laura Schmid,Se-Young Yun",49,74,7
"e659fa1e79a2a151be331125c14339988542aac3","https://www.semanticscholar.org/paper/e659fa1e79a2a151be331125c14339988542aac3",2,"Batch Prompting: Efficient Inference with Large Language Model APIs","Batch prompting, a simple alternative prompting approach that enables the LLM to run inference in batches, instead of one sample at a time, is proposed, which reduces both token and time costs while retaining downstream performance.","arXiv.org",2023,"Zhoujun Cheng,Jungo Kasai,Tao Yu",9,66,0
"b115c1e1e9e51f8ad7d47b745bc04e29a654b84d","https://www.semanticscholar.org/paper/b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",2,"Faithful Chain-of-Thought Reasoning","Faithful CoT, a reasoning framework involving two stages: Translation and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively, guarantees that the reasoning chain provides a faithful explanation of the final answer.","arXiv.org",2023,"QING LYU,Shreya Havaldar,Adam Stein,Li Zhang,D. Rao,Eric Wong,Marianna Apidianaki,Chris Callison-Burch",34,43,2
"3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e","https://www.semanticscholar.org/paper/3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",2,"Large Language Models Can Be Easily Distracted by Irrelevant Context","This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description.","International Conference on Machine Learning",2023,"Freda Shi,Xinyun Chen,Kanishka Misra,Nathan Scales,David Dohan,E. Chi,Nathanael Scharli,Denny Zhou",45,63,1
"69619a2a47faee7a29ec596db13172e2a42ff921","https://www.semanticscholar.org/paper/69619a2a47faee7a29ec596db13172e2a42ff921",2,"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models","Synthetic prompting is introduced, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning.","International Conference on Machine Learning",2023,"Zhihong Shao,Yeyun Gong,Yelong Shen,Minlie Huang,Nan Duan,Weizhu Chen",18,40,1
"40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a","https://www.semanticscholar.org/paper/40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a",2,"Techniques to Improve Neural Math Word Problem Solvers","A new encoder-decoder architecture that fully leverages the question text and preserves step-wise commutative law is proposed and outperforms state-of-the-art neural MWP solvers, showing the effectiveness of the techniques.","arXiv.org",2023,"Youyuan Zhang",0,36,0
"873a581320d928249609d3c07229d5af182a379c","https://www.semanticscholar.org/paper/873a581320d928249609d3c07229d5af182a379c",2,"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?","It is found that ChatGPT performs well on many tasks favoring reasoning capabilities while it still faces challenges when solving specific tasks such as sequence tagging, and with extensive empirical studies, both the effectiveness and limitations of the current version of ChatG PT are demonstrated.","arXiv.org",2023,"Chengwei Qin,Aston Zhang,Zhuosheng Zhang,Jiaao Chen,Michihiro Yasunaga,Diyi Yang",185,88,14
"5938abafd61881f6b23a2ba318d2d3d0327402c0","https://www.semanticscholar.org/paper/5938abafd61881f6b23a2ba318d2d3d0327402c0",2,"A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram","This work converts diagrams into basic textual clauses to describe diagram features effectively, and proposes a new neural solver called PGPSNet to fuse multi-modal information efficiently and promote geometric understanding and reasoning.","International Joint Conference on Artificial Intelligence",2023,"Ming-Liang Zhang,Fei Yin,Cheng-Lin Liu",0,46,0
"0d42221038c05cee8443c5b5af838505ee137dc3","https://www.semanticscholar.org/paper/0d42221038c05cee8443c5b5af838505ee137dc3",2,"ART: Automatic multi-step reasoning and tool-use for large language models","Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program, achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks.","arXiv.org",2023,"Bhargavi Paranjape,Scott M. Lundberg,Sameer Singh,Hanna Hajishirzi,Luke Zettlemoyer,Marco Tulio Ribeiro",32,40,5
"052a5e2bcc999810ee6f1eedcf758c528e4f125f","https://www.semanticscholar.org/paper/052a5e2bcc999810ee6f1eedcf758c528e4f125f",2,"Retrieving Multimodal Information for Augmented Generation: A Survey","This survey provides an in-depth review of retrieval-augmented generation in different modalities and discusses potential future directions of this emerging field.","arXiv.org",2023,"Ruochen Zhao,Hailin Chen,Weishi Wang,Fangkai Jiao,Xuan Long Do,Chengwei Qin,Bosheng Ding,Xiaobao Guo,Minzhi Li,Xingxuan Li,Shafiq R. Joty",6,161,0
"1d29334cfbe9a1a943082058876f0c22d44c62fd","https://www.semanticscholar.org/paper/1d29334cfbe9a1a943082058876f0c22d44c62fd",2,"A Survey of Large Language Models","A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.","arXiv.org",2023,"Wayne Xin Zhao,Kun Zhou,Junyi Li,Tianyi Tang,Xiaolei Wang,Yupeng Hou,Yingqian Min,Beichen Zhang,Junjie Zhang,Zican Dong,Yifan Du,Chen Yang,Yushuo Chen,Z. Chen,Jinhao Jiang,Ruiyang Ren,Yifan Li,Xinyu Tang,Zikang Liu,Peiyu Liu,J. Nie,Ji-rong Wen",354,415,29
"0061d6c1aa6c6032120974e8939ee11f5eed8813","https://www.semanticscholar.org/paper/0061d6c1aa6c6032120974e8939ee11f5eed8813",2,"Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding","An effective prompting approach that integrates self-evaluation guidance through stochastic beam search is proposed that explores the reasoning search space using a well-calibrated automatic criterion and enables an efficient search to produce higher-quality final predictions.","arXiv.org",2023,"Yuxi Xie,Kenji Kawaguchi,Yiran Zhao,Xu Zhao,MingSung Kan,Junxian He,Qizhe Xie",16,51,2
"1f07d5bf633fdaaf7dcb73397688cf256c6d709f","https://www.semanticscholar.org/paper/1f07d5bf633fdaaf7dcb73397688cf256c6d709f",2,"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought","RCoT (Reversing Chain-of-Thought) is proposed, a novel method to improve large language Models' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs' generated solutions and manually writing fine-grained feedback to guide LLMs in revising solutions.","arXiv.org",2023,"Tianci Xue,Ziqi Wang,Zhenhailong Wang,Chi Han,Pengfei Yu,Heng Ji",4,45,1
"4313b009cd30a469400d36b9fb20267620293d04","https://www.semanticscholar.org/paper/4313b009cd30a469400d36b9fb20267620293d04",2,"A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers","A data generation method for producing intricate mathematical derivations, and systematically perturb them with respect to syntax, structure, and semantics, employing symbolic algebra for scalable data production and augmentation.","arXiv.org",2023,"Jordan Meadows,Marco Valentino,Damien Teney,André Freitas",1,49,0
"073e6b91adc25c656d85002e3cb059e4530db20b","https://www.semanticscholar.org/paper/073e6b91adc25c656d85002e3cb059e4530db20b",2,"Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation","A novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles.","arXiv.org",2023,"Zhenwen Liang,W. Yu,Tanmay Rajpurohit,Peter Clark,Xiangliang Zhang,Ashwin Kaylan",2,64,0
"6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3","https://www.semanticscholar.org/paper/6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3",2,"Automatic Model Selection with Large Language Models for Reasoning","This work demonstrates that it is possible to combine the best of both worlds by using different models for different problems, employing a large language model (LLM) to perform model selection.","arXiv.org",2023,"Xu Zhao,Yuxi Xie,Kenji Kawaguchi,Junxian He,Qizhe Xie",3,33,0
"700f9c8a76d7af0fc550d119aa1d1164a496055e","https://www.semanticscholar.org/paper/700f9c8a76d7af0fc550d119aa1d1164a496055e",2,"Mixture of Prompt Experts for Generalizable and Interpretable Question Answering","A Mixture-of-Prompt-Experts (MOPE) system that ensembles multiple specialized LLMs that significantly outperforms any single specialized model on a collection of 12 QA datasets from four reasoning types.","arXiv.org",2023,"Chenglei Si,Weijia Shi,Chen Zhao,Luke Zettlemoyer,Jordan L. Boyd-Graber",0,45,0
"04270591b6006a83b0a8970ef80bcbfc26a835d9","https://www.semanticscholar.org/paper/04270591b6006a83b0a8970ef80bcbfc26a835d9",2,"Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models","This work explores code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps, and performs extensive ablation studies and error analyses and identifies several exclusive advantages of using symbolic promptings compared to natural language.","arXiv.org",2023,"Y. Hu,Haotong Yang,Zhouchen Lin,Muhan Zhang",4,34,0
"9efa81ec4954b0859c47dad8f42edfaf8bced69b","https://www.semanticscholar.org/paper/9efa81ec4954b0859c47dad8f42edfaf8bced69b",2,"Boosting Language Models Reasoning with Chain-of-Knowledge Prompting","This work proposes a novel Chain-of-Knowledge prompting, where it aims at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple, and introduces a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness.","arXiv.org",2023,"J. Wang,Qiushi Sun,Nuo Chen,Xiang Lorraine Li,Ming Gao",4,106,0
"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","https://www.semanticscholar.org/paper/0b47c8eae44c8c3c5f474d1b9869ba90a5199eae",2,"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements","This paper proposes a framework for MWP solvers based on the generation of linguistic variants of the problem text and shows that training on linguistic variant of problem statements and voting on candidate predictions improve the mathematical reasoning and robustness of the model.","Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",2023,"Syed Rifat Raiyan,Md. Nafis Faiyaz,S. Kabir,Mohsinul Kabir,H. Mahmud,Md. Kamrul Hasan",0,105,0
"3886f3bd2a0af9e75bf9fa5b7db4224969dbf346","https://www.semanticscholar.org/paper/3886f3bd2a0af9e75bf9fa5b7db4224969dbf346",2,"MathAttack: Attacking Large Language Models Towards Math Solving Ability","This work proposes a MathAttack model to attack MWP samples which are closer to the essence of security in solving math problems, and proposes a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability.","arXiv.org",2023,"Zihao Zhou,Qiufeng Wang,Mingyu Jin,Jie Yao,Jianan Ye,Wei Liu,Wei Wang,Xiaowei Huang,Kaizhu Huang",0,32,0
"4befd752d21a6231a9d930b1946177bd4cba30cb","https://www.semanticscholar.org/paper/4befd752d21a6231a9d930b1946177bd4cba30cb",2,"Mathematical Word Problem Solution Evaluation via Data Preprocessing Approach","An overview of methods for processing mathematical text problems and correspondent domain datasets is described and a new approach to estimate MWP solutions is proposed that could use more context around the problem.","International Conference ""Information Technology and Interactions""",2021,"Andrii D. Nikolaiev,A. Anisimov",0,21,0
"cc8417aa578016203cb52efc63592bba64b08bb3","https://www.semanticscholar.org/paper/cc8417aa578016203cb52efc63592bba64b08bb3",2,"FinMath: Injecting a Tree-structured Solver for Question Answering over Financial Reports","A new framework named FinMath is proposed, which improves the model’s numerical reasoning capacity by injecting a tree-structured neural model to perform multi-step numerical reasoning.","International Conference on Language Resources and Evaluation",2022,"Chenying Li,Wenbo Ye,Yilun Zhao",5,35,2
"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e",2,"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.","BIGSCIENCE",2022,"Sid Black,Stella Rose Biderman,Eric Hallahan,Quentin G. Anthony,Leo Gao,Laurence Golding,Horace He,Connor Leahy,Kyle McDonell,Jason Phang,M. Pieler,USVSN Sai Prashanth,Shivanshu Purohit,Laria Reynolds,J. Tow,Benqi Wang,Samuel Weinbach",288,142,35
"c9f48406851954cb098911eccb4124ea5f966675","https://www.semanticscholar.org/paper/c9f48406851954cb098911eccb4124ea5f966675",2,"A Survey on Multi-hop Question Answering and Generation","A general and formal definition of MHQA task is provided, the existing attempts to this highly interesting, yet quite challenging task are organized and summarized, and the best methods to createMHQA datasets are outlined.","arXiv.org",2022,"Vaibhav Mavi,Anubhav Jangra,A. Jatowt",13,182,0
"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a",2,"Exploring Length Generalization in Large Language Models","This paper establishes that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale, and shows that combining pretrained large language models' in-context learning abilities with scratchpad prompting results in a dramatic improvement in lengthgeneralization.","Neural Information Processing Systems",2022,"Cem Anil,Yuhuai Wu,Anders Andreassen,Aitor Lewkowycz,Vedant Misra,V. Ramasesh,Ambrose Slone,Guy Gur-Ari,Ethan Dyer,Behnam Neyshabur",57,33,15
"2fe6060ced80c1c245a718e6188b6516207bf0a8","https://www.semanticscholar.org/paper/2fe6060ced80c1c245a718e6188b6516207bf0a8",2,"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions","An augmented intelligence system for simplifying and enhancing the modeling experience for operations research is described that receives a suggested formulation of an optimization problem based on its description and enables the users to validate and edit the suggestions.","Conference on Empirical Methods in Natural Language Processing",2022,"Rindranirina Ramamonjison,Haley Li,Timothy T. Yu,Shiqi He,Vishnu Rengan,Amin Banitalebi-Dehkordi,Zirui Zhou,Yong Zhang",9,45,2
"7d645a3fd276918374fd9483fd675c28e46506d1","https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1",2,"Galactica: A Large Language Model for Science","Galactica is introduced: a large language model that can store, combine and reason about scientific knowledge, and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%.","arXiv.org",2022,"Ross Taylor,Marcin Kardas,Guillem Cucurull,Thomas Scialom,A. Hartshorn,Elvis Saravia,Andrew Poulton,Viktor Kerkez,Robert Stojnic",207,107,45
"7d5175db1b99552491063d2d9581b0b51e1d2932","https://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932",2,"Despite ""super-human"" performance, current LLMs are unsuited for decisions about ethics and safety","This work provides a simple new prompting strategy that leads to yet another supposedly ""super-human"" result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset).","arXiv.org",2022,"Joshua Albrecht,Ellie Kitanidis,Abraham J. Fetterman",5,42,0
"3f2cb353c7528efafb847309ab1e1e95245740a4","https://www.semanticscholar.org/paper/3f2cb353c7528efafb847309ab1e1e95245740a4",2,"Mathematical Capabilities of ChatGPT","It is found that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a Mathematical search engine and knowledge base interface, and GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty.","arXiv.org",2023,"Simon Frieder,Luca Pinchetti,Ryan-Rhys Griffiths,Tommaso Salvatori,Thomas Lukasiewicz,Philipp Christian Petersen,Alexis Chevalier,J. Berner",119,78,3
"cd0988714ea326642d2b1bb18753e187fec71e42","https://www.semanticscholar.org/paper/cd0988714ea326642d2b1bb18753e187fec71e42",2,"A Categorical Archive of ChatGPT Failures","","arXiv.org",2023,"A. Borji",137,72,11
"033275ccc2c7c5c38592ae893da0b5923cf90717","https://www.semanticscholar.org/paper/033275ccc2c7c5c38592ae893da0b5923cf90717",2,"Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases","A holistic survey of CLQA is provided with a detailed taxonomy studying the field from multiple angles, including graph types (modality, reasoning domain, background semantics), modeling aspects, modeling aspects (encoder, processor, decoder), supported queries, datasets, evaluation metrics, and applications.","arXiv.org",2023,"Hongyu Ren,Mikhail Galkin,M. Cochez,Zhaocheng Zhu,J. Leskovec",14,173,1
"dca6c3927ade6481a1ae080f5c24decbfeced1be","https://www.semanticscholar.org/paper/dca6c3927ade6481a1ae080f5c24decbfeced1be",2,"Boosted Prompt Ensembles for Large Language Models","A prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble'' that outperforms single-prompt output-space ensembles and bagged prompt-spaceEnsemble on the GSM8k and AQuA datasets, among others.","arXiv.org",2023,"Silviu Pitis,Michael Ruogu Zhang,Andrew Wang,Jimmy Ba",9,91,1
"95ca67ba607d7859ee8eec457f4b59b115d69bf5","https://www.semanticscholar.org/paper/95ca67ba607d7859ee8eec457f4b59b115d69bf5",2,"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models","This work proposes ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs that can effectively leverage the multi-turn conversation ability of chat- based LLMs, and integrate the thought chain following and tools manipulation in a unified way.","arXiv.org",2023,"Z. Chen,Kun Zhou,Beichen Zhang,Zheng Gong,Wayne Xin Zhao,Ji-rong Wen",5,31,0
"86b37cc52a41ce0cae618202bb746dd7802edc77","https://www.semanticscholar.org/paper/86b37cc52a41ce0cae618202bb746dd7802edc77",2,"Deductive Verification of Chain-of-Thought Reasoning","This work proposes Natural Program, a natural language-based deductive reasoning format that enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps and significantly enhances the rigor and trustfulness of generated reasoning steps.","arXiv.org",2023,"Z. Ling,Yunhao Fang,Xuanlin Li,Zhiao Huang,Mingu Lee,R. Memisevic,Hao Su",5,64,1
"50f9f33b284b7363fbd9b9d2da4939b989a1c7cd","https://www.semanticscholar.org/paper/50f9f33b284b7363fbd9b9d2da4939b989a1c7cd",2,"Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models","This perspective paper comprehensively review existing evaluations of Large Language Models using both standardized tests and ability-oriented benchmarks to highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting.","arXiv.org",2023,"Yuxi Ma,Chi Zhang,Song-Chun Zhu",2,65,0
"5bccb2d410bc64b92b15e3181ad6578e172ff895","https://www.semanticscholar.org/paper/5bccb2d410bc64b92b15e3181ad6578e172ff895",2,"A Comprehensive Overview of Large Language Models","This review article is intended to provide a systematic survey, but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research direction.","arXiv.org",2023,"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,N. Barnes,A. Mian",1,361,0
"c29dbfbc17fa190b787a2662d49f08a38c8bd166","https://www.semanticscholar.org/paper/c29dbfbc17fa190b787a2662d49f08a38c8bd166",2,"ARB: Advanced Reasoning Benchmark for Large Language Models","This work introduces ARB, a novel benchmark composed of advanced reasoning problems in multiple fields, featuring problems in mathematics, physics, biology, chemistry, and law, and introduces a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps.","arXiv.org",2023,"Tomohiro Sawada,Daniel Paleka,Alex Havrilla,Pranav Tadepalli,Paula Vidas,Alexander Kranias,John J. Nay,Kshitij Gupta,Aran Komatsuzaki",4,68,1
"506ad0e58f5d6b99bbcc5e8efb519ae6fa34d307","https://www.semanticscholar.org/paper/506ad0e58f5d6b99bbcc5e8efb519ae6fa34d307",2,"Guiding Mathematical Reasoning via Mastering Commonsense Formula Knowledge","A novel Formula-mastered Solver (FOMAS) is established with two systems drawing insight from the dual process theory, including a Knowledge System and a Reasoning System, to learn and apply formula knowledge, respectively to improve both reasoning accuracy and interpretability.","Knowledge Discovery and Data Mining",2023,"Jia-Yin Liu,Zhenya Huang,Zhiyuan Ma,Qi Liu,Enhong Chen,Tianhuang Su,Haifeng Liu",0,57,0
"1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc","https://www.semanticscholar.org/paper/1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",2,"Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification","The effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter is explored, and a novel and effective prompting method, explicit \uline{c}ode-based \ULine{s}elf-\uline {v}erification~(CSV), is proposed to further boost the mathematical reasoning potential of GPN.","arXiv.org",2023,"Aojun Zhou,Ke Wang,Zimu Lu,Weikang Shi,Sichun Luo,Zipeng Qin,Shaoqing Lu,Anya Jia,Linqi Song,Mingjie Zhan,Hongsheng Li",3,29,0
"537335d9aad0ddbaef93e7f88b0db096671ef6ec","https://www.semanticscholar.org/paper/537335d9aad0ddbaef93e7f88b0db096671ef6ec",2,"No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function","A method is proposed that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning in large language models without requiring additional fine-tuning or reinforcement learning with human feedback alignment.","arXiv.org",2023,"Haotian Xu",0,45,0
"62b4e06f5249d22e4a153ec4a2dc934c6a014372","https://www.semanticscholar.org/paper/62b4e06f5249d22e4a153ec4a2dc934c6a014372",2,"OWL: A Large Language Model for IT Operations","The OWL is introduced, a large language model trained on the authors' collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks.","arXiv.org",2023,"Hongcheng Guo,Jian Yang,Jiaheng Liu,Liqun Yang,Linzheng Chai,Jiaqi Bai,Junran Peng,Xiaorong Hu,Chao Chen,Dongfeng Zhang,Xu Shi,Tieqiao Zheng,Liangfan Zheng,Bo Zhang,Ke Xu,Zhoujun Li",0,73,0
"74a91bcbef04a7eeba4d6ac8dfa526f81badaff1","https://www.semanticscholar.org/paper/74a91bcbef04a7eeba4d6ac8dfa526f81badaff1",2,"Contrastive Decoding Improves Reasoning in Large Language Models","Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models.","arXiv.org",2023,"Sean O'Brien,Mike Lewis",0,33,0
"57d92f147794fdbae23773808792d31a84c7ed4a","https://www.semanticscholar.org/paper/57d92f147794fdbae23773808792d31a84c7ed4a",2,"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback","This work introduces MINT benchmark to evaluate LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback, and repurposes a diverse set of established datasets and tasks focusing on reasoning, coding, and decision-making for efficient evaluation.","",2023,"Xingyao Wang,Zihan Wang,Jiateng Liu,Yangyi Chen,Lifan Yuan,Hao Peng,Heng Ji",0,62,0
"c27f3904490b8e5f9f39fe2b36722090c189e916","https://www.semanticscholar.org/paper/c27f3904490b8e5f9f39fe2b36722090c189e916",2,"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF","This work describes the participation of the team in the ARQMath 2022 Lab, where two highly complementary methods for effective math answer and formula retrieval are applied, using a lexical sparse retriever and a fine-tuned bi-encoder dense retriever to capture contextual similarity and semantic matching.","Conference and Labs of the Evaluation Forum",2022,"Wei Zhong,Yuqing Xie,Jimmy J. Lin",4,58,0
"58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","https://www.semanticscholar.org/paper/58d5e4cbb5f3a944bae0bcc2bff93f06696f8196",2,"JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection","","",2022,"Shunsuke Onuma,Kazuma Kadowaki",2,17,1
"4f451ba06c4c9effd6c4ac0bae222495501a6200","https://www.semanticscholar.org/paper/4f451ba06c4c9effd6c4ac0bae222495501a6200",2,"Innovations in Neural Data-to-text Generation","This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella.","arXiv.org",2022,"Mandar Sharma,Ajay K. Gogineni,Naren Ramakrishnan",4,340,0
"b8eafd80fe009c4ca35dc26ee58a189bb66b89c2","https://www.semanticscholar.org/paper/b8eafd80fe009c4ca35dc26ee58a189bb66b89c2",2,"mGPT: Few-Shot Learners Go Multilingual","Two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus are introduced and compared with the state-of-the-art multilingual model XGLM.","arXiv.org",2022,"Oleh Shliazhko,Alena Fenogenova,M. Tikhonova,V. Mikhailov,A. Kozlova,Tatiana Shavrina",37,70,6
"8e5ca53f7633450e2756950c234c5f9d04b5c9f2","https://www.semanticscholar.org/paper/8e5ca53f7633450e2756950c234c5f9d04b5c9f2",2,"One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development","A domain-agnostic pretrained transformer model is able to effectively extract quantitative clinical measurements from diagnostic reports with a relatively small number of gold-standard annotations and may serve as a roadmap for other quantitative entity extraction.","JMIR Medical Informatics",2022,"Pulkit Singh,Julian S Haimovich,C. Reeder,S. Khurshid,Emily S. Lau,Jonathan W Cunningham,A. Philippakis,C. D. Anderson,J. Ho,S. Lubitz,P. Batra",0,37,0
"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","https://www.semanticscholar.org/paper/1f036b092a74f0c2f7eef37daa16eeb0f5954d9b",2,"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)","A comparison between human-developed machine learning model and models sampled through Neural Architecture Search (NAS) determine an efficient approach to solve the problem of addition using embedded hexadecimal digits.","arXiv.org",2022,"Victor Robila,Kexin Pei,Junfeng Yang",0,20,0
"857d4589b62085e900805fd5432f496e8fc07bd9","https://www.semanticscholar.org/paper/857d4589b62085e900805fd5432f496e8fc07bd9",2,"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model","The basic mathematical abilities often acquired by pre-trained language models are investigated, and mechanistic interpretability techniques are used to explain the (limited) mathematical abilities of GPT-2 small.","arXiv.org",2023,"Michael Hanna,Ollie Liu,Alexandre Variengien",3,35,0
"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","https://www.semanticscholar.org/paper/0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62",2,"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering","This model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers, which outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery.","Annual Meeting of the Association for Computational Linguistics",2023,"Vaishali Pal,Andrew Yates,E. Kanoulas,M. de Rijke",0,40,0
"9160673f89ecb326db8c64f8d09ab30f35a74e9d","https://www.semanticscholar.org/paper/9160673f89ecb326db8c64f8d09ab30f35a74e9d",2,"Arithmetic with Language Models: from Memorization to Computation","These findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.","arXiv.org",2023,"D. Maltoni,M. Ferrara",0,11,0
"5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e","https://www.semanticscholar.org/paper/5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e",2,"On the Paradox of Learning to Reason from Data","This study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has, in fact, learned statistical features that inherently exist in logical reasoning problems.","International Joint Conference on Artificial Intelligence",2022,"Honghua Zhang,Liunian Harold Li,Tao Meng,Kai-Wei Chang,Guy Van den Broeck",36,54,4
"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","https://www.semanticscholar.org/paper/1c1ca2392155ddf30408a442e6b504b5d60d4f2a",2,"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment","A novel moral chain of thought (MORALCOT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind.","Neural Information Processing Systems",2022,"Zhijing Jin,Sydney Levine,Fernando Gonzalez,Ojasv Kamal,Maarten Sap,Mrinmaya Sachan,Rada Mihalcea,J. Tenenbaum,B. Scholkopf",25,85,0
"47c3b8dd2c8a9326249ac98900b2c3fc71f46ab1","https://www.semanticscholar.org/paper/47c3b8dd2c8a9326249ac98900b2c3fc71f46ab1",2,"Self-planning Code Generation with Large Language Model","This paper proposes a self-planning code generation method with large language model, which consists of two phases, namely planning phase and implementation phase, which demonstrates a marked superiority over naive direct generation approaches with language model.","arXiv.org",2023,"Xue Jiang,Yihong Dong,Lecheng Wang,Qiwei Shang,Ge Li",13,79,0
"c715914c388fa64dd8686cd8755e5adfebbf2388","https://www.semanticscholar.org/paper/c715914c388fa64dd8686cd8755e5adfebbf2388",2,"REFINER: Reasoning Feedback on Intermediate Representations","REFINER is a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning that provides structured feedback that the reasoning uses to iteratively improve its intermediate arguments.","arXiv.org",2023,"Debjit Paul,Mete Ismayilzada,Maxime Peyrard,Beatriz Borges,Antoine Bosselut,Robert West,B. Faltings",26,61,1
"015f7ef29bebe85b705efead4c737e252baa3da7","https://www.semanticscholar.org/paper/015f7ef29bebe85b705efead4c737e252baa3da7",2,"Large Language Models Are Not Abstract Reasoners","This paper performs extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and investigates the reasons for this difference.","arXiv.org",2023,"Gaël Gendron,Qiming Bao,M. Witbrock,G. Dobbie",3,66,1
"b09420b30fc093d63fb2ee1aac26c71da81da437","https://www.semanticscholar.org/paper/b09420b30fc093d63fb2ee1aac26c71da81da437",2,"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks","Language-Interfaced Fine-Tuning is proposed and found that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks.","Neural Information Processing Systems",2022,"Tuan Dinh,Yuchen Zeng,Ruisu Zhang,Ziqian Lin,Shashank Rajput,Michael Gira,Jy-yong Sohn,Dimitris Papailiopoulos,Kangwook Lee",22,181,2
"e82e3f4347674b75c432cb80604d38ee630d4bf6","https://www.semanticscholar.org/paper/e82e3f4347674b75c432cb80604d38ee630d4bf6",2,"Transformers Learn Shortcuts to Automata","It is found that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics.","International Conference on Learning Representations",2022,"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang",34,107,1
"1e336827a740f0e4a3def14d261e55d1bda26d83","https://www.semanticscholar.org/paper/1e336827a740f0e4a3def14d261e55d1bda26d83",2,"Probing Representations of Numbers in Vision and Language Models","It is revealed that small numbers are processed differently from large numbers, as in biological systems, and a strong linguistic contribution is found in the structure of number representations in vision and language models, highlighting a difference between representations in biology and artificial systems.","",2022,"Ivana Kajić,Aida Nematzadeh",0,47,0
"2a83a92b08e0f3873d07162c73c67e533321112e","https://www.semanticscholar.org/paper/2a83a92b08e0f3873d07162c73c67e533321112e",2,"Aligning Generative Language Models with Human Values",,"NAACL-HLT",2022,"Ruibo Liu,Ge Zhang,Xinyu Feng,Soroush Vosoughi",10,55,4
"4497f3ed54ac520b50ffa05df04b37a59d4c1265","https://www.semanticscholar.org/paper/4497f3ed54ac520b50ffa05df04b37a59d4c1265",2,"Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET","It is shown that sample-based Minimum Bayes Risk decoding can be used to explore and quantify weaknesses in COMET models, and that these biases are hard to fully remove by simply training on additional synthetic data.","AACL",2022,"Chantal Amrhein,Rico Sennrich",25,51,2
"af46b5ee6d0c1aada1c482d53018a50909aa4c90","https://www.semanticscholar.org/paper/af46b5ee6d0c1aada1c482d53018a50909aa4c90",2,"Impact of Pretraining Term Frequencies on Few-Shot Reasoning","Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, the results raise the question of how much models actually generalize beyond pretraining data, and researchers are encouraged to take thepretraining data into account when interpreting evaluation results.","arXiv.org",2022,"Yasaman Razeghi,Robert L Logan IV,Matt Gardner,Sameer Singh",87,54,7
"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","https://www.semanticscholar.org/paper/4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80",2,"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence","A novel intermediate training task, names meaning-matching, designed to directly learn a meaning-text correspondence, is proposed that enables PLMs to learn lexical semantic information and is found to be a safe intermediate task that guarantees a similar or better performance of downstream tasks.","NAACL-HLT",2022,"Myeongjun Jang,Frank Mtumbuka,Thomas Lukasiewicz",4,53,1
"e02dce6ee032a13b1653f69b034a35676e7d4dc2","https://www.semanticscholar.org/paper/e02dce6ee032a13b1653f69b034a35676e7d4dc2",2,"Can language representation models think in bets?","It is shown that a model is able to ‘think in bets’ if it is first fine-tuned on bet questions with an identical structure, and that LRMs could potentially be applied to tasks that rely on cognitive decision-making skills, but that more research is necessary before these models can robustly make rational decisions.","Royal Society Open Science",2022,"Zhi–Bin Tang,M. Kejriwal",4,106,0
"71471561137956d6aeec39173db460a28d86c11b","https://www.semanticscholar.org/paper/71471561137956d6aeec39173db460a28d86c11b",2,"Analogical Math Word Problems Solving with Enhanced Problem-Solution Association","The proposed analogical learning strategy promotes the performance of MWP-BERT on Math23k over the state-of-the-art model Generate2Rank, with 5 times fewer parameters in the encoder.","Conference on Empirical Methods in Natural Language Processing",2022,"Zhenwen Liang,Jipeng Zhang,Xiangliang Zhang",2,51,0
"17a8b5e6fef1f69979d57021a8f30a5159e152c7","https://www.semanticscholar.org/paper/17a8b5e6fef1f69979d57021a8f30a5159e152c7",2,"Commonsense Reasoning for Conversational AI: A Survey of the State of the Art","A survey of recent conversational AI research focused on commonsense reasoning, including preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions.","arXiv.org",2023,"Christopher Richardson,Larry Heck",3,91,0
"856c342606aca05434e48f2e53cdbd6f6b886802","https://www.semanticscholar.org/paper/856c342606aca05434e48f2e53cdbd6f6b886802",2,"Pre‐trained language models: What do they know?","The behavior of pre‐trained language models (PLMs) in some inference tasks they were not initially trained for are studied, focusing on very recent research works related to the inference capabilities of PLMs in some selected tasks such as factual probing and common‐sense reasoning.","WIREs Data Mining and Knowledge Discovery",2023,"Nuno Guimarães,Ricardo Campos,Alípio Jorge",0,19,0
"867915b9587c046ce8e4b71ab4dee2a1d8bf0b48","https://www.semanticscholar.org/paper/867915b9587c046ce8e4b71ab4dee2a1d8bf0b48",2,"DocTime: A Document-level Temporal Dependency Graph Parser","DocTime - a novel temporal dependency graph (TDG) parser that takes as input a text document and produces a temporal dependencygraph outperforms previous BERT-based solutions by a relative 4-8% on three datasets from modeling the problem as a graph network with path-prediction loss to incorporate longer range dependencies.","North American Chapter of the Association for Computational Linguistics",2022,"Puneet Mathur,Vlad I. Morariu,Verena Kaynig-Fittkau,Jiuxiang Gu,Franck Dernoncourt,Quan Hung Tran,A. Nenkova,Dinesh Manocha,R. Jain",6,67,0
"30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0","https://www.semanticscholar.org/paper/30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0",2,"MC-LSTM: Mass-Conserving LSTM","This work proposes a novel Mass-Conserving LSTM (MC-LSTM), which sets a new state-of-the-art for neural arithmetic units at learning arithmetic operations, such as addition tasks, which have a strong conservation law, as the sum is constant over time.","International Conference on Machine Learning",2021,"Pieter-Jan Hoedt,Frederik Kratzert,D. Klotz,Christina Halmich,Markus Holzleitner,G. Nearing,S. Hochreiter,G. Klambauer",23,104,5
"4578747d52aa1b3537612287352a803a7b17e999","https://www.semanticscholar.org/paper/4578747d52aa1b3537612287352a803a7b17e999",2,"Asynchronous Neural Networks for Learning in Graphs","It is proved that AMP can simulate synchronous GNNs and that it can theoretically distinguish any pair of graphs, and it is shown that it might be better suited to propagate messages over large distances in graphs and performs well on several graph classification benchmarks.","arXiv.org",2022,"Lukas Faber,Roger Wattenhofer",1,62,0
"33149f835f391119d287cc2c6b009464e7d14fe4","https://www.semanticscholar.org/paper/33149f835f391119d287cc2c6b009464e7d14fe4",2,"On the Abilities of Mathematical Extrapolation with Implicit Models","This paper compares the robustness of implicitly-defined and classical deep learning models on a series of mathematical extrapolation tasks, where the models are tested with out-of-distribution samples during inference time to showcase implicit models’ unique advantages for mathematics extrapolation thanks to their flexible and selective framework.","",2022,"Juliette Decugis,Max Emerling,Ashwin Ganesh,Alicia Y. Tsai,L. Ghaoui",0,20,0
"26e07b6ac9b0f04396e2e7f7ca518b6fbae67ce7","https://www.semanticscholar.org/paper/26e07b6ac9b0f04396e2e7f7ca518b6fbae67ce7",2,"Improving the Robustness of Neural Multiplication Units with Reversible Stochasticity","It is shown that Neural Multiplication Units (NMUs) are unable to reliably learn tasks as simple as multiplying two inputs when given different training ranges, and stochasticity provides improved robustness with the potential to improve learned representations of upstream networks for numerical and image tasks.","arXiv.org",2022,"Bhumika Mistry,K. Farrahi,Jonathon S. Hare",0,26,0
"2af476f7c2a7c040fc9ab7750bf41a84f66aa947","https://www.semanticscholar.org/paper/2af476f7c2a7c040fc9ab7750bf41a84f66aa947",2,"Knowledge Based Multilingual Language Model","This work presents a novel framework to pretrain knowledge based multilingual language models (KMLMs) by generating a large amount of code-switched synthetic sentences and reasoning-based multilingual training data using the Wikidata knowledge graphs and designing pretraining tasks to facilitate knowledge learning.","arXiv.org",2021,"Linlin Liu,Xin Li,Ruidan He,Lidong Bing,Shafiq R. Joty,Luo Si",10,29,5
"5c8cb5d39aa8531fcb5962821ac404e4a0e109d1","https://www.semanticscholar.org/paper/5c8cb5d39aa8531fcb5962821ac404e4a0e109d1",2,"Is It Smaller Than a Tennis Ball? Language Models Play the Game of Twenty Questions","It is found that only the largest model has enough world knowledge to play the game of Twenty Questions well, although it still has difficulties with the shape and size of objects.","BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",2022,"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans",1,33,0
"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","https://www.semanticscholar.org/paper/f8a36a6c10b4f598f7a936f09c58de31c9e10bcd",2,"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning","Investigation of the underlying hypothesis in the most simple conceivable scenario – the addition of real numbers finds that two layer neural networks fail to learn the structure of this task and that growing the network’s width leads to a complex division of input space.","",2022,"",0,25,0
"2aba5bba16dac5cd62683bab9de5d6faaaed0de1","https://www.semanticscholar.org/paper/2aba5bba16dac5cd62683bab9de5d6faaaed0de1",2,"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach","An iterative context-aware prompter is proposed which addresses key limitations of existing prompting methods, namely they are either re- stricted to queries with a single identifiable re- 014 lation/predicate, or being agnostic to input con- 015 texts, which makes it difficult to capture vari- 016 abilities across different inference steps.","arXiv.org",2022,"Boshi Wang,Xiang Deng,Huan Sun",8,44,2
"fed460648303afa32247e493847e4dc73dc1a5b3","https://www.semanticscholar.org/paper/fed460648303afa32247e493847e4dc73dc1a5b3",2,"Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?","To endow PLMs with incremental reasoning skills, this work proposes a set of inference strategies on relevant facts and distractors allowing us to build automatically generated training datasets, and empirically shows the effectiveness of this proposal on multiple-choice question answering and reading comprehension.","International Conference on Computational Linguistics",2022,"Jesús Lovón-Melgarejo,José G. Moreno,Romaric Besançon,Olivier Ferret,L. Tamine",0,36,0
"011869f932f89d047ce2bd36d73a95cc04888193","https://www.semanticscholar.org/paper/011869f932f89d047ce2bd36d73a95cc04888193",2,"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms","A new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations and shows that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks.","Conference on Empirical Methods in Natural Language Processing",2020,"Pei Zhou,Rahul Khanna,Bill Yuchen Lin,Daniel Ho,J. Pujara,Xiang Ren",28,58,1
"eb2fc03b8865b8e1b4cb933d917ea269ebe14584","https://www.semanticscholar.org/paper/eb2fc03b8865b8e1b4cb933d917ea269ebe14584",2,"Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training","It is argued that mis-predictions can help locate such dominating patterns that harm language understanding and propose a new language pre-training method, MisPredictions as Harm Alerts (MPA), which expedites the pre- training of BERT and ELECTRA and improves their performances on downstream tasks.","",2020,"Chen Xing,Wenhao Liu,Caiming Xiong",0,41,0
"73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19","https://www.semanticscholar.org/paper/73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",2,"Measuring and Improving Consistency in Pretrained Language Models","The creation of PARAREL, a high-quality resource of cloze-style query English paraphrases, and analysis of the representational spaces of PLMs suggest that they have a poor structure and are currently not suitable for representing knowledge in a robust way.","Transactions of the Association for Computational Linguistics",2021,"Yanai Elazar,Nora Kassner,Shauli Ravfogel,Abhilasha Ravichander,E. Hovy,Hinrich Schütze,Yoav Goldberg",129,88,25
"008b9fc834f5839a25febe150f3076d550ee442f","https://www.semanticscholar.org/paper/008b9fc834f5839a25febe150f3076d550ee442f",2,"Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2",,"arXiv.org",2021,"Gregor Betz,Kyle Richardson,C. Voigt",19,50,2
"d65a064eb837f838faf6ff67781b62450b92b159","https://www.semanticscholar.org/paper/d65a064eb837f838faf6ff67781b62450b92b159",2,"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification","This work proposes gamification as a framework for data construction, and creates CommonsenseQA 2.0, which includes 14,343 yes/no questions, and demonstrates its difficulty for models that are orders-of-magnitude larger than the AI used in the game itself.","NeurIPS Datasets and Benchmarks",2021,"Alon Talmor,Ori Yoran,Ronan Le Bras,Chandrasekhar Bhagavatula,Yoav Goldberg,Yejin Choi,Jonathan Berant",70,63,14
"5aab57cc0530560d82c74c055f664280619d7e81","https://www.semanticscholar.org/paper/5aab57cc0530560d82c74c055f664280619d7e81",2,"PROST: Physical Reasoning about Objects through Space and Time","An extensive analysis demonstrates that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer options are presented to them, they struggle when the superlative in a question is inverted, and increasing the amount of pretraining data and parameters only yields minimal improvements.","Findings",2021,"Stephane T Aroca-Ouellette,Cory Paik,A. Roncone,Katharina Kann",22,54,5
"2cf3cd3a7a08fc91eecab45e73299940c9c439dc","https://www.semanticscholar.org/paper/2cf3cd3a7a08fc91eecab45e73299940c9c439dc",2,"Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills","This work proposes to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph, and shows that the model, PReasM, substantially outperforms T5, a popular pre-trained encoder-decoder model.","Annual Meeting of the Association for Computational Linguistics",2021,"Ori Yoran,Alon Talmor,Jonathan Berant",40,67,6
"e55391a9406245584b3e5b3225dad2e171b9a06b","https://www.semanticscholar.org/paper/e55391a9406245584b3e5b3225dad2e171b9a06b",2,"RuleBERT: Teaching Soft Rules to Pre-Trained Language Models","This work introduces a classification task where, given facts and soft rules, thePLM should return a prediction with a probability for a given hypothesis, and proposes a revised loss function that enables the PLM to learn how to predict precise probabilities for the task.","Conference on Empirical Methods in Natural Language Processing",2021,"Mohammed Saeed,N. Ahmadi,Preslav Nakov,Paolo Papotti",18,56,2
"a466d10b80dbdee3b130bef73ec62f3a89eb389b","https://www.semanticscholar.org/paper/a466d10b80dbdee3b130bef73ec62f3a89eb389b",2,"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples","This work explores methods to make better use of the multilingual annotation and language agnostic property of KG triples, and presents novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples.","Conference on Empirical Methods in Natural Language Processing",2021,"Linlin Liu,Xin Li,Ruidan He,Lidong Bing,Shafiq R. Joty,Luo Si",6,51,0
"e92653643e444d165523bb1f48763bf8d0f56772","https://www.semanticscholar.org/paper/e92653643e444d165523bb1f48763bf8d0f56772",2,"LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI","This work proposes an extensible framework to collectively yet categorically test diverse Logical reasoning capabilities required for NLI (and, by extension, NLU), and creates a semi-synthetic large test bench and associated framework that offers the following utilities.","arXiv.org",2021,"Ishan Tarunesh,Somak Aditya,M. Choudhury",4,70,1
"0b483b550b21ec42d693fc04a372dbb10dd07019","https://www.semanticscholar.org/paper/0b483b550b21ec42d693fc04a372dbb10dd07019",2,"Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge","It is found generalization does not improve over the course of pre-training, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.","North American Chapter of the Association for Computational Linguistics",2021,"Ian Porada,Alessandro Sordoni,J. Cheung",3,49,0
"7bebb48d34c219b119ca2d4ffc97d7fd4940c35c","https://www.semanticscholar.org/paper/7bebb48d34c219b119ca2d4ffc97d7fd4940c35c",2,"On the data requirements of probing","A novel method to estimate the required number of data samples in such experiments is presented and, across several case studies, it is verified that the estimations have sufficient statistical power.","Findings",2022,"Zining Zhu,Jixuan Wang,Bai Li,F. Rudzicz",2,67,0
"50f7d69ddd7f9b34f5121607fbdcc57236d65b8c","https://www.semanticscholar.org/paper/50f7d69ddd7f9b34f5121607fbdcc57236d65b8c",2,"Can Pre-trained Language Models Interpret Similes as Smart as Human?","This paper investigates the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, and shows that PLMs can infer similes’ shared properties while still underperforming humans.","Annual Meeting of the Association for Computational Linguistics",2022,"Qi He,Sijie Cheng,Zhixu Li,Rui Xie,Yanghua Xiao",10,55,2
"3f4d11971f2c64be9125a7fe99c019588bbebf16","https://www.semanticscholar.org/paper/3f4d11971f2c64be9125a7fe99c019588bbebf16",2,"Iteratively Prompt Pre-trained Language Models for Chain of Thought","An iterative prompting framework is explored, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference by learning to dynamically synthesize prompts conditioned on the current step’s contexts.","Conference on Empirical Methods in Natural Language Processing",2022,"Boshi Wang,Xiang Deng,Huan Sun",30,50,6
"cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","https://www.semanticscholar.org/paper/cd471b5ef162906ef3d9a84398b3f98e9ee4bf56",2,"A Review on Language Models as Knowledge Bases","This paper presents a set of aspects that it is deemed a pretrained Language Models should have to fully act as a KB, and reviews the recent literature with respect to those aspects.","arXiv.org",2022,"Badr AlKhamissi,Millicent Li,Asli Celikyilmaz,Mona T. Diab,Marjan Ghazvininejad",46,180,8
"a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7","https://www.semanticscholar.org/paper/a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7",2,"The Curious Case of Control","","Conference on Empirical Methods in Natural Language Processing",2022,"Elias Stengel-Eskin,Benjamin Van Durme",0,43,0
"e3dae33c5bdf397abdefd971ea34c48fb836dcc0","https://www.semanticscholar.org/paper/e3dae33c5bdf397abdefd971ea34c48fb836dcc0",2,"GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models","Interestingly, it is found that larger multilingual PLMs variants do not necessarily store geo-diverse concepts better than its smaller variant; and a language may better probe knowledge about a non-native country than its native country.","Conference on Empirical Methods in Natural Language Processing",2022,"Da Yin,Hritik Bansal,Masoud Monajatipoor,Liunian Harold Li,Kai-Wei Chang",12,48,1
"559bfba3bee31f6061a5d5c7061f22794de47e39","https://www.semanticscholar.org/paper/559bfba3bee31f6061a5d5c7061f22794de47e39",2,"State-of-the-art generalisation research in NLP: a taxonomy and review","A taxonomy for characterising and understanding generalisation research in NLP is presented, a taxonomy is used to present a comprehensive map of published generalisation studies, and recommendations for which areas might deserve attention in the future are made.","arXiv.org",2022,"D. Hupkes,Mario Giulianelli,Verna Dankers,Mikel Artetxe,Yanai Elazar,Tiago Pimentel,Christos Christodoulopoulos,Karim Lasri,Naomi Saphra,Arabella J. Sinclair,Dennis Ulmer,Florian Schottmann,Khuyagbaatar Batsuren,Kaiser Sun,Koustuv Sinha,Leila Khalatbari,Maria Ryskina,Rita Frieske,Ryan Cotterell,Zhijing Jin",24,696,2
"ad5573cb25fd403f7620332f363ae87327c69a49","https://www.semanticscholar.org/paper/ad5573cb25fd403f7620332f363ae87327c69a49",2,"The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning","This work revisits neuro-symbolic approaches and uses Language Models as Logic Programmer (LMLP) that learns from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog's backward chaining algorithm.","arXiv.org",2022,"Hanlin Zhang,Yi-Fan Zhang,Li Erran Li,Eric P. Xing",8,70,1
"da345b189e4faaaa489f7319640868a37a3932a1","https://www.semanticscholar.org/paper/da345b189e4faaaa489f7319640868a37a3932a1",2,"Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?","A skill tree on compositionality in arithmetic symbolic reasoning that defines the hierarchical levels of complexity along with three compositionality dimensions: systematicity, productivity, and substitutivity is introduced.","Conference of the European Chapter of the Association for Computational Linguistics",2023,"Keito Kudo,Y. Aoki,Tatsuki Kuribayashi,Ana Brassard,Masashi Yoshikawa,Keisuke Sakaguchi,Kentaro Inui",1,22,0
"06396c7cd5d223a1776abf8811359ec7bc05d420","https://www.semanticscholar.org/paper/06396c7cd5d223a1776abf8811359ec7bc05d420",2,"Knowledge-Augmented Methods for Natural Language Processing","This tutorial introduces the key steps in integrating knowledge into NLP, including knowledge grounding from text, knowledge representation and fusing, and introduces recent state-of-the-art applications in fusing knowledge into language understanding, language generation and commonsense reasoning.","Annual Meeting of the Association for Computational Linguistics",2023,"Chenguang Zhu,Yichong Xu,Xiang Ren,Bill Yuchen Lin,Meng Jiang,Wenhao Yu",9,85,0
"8018a68956d6751d7ea76110537d5a2e86ec05c4","https://www.semanticscholar.org/paper/8018a68956d6751d7ea76110537d5a2e86ec05c4",2,"The Life Cycle of Knowledge in Big Language Models: A Survey","This survey revisits PLMs as knowledge-based systems by dividing the life circle of knowledge in PLMs into five critical periods, and investigating how knowledge circulates when it is built, maintained and used.","arXiv.org",2023,"Boxi Cao,Hongyu Lin,Xianpei Han,Le Sun",2,174,0
"44772fe1c3fa422a3da7e25092db2544893d6bfb","https://www.semanticscholar.org/paper/44772fe1c3fa422a3da7e25092db2544893d6bfb",2,"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming","DSR-LM is proposed, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning, and efficiently learns weighted rules and applies semantic loss to further improve LMs.","Annual Meeting of the Association for Computational Linguistics",2023,"Hanlin Zhang,Jiani Huang,Ziyang Li,M. Naik,Eric P. Xing",7,59,1
"2db6c10f135d5701ae7aec45986124ce264c1344","https://www.semanticscholar.org/paper/2db6c10f135d5701ae7aec45986124ce264c1344",2,"Q UASI -N ATURAL L ANGUAGE","","",,"Kaiyu Yang,Jia Deng",0,60,0
"2cffec40f1d7cfc81d498b8939493243bbcadebe","https://www.semanticscholar.org/paper/2cffec40f1d7cfc81d498b8939493243bbcadebe",2,"Learning Symbolic Rules for Reasoning in Quasi-Natural Language","This work asks how to build a rule-based system that can reason with natural language input but without the manual construction of rules, and proposes MetaQNL, a ""Quasi-Natural"" language that can express both formal logic and natural language sentences, and MetaInduce, a learning algorithm that inducesMetaQNL rules from training data, with or without intermediate reasoning steps.","arXiv.org",2021,"Kaiyu Yang,Jia Deng",10,89,0
"30977afbd4501249e1a320bd2e48581197914ab8","https://www.semanticscholar.org/paper/30977afbd4501249e1a320bd2e48581197914ab8",2,"A Short Survey of Systematic Generalization","This survey includes systematic generalization and a history of how machine learning addresses it, and introduces Classicist and Connectionist, a first look at the definition of systematicgeneralization, and different types of Connectionists and how they approach the generalization.","arXiv.org",2022,"Yuanpeng Li",0,244,0
"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","https://www.semanticscholar.org/paper/237f5ca6fcccef2b77a2212b34fb06a1dbd09b72",2,"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting","This work collects and standardizes prompts from a diverse range of tasks for use with tasks they were not designed for, and evaluates these prompts across fixed multiple choice datasets for a quantitative analysis of how certain attributes of a prompt affect performance.","arXiv.org",2022,"Gabriel Orlanski",1,50,0
"2e01fdebbc780d3667ec3bf87a44927f0d9c188a","https://www.semanticscholar.org/paper/2e01fdebbc780d3667ec3bf87a44927f0d9c188a",2,"Decision-Oriented Dialogue for Human-AI Collaboration","A class of tasks, called decision-oriented dialogues, in which AI assistants must collaborate with one or more humans via natural language to help them make complex decisions is described, ranging from efficient communication to reasoning and optimization.","arXiv.org",2023,"Jessy Lin,Nicholas Tomlin,Jacob Andreas,J. Eisner",2,94,0
"67f23dec7687692660d8aa1315b9dbc8e1aacf22","https://www.semanticscholar.org/paper/67f23dec7687692660d8aa1315b9dbc8e1aacf22",2,"Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems","This study provides a comprehensive overview of the primary characteristics of a dialogue agent, the supporting tasks, their corresponding open-domain datasets, and the methods used to benchmark these datasets.","arXiv.org",2023,"Shivani Kumar,S. Bhatia,Milan Aggarwal,Tanmoy Chakraborty",0,198,0
"23446cc15226c46eae9e4414598f7375eacb6ea1","https://www.semanticscholar.org/paper/23446cc15226c46eae9e4414598f7375eacb6ea1",2,"Back to Square One: Bias Detection, Training and Commonsense Disentanglement in the Winograd Schema","Much of the apparent progress on Winograd Schema may not necessarily reflect progress in Commonsense reasoning, but much of it comes from supervised data, which is not likely to account for all the required commonsense reasoning skills and knowledge.","arXiv.org",2021,"Yanai Elazar,Hongming Zhang,Yoav Goldberg,D. Roth",3,67,1
"6bb369f874f49cd51415f216f1a3f635f2ca1eed","https://www.semanticscholar.org/paper/6bb369f874f49cd51415f216f1a3f635f2ca1eed",2,"ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations",,"Conference on Empirical Methods in Natural Language Processing",2021,"Rujun Han,I-Hung Hsu,Jiao Sun,J. Baylón,Qiang Ning,D. Roth,Nanyun Peng",24,34,6
"e6d84cf9ae6efa10919bff765613e883a761db62","https://www.semanticscholar.org/paper/e6d84cf9ae6efa10919bff765613e883a761db62",2,"Open Temporal Relation Extraction for Question Answering","This paper proposes to reformulate the task of TQA as open temporal relation extraction, which allows to learn context-agnostic, free-text-based relation representations that generalize across diﬀerent contexts and events, which leads to higher data eﬃciency.","Conference on Automated Knowledge Base Construction",2021,"Chao Shang,Peng Qi,Guangtao Wang,Jing Huang,Youzheng Wu,Bowen Zhou",3,25,0