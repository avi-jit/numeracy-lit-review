"id","url","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"28a5a53dafacebad8a7c47773079caeffb9a5baa","https://www.semanticscholar.org/paper/28a5a53dafacebad8a7c47773079caeffb9a5baa",9,"Representing Numbers in NLP: a Survey and a Vision","This work synthesizes best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.","North American Chapter of the Association for Computational Linguistics",2021,"Avijit Thawani,J. Pujara,Pedro A. Szekely,Filip Ilievski",44,68,3
"c2d50f17ea6769f6f5663ccac37a9627a0543184","https://www.semanticscholar.org/paper/c2d50f17ea6769f6f5663ccac37a9627a0543184",6,"I NVESTIGATING THE L IMITATIONS OF T RANSFORM ERS WITH S IMPLE A RITHMETIC T ASKS","It is concluded that modern pretrained language models can easily learn arithmetic from very few examples, as long as they use the proper surface representation, which bolsters evidence that subword tokenizers and positional encodings are compo-nents in current transformer designs that might need improvement.","",,"",0,0,0
"8424082e3bf4792462eb112d7ebcecf5b0dc3613","https://www.semanticscholar.org/paper/8424082e3bf4792462eb112d7ebcecf5b0dc3613",6,"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning","This survey paper discusses the performance of transformers on different reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning.","Conference on Automated Knowledge Base Construction",2021,"Chadi Helwe,C. Clavel,Fabian M. Suchanek",15,95,4
"d3a7a4543d83f568f79d1febe8379465ff0140c9","https://www.semanticscholar.org/paper/d3a7a4543d83f568f79d1febe8379465ff0140c9",5,"A Survey of Deep Learning for Mathematical Reasoning","This survey paper reviews the key tasks, datasets, and methods at the intersec-tion of mathematical reasoning and deep learning over the past decade, and evaluates existing benchmarks and methods and discusses future research directions.","ArXiv",2022,"Pan Lu,Liang Qiu,Wenhao Yu,S. Welleck,Kai-Wei Chang",1,216,1
"37588705a2af7d5b24d901dd33ade1ff293aabdd","https://www.semanticscholar.org/paper/37588705a2af7d5b24d901dd33ade1ff293aabdd",4,"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model","This work investigates the ability of text-to-text transfer learning model (T5), which has outperformed its predecessors in the conventional NLP tasks, to learn numeracy, and considers four numeracy tasks.","Conference on Empirical Methods in Natural Language Processing",2021,"K. Pal,Chitta Baral",9,18,1
"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","https://www.semanticscholar.org/paper/d331de3b6bebb0f9af1fddf1b730ec057a7026d4",4,"Relational World Knowledge Representation in Contextual Language Models: A Review","This work proposes to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision, and provides a high-level, extensible taxonomy for knowledge representation in L Ms.","Conference on Empirical Methods in Natural Language Processing",2021,"Tara Safavi,Danai Koutra",17,143,1
"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","https://www.semanticscholar.org/paper/0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",4,"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension","This study is the largest survey of the deep learning models in NLP field to date, providing an overview of the various formats and domains of the current resources, and highlighting the current lacunae for future work.","ACM Computing Surveys",2021,"Anna Rogers,Matt Gardner,Isabelle Augenstein",53,343,3
"1525c819ecaed99dad8117da1206eff3d470756d","https://www.semanticscholar.org/paper/1525c819ecaed99dad8117da1206eff3d470756d",4,"Benchmarks for Automated Commonsense Reasoning: A Survey","A survey of the development and uses of AI commonsense benchmarks, and it is argued that it is worthwhile to invest the work needed ensure that benchmark examples are consistently high quality.","ArXiv",2023,"E. Davis",1,168,0
"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",3,"PAL: Program-aided Language Models","Program-Aided Language models (P A L): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but ofﬂoads the solution step to a runtime such as a Python interpreter.","ArXiv",2022,"Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,Yiming Yang,Jamie Callan,Graham Neubig",16,57,6
"6756fcd998caeb7b23702e08559e63710179334c","https://www.semanticscholar.org/paper/6756fcd998caeb7b23702e08559e63710179334c",3,"Reasoning with Language Model Prompting: A Survey","This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting and introduces research works with comparisons and summaries and provides systematic resources to help beginners.","ArXiv",2022,"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen",6,150,0
"0f2199296f01694ee46b6059879260fb80a84fa6","https://www.semanticscholar.org/paper/0f2199296f01694ee46b6059879260fb80a84fa6",3,"Teaching Autoregressive Language Models Complex Tasks By Demonstration","The results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.","ArXiv",2021,"Gabriel Recchia",15,48,1
"1b6e810ce0afd0dd093f789d2b2742d047e316d5","https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",3,"Chain of Thought Prompting Elicits Reasoning in Large Language Models","Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.","ArXiv",2022,"Jason Wei,Xuezhi Wang,Dale Schuurmans,Maarten Bosma,E. Chi,Quoc Le,Denny Zhou",345,103,95
"3ee9c65366efbb17adf370c39f20dbef60d53670","https://www.semanticscholar.org/paper/3ee9c65366efbb17adf370c39f20dbef60d53670",3,"Towards Reasoning in Large Language Models: A Survey","A comprehensive overview of the current state of knowledge on reasoning in large language models, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, and suggestions on future directions are provided.","ArXiv",2022,"Jie Huang,K. Chang",4,103,0
"884c0b6db564208d99cadf2548f0aa96dee5f859","https://www.semanticscholar.org/paper/884c0b6db564208d99cadf2548f0aa96dee5f859",3,"Commonsense Reasoning with Implicit Knowledge in Natural Language","This work takes a middle ground where it uses smaller language models together with a relatively smaller but targeted natural language text corpora and proposes four methods competitive to state-of-the-art methods to reason with implicit commonsense.","Conference on Automated Knowledge Base Construction",2021,"Pratyay Banerjee,Swaroop Mishra",2,73,0
"def7c67c3688bc459fbef3ce50a466a0cbf411a9","https://www.semanticscholar.org/paper/def7c67c3688bc459fbef3ce50a466a0cbf411a9",3,"A Primer for Neural Arithmetic Logic Modules","Focusing on the shortcomings of NALU, an in-depth analysis is provided to reason about design choices of recent units to highlight inconsistencies in a fundamental experiment causing the inability to directly compare across papers.","ArXiv",2021,"Bhumika Mistry,K. Farrahi,Jonathon Hare",4,54,0
"c36277b67814e0a522e786a38e1768612b0e63f2","https://www.semanticscholar.org/paper/c36277b67814e0a522e786a38e1768612b0e63f2",3,"Exploring the Learning Mechanisms of Neural Division Modules","It is shown that robustly learning division in a systematic manner remains a challenge even at the simplest level of dividing two numbers, and a novel approach to division is proposed which is called the Neural Reciprocal Unit (NRU) and the Neural Multiplicative Reciproc Unit (NMRU).","",2022,"Bhumika Mistry",0,23,0
"ff8f3dfd9e2f4a92310999722abefab202935521","https://www.semanticscholar.org/paper/ff8f3dfd9e2f4a92310999722abefab202935521",3,"Do Language Models Understand Measurements?","This study shows that PLMs lack the capability required for reasoning over measurements, and proposes a simple embedding strategy to better distinguish between numbers and units, which leads to a significant improvement in the probing tasks.","Conference on Empirical Methods in Natural Language Processing",2022,"Sungjin Park,Seung-kook Ryu,E. Choi",0,26,0
"9a9e68d400069f023f7dc9b982226c95159a509d","https://www.semanticscholar.org/paper/9a9e68d400069f023f7dc9b982226c95159a509d",3,"Dissociating language and thought in large language models: a cognitive perspective","","ArXiv",2023,"Kyle Mahowald,Anna A. Ivanova,I. Blank,N. Kanwisher,J. Tenenbaum,Evelina Fedorenko",7,412,0
"ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb","https://www.semanticscholar.org/paper/ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb",3,"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks","This work presents an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which is called KG-to-task match and demonstrates that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHow-based KGs are the bestmatch for PIQA across all 3 analysis phases.","Conference of the European Chapter of the Association for Computational Linguistics",2021,"Lisa Bauer",10,40,1
"6597d61bdb531051678c773526758a6dc113b9ce","https://www.semanticscholar.org/paper/6597d61bdb531051678c773526758a6dc113b9ce",3,"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences","This work introduces a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs, and proposes a pairwise accuracy metric to reliably measure an agent’s ability to perform Commonsense reasoning over a given situation.","Findings",2021,"Shikhar Singh,Nuan Wen,Yu Hou,Pegah Alipoormolabashi,Te-Lin Wu,Xuezhe Ma,Nanyun Peng",13,51,6
"62953ca1252c9febe07c7007a10911726f37792d","https://www.semanticscholar.org/paper/62953ca1252c9febe07c7007a10911726f37792d",3,"TIMEDIAL: Temporal Commonsense Reasoning in Dialog","This paper presents the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set, TimeDial, and reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context.","Annual Meeting of the Association for Computational Linguistics",2021,"Lianhui Qin,Aditya Gupta,Shyam Upadhyay,Luheng He,Yejin Choi,Manaal Faruqui",23,48,3
"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","https://www.semanticscholar.org/paper/c9343c26a0e604f7afd94b7290bbdf8d96cd65b6",3,"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals","This paper measures the NCS acquired by existing neural language models using a masked numeral prediction task as an evaluation task and proposes methods to reflect not only the symbolic aspect but also the quantitative aspect of numerals in the training of language models, using a loss function that depends on the magnitudes of the numerals and a regression model for the masked numal prediction task.","Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out",2021,"Taku Sakamoto,Akiko Aizawa",1,23,0
"957424a1f3319a2aab1a91539a00e712477b4b4a","https://www.semanticscholar.org/paper/957424a1f3319a2aab1a91539a00e712477b4b4a",3,"Self-Attention Networks Can Process Bounded Hierarchical Languages","It is proved that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language.","Annual Meeting of the Association for Computational Linguistics",2021,"Shunyu Yao,Binghui Peng,C. Papadimitriou,Karthik Narasimhan",9,52,2
"f729e16ee6765e2fd76772cbf9d3dd17b4d25438","https://www.semanticscholar.org/paper/f729e16ee6765e2fd76772cbf9d3dd17b4d25438",2,"Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems","This work inspects non-neural and neural methods to solve math word problems narrated in a natural language, and highlights the ability of these methods to be generalizable, mathematically reasonable, interpretable, and explainable.","ArXiv",2021,"Keyur Faldu,A. Sheth,Prashant Kikani,Manas Gaur,Aditi Avasthi",6,75,2
"c9f48406851954cb098911eccb4124ea5f966675","https://www.semanticscholar.org/paper/c9f48406851954cb098911eccb4124ea5f966675",2,"A Survey on Multi-hop Question Answering and Generation","A general and formal definition of MHQA task is provided, the existing attempts to this highly interesting, yet quite challenging task are summarized, and the best methods to createMHQA datasets are outlined.","ArXiv",2022,"Vaibhav Mavi,Anubhav Jangra,A. Jatowt",4,183,0
"2fe6060ced80c1c245a718e6188b6516207bf0a8","https://www.semanticscholar.org/paper/2fe6060ced80c1c245a718e6188b6516207bf0a8",2,"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions","An augmented intelligence system for simplifying and enhancing the modeling experience for operations research is described that receives a suggested formulation of an optimization problem based on its description and enables the users to validate and edit the suggestions.","ArXiv",2022,"Rindranirina Ramamonjison,Haley Li,Timothy T. Yu,Shiqi He,Vishnu Rengan,Amin Banitalebi-Dehkordi,Zirui Zhou,Yong Zhang",6,40,1
"262d6a0f6eb3f4c81e47fecd7e14be004295a7cf","https://www.semanticscholar.org/paper/262d6a0f6eb3f4c81e47fecd7e14be004295a7cf",2,"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models","This work proposes a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands and math operators on the output solution, and shows that robustness does not appear to continuously improve as a function of scale, but that the recent GPT-3-Instruct achieves a dra-matic improvement in both robustness and sensitivity, compared to all other GPT variants.","ArXiv",2022,"Alessandro Stolfo,Zhijing Jin,K. Shridhar,B. Schölkopf,Mrinmaya Sachan",2,55,0
"598d9b235f5ab148fc757240d9bc39a47b8eaf72","https://www.semanticscholar.org/paper/598d9b235f5ab148fc757240d9bc39a47b8eaf72",2,"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks","Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoT performance on all math problem datasets and near-SoTA performance on ﬁnancial datasets.","ArXiv",2022,"Wenhu Chen,Xueguang Ma,Xinyi Wang,William W. Cohen",19,38,5
"72d53421ec2179e1116b7aada77941213213135c","https://www.semanticscholar.org/paper/72d53421ec2179e1116b7aada77941213213135c",2,"A Categorical Archive of ChatGPT Failures","","ArXiv",2023,"A. Borji",1,33,0
"c78f267dfcca001384cb0d4e3b8de0173adc162d","https://www.semanticscholar.org/paper/c78f267dfcca001384cb0d4e3b8de0173adc162d",2,"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF","This work describes the participation of the team in the ARQMath 2022 Lab, where two highly complementary methods for effective math answer and formula retrieval are applied, using a lexical sparse retriever and a fine-tuned bi-encoder dense retriever to capture contextual similarity and semantic matching.","Conference and Labs of the Evaluation Forum",2022,"Wei Zhong,Yuqing Xie,Jimmy J. Lin",2,58,0
"a5808ccc50f77083bd3be926fb2af05cf34563ff","https://www.semanticscholar.org/paper/a5808ccc50f77083bd3be926fb2af05cf34563ff",2,"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition","The ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on is evaluated.","International Conference on Language Resources and Evaluation",2022,"Matteo Muffo,A. Cocco,Enrico Bertino",3,22,0
"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","https://www.semanticscholar.org/paper/1f036b092a74f0c2f7eef37daa16eeb0f5954d9b",2,"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)","A comparison between human-developed machine learning model and models sampled through Neural Architecture Search (NAS) determine an efficient approach to solve the problem of addition using embedded hexadecimal digits.","ArXiv",2022,"Victor Robila,Kexin Pei,Junfeng Yang",0,20,0
"1a174b63d294f96568517b91f2c8d6c9362118b5","https://www.semanticscholar.org/paper/1a174b63d294f96568517b91f2c8d6c9362118b5",2,"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems","This paper proposes to conduct numerical capability diagnosis on a series of Numerical Question Answering systems and datasets and investigates the effectiveness of applying perturbations as data augmentation to relieve systems’ lack of robust numerical capabilities.","Conference on Empirical Methods in Natural Language Processing",2022,"Jialiang Xu,Mengyu Zhou,Xinyi He,Shi Han,Dongmei Zhang",0,47,0
"965e409a3e7b5670d609837fac9823b160d6639c","https://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c",2,"Logical Tasks for Measuring Extrapolation and Rule Comprehension","This work describes and characterize logical tasks and discusses system requirements for their solution, and discusses the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias.","ArXiv",2022,"Ippei Fujisawa,R. Kanai",0,61,0
"4d17732d90440682b0500f4e209c6cc4fac20e0e","https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e",2,"Teaching Algorithmic Reasoning via In-context Learning","This work shows that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which it is referred to as algorithmic prompting, and evaluates the approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrates boosts in performance over existing prompting techniques.","ArXiv",2022,"Hattie Zhou,Azade Nova,H. Larochelle,Aaron C. Courville,Behnam Neyshabur,Hanie Sedghi",6,70,3
"4c2534f9b03ac2f3810c07abc398a11bcf47258e","https://www.semanticscholar.org/paper/4c2534f9b03ac2f3810c07abc398a11bcf47258e",2,"Transformers Learn Shortcuts to Automata","The theoretical results completely characterize shortcut solutions, whereby a shallow Transformer with only o(T ) layers can exactly replicate the computation of an automaton on an input sequence of length T .","ArXiv",2022,"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang",4,104,0
"41f44979cf1cd3f4cbd615dc130bc33721f5281b","https://www.semanticscholar.org/paper/41f44979cf1cd3f4cbd615dc130bc33721f5281b",2,"MemPrompt: Memory-assisted Prompt Editing with User Feedback","It is shown how a (simulated) user can interac-tively teach a deployed GPT -3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT 3, a step towards the low-cost utility enhancement for very large pre-trained LMs.","",2022,"Aman Madaan,Niket Tandon,Peter Clark,Yiming Yang",1,41,0
"5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e","https://www.semanticscholar.org/paper/5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e",2,"On the Paradox of Learning to Reason from Data","This study pro-vides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has in fact learned statistical features that inherently exist in logical reasoning problems.","ArXiv",2022,"Honghua Zhang,Liunian Harold Li,Tao Meng,Kai-Wei Chang,Guy Van den Broeck",18,41,4
"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","https://www.semanticscholar.org/paper/1c1ca2392155ddf30408a442e6b504b5d60d4f2a",2,"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment","This paper presents a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule- Breaking – inspired by recent moral psychology studies and proposes a novel moral chain of thought prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments.","ArXiv",2022,"Zhijing Jin,Sydney Levine,Fernando Gonzalez,Ojasv Kamal,Maarten Sap,Mrinmaya Sachan,Rada Mihalcea,J. Tenenbaum,B. Schölkopf",1,78,0
"833a2f1817cb9aeb292620454889cae78e26dda4","https://www.semanticscholar.org/paper/833a2f1817cb9aeb292620454889cae78e26dda4",2,"Impact of Pretraining Term Frequencies on Few-Shot Reasoning","Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, the results raise the question of how much models actually generalize beyond pretraining data, and researchers are encouraged to take thepretraining data into account when interpreting evaluation results.","ArXiv",2022,"Yasaman Razeghi,Robert L Logan IV,Matt Gardner,Sameer Singh",53,54,4
"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","https://www.semanticscholar.org/paper/4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80",2,"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence","A novel intermediate training task, names meaning-matching, designed to directly learn a meaning-text correspondence, is proposed that enables PLMs to learn lexical semantic information and is found to be a safe intermediate task that guarantees a similar or better performance of downstream tasks.","NAACL-HLT",2022,"Myeongjun Jang,Frank Mtumbuka,Thomas Lukasiewicz",1,53,0
"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","https://www.semanticscholar.org/paper/0b7e9b6b588baa3f24fdde06feec26a067aa74bd",2,"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data","A new large-scale benchmark, MultiHiertt, with QA pairs over Multi Hierarchical Tabular and Textual data is constructed and a novel QA model termed MT2Net is introduced, which first applies facts retrieving to extract relevant supporting facts from both tables and text and then uses a reasoning module to perform symbolic reasoning over retrieved facts.","Annual Meeting of the Association for Computational Linguistics",2022,"Yilun Zhao,Yunxiang Li,Chenying Li,Rui Zhang",13,63,4
"20fae749e3d469c331731ffa2f811079db792fdc","https://www.semanticscholar.org/paper/20fae749e3d469c331731ffa2f811079db792fdc",2,"A Simple, Yet Effective Approach to Finding Biases in Code Generation","This work shows that current code generation systems exhibit biases inherited from large language model backbones, which might leak into generated code under specific circumstances, and proposes a framework that automatically removes hints and exposes various biases that these code generation models use.","ArXiv",2022,"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski",2,43,0
"6681f0a0cc6ddaa70cdea109b941c47538caaa27","https://www.semanticscholar.org/paper/6681f0a0cc6ddaa70cdea109b941c47538caaa27",2,"Judicial knowledge-enhanced magnitude-aware reasoning for numerical legal judgment prediction","","Artificial Intelligence and Law",2022,"Sheng Bi,Zhiyao Zhou,Lu Pan,G. Qi",1,27,0
"30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0","https://www.semanticscholar.org/paper/30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0",2,"MC-LSTM: Mass-Conserving LSTM","This work proposes a novel Mass-Conserving LSTM (MC-LSTM), which sets a new state-of-the-art for neural arithmetic units at learning arithmetic operations, such as addition tasks, which have a strong conservation law, as the sum is constant over time.","International Conference on Machine Learning",2021,"Pieter-Jan Hoedt,Frederik Kratzert,D. Klotz,Christina Halmich,Markus Holzleitner,G. Nearing,S. Hochreiter,G. Klambauer",13,108,4
"4578747d52aa1b3537612287352a803a7b17e999","https://www.semanticscholar.org/paper/4578747d52aa1b3537612287352a803a7b17e999",2,"Asynchronous Neural Networks for Learning in Graphs","It is proved that AMP can simulate synchronous GNNs and that it can theoretically distinguish any pair of graphs and experimentally validate AMP’s expressiveness.","ArXiv",2022,"Lukas Faber,Roger Wattenhofer",0,62,0
"0ce6a798f8222ed8f221326ca566311c648cb4dc","https://www.semanticscholar.org/paper/0ce6a798f8222ed8f221326ca566311c648cb4dc",2,"Learning Division with Neural Arithmetic Logic Modules","It is shown that robustly learning division in a systematic manner remains a challenge even at the simplest level of dividing two numbers, and two novel approaches for division are proposed which are called the Neural Reciprocal Unit (NRU) and the Neural Multiplicative Reciproc Unit (NMRU).","ArXiv",2021,"Bhumika Mistry,K. Farrahi,Jonathon Hare",0,13,0
"33149f835f391119d287cc2c6b009464e7d14fe4","https://www.semanticscholar.org/paper/33149f835f391119d287cc2c6b009464e7d14fe4",2,"On the Abilities of Mathematical Extrapolation with Implicit Models","This paper compares the robustness of implicitly-defined and classical deep learning models on a series of mathematical extrapolation tasks, where the models are tested with out-of-distribution samples during inference time to showcase implicit models’ unique advantages for mathematics extrapolation thanks to their flexible and selective framework.","",2022,"Juliette Decugis,Max Emerling,Ashwin Ganesh,Alicia Y. Tsai,L. Ghaoui",0,20,0
"26e07b6ac9b0f04396e2e7f7ca518b6fbae67ce7","https://www.semanticscholar.org/paper/26e07b6ac9b0f04396e2e7f7ca518b6fbae67ce7",2,"Improving the Robustness of Neural Multiplication Units with Reversible Stochasticity","It is shown that Neural Multiplication Units (NMUs) are unable to reliably learn tasks as simple as multiplying two inputs when given different training ranges, and stochasticity provides improved robustness with the potential to improve learned representations of upstream networks for numerical and image tasks.","ArXiv",2022,"Bhumika Mistry,K. Farrahi,Jonathon Hare",0,26,0
"eb2fc03b8865b8e1b4cb933d917ea269ebe14584","https://www.semanticscholar.org/paper/eb2fc03b8865b8e1b4cb933d917ea269ebe14584",2,"Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training","It is argued that mis-predictions can help locate such dominating patterns that harm language understanding and propose a new language pre-training method, MisPredictions as Harm Alerts (MPA), which expedites the pre- training of BERT and ELECTRA and improves their performances on downstream tasks.","",2020,"Chen Xing,Wenhao Liu,Caiming Xiong",0,41,0
"011869f932f89d047ce2bd36d73a95cc04888193","https://www.semanticscholar.org/paper/011869f932f89d047ce2bd36d73a95cc04888193",2,"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms","A new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations and shows that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks.","Conference on Empirical Methods in Natural Language Processing",2020,"Pei Zhou,Rahul Khanna,Bill Yuchen Lin,Daniel Ho,J. Pujara,Xiang Ren",23,55,1
"73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19","https://www.semanticscholar.org/paper/73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",2,"Measuring and Improving Consistency in Pretrained Language Models","The creation of PARAREL, a high-quality resource of cloze-style query English paraphrases, and analysis of the representational spaces of PLMs suggest that they have a poor structure and are currently not suitable for representing knowledge in a robust way.","Transactions of the Association for Computational Linguistics",2021,"Yanai Elazar,Nora Kassner,Shauli Ravfogel,Abhilasha Ravichander,E. Hovy,Hinrich Schütze,Yoav Goldberg",66,83,20
"008b9fc834f5839a25febe150f3076d550ee442f","https://www.semanticscholar.org/paper/008b9fc834f5839a25febe150f3076d550ee442f",2,"Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2","","ArXiv",2021,"Gregor Betz,Kyle Richardson,C. Voigt",11,58,2
"5aab57cc0530560d82c74c055f664280619d7e81","https://www.semanticscholar.org/paper/5aab57cc0530560d82c74c055f664280619d7e81",2,"PROST: Physical Reasoning about Objects through Space and Time","It is demonstrated that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer options are presented to them, they struggle when the superlative in a question is inverted, and increasing the amount of pretraining data and parameters only yields minimal improvements.","Findings",2021,"Stephane T Aroca-Ouellette,Cory Paik,A. Roncone,Katharina Kann",12,54,3
"d65a064eb837f838faf6ff67781b62450b92b159","https://www.semanticscholar.org/paper/d65a064eb837f838faf6ff67781b62450b92b159",2,"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification","This work proposes gamiﬁcation as a framework for data construction and creates CommonsenseQA 2.0, which includes 14,343 yes/no questions, and demonstrates its difﬂculty for models that are orders-of-magnitude larger than the AI used in the game itself.","NeurIPS Datasets and Benchmarks",2021,"Alon Talmor,Ori Yoran,Ronan Le Bras,Chandrasekhar Bhagavatula,Yoav Goldberg,Yejin Choi,Jonathan Berant",40,61,7
"e55391a9406245584b3e5b3225dad2e171b9a06b","https://www.semanticscholar.org/paper/e55391a9406245584b3e5b3225dad2e171b9a06b",2,"RuleBERT: Teaching Soft Rules to Pre-Trained Language Models","This work introduces a classification task where, given facts and soft rules, thePLM should return a prediction with a probability for a given hypothesis, and proposes a revised loss function that enables the PLM to learn how to predict precise probabilities for the task.","Conference on Empirical Methods in Natural Language Processing",2021,"Mohammed Saeed,N. Ahmadi,Preslav Nakov,Paolo Papotti",9,53,1
"2af476f7c2a7c040fc9ab7750bf41a84f66aa947","https://www.semanticscholar.org/paper/2af476f7c2a7c040fc9ab7750bf41a84f66aa947",2,"Knowledge Based Multilingual Language Model","This work presents a novel framework to pretrain knowledge based multilingual language models (KMLMs) by generating a large amount of code-switched synthetic sentences and reasoning-based multilingual training data using the Wikidata knowledge graphs and designing pretraining tasks to facilitate knowledge learning.","ArXiv",2021,"Linlin Liu,Xin Li,Ruidan He,Lidong Bing,Shafiq R. Joty,Luo Si",8,29,4
"f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884","https://www.semanticscholar.org/paper/f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884",2,"LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI","This work proposes an extensible framework to collectively yet categorically test diverse LOgical reasoning capabilities required for NLI (and by extension, NLU) and creates a semi-synthetic large test-bench that offers following utilities: individually test and analyze reasoning capabilities along 17 reasoning dimensions (including pragmatic reasoning).","ArXiv",2021,"Ishan Tarunesh,Somak Aditya,M. Choudhury",4,58,1
"06396c7cd5d223a1776abf8811359ec7bc05d420","https://www.semanticscholar.org/paper/06396c7cd5d223a1776abf8811359ec7bc05d420",2,"Knowledge-Augmented Methods for Natural Language Processing","This tutorial introduces the key steps in integrating knowledge into NLP, including knowledge grounding from text, knowledge representation and fusing, and introduces recent state-of-the-art applications in fusing knowledge into language understanding, language generation and commonsense reasoning.","Annual Meeting of the Association for Computational Linguistics",2022,"Chenguang Zhu,Yichong Xu,Xiang Ren,Bill Yuchen Lin,Meng Jiang,Wenhao Yu",4,83,0
"44772fe1c3fa422a3da7e25092db2544893d6bfb","https://www.semanticscholar.org/paper/44772fe1c3fa422a3da7e25092db2544893d6bfb",2,"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming","DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion and leads to improved logical reasoning of pre-trained LMs.","",2022,"Hanlin Zhang,Jian-Hui Huang,Ziyang Li,M. Naik,Eric Xing",3,43,1
"fed460648303afa32247e493847e4dc73dc1a5b3","https://www.semanticscholar.org/paper/fed460648303afa32247e493847e4dc73dc1a5b3",2,"Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?","To endow PLMs with incremental reasoning skills, this work proposes a set of inference strategies on relevant facts and distractors allowing us to build automatically generated training datasets, and empirically shows the effectiveness of this proposal on multiple-choice question answering and reading comprehension.","International Conference on Computational Linguistics",2022,"Jesús Lovón-Melgarejo,Jose G. Moreno,Romaric Besançon,Olivier Ferret,L. Tamine",0,36,0
"37bf0bf34603145246c3311df19e2afdf6e0270a","https://www.semanticscholar.org/paper/37bf0bf34603145246c3311df19e2afdf6e0270a",2,"JAKET: Joint Pre-training of Knowledge Graph and Language Understanding","A novel joint pre-training framework, JAKET, to model both the knowledge graph and language, which enables the pre-trained model to easily adapt to unseen knowledge graphs in new domains.","AAAI Conference on Artificial Intelligence",2020,"Donghan Yu,Chenguang Zhu,Yiming Yang,Michael Zeng",47,50,5
"a466d10b80dbdee3b130bef73ec62f3a89eb389b","https://www.semanticscholar.org/paper/a466d10b80dbdee3b130bef73ec62f3a89eb389b",2,"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples","This work explores methods to make better use of the multilingual annotation and language agnostic property of KG triples, and presents novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples.","Conference on Empirical Methods in Natural Language Processing",2021,"Linlin Liu,Xin Li,Ruidan He,Lidong Bing,Shafiq R. Joty,Luo Si",2,49,0
"0b483b550b21ec42d693fc04a372dbb10dd07019","https://www.semanticscholar.org/paper/0b483b550b21ec42d693fc04a372dbb10dd07019",2,"Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge","It is found generalization does not improve over the course of pre-training, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.","North American Chapter of the Association for Computational Linguistics",2021,"Ian Porada,Alessandro Sordoni,J. C. Cheung",1,47,0
"7bebb48d34c219b119ca2d4ffc97d7fd4940c35c","https://www.semanticscholar.org/paper/7bebb48d34c219b119ca2d4ffc97d7fd4940c35c",2,"On the data requirements of probing","A novel method to estimate the required number of data samples in such experiments is presented and, across several case studies, it is verified that the estimations have sufficient statistical power.","Findings",2022,"Zining Zhu,Jixuan Wang,Bai Li,F. Rudzicz",2,66,0
"50f7d69ddd7f9b34f5121607fbdcc57236d65b8c","https://www.semanticscholar.org/paper/50f7d69ddd7f9b34f5121607fbdcc57236d65b8c",2,"Can Pre-trained Language Models Interpret Similes as Smart as Human?","This paper investigates the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, and shows that PLMs can infer similes’ shared properties while still underperforming humans.","Annual Meeting of the Association for Computational Linguistics",2022,"Qi He,Sijie Cheng,Zhixu Li,Rui Xie,Yanghua Xiao",4,54,0
"2aba5bba16dac5cd62683bab9de5d6faaaed0de1","https://www.semanticscholar.org/paper/2aba5bba16dac5cd62683bab9de5d6faaaed0de1",2,"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach","An iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference tasks, and proposes an iterative context-aware prompter, which addresses limitations by learning to dynamically synthesize prompts conditioned on the current step’s contexts.","ArXiv",2022,"Boshi Wang,Xiang Deng,Huan Sun",4,44,2
"3f4d11971f2c64be9125a7fe99c019588bbebf16","https://www.semanticscholar.org/paper/3f4d11971f2c64be9125a7fe99c019588bbebf16",2,"Iteratively Prompt Pre-trained Language Models for Chain of Thought","An iterative prompting framework is explored, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference by learning to dynamically synthesize prompts conditioned on the current step’s contexts.","Conference on Empirical Methods in Natural Language Processing",2022,"Boshi Wang,Xiang Deng,Huan Sun",4,48,3
"706c6b3781374b0b11f98f204a4ddd05b26ed009","https://www.semanticscholar.org/paper/706c6b3781374b0b11f98f204a4ddd05b26ed009",2,"Knowledge Infused Decoding","Knowledge Infused Decoding (KID)—a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding, which maintains a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning.","International Conference on Learning Representations",2022,"Ruibo Liu,Guoqing Zheng,Shashank Gupta,Radhika Gaonkar,Chongyang Gao,Soroush Vosoughi,Milad Shokouhi,A. Awadallah",6,98,1
"cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","https://www.semanticscholar.org/paper/cd471b5ef162906ef3d9a84398b3f98e9ee4bf56",2,"A Review on Language Models as Knowledge Bases","This paper presents a set of aspects that it is deemed an LM should have to fully act as a KB, and reviews the recent literature with respect to those aspects.","ArXiv",2022,"Badr AlKhamissi,Millicent Li,Asli Celikyilmaz,Mona T. Diab,Marjan Ghazvininejad",15,167,5
"e3dae33c5bdf397abdefd971ea34c48fb836dcc0","https://www.semanticscholar.org/paper/e3dae33c5bdf397abdefd971ea34c48fb836dcc0",2,"GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models","Interestingly, it is found that larger multilingual PLMs variants do not necessarily store geo-diverse concepts better than its smaller variant; and a language may better probe knowledge about a non-native country than its native country.","Conference on Empirical Methods in Natural Language Processing",2022,"Da Yin,Hritik Bansal,Masoud Monajatipoor,Liunian Harold Li,Kai-Wei Chang",6,45,0
"2db6c10f135d5701ae7aec45986124ce264c1344","https://www.semanticscholar.org/paper/2db6c10f135d5701ae7aec45986124ce264c1344",2,"Learning Symbolic Rules for Reasoning in Quasi-Natural Language","This work proposes MetaQNL, a “Quasi-Natural” language that can express both formal logic and natural language sentences, and MetaInduce, a learning algorithm that inducesMetaQNL rules from training data consisting of questions and answers, with or without intermediate reasoning steps.","ArXiv",2021,"Kaiyu Yang,Jia Deng",5,74,0
"108c25905be36b2a7a0fc7256ac314985ecd9699","https://www.semanticscholar.org/paper/108c25905be36b2a7a0fc7256ac314985ecd9699",2,"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models","This work demonstrates that large language models can succeed in extrapolation without modifying their architecture or training procedure, and shows how generating step-by-step rationales and introducing marker tokens are both required for effective extrapolation.","MATHNLP",2022,"M. Bueno,Carlos Gemmel,Jeffrey Stephen Dalton,R. Lotufo,Rodrigo Nogueira",1,65,0
"30977afbd4501249e1a320bd2e48581197914ab8","https://www.semanticscholar.org/paper/30977afbd4501249e1a320bd2e48581197914ab8",2,"A Short Survey of Systematic Generalization","This survey includes systematic generalization and aory of how machine learning addresses it, and looks into sys- tematic generalization in language, vision, and VQA ﬁelds.","ArXiv",2022,"Yuanpeng Li",0,241,0
"3d2035edd4dd48e1e638279409e11bf689c461e1","https://www.semanticscholar.org/paper/3d2035edd4dd48e1e638279409e11bf689c461e1",2,"Temporal Reasoning in Natural Language Inference","Five new natural language inference (NLI) datasets focused on temporal reasoning are introduced and four existing datasets annotated for event duration and event ordering are recast into more than one million NLI examples.","Findings",2020,"Siddharth Vashishtha,Adam Poliak,Yash Kumar Lal,Benjamin Van Durme,Aaron Steven White",20,60,3
"d865c87f6ce4e0be29ae1e3780fae66b8034d04b","https://www.semanticscholar.org/paper/d865c87f6ce4e0be29ae1e3780fae66b8034d04b",2,"TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation","This paper examines the ability of large-scale pre-trained language models to distinguish commonsense from non-commonsense statements, and the utility of external resources that aim to supplement the world knowledge inherent in such language models, including commonsense knowledge graph embedding models, word concreteness ratings, and text-to-image generation models.","International Workshop on Semantic Evaluation",2020,"Don Teo",2,54,0
"9793b07ba09d9f2ac9cabd8117daa93bf3db4346","https://www.semanticscholar.org/paper/9793b07ba09d9f2ac9cabd8117daa93bf3db4346",2,"DEER: A Data Efficient Language Model for Event Temporal Reasoning","This work proposes DEER, a language model that is trained to focus on event temporal relations and performs better under low-resource settings than original LMs and uses a generator-discriminator structure to reinforce the LMs' capability of event temporal reasoning.","ArXiv",2020,"Rujun Han,Xiang Ren,Nanyun Peng",11,16,1
"e6d84cf9ae6efa10919bff765613e883a761db62","https://www.semanticscholar.org/paper/e6d84cf9ae6efa10919bff765613e883a761db62",2,"Open Temporal Relation Extraction for Question Answering","This paper decomposes each question into a question event and an open temporal relation (OTR) which is not pre-defined nor with timestamps, and ground the former in the context while sharing the representation of the latter across contexts.","Conference on Automated Knowledge Base Construction",2021,"Chao Shang,Peng Qi,Guangtao Wang,Jing Huang,Youzheng Wu,Bowen Zhou",2,25,0
"6bb369f874f49cd51415f216f1a3f635f2ca1eed","https://www.semanticscholar.org/paper/6bb369f874f49cd51415f216f1a3f635f2ca1eed",2,"ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations","","Conference on Empirical Methods in Natural Language Processing",2021,"Rujun Han,I-Hung Hsu,Jiao Sun,J. Baylón,Qiang Ning,D. Roth,Nanyun Peng",13,36,2
"30602e3382df3abedb5f225b55b7efce8580f74d","https://www.semanticscholar.org/paper/30602e3382df3abedb5f225b55b7efce8580f74d",2,"ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data","This work aims to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data, and introduces ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts.","Annual Meeting of the Association for Computational Linguistics",2020,"Woojeong Jin,Suji Kim,Rahul Khanna,Dong-Ho Lee,Fred Morstatter,A. Galstyan,Xiang Ren",12,59,1
"9d436d25981ea61db728bd490f0d54376d08953e","https://www.semanticscholar.org/paper/9d436d25981ea61db728bd490f0d54376d08953e",2,"Temporal Reasoning on Implicit Events from Distant Supervision","A neuro-symbolic temporal reasoning model, SymTime, is proposed, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times and generalizes to other temporal reasoning tasks.","North American Chapter of the Association for Computational Linguistics",2020,"Ben Zhou,Kyle Richardson,Qiang Ning,Tushar Khot,Ashish Sabharwal,D. Roth",36,55,6
"6eee69031d2e11aa03a5a8fcb219cff4562863be","https://www.semanticscholar.org/paper/6eee69031d2e11aa03a5a8fcb219cff4562863be",2,"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning","A continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations and design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts.","Conference on Empirical Methods in Natural Language Processing",2020,"Rujun Han,Xiang Ren,Nanyun Peng",12,38,3
"33b06c74eea3f400b6f5ef14ef163aef1db42d16","https://www.semanticscholar.org/paper/33b06c74eea3f400b6f5ef14ef163aef1db42d16",2,"Conditional Generation of Temporally-ordered Event Sequences","A single model is proposed that addresses both temporal ordering, sorting given events into the order they occurred, and event infilling, predicting new events which fit into an existing temporally-ordered sequence.","Annual Meeting of the Association for Computational Linguistics",2020,"Shih-Ting Lin,Nathanael Chambers,Greg Durrett",10,63,2
"d6c919ee0d51432496513ef4b6b2dbd128819779","https://www.semanticscholar.org/paper/d6c919ee0d51432496513ef4b6b2dbd128819779",2,"Microtask Detection","This article introduces the novel challenge of microtask detection, and it presents machine-learned models for automatically determining which tasks are actionable and which of these actionable tasks are microtasks, which have implications for the design of systems to help people make the most of their time.","ACM Trans. Inf. Syst.",2021,"Ryen W. White,E. Nouri,James Woffinden-Luey,Mark J. Encarnación,S. Jauhar",4,149,0
"c5943bc4b22a63f595cb1c2823a449e03aad4787","https://www.semanticscholar.org/paper/c5943bc4b22a63f595cb1c2823a449e03aad4787",2,"AR-LSAT: Investigating Analytical Reasoning of Text","This paper studies the challenge of analytical reasoning of text and introduces a new dataset consisting of questions from the Law School Admission Test from 1991 to 2016, and designs two different baselines which struggle to solve this task.","ArXiv",2021,"Wanjun Zhong,Siyuan Wang,Duyu Tang,Zenan Xu,Daya Guo,Jiahai Wang,Jian Yin,Ming Zhou,Nan Duan",8,43,0
"2ef4be35f8424ea768aa2e1b44392b3eddbc780b","https://www.semanticscholar.org/paper/2ef4be35f8424ea768aa2e1b44392b3eddbc780b",2,"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema","It is suggested that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning, and the observed progress is mostly due to the use of supervision in training WS models, which is not likely to successfully support all the required Commonsense reasoning skills and knowledge.","Conference on Empirical Methods in Natural Language Processing",2021,"Yanai Elazar,Hongming Zhang,Yoav Goldberg,D. Roth",27,74,2
"c7f977f556d2060238fdc1286d057d46958afaf9","https://www.semanticscholar.org/paper/c7f977f556d2060238fdc1286d057d46958afaf9",2,"ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning","ESTER, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning is introduced, which study five most commonly used event semantic relations and formulate them as questionanswering tasks.","ArXiv",2021,"Rujun Han,I-Hung Hsu,Jiao Sun,J. Baylón,Qiang Ning,D. Roth,Nanyun Pen",5,38,2
"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","https://www.semanticscholar.org/paper/7fa273f450251523e6b7fcc2eb3fdbdfd4a30493",2,"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP","This paper presents the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format, and reveals that the few- shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks.","Conference on Empirical Methods in Natural Language Processing",2021,"Qinyuan Ye,Bill Yuchen Lin,Xiang Ren",69,176,18
"174a0e6da0dfb7f96d4a0a4076eed154c439e41a","https://www.semanticscholar.org/paper/174a0e6da0dfb7f96d4a0a4076eed154c439e41a",2,"Probing Language Models for Understanding of Temporal Expressions","It is found that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.","BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",2021,"Shivin Thukral,Kunal Kukreja,Christian Kavouras",4,29,1
"d83c7aa12420d5d035f43d7737bfa6893e9e2c61","https://www.semanticscholar.org/paper/d83c7aa12420d5d035f43d7737bfa6893e9e2c61",2,"Exploring Universal Intrinsic Task Subspace via Prompt Tuning","Evidence is shown indicating that the adaptations of PLMs to various few-shot tasks can be reparameterized as optimizing only a few free parameters in a low-dimensional intrinsic task subspace, which may help to understand why PLMs could easily adapt to various NLP tasks with small-scale data.","",2021,"Yujia Qin,Xiaozhi Wang,Yusheng Su,Yankai Lin,Ning Ding,Jing Yi,Weize Chen,Zhiyuan Liu,Juanzi Li,Lei Hou,Peng Li,Maosong Sun,Jie Zhou",2,125,0
"b672d2ec81c9395c312d57a27931864d07664592","https://www.semanticscholar.org/paper/b672d2ec81c9395c312d57a27931864d07664592",2,"A Meta-framework for Spatiotemporal Quantity Extraction from Text","This paper formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it, which contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models.","Annual Meeting of the Association for Computational Linguistics",2022,"Qiang Ning,Ben Zhou,Hao Wu,Haoruo Peng,Chuchu Fan,Matt Gardner",0,50,0
"7db7fb25a8753c9efc6b3722d178f94fcc1f82d3","https://www.semanticscholar.org/paper/7db7fb25a8753c9efc6b3722d178f94fcc1f82d3",2,"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP","It is demonstrated that the proposed Knowledge Mixture enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences).","ArXiv",2022,"Yufei Wang,Jiayi Zheng,Can Xu,Xiubo Geng,Tao Shen,Chongyang Tao,Daxin Jiang",0,118,0
"82c6f5ffd4d2d43ae7684601f607eae26a759a5a","https://www.semanticscholar.org/paper/82c6f5ffd4d2d43ae7684601f607eae26a759a5a",2,"Analytical Reasoning of Text","This paper studies the challenge of analytical reasoning of text and collects a new dataset consisting of questions from the Law School Admission Test from 1991 to 2016, and presents an approach dubbed ARM, which outperforms pre-trained models significantly.","NAACL-HLT",2022,"Wanjun Zhong,Siyuan Wang,Duyu Tang,Zenan Xu,Daya Guo,Yining Chen,Jiahai Wang,Jian Yin,Ming Zhou,Nan Duan",0,55,0
"b8768fbba4d356a1c49dcd8e4cd33aea0c6472bc","https://www.semanticscholar.org/paper/b8768fbba4d356a1c49dcd8e4cd33aea0c6472bc",2,"How about Time? Probing a Multilingual Language Model for Temporal Relations","This paper presents a comprehensive set of probing experiments using a multilingual language model, XLM-R, for temporal relation classification between events in four languages, and results show an advantage of contextualized embeddings over static ones and a detrimen- tal role of sentence level embeddeddings.","International Conference on Computational Linguistics",2022,"Tommaso Caselli,Irene Dini,F. Dell’Orletta",0,39,0
"8b0b5b998d65add5c036b45c36afa3f9df96f4b0","https://www.semanticscholar.org/paper/8b0b5b998d65add5c036b45c36afa3f9df96f4b0",2,"Toward Building a Language Model for Understanding Temporal Commonsense","This paper focuses on the development of language models for temporal commonsense inference over several pre-trained language models, and relies on multi-step fine-tuning using multiple corpora, and masked language modeling to predict masked temporal indicators that are crucial for temporal Commonsense reasoning.","AACL",2022,"Mayuko Kimura,L. Pereira,Ichiro Kobayashi",0,24,0
"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","https://www.semanticscholar.org/paper/9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3",2,"20Q: Overlap-Free World Knowledge Benchmark for Language Models","20Q is introduced, a novel benchmark using the Twenty Questions game to evaluate world knowledge and common sense of language models and shows that in-context learning is inefficient for evaluating language models’ world knowledge — fine-tuning is necessary to show their true capabilities.","IEEE Games Entertainment Media Conference",2022,"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans",0,36,0
"cbdb45fc16b0885905b91d84281c310e6cb49e9c","https://www.semanticscholar.org/paper/cbdb45fc16b0885905b91d84281c310e6cb49e9c",2,"Cross-Task Generalization via Natural Language Crowdsourcing Instructions","This work introduces NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances, and adopts generative pre-trained language models to encode task-specific instructions along with input and generate task output.","Annual Meeting of the Association for Computational Linguistics",2021,"Swaroop Mishra,Daniel Khashabi,Chitta Baral,Hannaneh Hajishirzi",115,54,15
"ac8d33e4c0a45e227a47353f3f26fbb231482dc1","https://www.semanticscholar.org/paper/ac8d33e4c0a45e227a47353f3f26fbb231482dc1",2,"Time-Aware Language Models as Temporal Knowledge Bases","This work proposes a simple technique for jointly modeling text with its timestamp that improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods and shows that models trained with temporal context can be efficiently ""refreshed"" as new data arrives.","International Conference on Topology, Algebra and Categories in Logic",2021,"Bhuwan Dhingra,Jeremy R. Cole,Julian Martin Eisenschlos,D. Gillick,Jacob Eisenstein,William W. Cohen",64,54,13
"47df3fd32d00220c85c2c51a571254fd99b2ecc7","https://www.semanticscholar.org/paper/47df3fd32d00220c85c2c51a571254fd99b2ecc7",2,"MetaICL: Learning to Learn In Context","This work introduces MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks.","North American Chapter of the Association for Computational Linguistics",2021,"Sewon Min,M. Lewis,Luke Zettlemoyer,Hannaneh Hajishirzi",76,136,18
"c3a454e50ec0610f1380d55b1988a5eb5d45207b","https://www.semanticscholar.org/paper/c3a454e50ec0610f1380d55b1988a5eb5d45207b",2,"Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization","Four translation methods that can translate natural questions into cloze-style sentences to better solicit commonsense knowledge from language models are investigated, including a syntactic-based model, an unsupervised neural model, and two supervised neural models.","AAAI Conference on Artificial Intelligence",2022,"Zi-Yi Dou,Nanyun Peng",7,43,1
"7e5ca499cd9b932921bda84db98f75087d0b0683","https://www.semanticscholar.org/paper/7e5ca499cd9b932921bda84db98f75087d0b0683",2,"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey","A survey of commonsense knowledge acquisition and reasoning tasks, the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions are presented.","AAAI Conference on Artificial Intelligence",2022,"Prajjwal Bhargava,Vincent Ng",9,82,0
"078f4efd448822b0e25d3ee0aec842ced606a595","https://www.semanticscholar.org/paper/078f4efd448822b0e25d3ee0aec842ced606a595",2,"Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts","A Plan-and-Write framework enhanced by reinforcement learning to generate storylines and stories end-to-end using structured storylines to encode events and their pair-wise temporal relations as **temporal prompts** that guide how stories should unfold temporally.","North American Chapter of the Association for Computational Linguistics",2022,"Rujun Han,Hong Chen,Yufei Tian,Nanyun Peng",5,50,0
"e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e","https://www.semanticscholar.org/paper/e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e",2,"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP","The goal of KoMT is to condense diverse NLP task-speciﬁc knowledge into the single KnowDA model (i.e., all-in-one) such that KnowDA could utilize these knowledge to quickly grasp the inherent synthesis law of the target task through limited training instances.","",2022,"Yufei Wang,Jiayi Zheng,Can Xu,Xiubo Geng,Tao Shen,Chongyang Tao,Daxin Jiang",0,117,0
"98f19ca97512361b12475b42b67a617de14d33a1","https://www.semanticscholar.org/paper/98f19ca97512361b12475b42b67a617de14d33a1",2,"Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic","A novel neural-logic based Soft Logic Enhanced Event Temporal Reasoning (SLEER) model is proposed for acquiring unbiased TCS knowledge, in which the complementary relationship among dimensions are explicitly represented as logic rules and modeled by t-norm fuzzy logics.","AAAI Conference on Artificial Intelligence",2022,"Bibo Cai,Xiao Ding,Bowen Chen,L. Du,Ting Liu",0,36,0
"15bacb240e2598457af4ded3039b6988aa9706f0","https://www.semanticscholar.org/paper/15bacb240e2598457af4ded3039b6988aa9706f0",2,"Few-shot Adaptation Works with UnpredicTable Data","This work automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets - and finds that narrow subsets of the authors' dataset sometimes outperform more diverse datasets.","ArXiv",2022,"Jun Shern Chan,M. Pieler,Jonathan Jao,J. Scheurer,Ethan Perez",2,144,0
"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","https://www.semanticscholar.org/paper/9a3f1a51ab2b3816655e4c4f58a022421ea7b34b",2,"""John is 50 years old, can his son be 65?"" Evaluating NLP Models' Understanding of Feasibility","This work introduces FeasibilityQA, a question-answering dataset involving binary classification (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility, and shows that even state-of-the-art models such as GPT-3, G PT-2, and T5 struggle to answer the feasibility questions correctly.","ArXiv",2022,"Himanshu Gupta,Neeraj Varshney,Swaroop Mishra,Kuntal Kumar Pal,Saurabh Arjun Sawant,Kevin Scaria,Siddharth Goyal,Chitta Baral",1,44,0
"1d417bdd331912a458de920459f23fcc7f6e8699","https://www.semanticscholar.org/paper/1d417bdd331912a458de920459f23fcc7f6e8699",2,"Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts","It is shown that with such intermediate pre-training, developing robust decomposition-based models for a diverse range of tasks becomes more feasible, and the model, DecompT5, improves 20% to 30% on two datasets, Overnight and TORQUE, over the baseline language model.","Conference on Empirical Methods in Natural Language Processing",2022,"Ben Zhou,Kyle Richardson,Xiaodong Yu,D. Roth",3,38,0
"1eba9c79a7064a7e04bd387767f87b9f891472e2","https://www.semanticscholar.org/paper/1eba9c79a7064a7e04bd387767f87b9f891472e2",2,"Test of Time: Instilling Video-Language Models with a Sense of Time","This paper proposes a temporal adaptation recipe on top of one video-language model, VideoCLIP, based on post-pretraining on a small amount of video-text data, and conducts a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require a varying degree of time awareness.","ArXiv",2023,"Piyush Bagad,Makarand Tapaswi,Cees G. M. Snoek",0,127,0
"a7e23ffb6c692e54c1fc138a2690e06a56da14ee","https://www.semanticscholar.org/paper/a7e23ffb6c692e54c1fc138a2690e06a56da14ee",2,"tieval: An Evaluation Framework for Temporal Information Extraction Systems","T tieval, a Python library that provides a concise interface for importing different corpora and is equipped with domain-specific operations that facilitate system evaluation, is presented and its most relevant features are highlighted.","ArXiv",2023,"Hugo Sousa,A. Jorge,Ricardo Campos",0,35,0
"72c47862e2eee0e65bc25b6cd6baeb2a50ef4bc7","https://www.semanticscholar.org/paper/72c47862e2eee0e65bc25b6cd6baeb2a50ef4bc7",2,"tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation","This work identifies patterns across previous preprocessings, e.g. mapping of column names, and extraction of a specific sub-field from structured data in a column, and proposes a structured annotation framework that makes the authors' annotations fully exposed and not buried in unstructured code.","ArXiv",2023,"Damien Sileo",0,145,0
"9574d10f55e77d432f923ee71e3205fc64a3104a","https://www.semanticscholar.org/paper/9574d10f55e77d432f923ee71e3205fc64a3104a",2,"Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?","Meta prompt tuning is studied to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks and an in-depth analysis from the perspective of task similarity.","",2023,"Chengwei Qin,Shafiq R. Joty,Q. Li,Ruochen Zhao",0,161,0
"40a1f266bb5ca853837355bfff272a55f0049c81","https://www.semanticscholar.org/paper/40a1f266bb5ca853837355bfff272a55f0049c81",2,"Mining Numbers in Text: A Survey","A quick overview of the history and recent advances of the research of mining such relations between numerals and words found in text data is provided.","Information Systems",2021,"Minoru Yoshida,K. Kita",2,47,0
"320c1c6647a5b975c901347f71638c881888686b","https://www.semanticscholar.org/paper/320c1c6647a5b975c901347f71638c881888686b",2,"NQuAD: 70,000+ Questions for Machine Comprehension of the Numerals in Text","A Numeral-related Question Answering Dataset, NQuAD, for fine-grained numeracy, is presented, and several baselines for future works are proposed and it is shown that N QuAD is more challenging than the numeral- related questions in other datasets.","International Conference on Information and Knowledge Management",2021,"Chung-Chi Chen,Hen-Hsen Huang,Hsin-Hsi Chen",0,28,0
"251c3afbaafcc9b5178534be9109f644bfc5e912","https://www.semanticscholar.org/paper/251c3afbaafcc9b5178534be9109f644bfc5e912",2,"Enhancing Knowledge Bases with Quantity Facts","A recall-oriented approach to close the gap in knowledge-base coverage, based on iterative learning for extracting quantity facts, with two novel contributions to boost recall for KB augmentation without sacrificing the quality standards of the knowledge base.","The Web Conference",2022,"Vinh Thinh Ho,D. Stepanova,Dragan Milchevski,Jannik Strötgen,G. Weikum",2,48,0
"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","https://www.semanticscholar.org/paper/237f5ca6fcccef2b77a2212b34fb06a1dbd09b72",2,"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting","Collect and standardize prompts from a diverse range of tasks for use with tasks they were not designed for and evaluate these prompts across multiple choice datasets for a quantitative analysis of how certain attributes of a prompt affect performance.","ArXiv",2022,"Gabriel Orlanski",0,47,0
"75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1","https://www.semanticscholar.org/paper/75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1",2,"On the Computational Power of Transformers and Its Implications in Sequence Modeling","This paper provides an alternate and simpler proof to show that vanilla Transformers are Turing-complete and proves that Transformers with only positional masking and without any positional encoding are also Turing- complete.","Conference on Computational Natural Language Learning",2020,"S. Bhattamishra,Arkil Patel,Navin Goyal",19,41,4
"c2b4d96db34bd472e84c9234838cc4e808eb1ba9","https://www.semanticscholar.org/paper/c2b4d96db34bd472e84c9234838cc4e808eb1ba9",2,"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language","A new model architecture for learning multi-modal neuro-symbolic representations for video captioning using a dictionary learning-based method that incorporates modality-specific inductive biases for the captioning task is proposed.","ArXiv",2020,"Hassan Akbari,H. Palangi,Jianwei Yang,Sudha Rao,Asli Celikyilmaz,Roland Fernandez,P. Smolensky,Jianfeng Gao,Shih-Fu Chang",2,81,0
"ac4ae352e2434d4a71c6a79bf5f93df5f600b058","https://www.semanticscholar.org/paper/ac4ae352e2434d4a71c6a79bf5f93df5f600b058",2,"Increasing Learning Efficiency of Self-Attention Networks through Direct Position Interactions, Learnable Temperature, and Convoluted Attention","This work investigates three modifications to SANs: direct position interactions, learnable temperature, and convoluted attention that enable faster learning, i.e., higher accuracies after fewer update steps.","International Conference on Computational Linguistics",2020,"Philipp Dufter,Martin Schmitt,Hinrich Schütze",3,27,0
"89bc3b21c15c0656643918488ea25af939d0881a","https://www.semanticscholar.org/paper/89bc3b21c15c0656643918488ea25af939d0881a",2,"MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with Just One Transformer VAE","Experiments show that MuseMorphose outperforms recurrent neural network (RNN) based baselines on numerous widely-used metrics for style transfer tasks.","ArXiv",2021,"Shih-Lun Wu,Yi-Hsuan Yang",20,79,1
"976a609cf540d1ded373b872d34779f7164d840a","https://www.semanticscholar.org/paper/976a609cf540d1ded373b872d34779f7164d840a",2,"Rethinking the Design Principles of Robust Vision Transformer","RVT is a new vision transformer, which has superior performance and strong robustness, and two new plug-and-play techniques called position-aware attention rescaling and patch-wise augmentation to train the authors' RVT are proposed.","",2021,"Xiaofeng Mao,Gege Qi,Yuefeng Chen,Xiaodan Li,Shaokai Ye,Yuan He,Hui Xue",17,39,0
"52d01d9f71caf0d021fccb75b75b7d3dfc7460f5","https://www.semanticscholar.org/paper/52d01d9f71caf0d021fccb75b75b7d3dfc7460f5",2,"On Scalar Embedding of Relative Positions in Attention Models","This work shows that SRPE in attention has an elegant probabilistic interpretation and proposes a new SRPE (AT5) that adopts a learnable bucketization protocol and automatically adapts to the dependency range specific to the learning task.","AAAI Conference on Artificial Intelligence",2021,"Junshuang Wu,Richong Zhang,Yongyi Mao,Junfan Chen",2,21,0
"a28861bdef17182df3b64fc99e9359e2e23e7672","https://www.semanticscholar.org/paper/a28861bdef17182df3b64fc99e9359e2e23e7672",2,"Transformer with Syntactic Position Encoding for Machine Translation","This work proposes global positional encoding for dependency tree, a new scheme that facilitates syntactic relation modeling between any two words with keeping exactness and without immediate neighbor constraint, which is more effective than existing approaches.","Recent Advances in Natural Language Processing",2021,"Yikuan Xie,Wenyong Wang,Mingqian Du,Qing He",0,28,0
"8babeaffc8413747412765803a50d2c3adbbbbdd","https://www.semanticscholar.org/paper/8babeaffc8413747412765803a50d2c3adbbbbdd",2,"DEBERTA: DECODING-ENHANCED BERT","A new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) is proposed that improves the BERT and RoBERTa models using two novel techniques that significantly improve the efficiency of model pre-training and the performance of both natural language understand (NLU) and natural langauge generation (NLG) downstream tasks.","",2021,"Pengcheng He,Xiaodong Liu,Jianfeng Gao,Weizhu Chen",0,61,0
"8d6b1929b92211ad0eb3e14b2c2b41789ccf053a","https://www.semanticscholar.org/paper/8d6b1929b92211ad0eb3e14b2c2b41789ccf053a",2,"Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models","This paper investigates the potential problems in Shaw-R PE and XL-RPE, and proposes two novel RPEs called Low-level Fine-grained High-level Coarse- grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) R PE.","Conference on Empirical Methods in Natural Language Processing",2021,"Anlin Qu,Jianwei Niu,Shasha Mo",3,33,0
"2a405483796dfedf5d95483aa8880c57626e0e9f","https://www.semanticscholar.org/paper/2a405483796dfedf5d95483aa8880c57626e0e9f",2,"Integrating Tree Path in Transformer for Code Representation","This paper investigates two representative path encoding methods shown in previous research work and integrates them into the attention module of Transformer, leading to the novel state-of-theart representation model TPTrans, which finally outperforms strong baselines.","Neural Information Processing Systems",2021,"Han Peng,Ge Li,Wenhan Wang,Yunfei Zhao,Zhi Jin",15,34,1
"c687acfeaafb86ab045f9fcbb6e2ff691827a0c2","https://www.semanticscholar.org/paper/c687acfeaafb86ab045f9fcbb6e2ff691827a0c2",2,"On Position Embeddings in BERT O N P OSITION E MBEDDINGS IN BERT","","",2021,"Jakob Grue",0,35,0
"05f5f8b2065a520846d89771ebaea2bb1534e9c6","https://www.semanticscholar.org/paper/05f5f8b2065a520846d89771ebaea2bb1534e9c6",2,"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","A new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) is proposed that improves the BERT and RoBERTa models using two novel techniques that significantly improve the efficiency of model pre-training and performance of downstream tasks.","International Conference on Learning Representations",2020,"Pengcheng He,Xiaodong Liu,Jianfeng Gao,Weizhu Chen",722,64,183
"787119e3c3f819244c82b7d97779473773e60696","https://www.semanticscholar.org/paper/787119e3c3f819244c82b7d97779473773e60696",2,"MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers","MaX-DeepLab, the first end-to-end model for panoptic segmentation, is presented, and shows a significant 7.1% PQ gain in the box-free regime on the challenging COCO dataset, closing the gap between box-based and box- free methods for the first time.","Computer Vision and Pattern Recognition",2020,"Huiyu Wang,Yukun Zhu,Hartwig Adam,A. Yuille,Liang-Chieh Chen",247,112,17
"0822f8d7e6a72a65e65f147d3a8d8fccd485da40","https://www.semanticscholar.org/paper/0822f8d7e6a72a65e65f147d3a8d8fccd485da40",2,"Shortformer: Better Language Modeling using Shorter Inputs","This work identifies conditions where shorter inputs are not harmful, and achieves perplexity and efficiency improvements through two new methods that decrease input length, and shows how to improve the efficiency of recurrence methods in transformers.","Annual Meeting of the Association for Computational Linguistics",2021,"Ofir Press,Noah A. Smith,M. Lewis",38,24,7
"f5a2ce064ea0efc3d7f26acd91041824c3254cd8","https://www.semanticscholar.org/paper/f5a2ce064ea0efc3d7f26acd91041824c3254cd8",2,"NRTSI: Non-Recurrent Time Series Imputation for Irregularly-sampled Data","This work views the imputation task from the perspective of permutation equivariant modeling of sets and proposes a novel imputation model called NRTSI without any recurrent modules, which achieves state-of-the-art performance across a wide range of commonly used time series imputation benchmarks.","ArXiv",2021,"Siyuan Shan,Junier B. Oliva",1,44,0
"594db112d65891fbaf45b27f17d2f9de88ddcd82","https://www.semanticscholar.org/paper/594db112d65891fbaf45b27f17d2f9de88ddcd82",2,"Revisiting Language Encoding in Learning Multilingual Representations","This paper revisits the use of language embedding and identifies several problems in the existing formulations and proposes a new approach called Cross-lingual Language Projection (XLP), which achieves the purpose of appropriately encoding “language” in a multilingual Transformer model.","ArXiv",2021,"Shengjie Luo,Kaiyuan Gao,Shuxin Zheng,Guolin Ke,Di He,Liwei Wang,Tie-Yan Liu",2,39,0
"eaa88d697f92739f3569564329e9d037aabbe2d7","https://www.semanticscholar.org/paper/eaa88d697f92739f3569564329e9d037aabbe2d7",2,"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics","Models show a gap toward human-level generalization when tested with new concepts in a few-shot setting, and the results suggest that current models still struggle in extrapolation to long-range syntactic dependency and semantics.","",2021,"Qing Li,Siyuan Huang,Yining Hong,Yixin Zhu,Y. Wu,Song-Chun Zhu",1,105,1
"a1ad15d2333cf9d9b55bbc97a3aacd244a8b9fdf","https://www.semanticscholar.org/paper/a1ad15d2333cf9d9b55bbc97a3aacd244a8b9fdf",2,"Demystifying the Better Performance of Position Encoding Variants for Transformer","This work demonstrates a simple yet effective way to encode position and segment into the Transformer models and performs on par with SOTA on GLUE, XTREME and WMT benchmarks while saving computation costs.","ArXiv",2021,"Pu-Chin Chen,Henry Tsai,Srinadh Bhojanapalli,Hyung Won Chung,Yin-Wen Chang,Chun-Sung Ferng",6,22,1
"db46b0de44c5113c47f0ec5392eb91d0726497bf","https://www.semanticscholar.org/paper/db46b0de44c5113c47f0ec5392eb91d0726497bf",2,"A Simple and Effective Positional Encoding for Transformers","This work introduces Decoupled Positional Attention for Transformers (DIET), a simple yet effective mechanism to encode position and segment information into the Transformer models that has faster training and inference time, while achieving competitive performance on GLUE, XTREME and WMT benchmarks.","Conference on Empirical Methods in Natural Language Processing",2021,"Pu-Chin Chen,Henry Tsai,Srinadh Bhojanapalli,Hyung Won Chung,Yin-Wen Chang,Chun-Sung Ferng",10,22,1
"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","https://www.semanticscholar.org/paper/66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",2,"RoFormer: Enhanced Transformer with Rotary Position Embedding","This paper investigates various methods to integrate positional information into the learning process of transformer-based language models and proposes a novel method named Rotary Position Embedding (RoPE), which encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation.","ArXiv",2021,"Jianlin Su,Yu Lu,Shengfeng Pan,Bo Wen,Yunfeng Liu",134,45,23
"fda805c6e85a03d10549acdc5489420ca8f3d405","https://www.semanticscholar.org/paper/fda805c6e85a03d10549acdc5489420ca8f3d405",2,"MuseMorphose: Full-Song and Fine-Grained Piano Music Style Transfer with One Transformer VAE","Experiments show that MuseMorphose outperforms recurrent neural network based baselines on numerous widely-used metrics for style transfer tasks, and is interested in bringing the two together to construct a single model that exhibits both strengths.","",2021,"Shih-Lun Wu,Yi-Hsuan Yang",0,68,0
"08ffdec40291a2ccb5f8a6cc048b01247fb34b96","https://www.semanticscholar.org/paper/08ffdec40291a2ccb5f8a6cc048b01247fb34b96",2,"Relative Positional Encoding for Transformers with Linear Complexity","Stochastic Positional Encoding is presented as a way to generate PE that can be used as a replace-ment to the classical additive (sinusoidal) PE and provably behaves like RPE.","International Conference on Machine Learning",2021,"A. Liutkus,Ondřej Cífka,Shih-Lun Wu,Umut Simsekli,Yi-Hsuan Yang,Gaël Richard",21,70,4
"d8e7bad2681ce70277c900c77a22181d4b03d705","https://www.semanticscholar.org/paper/d8e7bad2681ce70277c900c77a22181d4b03d705",2,"The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models","Analysis of position embeddings of existing language models finds strong evidence of translation invariance, which leads to translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embedDings.","Annual Meeting of the Association for Computational Linguistics",2021,"Ulme Wennberg,G. Henter",8,31,1
"7509c66a666e2e3f14bc8676b969b945ee6e136f","https://www.semanticscholar.org/paper/7509c66a666e2e3f14bc8676b969b945ee6e136f",2,"CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings","This paper proposes an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute and relative positions and leads to better generalization performance as well as increased stability with respect to training hyper-parameters.","Neural Information Processing Systems",2021,"T. Likhomanenko,Qiantong Xu,Ronan Collobert,Gabriel Synnaeve,A. Rogozhnikov",16,56,3
"47ae807cd511b35e78a2cd4e198283dea6dafd41","https://www.semanticscholar.org/paper/47ae807cd511b35e78a2cd4e198283dea6dafd41",2,"Do Transformers Really Perform Bad for Graph Representation?","This paper presents Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge.","Neural Information Processing Systems",2021,"Chengxuan Ying,Tianle Cai,Shengjie Luo,Shuxin Zheng,Guolin Ke,Di He,Yanming Shen,Tie-Yan Liu",273,63,70
"1dbb523a6555d6e0c5727620e2b57daaa5b79dc0","https://www.semanticscholar.org/paper/1dbb523a6555d6e0c5727620e2b57daaa5b79dc0",2,"Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models","Composite attention is proposed, which unites previous relative position encoding methods under a convolutional framework, and finds that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings.","Annual Meeting of the Association for Computational Linguistics",2021,"Tyler A. Chang,Yifan Xu,Weijian Xu,Z. Tu",5,32,1
"8835d7e27472e6041b640c869ce6b94021f8c851","https://www.semanticscholar.org/paper/8835d7e27472e6041b640c869ce6b94021f8c851",2,"Do Large Scale Molecular Language Representations Capture Important Structural Information?","Experiments show that the learned molecular representation, MOLFORMER, performs competitively, when compared to existing graph-based and fingerprint-based supervised learning baselines, on the challenging tasks of predicting properties of QM8 and QM9 molecules.","ArXiv",2021,"Jerret Ross,Brian M. Belgodere,Vijil Chenthamarakshan,Inkit Padhi,Youssef Mroueh,Payel Das",11,73,2
"0d508600d77d8a7e6a655cdb6d139779732f649f","https://www.semanticscholar.org/paper/0d508600d77d8a7e6a655cdb6d139779732f649f",2,"Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding","A novel way to accelerate attention calculation for Transformers with RPE on top of the kernelized attention using Fast Fourier Transform (FFT), and it is demonstrated that properly using relative positional encoding can mitigate the training instability problem of vanilla Kernelized attention.","Neural Information Processing Systems",2021,"Shengjie Luo,Shanda Li,Tianle Cai,Di He,Dinglan Peng,Shuxin Zheng,Guolin Ke,Liwei Wang,Tie-Yan Liu",23,53,2
"c6dad3f9c9f29602b8c89585c23c73377ef00601","https://www.semanticscholar.org/paper/c6dad3f9c9f29602b8c89585c23c73377ef00601",2,"Bootstrap Representation Learning for Segmentation on Medical Volumes and Sequences","This work proposed an asymmetric network with an attention-guided predictor to enforce distance-specific prediction and supervision on slices within and across volumes/sequences and introduced a novel prototype-based foreground-background calibration module to enhance representation consistency.","ArXiv",2021,"Zejian Chen,Wei Zhuo,Tianfu Wang,Wufeng Xue,Dong Ni",2,59,2
"fbac64d617914ab8dac0682639fbd6012faed771","https://www.semanticscholar.org/paper/fbac64d617914ab8dac0682639fbd6012faed771",2,"Rethinking Positional Encoding","It is shown that alternative non-Fourier embedding functions can indeed be used for positional encoding and that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates.","ArXiv",2021,"Jianqiao Zheng,Sameera Ramasinghe,S. Lucey",23,38,0
"d3f51870f4da5dd9c2a08a55cfa8a380b8d49208","https://www.semanticscholar.org/paper/d3f51870f4da5dd9c2a08a55cfa8a380b8d49208",2,"Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation","This work introduces the low-rank decomposed self-attention, which projects user's historical items into a small constant number of latent interests and leverages item-to-interest interaction to generate the context-aware representation and designs the decoupled position encoding, which models the sequential relations between items more precisely.","Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",2021,"Xinyan Fan,Zheng Liu,Jianxun Lian,Wayne Xin Zhao,Xing Xie,Ji-Rong Wen",21,18,3
"79678d2f10bddf14b2aedf3427f8a4c39908931f","https://www.semanticscholar.org/paper/79678d2f10bddf14b2aedf3427f8a4c39908931f",2,"Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding","A new self-attention based model for music score infilling, i.e., to generate a polyphonic music sequence that fills in the gap between given past and future contexts, that can infill a variable number of notes for different time spans is proposed.","International Society for Music Information Retrieval Conference",2021,"Chin-Jui Chang,Chun-Yi Lee,Yi-Hsuan Yang",7,44,0
"ed535e93d5b5a8b689e861e9c6083a806d1535c2","https://www.semanticscholar.org/paper/ed535e93d5b5a8b689e861e9c6083a806d1535c2",2,"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers","By revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, this work can drastically improve the performance of Transformers on systematic generalization.","Conference on Empirical Methods in Natural Language Processing",2021,"R. Csordás,Kazuki Irie,J. Schmidhuber",55,55,11
"51b5db5c679be0ce9a39a2ee21def42bca165efe","https://www.semanticscholar.org/paper/51b5db5c679be0ce9a39a2ee21def42bca165efe",2,"Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning","An alternative self-attention architecture, Shatter, that more efficiently encodes sequence information by softly partitioning the space of relative positions and applying different value matrices to different parts of the sequence, making the cost of pretraining much more affordable.","ArXiv",2021,"Ran Tian,Joshua Maynez,Ankur P. Parikh",2,42,0
"aec7e7143bfe082752f428fe01e4090dd7fc411c","https://www.semanticscholar.org/paper/aec7e7143bfe082752f428fe01e4090dd7fc411c",2,"Ultra-high Resolution Image Segmentation via Locality-aware Context Fusion and Alternating Local Enhancement","This paper introduces a novel locality-aware context fusion based segmentation model to process local patches, where the relevance between local patch and its various contexts are jointly and complementarily utilized to handle the semantic regions with large variations.","",2021,"Wenxi Liu,Qi Li,Xin Lin,Weixiang Yang,Shengfeng He,Yuanlong Yu",0,73,0
"33c831326bb47b2ba2031fd7213b6918d23eb01e","https://www.semanticscholar.org/paper/33c831326bb47b2ba2031fd7213b6918d23eb01e",2,"The Impact of Positional Encodings on Multilingual Compression","While sinusoidal positional encodings were designed for monolingual applications, they are particularly useful in multilingual language models, because they were explicitly designed to facilitate compositionality by allowing linear projections over arbitrary time steps.","Conference on Empirical Methods in Natural Language Processing",2021,"Vinit Ravishankar,Anders Søgaard",3,20,0
"232acef2483a26fe95c70b619f88fa0b82c1a105","https://www.semanticscholar.org/paper/232acef2483a26fe95c70b619f88fa0b82c1a105",2,"Multiplicative Position-aware Transformer Models for Language Understanding","It is shown that the proposed embedding method, which served as a drop-in replacement of the default absolute position embedding, can improve the RoberTa-base and RoBERTa-large models on SQuAD1.1 and SQuad2.0 datasets.","ArXiv",2021,"Zhiheng Huang,Davis Liang,Peng Xu,Bing Xiang",0,23,0
"87e879c2465b2414a58dd3a1184f8b346d48f3e7","https://www.semanticscholar.org/paper/87e879c2465b2414a58dd3a1184f8b346d48f3e7",2,"Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer","This paper presents a novel Dual-Aspect Collaborative Transformer to learn embeddings for the node and positional features separately, instead of fusing them together as done in existing ones, so as to avoid potential noises and incompatible correlations.","Neural Information Processing Systems",2021,"Yining Ma,Jingwen Li,Zhiguang Cao,Wen Song,Le Zhang,Zhenghua Chen,Jing Tang",28,39,5
"c501d6f1eecf3b0fdae7d428e1829bb6607a6a37","https://www.semanticscholar.org/paper/c501d6f1eecf3b0fdae7d428e1829bb6607a6a37",2,"Relative Molecule Self-Attention Transformer","Relative Molecule Attention Transformer (R-MAT) is identified: a novel Transformer-based model based on the developed self-attention layer that achieves state-of-the-art or very competitive results across a wide range of molecule property prediction tasks.","ArXiv",2021,"Lukasz Maziarka,Dawid Majchrowski,Tomasz Danel,Piotr Gainski,J. Tabor,I. Podolak,Pawel M. Morkisz,Stanislaw Jastrzebski",8,55,2
"91a4cbae6553e975ddc3b2f6850ed725ff475307","https://www.semanticscholar.org/paper/91a4cbae6553e975ddc3b2f6850ed725ff475307",2,"SwinTrack: A Simple and Strong Baseline for Transformer Tracking","This paper proposes a simple yet efﬁcient fully-attentional tracker, dubbed SwinTrack, within classic Siamese framework that leverages the Transformer architecture, enabling better feature interactions for tracking than pure CNN or hybrid CNN-Transformer frameworks.","ArXiv",2021,"Liting Lin,Heng Fan,Yong Xu,Haibin Ling",26,63,7
"9fe29c834afbe1848d9df713ae6e0ca3bd053605","https://www.semanticscholar.org/paper/9fe29c834afbe1848d9df713ae6e0ca3bd053605",2,"Probing the Role of Positional Information in Vision-Language Models","A case study of LXMERT, a state-of-the-art VL model, which probes the use of the PI in the representation and study its effect on Visual Question Answering, it is shown that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs.","NAACL-HLT",2022,"Philipp J. Rösch,Jindřich Libovický",2,44,0
"08013183713f427258d36332b1bee9d6ce37cf30","https://www.semanticscholar.org/paper/08013183713f427258d36332b1bee9d6ce37cf30",2,"S TRUCTURE -A WARE T RANSFORMER P OLICY FOR I NHOMOGENEOUS M ULTI -T ASK R EINFORCEMENT L EARNING","It is empirically show that the morphological information is crucial for modular reinforcement learning, substantially outperforming prior state-of-the-art methods on multi-task learning as well as transfer learning settings with different state and action space dimensions.","",2022,"Sunghoon Hong,Deunsol Yoon,Kee-Eung Kim",0,34,0
"26ce5dd8221efc307fceef111e003bdb0ffe8ed5","https://www.semanticscholar.org/paper/26ce5dd8221efc307fceef111e003bdb0ffe8ed5",2,"Positional Attention Guided Transformer-like Architecture for Visual Question Answering","A novel positional attention guided Transformer-like architecture which can adaptively extracts positional information within and across the visual and language modalities, and use this information to guide high-level interactions in inter- and intra- modality information flows is proposed.","IEEE transactions on multimedia",2022,"Aihua Mao,Zhi Yang,Ken Lin,Jun-ying Xuan,Yong-Jin Liu",0,51,0
"6aded7d6f93fd93c5632b627b6520830a3999ad4","https://www.semanticscholar.org/paper/6aded7d6f93fd93c5632b627b6520830a3999ad4",2,"Molformer: Large Scale Chemical Language Representations Capture Molecular Structure and Properties","Experiments show that utilizing the learned molecular representation outperforms existing baselines on downstream tasks, including supervised and self-supervised graph neural net baselines and language models, on several classiﬁcation and regression tasks from ten benchmark datasets while performing competitively on two others.","",2022,"Jerret Ross,Brian M. Belgodere,V. Chenthamarakshan,Inkit Padhi,Youssef Mroueh,Payel Das",3,69,1
"2a98fd84fd7a1eadbeb4513ad322d13286ecb9ac","https://www.semanticscholar.org/paper/2a98fd84fd7a1eadbeb4513ad322d13286ecb9ac",2,"Graph Hawkes Transformer for Extrapolated Reasoning on Temporal Knowledge Graphs","A Graph Hawkes Transformer (GHT) is proposed, which combines two variants of Transformer, which capture the instantaneous structural information and temporal evolution information, respectively, and a new relational continuous-time encoding function to facilitate feature evolution with the Hawkes process.","Conference on Empirical Methods in Natural Language Processing",2022,"Haohai Sun,Shangyi Geng,Jialu Zhong,Han Hu,Kun He",0,40,0
"20333c34f892c8e0c2f4e6c37295a8b43ef35c02","https://www.semanticscholar.org/paper/20333c34f892c8e0c2f4e6c37295a8b43ef35c02",2,"Rethinking Positional Encoding in Tree Transformer for Code Representation","This work proposes a novel tree Transformer encoding node positions based on a new description method for tree structures that outperforms strong baselines on code summarization and completion tasks across two languages, demonstrating the model’s effectiveness.","Conference on Empirical Methods in Natural Language Processing",2022,"Han Peng,Ge Li,Yunfei Zhao,Zhi Jin",0,44,0
"7072db6eddb85ecd2c117365d91bd694760f726e","https://www.semanticscholar.org/paper/7072db6eddb85ecd2c117365d91bd694760f726e",2,"Position Information in Transformers: An Overview","An overview and theoretical comparison of existing methods to incorporate position information into Transformer models is provided and what characteristics of an application should be taken into account when selecting a position encoding is indicated.","Computational Linguistics",2021,"Philipp Dufter,Martin Schmitt,Hinrich Schütze",35,67,2
"b8cee43a51c44f8f4448e78e41ecf081987707cf","https://www.semanticscholar.org/paper/b8cee43a51c44f8f4448e78e41ecf081987707cf",2,"Towards Robust Vision Transformer","Robust Vision Transformer (RVT) is proposed, which is a new vision transformer and has superior performance with strong robustness and generalization ability compared with previous ViTs and state-of-the-art CNNs.","Computer Vision and Pattern Recognition",2021,"Xiaofeng Mao,Gege Qi,Yuefeng Chen,Xiaodan Li,Ranjie Duan,Shaokai Ye,Yuan He,Hui Xue",58,53,7
"d8d2e574965fe733eb1416e03df2b5c2914fc530","https://www.semanticscholar.org/paper/d8d2e574965fe733eb1416e03df2b5c2914fc530",2,"A Survey of Transformers","This survey provides a comprehensive review of various Transformer variants and proposes a new taxonomy of X-formers from three perspectives: architectural modification, pre-training, and applications.","AI Open",2021,"Tianyang Lin,Yuxin Wang,Xiangyang Liu,Xipeng Qiu",172,171,5
"df59d0098c1b2c1ee8995da802dd6b12d158c2b8","https://www.semanticscholar.org/paper/df59d0098c1b2c1ee8995da802dd6b12d158c2b8",2,"Large-scale chemical language representations capture molecular structure and properties","Encouraging evidence is provided that large-scale molecular language models can capture sufficient chemical and structural information to predict various distinct molecular properties, including quantum-chemical properties.","Nature Machine Intelligence",2021,"Jerret Ross,Brian M. Belgodere,V. Chenthamarakshan,Inkit Padhi,Youssef Mroueh,Payel Das",1,75,0
"87e823d2cb58e741230c0fa3b83f3459c7e32241","https://www.semanticscholar.org/paper/87e823d2cb58e741230c0fa3b83f3459c7e32241",2,"PiSLTRc: Position-Informed Sign Language Transformer With Content-Aware Convolution","This work proposes a new model architecture, namely PiSLTRc, with two distinctive characteristics: content-aware and position-aware convolution layers, thus generating robust neighborhood-enhanced sign representation and achieves state-of-the-art performance on translation quality with $+1.6$ BLEU improvements.","IEEE transactions on multimedia",2021,"Pan Xie,Mengyi Zhao,Xiaohui Hu",5,52,0
"9ca329408813d209b1dcb36936f7f9cba82506bd","https://www.semanticscholar.org/paper/9ca329408813d209b1dcb36936f7f9cba82506bd",2,"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation","This work shows that extrapolation can be enabled by simply changing the position representation method, though it finds that current methods do not allow for efficient extrapolation, and introduces a simpler and more efficient position method, Attention with Linear Biases (ALiBi).","International Conference on Learning Representations",2021,"Ofir Press,Noah A. Smith,M. Lewis",67,48,14
"155f8b1907f0ca6bdc3f6a7c7e12ae666cf5d944","https://www.semanticscholar.org/paper/155f8b1907f0ca6bdc3f6a7c7e12ae666cf5d944",2,"Transformers in Time Series: A Survey","This paper systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations, and is the first work to comprehensively and systematically summarize the recent advances of Transformers for modeling time series data.","ArXiv",2022,"Qingsong Wen,Tian Zhou,Chao Zhang,Weiqiu Chen,Ziqing Ma,Junchi Yan,Liang Sun",34,89,4
"50af83ea20201b51014358534650213e6133650c","https://www.semanticscholar.org/paper/50af83ea20201b51014358534650213e6133650c",2,"FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks","Improved Linear Transformer with Shift-Invariant Kernel Function SIKF and FastRPB1 which stands for Fast Relative Positional Bias, which efficiently adds positional information to self-attention using Fast Fourier Transformation, which significantly outperforms another efficient positional encodings method in accuracy.","ArXiv",2022,"Maksim Zubkov,Daniil Gavrilov",0,23,0
"4e9cd6be4a8fcad2ca562fcf41a1f882387a3167","https://www.semanticscholar.org/paper/4e9cd6be4a8fcad2ca562fcf41a1f882387a3167",2,"LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network","This work proposes an efficient network, LGT-Net, for room layout estimation, which contains a novel Transformer architecture called SWG-Transformer to model geometry relations, and designs a novel relative position embedding of Transformer to enhance the spatial identification ability for the panorama.","Computer Vision and Pattern Recognition",2022,"Zhigang Jiang,Zhongzheng Xiang,Jinhua Xu,Mingbi Zhao",2,34,1
"838a2297b94f7bad96c4f8370a5f58487f194f44","https://www.semanticscholar.org/paper/838a2297b94f7bad96c4f8370a5f58487f194f44",2,"Visual Abductive Reasoning","A new task and dataset, Visual Abductive Reasoning (VAR), for examining abductive reasoning ability of machine intelligence in everyday visual situations, and devise a strong baseline model, REASONER (causal-and-cascaded reasoning Transformer), which surpasses many famous video-language models, while still being far behind human performance.","Computer Vision and Pattern Recognition",2022,"Chen Liang,Wenguan Wang,Tianfei Zhou,Yi Yang",5,85,0
"2e03ee15c00561a891eaf197515da720d1566411","https://www.semanticscholar.org/paper/2e03ee15c00561a891eaf197515da720d1566411",2,"Causal Transformer for Estimating Counterfactual Outcomes","A novel Causal Transformer for estimating counterfactual outcomes over time that aims to learn adversarial balanced representations, so that they are predictive of the next outcome but non-predictive of the current treatment assignment.","International Conference on Machine Learning",2022,"Valentyn Melnychuk,Dennis Frauen,S. Feuerriegel",12,61,1
"633851a323990270461b49e8068a2c9ca1cea530","https://www.semanticscholar.org/paper/633851a323990270461b49e8068a2c9ca1cea530",2,"Dynamic Position Encoding for Transformers","This paper proposes a novel architecture with new position embeddings that take the order of the target words into consideration and generates new embedDings to refine each word’s position information as dynamic position encoding (DPE).","International Conference on Computational Linguistics",2022,"Joyce Zheng,Mehdi Rezagholizadeh,P. Passban",0,21,0
"2193d898e556ad2ac14f0ababf02f90e6fdfe663","https://www.semanticscholar.org/paper/2193d898e556ad2ac14f0ababf02f90e6fdfe663",2,"DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks","This work focuses on improving the position encoding ability of BERT with the causal attention masks, and proposes a new pre-trained language model DecBERT and evaluates it on the GLUE benchmark.","NAACL-HLT",2022,"Ziyang Luo,Yadong Xi,Jing Ma,Zhiwei Yang,Xiao-Xi Mao,Changjie Fan,Rongsheng Zhang",1,54,0
"b9a701c90f3d3df27366f5b29a97f798eb940ac7","https://www.semanticscholar.org/paper/b9a701c90f3d3df27366f5b29a97f798eb940ac7",2,"ChapterBreak: A Challenge Dataset for Long-Range Language Models","This work introduces C HAPTER B REAK, a challenge dataset that provides an LRLM with a long segment from a narrative that ends at a chapter boundary and asks it to distinguish the beginning of the ground-truth next chapter from a set of negative segments sampled from the same narrative.","North American Chapter of the Association for Computational Linguistics",2022,"Simeng Sun,Katherine Thai,Mohit Iyyer",4,39,1
"9d03a164759bb5cc2fa6b575254b58f790ab6785","https://www.semanticscholar.org/paper/9d03a164759bb5cc2fa6b575254b58f790ab6785",2,"Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction","This paper investigates the effectiveness of sentence-level transformers for zero-shot offensive span identification on a code-mixed Tamil dataset and finds both LIME and IG to show significant improvement with Masked Data Augmentation and Multilabel Training.","DRAVIDIANLANGTECH",2022,"Manikandan Ravikiran,Bharathi Raja Chakravarthi",1,19,0
"6e98a35c439c5c7387de6bffd96dea9b2943a548","https://www.semanticscholar.org/paper/6e98a35c439c5c7387de6bffd96dea9b2943a548",2,"Trading Positional Complexity vs. Deepness in Coordinate Networks","This paper shows that alternative non-Fourier embedding functions can indeed be used for positional encoding, and shows that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates.","European Conference on Computer Vision",2022,"Jianqiao Zheng,Sameera Ramasinghe,Xueqian Li,S. Lucey",0,39,0
"d6c5aab433d9871cabc01ffb1e5e1ea89141155b","https://www.semanticscholar.org/paper/d6c5aab433d9871cabc01ffb1e5e1ea89141155b",2,"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation","KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences, is proposed using conditionally positive deﬁnite (CPD) kernels, and it is shown that a CPD kernel can be transformed into a PD kernel by adding a constant offset.","ArXiv",2022,"Ta-Chung Chi,Ting-Han Fan,P. Ramadge,Alexander I. Rudnicky",1,54,0
"e1461a3aa9f4bef86435fa2cc87b16d9f34d5ab0","https://www.semanticscholar.org/paper/e1461a3aa9f4bef86435fa2cc87b16d9f34d5ab0",2,"Do we really need temporal convolutions in action segmentation?","A pure Transformer-based model without temporal convolutions by incorporating temporal sampling, called Temporal U-Transformer (TUT), which reduces complexity while introducing an induc- tive bias that adjacent frames are more likely to belong to the same class, but the introduction of coarse resolutions in the misclassiﬁcation of boundaries.","",2022,"Dazhao Du,Bing Su,Yu Li,Zhongang Qi,Lingyu Si,Ying Shan",1,51,0
"3884307cb95329275755baaf99600e7431be695d","https://www.semanticscholar.org/paper/3884307cb95329275755baaf99600e7431be695d",2,"Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation","A pure Transformer-based model without temporal convolution-based models by incorporating the U-Net architecture is designed and a boundary-aware loss based on the distribution of similarity scores between frames from attention modules is proposed to enhance the ability to recognize boundaries.","ArXiv",2022,"Dazhao Du,Bing Su,Yu Li,Zhongang Qi,Lingyu Si,Ying Shan",2,57,0
"746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a","https://www.semanticscholar.org/paper/746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a",2,"Your Transformer May Not be as Powerful as You Expect","This work mathematically analyzes the power of RPE-based Transformers regarding whether the model is capable of approximating any continuous sequence-to-sequence functions and develops a novel attention module, called Universal R PE-based (URPE) Attention, which sitsisﬁes the conditions.","ArXiv",2022,"Shengjie Luo,Shanda Li,Shuxin Zheng,Tie-Yan Liu,Liwei Wang,Di He",5,78,1
"8e6ada0242dc9b3680ccf6ddeec9e7c3cd41766f","https://www.semanticscholar.org/paper/8e6ada0242dc9b3680ccf6ddeec9e7c3cd41766f",2,"Dynamically Relative Position Encoding-Based Transformer for Automatic Code Edit","DTrans is designed with dynamically relative position encoding in the multi-head attention of Transformer, which can more accurately generate patches than the state-of-the-art methods and locate the lines to change with higher accuracy than the existing methods.","IEEE Transactions on Reliability",2022,"Shiyi Qi,Yaoxian Li,Cuiyun Gao,Xiaohong Su,Shuzheng Gao,Zibin Zheng,Chuanyi Liu",0,74,0
"31a9744bd5421b3fbbad2ab38ce33bb2f352c77a","https://www.semanticscholar.org/paper/31a9744bd5421b3fbbad2ab38ce33bb2f352c77a",2,"CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation","Clustering Mask Transformer (CMT-DeepLab) is proposed, a transformer-based framework for panoptic segmentation designed around clustering that improves the performance over prior art significantly and achieves a new state-of-the-art of 55.7% PQ on the COCO test-dev set.","Computer Vision and Pattern Recognition",2022,"Qihang Yu,Huiyu Wang,Dahun Kim,Siyuan Qiao,Maxwell D. Collins,Yukun Zhu,Hartwig Adam,A. Yuille,Liang-Chieh Chen",12,102,0
"31a84391e1b47fa15f3a521c43d62385a7757637","https://www.semanticscholar.org/paper/31a84391e1b47fa15f3a521c43d62385a7757637",2,"MetaTPTrans: A Meta Learning Approach for Multilingual Code Representation Learning","MetaTPTrans is proposed, a meta learn- ing approach for multilingual code representation learning that generates different parameters for the feature extractor according to the programming language type of the input code snippet, enabling the model to learn both language-agnostic and language-speciﬁc information with dynamic parameters in the feature Extractor.","ArXiv",2022,"Weiguo Pian,Hanyu Peng,Xunzhu Tang,Tiezhu Sun,Haoye Tian,Andrew Habib,Jacques Klein,Tegawend'e F. Bissyand'e",1,49,0
"e28adeb4db46469df9f9bd653501871ddc5f4318","https://www.semanticscholar.org/paper/e28adeb4db46469df9f9bd653501871ddc5f4318",2,"MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization","MuchSUM is a multi-channel graph convolutional network designed to explicitly incorporate multiple salient summary-worthy features under bipartite word-sentence heterogeneous graphs and can achieve considerable performance compared with some BERT-initialized graph-based extractive summarization systems.","Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",2022,"Qianren Mao,Hongdong Zhu,J. Liu,Cheng Ji,Hao Peng,Jianxin Li,Lihong Wang,Z. Wang",0,37,0
"574bcb4aba88cd7a296d584a5bcb99bd769705d8","https://www.semanticscholar.org/paper/574bcb4aba88cd7a296d584a5bcb99bd769705d8",2,"k-means Mask Transformer","The relationship between pixels and object queries is rethink, and a k -means clustering algorithm is proposed to reformulate the cross-attention learning as a clustering process, which improves the state-of-the-art, but also enjoys a simple and elegant design.","European Conference on Computer Vision",2022,"Qihang Yu,Huiyu Wang,Siyuan Qiao,Maxwell D. Collins,Yukun Zhu,Hatwig Adam,A. Yuille,Liang-Chieh Chen",9,116,3
"9692886ba8e2c9d8990b0505e9c67a696d9f28a7","https://www.semanticscholar.org/paper/9692886ba8e2c9d8990b0505e9c67a696d9f28a7",2,"A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities","The results suggest that Transformer based on abstract syntax trees (ASTs) shows more robust performance than the model based on only code sequence under most code transformations, and the design of positional encoding can impact the robustness of Transformer under code transformation.","ArXiv",2022,"Yaoxian Li,Shiyi Qi,Cuiyun Gao,Yun Peng,David Lo,Zenglin Xu,Michael R. Lyu",0,87,0
"4d80dae4039b4de6c1ba30227d5981fd1cbb4a1f","https://www.semanticscholar.org/paper/4d80dae4039b4de6c1ba30227d5981fd1cbb4a1f",2,"Parameterization of Cross-token Relations with Relative Positional Encoding for Vision MLP","A new positional spacial gating unit (PoSGU) is proposed that exploits the attention formulations used in the classical relative positional encoding (RPE), to efficiently encode the cross-token relations for token mixing and can successfully reduce the current quadratic parameter complexity O(N2) of vision MLPs.","ACM Multimedia",2022,"Zhicai Wang,Y. Hao,Xingyu Gao,Hao Zhang,Shuo Wang,Tingting Mu,Xiangnan He",0,55,0
"4299353235595391e2b4f7298baffd00b5acf9d1","https://www.semanticscholar.org/paper/4299353235595391e2b4f7298baffd00b5acf9d1",2,"LordBERT: Embedding Long Text by Segment Ordering with BERT","A novel multi-task learning framework, named LordBERT, which fully exploits both intra- and inter-segment information in long text by segment ordering with BERT and utilizes an auxiliary segment ordering module to reorder disordered segments.","IEEE International Joint Conference on Neural Network",2022,"Borun Chen,Rongyi Sun,Yimeng Dai,Haitao Zheng,Rui Zhang",0,35,0
"e9d1a851d4f7950db360d12fe7f96647aaf547da","https://www.semanticscholar.org/paper/e9d1a851d4f7950db360d12fe7f96647aaf547da",2,"Generalized Attention Mechanism and Relative Position for Transformer","A new relative position representation within the framework of GAM can be easily utilized for cases in which elements next to each other in input sequence can be at random locations in actual dataset/corpus.","ArXiv",2022,"R. Pandya",0,5,0
"4f451ba06c4c9effd6c4ac0bae222495501a6200","https://www.semanticscholar.org/paper/4f451ba06c4c9effd6c4ac0bae222495501a6200",2,"Innovations in Neural Data-to-text Generation","This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella.","ArXiv",2022,"Mandar Sharma,Ajay K. Gogineni,Naren Ramakrishnan",1,347,0
"076deb54a5d776cd21eabf2c40cdd839f53d6d77","https://www.semanticscholar.org/paper/076deb54a5d776cd21eabf2c40cdd839f53d6d77",2,"giMLPs: Gate with Inhibition Mechanism in MLPs","The gate with inhibition on CycleMLP (gi-CycleMLP) can produce the equal performance on ImageNet classiﬁcation task, and it also improves the BERT, RoBERTa and DeBERTaV3 models depending on two novel techniques.","ArXiv",2022,"C. Kang,Jindich Prokop,Lei Tong,Huiyu Zhou,Yong Hu,Daneil Novak",0,71,0
"42bc0824d8ca35105d181aaa0183654535325f55","https://www.semanticscholar.org/paper/42bc0824d8ca35105d181aaa0183654535325f55",2,"Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages","Structural encoding methods make Transformers more sample-efﬁcient, enabling them to perform better from smaller amounts of data, and investigates two methods for building in such a bias.","ArXiv",2022,"Paul Soulos,Sudha Rao,Caitlin Smith,Eric Rosen,Asli Celikyilmaz,R.,Tom McCoy,Yichen Jiang,Coleman Haley,Roland Fernandez,H. Palangi,Jianfeng Gao,P. Smolensky",1,43,0
"03b7d2e942eafabd14b229f8962fa6e943053f75","https://www.semanticscholar.org/paper/03b7d2e942eafabd14b229f8962fa6e943053f75",2,"Melody Infilling with User-Provided Structural Context","This paper proposes a structure-aware conditioning approach that employs a novel attention-selecting module to supply user-provided structure-related information to the Transformer for inﬁlling and shows that the proposed model can harness the structural information effectively and generate melodies in the style of pop of higher quality than the two existing structure-agnostic in-lling models.","ArXiv",2022,"Chih-Pin Tan,A. Su,Yi-Hsuan Yang",0,39,0
"a0b4e5f73adb829f2b7c51eb373f0559c110e2ac","https://www.semanticscholar.org/paper/a0b4e5f73adb829f2b7c51eb373f0559c110e2ac",2,"Better Pre-Training by Reducing Representation Confusion","Two novel techniques to improve pre-trained language models are proposed: Decoupled Directional Relative Position encoding and MTH pre-training objective, which allow the model to capture different categories of information more clearly, as a way to alleviate information confusion in representation learning for better optimization.","",2022,"Haojie Zhang,Mingfei Liang,Ruobing Xie,Zhen Sun,Bo Zhang,Leyu Lin",0,32,0
"5cdb940cb0e8158bc5bc5aabcee84ffeee0c30fe","https://www.semanticscholar.org/paper/5cdb940cb0e8158bc5bc5aabcee84ffeee0c30fe",2,"Improve Transformer Pre-Training with Decoupled Directional Relative Position Encoding and Representation Differentiations","Two novel techniques to improve pre-trained language models are proposed: Decoupled Directional Relative Position (DDRP) encoding and MTH pre-training objective, which decouples the relative distance features and the directional features in classical relative position encoding for better position information understanding.","ArXiv",2022,"Haojie Zhang,Mingfei Liang,Ruobing Xie,Zhen Sun,Bo Zhang,Leyu Lin",0,24,0
"97833e2aa0da5240e62436373b58af988a4ab6ab","https://www.semanticscholar.org/paper/97833e2aa0da5240e62436373b58af988a4ab6ab",2,"The Curious Case of Absolute Position Embeddings","It is observed that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information, which raises questions about the efficacy of APEs to model the relativity of position information.","Conference on Empirical Methods in Natural Language Processing",2022,"Koustuv Sinha,Amirhossein Kazemnejad,Siva Reddy,J. Pineau,D. Hupkes,Adina Williams",2,56,0
"bb1486013aedbbb4f9a960c83731fbb91a04b1f3","https://www.semanticscholar.org/paper/bb1486013aedbbb4f9a960c83731fbb91a04b1f3",2,"Mitigation of Spatial Nonstationarity with Vision Transformers","","ArXiv",2022,"Lei Liu,Javier E. Santos,Mavsa Prodanovi'c,M. Pyrcz",0,40,0