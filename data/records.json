{"papers":[{"url":"https://www.semanticscholar.org/paper/a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":23,"citationCount":16,"influentialCitationCount":0,"publicationDate":2022,"authors":"Matteo Muffo,A. Cocco,Enrico Bertino","citations":[{"paperId":"4372100e5f676b0a2db93a97b6b1e45ae593b75e","title":"Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought"},{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners"},{"paperId":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator"},{"paperId":"a37d5620210276e47cf0c9dd2898c2a82c9d0422","title":"Simple synthetic data reduces sycophancy in large language models"},{"paperId":"9160673f89ecb326db8c64f8d09ab30f35a74e9d","title":"Arithmetic with Language Models: from Memorization to Computation"},{"paperId":"7ef0cc95ff71c1098414cd61e88ac373ea2db4c4","title":"Performance of Generative Large Language Models on Ophthalmology Board Style Questions."},{"paperId":"24a285b1e7ee9acfec9a82f7123563b62f532203","title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"ade6d2808283ab8372db701ede6ee145a5445a95","title":"SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models"},{"paperId":"763f47b73d0273a93a52a8ee8a63cf19009df6e1","title":"Injecting the BM25 Score as Text Improves BERT-Based Re-rankers"},{"paperId":"651dac86d8bf847ec6780a878cb1e04d3d41f356","title":"GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities"},{"paperId":"36f46391b00a7eb4ffc991f964a36b264811057d","title":"Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks"}],"references":[{"paperId":"ac34c70ee85b048ad97328713c790f389656e4eb","title":"Ecco: An Open Source Library for the Explainability of Transformer Language Models"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"b042444e4e9501ab2fcae6b1f6b9d834146b3fc9","title":"Do Language Embeddings capture Scales?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"53c8b98eb9180ed9f46820627715c7ae2803cee7","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1a2118bed729579528deb51e745d58dd3629baf6","title":"Learning Important Features Through Propagating Activation Differences"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"a9075f6332542e12b2bf3cdbdb3a6ed44733fb41","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"13167f9cd8c7906ca808b01d28dca6dd951da8a5","title":"of the Association for Computational Linguistics"},{"paperId":null,"title":"3374–3380, Florence, Italy,"}],"id":"a5808ccc50f77083bd3be926fb2af05cf34563ff","summary":"The ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on is evaluated."},{"url":"https://www.semanticscholar.org/paper/c2d50f17ea6769f6f5663ccac37a9627a0543184","title":"I NVESTIGATING THE L IMITATIONS OF T RANSFORM ERS WITH S IMPLE A RITHMETIC T ASKS","venue":"","year":null,"referenceCount":49,"citationCount":73,"influentialCitationCount":0,"publicationDate":null,"authors":"","citations":[{"paperId":"17ba73a2a332a44bb1a00622beab96f33d4b1ba7","title":"RORA: Robust Free-Text Rationale Evaluation"},{"paperId":"f740a2474b52675287166a003bd1313f8aabcd68","title":"BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning"},{"paperId":"6f63ad08cf27183ceb2976b9fb6599ed9b31a522","title":"OmniPred: Language Models as Universal Regressors"},{"paperId":"49c45d2a2773c537804c38d69cde67e00fbad6fe","title":"Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs"},{"paperId":"61547f92cbba68629f470ece8019c4140c506706","title":"Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models"},{"paperId":"bc4e1552d3c12180fd13d59e155861a6eb2fbdab","title":"Lissard: Long and Simple Sequential Reasoning Datasets"},{"paperId":"4372100e5f676b0a2db93a97b6b1e45ae593b75e","title":"Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought"},{"paperId":"d9e76ae6480114d81da2e9eb98f848df120be057","title":"Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors"},{"paperId":"a640215755e23e2649f4b3d3246a47b14fea93f7","title":"Getting the most out of your tokenizer for pre-training and domain adaptation"},{"paperId":"2aeaad5548229dec7fdf716f7e83a5a359665852","title":"Carrying over algorithm in transformers"},{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"4726d1dc54851db99c29180127d840bd19f20afc","title":"Positional Description Matters for Transformers Arithmetic"},{"paperId":"dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills"},{"paperId":"1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b","title":"What Algorithms can Transformers Learn? A Study in Length Generalization"},{"paperId":"200fcd964f41efe0c35a3f888a520ede08a3269c","title":"From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers"},{"paperId":"174a9b78350e9561555052bc6901cc44782f4c62","title":"Estimating Numbers without Regression"},{"paperId":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners"},{"paperId":"0b778079946764292de3771a489d5ce9e1868a8b","title":"The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models"},{"paperId":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator"},{"paperId":"cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6","title":"Can transformers learn the greatest common divisor?"},{"paperId":"edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a","title":"It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models"},{"paperId":"9160673f89ecb326db8c64f8d09ab30f35a74e9d","title":"Arithmetic with Language Models: from Memorization to Computation"},{"paperId":"72a9187b489992cad3d54420611d5039eb6b9d86","title":"One Blade for One Purpose: Advancing Math Information Retrieval using Hybrid Search"},{"paperId":"8f1b0c247171510fc68da27a16a377456376a5a7","title":"Assessing GPT-3.5 and GPT-4 in Generating International Classification of Diseases Billing Codes"},{"paperId":"0db0af0cd3ceb0531a050a03e6ceb849580ff53b","title":"Teaching Arithmetic to Small Transformers"},{"paperId":"be3a74f9f6010889d049060eab2a4b09eb48bbfb","title":"The Value of Numbers in Clinical Text Classification"},{"paperId":"49d340b7d6108ce5ef2277a165ea83f254671763","title":"The use of weather nowcasting convolutional neural network extrapolators in cardiac PET imaging"},{"paperId":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers"},{"paperId":"b43383f10634f7e610f22badd4f42c93e5dcb947","title":"Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI"},{"paperId":"3e7a0dc5795dc108c78993bcf3624fc626a9f9cf","title":"Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks"},{"paperId":"d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","title":"Exposing Attention Glitches with Flip-Flop Language Modeling"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering"},{"paperId":"aec826ff336ca442697d5f908ab1668f1ea18987","title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","title":"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)"},{"paperId":"49e46e615747f258517248de5a736814fada17ee","title":"What is my math transformer doing? - Three results on interpretability and generalization"},{"paperId":"e82e3f4347674b75c432cb80604d38ee630d4bf6","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"2a7ae3e98357569c41424dacd60c62d3df78a0db","title":"Limitations of Language Models in Arithmetic and Symbolic Induction"},{"paperId":"4f451ba06c4c9effd6c4ac0bae222495501a6200","title":"Innovations in Neural Data-to-text Generation"},{"paperId":"40f73969cd3415e9c1b03796cbb5a50c79ebd448","title":"SALSA: Attacking Lattice Cryptography with Transformers"},{"paperId":"b92628d13e8d090d042232fe6ae0b8998634b893","title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"},{"paperId":"d2018d1b0f69ca6805aa18a22be95cab9b5c44c7","title":"On Neural Architecture Inductive Biases for Relational Tasks"},{"paperId":"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence"},{"paperId":"7a25155364476839b6d1fc0653cd8611327ab9ba","title":"mGPT: Few-Shot Learners Go Multilingual"},{"paperId":"edd80013e8b9fcba9231cf99884f32e5236ff329","title":"AA-TransUNet: Attention Augmented TransUNet For Nowcasting Tasks"},{"paperId":"45ece6f3b0a319dba60c20b3013b5161dd49c58b","title":"Linear algebra with transformers"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"6c84fc8c5ec823342f34a020f8b92b7064e96ca2","title":"Towards Efficient and Robust Out-of-Distribution Deep Learning with Implicit Models"},{"paperId":"58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","title":"JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection"},{"paperId":"c27f3904490b8e5f9f39fe2b36722090c189e916","title":"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF"},{"paperId":"346accd49d1359aa3985c7b298bc2057ae642271","title":"One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development"},{"paperId":"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","title":"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning"},{"paperId":"5df55d94ab5026ff84ec01871592108fbadbddbe","title":"Measurement Extraction with Natural Language Processing: A Review"},{"paperId":"e39dfab0477ae28dbb18d49beb9cd4c7ea30486c","title":"On the Abilities of Mathematical Extrapolation with Implicit Models"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":"b16d916ce411be0b1c7b6139317c652927984f46","title":"Tokenization on the Number Line is All You Need"}],"references":[{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"4383a975c09b72ba2f1a77cd779bb6965dbfb2fb","title":"Scaling Laws for Transfer"},{"paperId":"02791e807dc9a91f854a1f3d5f6005122a546109","title":"Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"45cdda3f96ae32927c7b073ebbedf46f5e0fbdc5","title":"Enhancing the Numeracy of Word Embeddings: A Linear Algebraic Perspective"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"b042444e4e9501ab2fcae6b1f6b9d834146b3fc9","title":"Do Language Embeddings capture Scales?"},{"paperId":"84476fdf6ead3553f4493dff8e02308439d6222b","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"8256f48f759cf85044db251cc512f965834945b3","title":"Rethinking Positional Encoding in Language Pre-training"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"ac67ec5b985a30239926c3362fa45a9a03f017af","title":"Learning to Generate Correct Numeric Values in News Headlines"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"d0e28f5dc1feae19e41087a92a87992977fd85af","title":"Encoding word order in complex embeddings"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"136396e988dbbdaca433096e931b22732c3d95dd","title":"Text2Math: End-to-end Parsing Text into Math Expressions"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"5435f997d98754f68492334eeb87d027047e60cb","title":"Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"0bb487b9cfefd56e79be0a5be5f1e05742683301","title":"Extensions and Limitations of the Neural GPU"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"5b791cd374c7109693aaddee2c12d659ae4e3ec0","title":"Grid Long Short-Term Memory"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"319f22bd5abfd67ac15988aa5c7f705f018c3ccd","title":"Learning internal representations by error propagation"},{"paperId":null,"title":"st Mathematical Reasoning in General Artiﬁcial Intelligence Workshop"},{"paperId":"5414be4432fcfe1e83932127ee2463d107e5b61c","title":"A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"},{"paperId":null,"title":"Character-level models such as ELMO"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"The architecture of the transformer"}],"id":"c2d50f17ea6769f6f5663ccac37a9627a0543184","summary":"It is concluded that modern pretrained language models can easily learn arithmetic from very few examples, as long as they use the proper surface representation, which bolsters evidence that subword tokenizers and positional encodings are compo-nents in current transformer designs that might need improvement."},{"url":"https://www.semanticscholar.org/paper/ecf0cb0725de18659f9ba25a8cf65a1085564006","title":"Understanding Arithmetic Reasoning in Language Models using Causal Mediation Analysis","venue":"arXiv.org","year":2023,"referenceCount":51,"citationCount":5,"influentialCitationCount":1,"publicationDate":"24/05/2023","authors":"Alessandro Stolfo,Yonatan Belinkov,Mrinmaya Sachan","citations":[{"paperId":"a05148b077418259989511ed8031ce05689a16aa","title":"AtP*: An efficient and scalable method for localizing LLM behaviour to components"},{"paperId":"49c45d2a2773c537804c38d69cde67e00fbad6fe","title":"Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs"},{"paperId":"a14e999e8826f98cfd1cac9dd7baa4d0b61f7266","title":"Increasing Trust in Language Models through the Reuse of Verified Circuits"},{"paperId":"f5df0667365764a970fc6abfa0a68b7d1d0ae413","title":"Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models"},{"paperId":"9160673f89ecb326db8c64f8d09ab30f35a74e9d","title":"Arithmetic with Language Models: from Memorization to Computation"}],"references":[{"paperId":"405f8f5f1c6df1b3343c812832479aad5180b65f","title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"12910786da7a34c9ee26798fd81b0ed7b0e38789","title":"Finding Neurons in a Haystack: Case Studies with Sparse Probing"},{"paperId":"133b97e40017a9bbbadd10bcd7f13088a97ca3cc","title":"Dissecting Recall of Factual Associations in Auto-Regressive Language Models"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"574beee702be3856d60aa482ec725168fe64fc99","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"dcd8dff20f4f490acd5f94001d34c774167f053e","title":"Jump to Conclusions: Short-Cutting Transformers With Linear Transformations"},{"paperId":"762ca2711eb167f19b79e39c175708ca15e1f5d7","title":"Eliciting Latent Predictions from Transformers with the Tuned Lens"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"f680d47a51a0e470fcb228bf0110c026535ead1b","title":"Progress measures for grokking via mechanistic interpretability"},{"paperId":"89c3bd70ad33c4f8832f00ab98872b77861ee0ec","title":"Discovering Latent Knowledge in Language Models Without Supervision"},{"paperId":"6edd112383ad494f5f2eba72b6f4ffae122ce61f","title":"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"c90a99eeb57019732a6cc996bb9eaf13faedf00f","title":"In-context Learning and Induction Heads"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"1bcde55995a957b3e8a595d536b816cb8989cf1d","title":"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cf36236015c9f93f15bfafbf282f69e08bdc9c16","title":"Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"},{"paperId":"996445d847f06e99b0bd259345408a0cf1bce87e","title":"Locating and Editing Factual Associations in GPT"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"aaf3ebaf12baeb366ce6ff32aa36d608a7eab583","title":"On the Pitfalls of Analyzing Individual Neurons in Language Models"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"488de57dde884402d88ccecfd02347dd7e4d01a1","title":"Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models"},{"paperId":"57a1258571a21817d89197dc84c986861fb6e580","title":"Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning."},{"paperId":"5951ede73a1dbb9c5ea8b4d95f31c5ed646aacbd","title":"Causal Abstractions of Neural Networks"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"b5ba72aaaef1ae5dccb313c64a5cfb5de3e2b442","title":"Multimodal Neurons in Artificial Neural Networks"},{"paperId":"9558d5637955bc444b45ee867a2959d98b0d3e6c","title":"Visualizing Weights"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"4a54d58a4b20e4f3af25cea3c188a12082a95e02","title":"Transformer Feed-Forward Layers Are Key-Value Memories"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"54a13bcc9613dcaa76fb25fbe96572f376cfcca9","title":"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b8aa08f1ae7ffa568b3ea925af121d629e862b99","title":"The Building Blocks of Inter-operability"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"40be3888daa5c2e5af4d36ae22f690bcc8caf600","title":"Visualizing and Understanding Recurrent Networks"},{"paperId":"d8574a62874312b81347438b1566cdb1c6d5abe5","title":"Direct and Indirect Effects"},{"paperId":"fca0e8bb93a8242c153ef966cefade5e19e5ed21","title":"Feature Visualization"},{"paperId":"a0117b72a4f68d0a134e24f674ca7fd0b42663b7","title":"Causality"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":null,"title":"A mathematical framework for transformer circuits"},{"paperId":"78d08b8ab4132defffe98ec7f80a51452203f70d","title":"Investigating Gender Bias in Language Models Using Causal Mediation Analysis"},{"paperId":null,"title":"2023. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks"},{"paperId":null,"title":"We measure the causal effect of the intervention on variables m ( l ) t and a ( l ) t on the model’s prediction by computing the change in the probability values assigned to the results r and r ′"},{"paperId":null,"title":"During the forward pass with input p 1 , we store the activation values ¯ m ( l ) t := MLP ( l ) ( h ( l − 1) t ) , and ¯ a ( l ) t := A ( l ) ( h ( l − 1) 1 , . . . , h ( l − 1) t )"},{"paperId":null,"title":"f O"},{"paperId":null,"title":"Natural Language Processing (Volume 1"},{"paperId":null,"title":"2022b. Chain of thought prompting elicits reasoning in large language models"}],"id":"ecf0cb0725de18659f9ba25a8cf65a1085564006","summary":"A mechanistic interpretation of LLMs for arithmetic-based questions using a causal mediation analysis framework, which identifies the subset of parameters responsible for specific predictions and investigates the role of the attention mechanism."},{"url":"https://www.semanticscholar.org/paper/073e6b91adc25c656d85002e3cb059e4530db20b","title":"Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":13,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Zhenwen Liang,W. Yu,Tanmay Rajpurohit,Peter Clark,Xiangliang Zhang,Ashwin Kaylan","citations":[{"paperId":"edc5b4b2b89718b42dc90192960084166b7987d8","title":"Multi-Intent Attribute-Aware Text Matching in Searching"},{"paperId":"7f4bdef8c9d660af6b18a55de0699e5e65ce3b54","title":"Plan-Grounded Large Language Models for Dual Goal Conversational Settings"},{"paperId":"42445823fb0156afddc8c72eaa5ee81dded5b965","title":"Large Language Models for Mathematical Reasoning: Progresses and Challenges"},{"paperId":"d43587743abd006be30756b67efd87f61671412b","title":"Tartare: Automatic Generation of C Pointer Statements and Feedback"},{"paperId":"8f070e301979732e0dd73f6aa6170309cf73aa7d","title":"Large Language Model based Multi-Agents: A Survey of Progress and Challenges"},{"paperId":"04cc5744fe8b436fb0f8b2763ed8ac7e4a60f7d1","title":"Combining GPT and Colab as learning tools for students to explore the numerical solutions of difference equations"},{"paperId":"1a6bd4f70bceb26ddb722ce98c0eae2a64147048","title":"Knowledge-Based and Generative-AI-Driven Pedagogical Conversational Agents: A Comparative Study of Grice's Cooperative Principles and Trust"},{"paperId":"631b5baa2c34f7095ccdd8761086b49148071d78","title":"Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments"},{"paperId":"98b607e7cb84e1a5c87c8a49562ae35435e6722d","title":"Learning From Mistakes Makes LLM Better Reasoner"},{"paperId":"e1414fc1e1a6752524a1807a29ee406e8d808849","title":"Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models"},{"paperId":"c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36","title":"MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning"},{"paperId":"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","title":"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements"},{"paperId":"3dc9eb80b806f2db14a45216a7fd840153f45a07","title":"Don’t be Blind to Questions: Question-Oriented Math Word Problem Solving"}],"references":[{"paperId":"3fc3460c4554a28e489a0ea6ef067b79b7d301d9","title":"Active Prompting with Chain-of-Thought for Large Language Models"},{"paperId":"69619a2a47faee7a29ec596db13172e2a42ff921","title":"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","title":"Large Language Models Are Reasoning Teachers"},{"paperId":"126a4776ff8315fd506766cb8f3c722cf746ad9e","title":"Teaching Small Language Models to Reason"},{"paperId":"71471561137956d6aeec39173db460a28d86c11b","title":"Analogical Math Word Problems Solving with Enhanced Problem-Solution Association"},{"paperId":"e6745fb621481ccb0ed53c267a37292e499c1b42","title":"Automatic Generation of Socratic Subquestions for Teaching Math Word Problems"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"95d54e3ce577f7d91ab4b2c52c73b501245e484d","title":"ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback"},{"paperId":"7d29a84a589aa5655e5d3fed8d725ea472816599","title":"Explanations from Large Language Models Make Small Reasoners Better"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"4feb4f49c5837cbb1c9c8e61d4e8056cafb102e6","title":"Unbiased Math Word Problems Benchmark for Mitigating Solving Bias"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"a376e6b118a10a8cb8a2920f6b83a70f087579f5","title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"},{"paperId":"7955b46e640fe460d9c781263c498a19a460bb9b","title":"PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks"},{"paperId":"2145fcceeb69385e108bf1796d52f974854d4c0b","title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"5e8d3c2dc0fc53949794fc00600e25558c4a2441","title":"WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation"},{"paperId":"110c359de78cf1071a3d7a1cd61396b3b8f2722e","title":"Knowledge Tracing: A Survey"},{"paperId":"39df1a17da84f02bfcb8751de8965b798653a5ee","title":"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems"},{"paperId":"521ccc898395a2818fced22b4cf371b0e5121f94","title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models"},{"paperId":"81c241c2cb8e86a5574797424bc281e323cd831b","title":"Recall and Learn: A Memory-augmented Solver for Math Word Problems"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"5926be5b2832d69e97f50588e1f5005b892743a7","title":"Solving Math Word Problems with Teacher Supervision"},{"paperId":"f8e0f7babe7cd858aecfafb03375bf578fe161f7","title":"MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"eb2f1480eca6195fe8dbab5de8bbac8749e9be58","title":"RODA: Reverse Operation Based Data Augmentation for Solving Math Word Problems"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"6605bba6e0caabda06b090d67698a5683eba4dfa","title":"Translating a Math Word Problem to an Expression Tree"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"c20584e878efbc4d716710b9aa2f4954c4014638","title":"Learning Fine-Grained Expressions to Solve Math Word Problems"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"f6b5335f27b9583dd152d8cd4ea9134e24bd297b","title":"Learning To Use Formulas To Solve Simple Arithmetic Problems"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"acfe603d8a90ee3d4187e426b26c4d7bf394b4c8","title":"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"e2694d343993482626163fa9e4f3d3d0ba5f8efd","title":"A review of methods for automatic understanding of natural language mathematical problems"},{"paperId":"5b3ffcbd38b130e54e161b22e513302ad3f186ce","title":"The Real Story Behind Story Problems: Effects of Representations on Quantitative Reasoning"},{"paperId":"34c0410aaaed15350e3c576ad2bc327e8a444c65","title":"The power of feedback."},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"ca1a53c7cadfca7e3201568ba84bba490e6077b1","title":"Comprehension of arithmetic word problems: A comparison of successful and unsuccessful problem solvers."},{"paperId":"43489fda8a0c6276a3aa8f068892e31ebfd63c13","title":"Understanding and solving arithmetic word problems: A computer simulation"},{"paperId":"72123a86eae2cb5c4eae8650f43524039d48875d","title":"Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions"},{"paperId":"25eba922c7076e1a8a0a86fd0eed2bfcdbe0bbc3","title":"AugESC: Large-scale Data Augmentation for Emotional Support Conversation with Pre-trained Language Models"},{"paperId":null,"title":"Gpt-j-6b: A 6 billion parameter autoregressive language model"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Personalized learning: A guide for engaging students with technology"},{"paperId":"645b2c28c28bd28eaa187a2faafa5ec12bc12e3a","title":"Knowledge tracing: Modeling the acquisition of procedural knowledge"}],"id":"073e6b91adc25c656d85002e3cb059e4530db20b","summary":"A novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles."},{"url":"https://www.semanticscholar.org/paper/13ac3b7f1ace63c76c1e081898369f8e0505411c","title":"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/08/2023","authors":"Dingzirui Wang,Longxu Dou,Wenbin Zhang,Junyu Zeng,Wanxiang Che","citations":[{"paperId":"dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills"}],"references":[{"paperId":"1e2eba005ccd8ab7a668a525c5b43245853bdaf1","title":"Reasoning in Large Language Models Through Symbolic Math Word Problems"},{"paperId":"8568d7bd9dfb5ba0b91940b938b44a88fafdf95b","title":"Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models"},{"paperId":"d75d11d2c89c01cd284383546ae057cb827dc272","title":"Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning"},{"paperId":"6289de84a02f0c27734f295ada565603ac958948","title":"Tab-CoT: Zero-shot Tabular Chain of Thought"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"261549439aebdda72b648ecc462448fd24857ac1","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"57100e39d0413ee585b381ba9ab366e8a6cf2866","title":"Solving Math Word Problems by Combining Language Models With Symbolic Solvers"},{"paperId":"574beee702be3856d60aa482ec725168fe64fc99","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"b626560f19f815808a289ef5c24a17c57320da70","title":"MathPrompter: Mathematical Reasoning using Large Language Models"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"e826ac71dad8c4ce36d82fb7add43e3d306bb7e1","title":"Making Language Models Better Reasoners with Step-Aware Verifier"},{"paperId":"5fa148e8bfcd5fa3df05311c0ec8abb13caf1c05","title":"GraphQ IR: Unifying the Semantic Parsing of Graph Query Languages with One Intermediate Representation"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"379e9d5bba8a617b3114bd5b562b14aa6abc5282","title":"Natural SQL: Making SQL Easier to Infer from Natural Language Specifications"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"34c8b5b816d3ae6d433ca5e5735f264754ff4c2c","title":"N-LTP: An Open-source Neural Language Technology Platform for Chinese"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"b9372e972997c5056bb79c70526230baed2e372b","title":"Multi-hop Reading Comprehension through Question Decomposition and Rescoring"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"676a7e362e283cb953135f506eb0a07a8b0be0ed","title":"A Survey on Semantic Parsing"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"e889e2dd1647fdfc28edb9abe08a863b704f08be","title":"Using Intermediate Representations to Solve Math Word Problems"},{"paperId":"ad406dea8c9c616402c000f32261700b77f9ac3a","title":"Multi-hop Inference for Sentence-level TextGraphs: How Challenging is Meaningfully Combining Information for Science Question Answering?"},{"paperId":"3d1b34384499f77bf317b01594c8c60af212582b","title":"SymPy: Symbolic computing in Python"},{"paperId":"2d7fcca1b0bde8e3f0450f4c8e67e6cbf519bff1","title":"Equation Parsing : Mapping Sentences to Grounded Equations"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"e72e5ee5de14fd463ab58ce830474157258e3578","title":"Abstract Meaning Representation for Sembanking"},{"paperId":"6eac5e325302826c6778ca7dfa92fb1ceb28f02e","title":"Word Problems"},{"paperId":"700bde5ae8a045bd8519ea45180906f04ebc8dea","title":"Exploring the Secrets Behind the Learning Difficulty of Meaning Representations for Semantic Parsing"},{"paperId":null,"title":"Abstract syntax tree implementation idioms"},{"paperId":null,"title":"Decomposition enhances reasoning via self-evaluation"},{"paperId":null,"title":"A theorem-driven question"}],"id":"13ac3b7f1ace63c76c1e081898369f8e0505411c","summary":"This paper presents a method called Boosting Numerical Reason\\textbfing by Decomposing the Generation of Equations (Bridge), which can improve the accuracy of LLMs in generating equations as IMRs by reducing the tendency of generating constant expressions and programs."}]}