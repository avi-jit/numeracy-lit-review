{"papers":[{"url":"https://www.semanticscholar.org/paper/c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations","venue":"North American Chapter of the Association for Computational Linguistics","year":2018,"referenceCount":14,"citationCount":1493,"influentialCitationCount":176,"publicationDate":"06/03/2018","authors":"Peter Shaw,Jakob Uszkoreit,Ashish Vaswani","citations":[{"paperId":"fe518b7bfc287b5cefef817095ef4aab29696181","title":"Improving attention network to realize joint extraction for the construction of equipment knowledge graph"},{"paperId":"4dc935e258ad9df3a76958d9824073c52155ff35","title":"DETR Doesn't Need Multi-Scale or Locality Design"},{"paperId":"625d5162adbc5db195fdb4e1d9f625c309a7e6b1","title":"Lightweight Convolutional-iConformer for Sound Event Detection"},{"paperId":"2ab3e082ee8a90d2a452c2a257915ff70d0b8911","title":"A Survey on Data-Driven Runoff Forecasting Models Based on Neural Networks"},{"paperId":"7da05f84e16a6003f7167ec004c413f3b1e91fa2","title":"Don't be so negative! Score-based Generative Modeling with Oracle-assisted Guidance"},{"paperId":"07bc02bd16f6fe78a7ea3bb8d966fcc6e3893195","title":"Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners"},{"paperId":"16be9876eebcda031c15efaf4cfe3ba49146b329","title":"ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis"},{"paperId":"58d1a002a0ff0aa40b6633f0a7073d48f1cdff53","title":"Empower Your Model with Longer and Better Context Comprehension"},{"paperId":"e54c859515b68c5c7a56f66382b9e9605953df0b","title":"3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding"},{"paperId":"3d04f5bb0599ce02d8fa47420f72f500758c4660","title":"How to Scale Your EMA"},{"paperId":"0fa35c54e165f3c8f30a40bf05f4e60cfd93267e","title":"Chinese medical entity recognition based on the dual-branch TENER model"},{"paperId":"9e3e56957e249cdebdd8673fd1174980ed694560","title":"Efficient Beam Tree Recursion"},{"paperId":"6b924da91712d0972dc98e778b2683912196308f","title":"Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model"},{"paperId":"8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8","title":"Linearized Relative Positional Encoding"},{"paperId":"f490898ec8363d9dd0040479ed392745ff40c425","title":"ECSIC: Epipolar Cross Attention for Stereo Image Compression"},{"paperId":"453e3b4e7ee2b0a8ad0a4fc14467a29c347fff69","title":"A Review of Text Sentiment Analysis Methods and Applications"},{"paperId":"b550e855522f05816dafe39ad521c655b7f84664","title":"Transformers are Universal Predictors"},{"paperId":"473d5b35859b1515e99564a44547426298947cc8","title":"Learning to Identify Graphs from Node Trajectories in Multi-Robot Networks"},{"paperId":"9acd6d8b695187831222d94277f3ffcf1d561d10","title":"CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training"},{"paperId":"b036e281c6d73d352b963e97f3b36bec549837f8","title":"VampNet: Music Generation via Masked Acoustic Token Modeling"},{"paperId":"177c3a4fc5e6603f09b08568e5f93bf78bcafe21","title":"The Ethical Implications of Generative Audio Models: A Systematic Literature Review"},{"paperId":"c4d4f66681b99a51908fa06d7d5fc7f534d11cd8","title":"LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias"},{"paperId":"76e8dd7acb3a9e4ae65189817bfa7e1adedf805a","title":"Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation"},{"paperId":"956697da5f282c1bc89b19b4127f7c040aa2268e","title":"Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation"},{"paperId":"764299bcfdbc16361e57e3045a3921f284adf399","title":"Self-attention mechanism at the token level: Gradient analysis and algorithm optimization"},{"paperId":"7275318008bad0c141d3516a25903b37014590a1","title":"Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation"},{"paperId":"5d433a43aa8b5c4687e9edebf2fac6e8e248bb10","title":"Predicting Music Hierarchies with a Graph-Based Neural Decoder"},{"paperId":"fecbf14c6afa8d17a0cae8697db8abbadb7abe7d","title":"A Hybrid System for Systematic Generalization in Simple Arithmetic Problems"},{"paperId":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers"},{"paperId":"d560d653544a23347cb8ab17b44c3f2cc85b3e7b","title":"Exploiting Pseudo Future Contexts for Emotion Recognition in Conversations"},{"paperId":"5aeecb086d34368a5539e449d5310ed114a29b8a","title":"Toward a Spectral Foundation Model: An Attention-Based Approach with Domain-Inspired Fine-Tuning and Wavelength Parameterization"},{"paperId":"d8088d6b45e3ae523b9472745971f698a3b1d7f5","title":"SKIER: A Symbolic Knowledge Integrated Model for Conversational Emotion Recognition"},{"paperId":"df6ba91e1a63ba62d348912ee1983358c642d3b4","title":"AI in drug discovery and its clinical relevance"},{"paperId":"92630c3017c9a878dbed0178e66a9498abe2daaf","title":"Sumformer: recursive positional encoding for transformer in short text classification"},{"paperId":"5c828011a508611df4d58cced9cc48d049dc4eb9","title":"A Data Source for Reasoning Embodied Agents"},{"paperId":"a6159daf277e73ca511da98a0d05432f6bab0de7","title":"LightGlue: Local Feature Matching at Light Speed"},{"paperId":"d2d0371158803df93a249c9f7237ffd79b875816","title":"Sparse Modular Activation for Efficient Sequence Modeling"},{"paperId":"072e2deab443a135d922f7cb03b8b1f274712ec2","title":"A Transformer-Based Cross-Window Aggregated Attentional Image Inpainting Model"},{"paperId":"a249bc9a63fcd2f39d0b61c02221ce46f306cb70","title":"A transformers-based approach for fine and coarse-grained classification and generation of MIDI songs and soundtracks"},{"paperId":"ea3a73be965c6ddc85a6d90dad06d20d287909b5","title":"Passive Vital Sign Monitoring via Facial Vibrations Leveraging AR/VR Headsets"},{"paperId":"e23ea2db9561cad034a77eb00e661cf04b89d539","title":"Generating Questions via Unexploited OCR Texts: Prompt-Based Data Augmentation for TextVQA"},{"paperId":"ba591af0202f58875ab5b51c10c2e8dde50e8e68","title":"A Semantic and Structural Transformer for Code Summarization Generation"},{"paperId":"400cee04737f6a2012a2677acd8c052d233335eb","title":"Understanding Parameter Sharing in Transformers"},{"paperId":"5654655944c4a07646dfd280bdb819080ad35fa6","title":"Research on Named Entity Recognition in Improved transformer with R-Drop structure"},{"paperId":"9ced9dac3aca804522f474d39595b4c44d277303","title":"Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series"},{"paperId":"906ff01d437489fb505e634b956cdb7990c2547a","title":"Partially Trained Music Generation based on Transformer"},{"paperId":"a9a46d098a453d635355517b7d977c260c48d8ff","title":"PauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and Pause-based Prosody Modeling"},{"paperId":"345ba0a4fad9c762d87ef0268c3c047f774e9b5c","title":"SSIN: Self-Supervised Learning for Rainfall Spatial Interpolation"},{"paperId":"12d1ea06e103b0c0059a9388a0b7157525e93f4a","title":"HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models"},{"paperId":"c5e3e543d55a145fe4c7e5dd7fd97b9f30054f46","title":"E(2)-Equivariant Vision Transformer"},{"paperId":"7d22e6d110a2be18ef7236fb2238fd85463de4b2","title":"DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles"},{"paperId":"f03fa16bee6b7296ea86fa4fc3a0416ef135f77d","title":"Improving Long Context Document-Level Machine Translation"},{"paperId":"b8e57e8c81877d18998413b3e3c724fe142bb34e","title":"CoAt-Mixer: Self-attention deep learning framework for left ventricular hypertrophy using electrocardiography"},{"paperId":"6321a92d3e4fbe34ed6688da0a1164528d053092","title":"Learning Probabilistic Coordinate Fields for Robust Correspondences"},{"paperId":"2d89a84314f1b83e58f3cb4088bd4b95663b40c7","title":"Everybody Compose: Deep Beats To Music"},{"paperId":"a559a59ac6bc9c7c90caec294216de11a3e78703","title":"Human-imperceptible, Machine-recognizable Images"},{"paperId":"ccef05840870479632f9055479a1269d53033b7b","title":"Certified Reasoning with Language Models"},{"paperId":"c7889e7180490b7f829777f6cc97bbb4686671a8","title":"A2B: Anchor to Barycentric Coordinate for Robust Correspondence"},{"paperId":"357c9cbdcd164d86d3a559ef9bab750c996da13b","title":"Classification of Edge-dependent Labels of Nodes in Hypergraphs"},{"paperId":"81cac3c2a2eccf20dac0489f8a16e06f98a2d506","title":"Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning"},{"paperId":"fc1774419f418fb79451c0bde5d64ae4ce8d5b31","title":"Systematic Visual Reasoning through Object-Centric Relational Abstraction"},{"paperId":"425f4f6f96d6066467ab640b29dd444a7bbdfa92","title":"Mixed-former: multi-fusion remote sensing change detection"},{"paperId":"963cf5270b0ecf4d1f4b7f0477977604f6ea6f9b","title":"The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles"},{"paperId":"14347ae05bebfd9cb594ca2d2ebf07dd179962b7","title":"Data-Efficient French Language Modeling with CamemBERTa"},{"paperId":"9b417791b2e908d1e2a5486578642da19a0817b7","title":"Effect of Feedback on Drug Consumption Disclosures on Social Media"},{"paperId":"52b10ae66d025e99fbb602935e155f97f4f0696f","title":"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance"},{"paperId":"fb70eac98eb79ce1639b2cd8db84f4f805cffccd","title":"An End-to-End Blind Image Quality Assessment Method Using a Recurrent Network and Self-Attention"},{"paperId":"df55f9f174bd2343356ea43dd3ef2a6ac6951b59","title":"FaceFormer: Aggregating Global and Local Representation for Face Hallucination"},{"paperId":"4fac904a7f23d69db347441036cb9d5e44a233ab","title":"GPT based model with Relative Attention for De Novo Drug Design"},{"paperId":"08e5d8d1a9d481204b4caff58490755f59b0e88f","title":"Monotonic Location Attention for Length Generalization"},{"paperId":"6f6e2e0311589a9af045f6acd00b7dee6d19fce4","title":"The Impact of Positional Encoding on Length Generalization in Transformers"},{"paperId":"93be625e495c685301023774647d4446beeabc33","title":"Inverse-design of nonlinear mechanical metamaterials via video denoising diffusion models"},{"paperId":"1355d894cc9a5c9aaf7b1933cb582bbacc148943","title":"RINGER: Rapid Conformer Generation for Macrocycles with Sequence-Conditioned Internal Coordinate Diffusion"},{"paperId":"ad4be183eef6aa8d81b3a7cf3c06250e5b983583","title":"SimpSON: Simplifying Photo Cleanup with Single-Click Distracting Object Segmentation Network"},{"paperId":"119a3ed0898499fce0ce6af6958d566d82390ba5","title":"GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning"},{"paperId":"16e2cabe68ad1d651b21d0e7d1cea6e0a293741c","title":"Identity-Guided Spatial Attention for Vehicle Re-Identification"},{"paperId":"905a401efd96d4a5705f64b67cef486b43e83e20","title":"Improving Position Encoding of Transformers for Multivariate Time Series Classification"},{"paperId":"b4ee74a1f294cb5bb028a958ce43219cabb0f347","title":"TranSFormer: Slow-Fast Transformer for Machine Translation"},{"paperId":"43972d51d944c6a1499d1bebd4421768a872c379","title":"FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models"},{"paperId":"8556212e1f4ebfd7df71dc7f0cfdc88f0995d5e6","title":"Research on abnormal detection of gas load based on LSTM-WGAN"},{"paperId":"639977efc7e5e6f0e7c67cfd25536501748e6c5b","title":"Duplex Diffusion Models Improve Speech-to-Speech Translation"},{"paperId":"0d4354ed28ff3924a8cc39725b11587ab5aa4aae","title":"RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration"},{"paperId":"0bec2d1e1ff6d2b11ca2dad729cbfe13f3bc07aa","title":"Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness"},{"paperId":"cb05d0ec00a49afce080eb537131adac7b9ca846","title":"Learning Sequence Descriptor based on Spatiotemporal Attention for Visual Place Recognition"},{"paperId":"279a2ba6f22a543e8235fe95603f034d11adc6ad","title":"Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization"},{"paperId":"b91083c6cb7490f039b22319b5932f7138282593","title":"Deep Multiple Instance Learning with Distance-Aware Self-Attention"},{"paperId":"9fe29c834afbe1848d9df713ae6e0ca3bd053605","title":"Probing the Role of Positional Information in Vision-Language Models"},{"paperId":"45a647629c5f9e58ad4463ba9b9087445720e2fc","title":"Five A+ Network: You Only Need 9K Parameters for Underwater Image Enhancement"},{"paperId":"79c27ab1c7b37e539e343611ecf149ce63446e5c","title":"MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation"},{"paperId":"02bdb2ba006344464ead68ad672a51327cddf21d","title":"Toward Moiré-Free and Detail-Preserving Demosaicking"},{"paperId":"be20e7d79f494cab1e3cef573c138ea262ecf533","title":"GCNSLIM: Graph Convolutional Network with Sparse Linear Methods for E-government Service Recommendation"},{"paperId":"04b7ad34b7389fe8730178a13b176defc00fc7f3","title":"Self-Supervised Sentence Compression for Meeting Summarization"},{"paperId":"8e65c94ba0664735bfe15c8dea4baf69a3a4d905","title":"Meta-Polyp: A Baseline for Efficient Polyp Segmentation"},{"paperId":"2f16e71bed0a4ec47ceee98dffd80a13fb6af092","title":"Sinogram Domain Angular Upsampling of Sparse-View Micro-CT with Dense Residual Hierarchical Transformer and Noise-Aware Loss"},{"paperId":"4755545bdd5c3eaef08cd0f5a45ccde5f03b93b2","title":"RAANet: Residual Aggregation Attention Network for Classification of Small Intestinal Endoscopic Images"},{"paperId":"29bce162bd5b83cbc4c00732d03844b33b8a6007","title":"QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing"},{"paperId":"901de03bb54f7025f11340ed24252a48fa8415bc","title":"What should be encoded by position embedding for neural network language models?"},{"paperId":"5e8427b13f8691697564b565f9d03f3e3e294b0b","title":"Understanding Gaussian Attention Bias of Vision Transformers Using Effective Receptive Fields"},{"paperId":"f35f5aedc30e2c5ded210d9c91ba6e84bd029425","title":"Toeplitz Neural Network for Sequence Modeling"},{"paperId":"53f59f389ed55b1576f65058123f5234f65b576e","title":"Laziness Is a Virtue When It Comes to Compositionality in Neural Semantic Parsing"},{"paperId":"bb624a2a88a3c960d5b59360baf9a8259cbcfa81","title":"Hierarchical Transformer for Scalable Graph Learning"},{"paperId":"82e0cae01a57fd1d18f80dffd6367f85a4c92536","title":"Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks"},{"paperId":"3bbc8841012fdb5971d1c86dff528edd8590f1b8","title":"SLTUNET: A Simple Unified Model for Sign Language Translation"},{"paperId":"b02ea0ddcc8515fdb94ffc5c4355132335ef71b4","title":"An Extensive Study of the Structure Features in Transformer-based Code Semantic Summarization"},{"paperId":"6fa1e9593fdfbc0688e6bb53ee19228bf2232aa7","title":"Self-attention enhanced deep residual network for spatial image steganalysis"},{"paperId":"57c289c912c43ab6f2388766a0329d89bbff1534","title":"VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning"},{"paperId":"631e3879b7daa24da4014a797850146bfd9f7926","title":"Cross-Shaped Windows Transformer with Self-supervised Pretraining for Clinically Significant Prostate Cancer Detection in Bi-parametric MRI"},{"paperId":"74fa10978af75a3af8dc87eb5b3b91e6aa9cbb5e","title":"Comparison and analysis of computer vision models based on images of catamount and canid"},{"paperId":"5c371638a8a8382bd885dc45629ccf3d9a06d4e1","title":"Learning Neural PDE Solvers with Parameter-Guided Channel Attention"},{"paperId":"b5416fc248f3dbcece522ef59f571a1f56e5e9a4","title":"Technical Report on Token Position Bias in Transformers"},{"paperId":"b4d2d8ba34a98d71af861bfc5a11f630bd66b8db","title":"Streamlined Global and Local Features Combinator (SGLC) for High Resolution Image Dehazing"},{"paperId":"8dc22bea5ddb1b3e4d5f522d3e6dbf9ebf6d531e","title":"Img2Vec: A Teacher of High Token-Diversity Helps Masked AutoEncoders"},{"paperId":"8a94922c7d45df945a41b950c8d05f23c71001cf","title":"MMST: A Multi-Modal Ground-Based Cloud Image Classification Method"},{"paperId":"753ca5e059089a3eed0872c5caadad6f05f502e7","title":"SACANet: scene-aware class attention network for semantic segmentation of remote sensing images"},{"paperId":"c07d5683f4e9587be87ee01a74a0ab8b53656b89","title":"Multi-Modal Deep Learning for Credit Rating Prediction Using Text and Numerical Data Streams"},{"paperId":"458c8a10ec37466a6a554202c472e8faea0834ad","title":"Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers"},{"paperId":"850b8f31a1bb762544bd35163923784a664b315a","title":"Prompt-Learning for Cross-Lingual Relation Extraction"},{"paperId":"a955eeda762dce23d254fb76e1071ebaf7d78fce","title":"STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning"},{"paperId":"ad426f36a41d03e97b09108b5c50260efff7deb5","title":"Region-Enhanced Feature Learning for Scene Semantic Segmentation"},{"paperId":"c93fc6c8240453fc5cf5bff057547e67968e00a8","title":"Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding"},{"paperId":"ba7592b6052ad470eb75bd47659e27b58ed81631","title":"Dynamic Graph Representation Learning with Neural Networks: A Survey"},{"paperId":"b032f324a0d4a24fd917551345bd100dc368e41a","title":"Diffusion Models as Masked Autoencoders"},{"paperId":"79a574788b99f3a065cdc972c94497a921af5239","title":"Inductive biases in deep learning models for weather prediction"},{"paperId":"dc26a39c9a02d805dfd96742d66718e8d931e5c0","title":"OVERVIEW OF TRANSFORMER-BASED MODELS FOR MEDICAL IMAGE SEGMENTATION"},{"paperId":"6c9d243af44f8e8d0614d4951fe8672af84f6daa","title":"Ti-SSE: a time interval based sequential recommendation model via stochastic shared embeddings"},{"paperId":"2edb3554b2e8bd5ec09bdff391a938b4cf930997","title":"Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe Based Motion Interpolation"},{"paperId":"9f9f5f10e804cbec812a8f8639769e9a03dee44c","title":"TransCODE: Co-design of Transformers and Accelerators for Efficient Training and Inference"},{"paperId":"bb7d682e5f9c4a8985d93673a07a267f4080739a","title":"CoRe-Sleep: A Multimodal Fusion Framework for Time Series Robust to Imperfect Modalities"},{"paperId":"d11ee2658083e6dceccf6dfcf699aa5270f34dcb","title":"S-ViT: Sparse Vision Transformer for Accurate Face Recognition"},{"paperId":"b695d8e4edf66d7aa7a049cfb48038e35b0845b1","title":"EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms"},{"paperId":"3ce0a1ae648d6727ca8b336c15a1378e1cfd0969","title":"Plotting Behind the Scenes: Towards Learnable Game Engines"},{"paperId":"3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","title":"EVA-02: A Visual Representation for Neon Genesis"},{"paperId":"24bcdce51edb8e1174fbabd072a0c07bf7362d57","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"e6a39ffc46bf7ba050da9d803ce2da72d67f196c","title":"Digital future beyond pandemic outbreak: systematic review of the impact of COVID-19 outbreak on digital psychology"},{"paperId":"4b572031d82294d4392723b746518f8f53d9fe3f","title":"ViTO: Vision Transformer-Operator"},{"paperId":"513dd6422b5e3f839e34e3a77b58a92b48e433ad","title":"AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+"},{"paperId":"abe4ea0e1f1d04288406818048bc3f973d930ec9","title":"A comprehensive study of automatic video summarization techniques"},{"paperId":"2be8df3ffdb0095bc18acb5e3f189736ba93e308","title":"Stabilizing Transformer Training by Preventing Attention Entropy Collapse"},{"paperId":"edef246794a1eb2df69e9d6d1a3c72ca0516f124","title":"Xformer: Hybrid X-Shaped Transformer for Image Denoising"},{"paperId":"bb3faec0406af580a5c1dfe05bba59295a8272f7","title":"Unifying Layout Generation with a Decoupled Diffusion Model"},{"paperId":"c40ec51ddd4145402bd95eeb3ce6977778d87881","title":"Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling"},{"paperId":"4a6bf17538ded4eae48629728ee17adebb688094","title":"EG-TransUNet: a transformer-based U-Net with enhanced and guided models for biomedical image segmentation"},{"paperId":"86ddf98d18ef3839e97f7d7ae6d017610d4054f7","title":"Less Is More: Brain Functional Connectivity Empowered Generalizable Intention Classification With Task-Relevant Channel Selection"},{"paperId":"3c8b0cfd1bdf8aac541f033181aac8b005417d35","title":"Cross-Temporal Snapshot Alignment for Dynamic Networks"},{"paperId":"a0d0002be16a1b70dcbb4b152813da55a81c2201","title":"Diffusing Graph Attention"},{"paperId":"e48e7b8c8faa8705c5d1155c12d403f1c13653b6","title":"ADE-CycleGAN: A Detail Enhanced Image Dehazing CycleGAN Network"},{"paperId":"b596fb7e6fcb32344f47ebe945a6c493454dbf7b","title":"Slice-Based Code Change Representation Learning"},{"paperId":"cc5e79419a0bfaa067e897d35c23a69774d7c2b5","title":"A Novel Cross Grouping CG MLP based on local mechanism"},{"paperId":"c4a10d06a6919137f8507dfd3a57f33a3dbeaa20","title":"Applying Plain Transformers to Real-World Point Clouds"},{"paperId":"43d8c95ae1c975657771bc6dd58c747c5cf6c874","title":"A Novel Transformer-Based IMU Self-Calibration Approach through On-Board RGB Camera for UAV Flight Stabilization"},{"paperId":"7eaf4a0fe5a4199c109dc8ae9add981dbeba1e69","title":"Research on Chinese traditional opera costume recognition based on improved YOLOv5"},{"paperId":"5c9ccbeaeadffa6ae1a966d52e3373c6b525f971","title":"SATBA: An Invisible Backdoor Attack Based On Spatial Attention"},{"paperId":"b45ea6ddc9d964116addaf1dafd0641d78a6228e","title":"Sequential Query Encoding For Complex Query Answering on Knowledge Graphs"},{"paperId":"44ca4241feef086a19e90902ba1ad27ed2f82edd","title":"Named entity recognition for Chinese based on global pointer and adversarial training"},{"paperId":"b94f2e22dd30bb9e016e88e053c91e2794213b82","title":"Dual channel Chinese sentiment analysis of characters and words based on deep learning"},{"paperId":"77786eda28e5f0b25d682c27846334d53daf43e4","title":"Complex-Valued Relative Positional Encodings for Transformer"},{"paperId":"5dfe7236c85ce5569be48a4f567b85d0ebd7e5be","title":"Spatially Aware Transformer Networks for Contextual Prediction of Diabetic Nephropathy Progression from Whole Slide Images"},{"paperId":"a6f9da52f9ef80249aaf9e3a54e0afb6d35c7ac0","title":"Embeddings for Tabular Data: A Survey"},{"paperId":"1592dbe6a111ce94fcb562fc7d910978a8a5ca1e","title":"Reference aware attention based medical image diagnosis"},{"paperId":"ade3705d102ffb574bd355e58f7f8066f2aa73fc","title":"The Virtual Sleep Lab—A Novel Method for Accurate Four-Class Sleep Staging Using Heart-Rate Variability from Low-Cost Wearables"},{"paperId":"b8c236dc5963dac36b0d8e419beb5876e3a18f96","title":"Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation"},{"paperId":"4d1be5f81204f968a34a08975fece5f626f79618","title":"Learning Language Representations with Logical Inductive Bias"},{"paperId":"e5469d770062491dd9e2b27d77c4295aa296809a","title":"GH-DDM: the generalized hybrid denoising diffusion model for medical image generation"},{"paperId":"b621b2ecbeef82cee82570297700a8dcd4cfe94e","title":"Semiconductor Fab Scheduling with Self-Supervised and Reinforcement Learning"},{"paperId":"9e449d611bec60ff2e3faaec38dce3b79abac44e","title":"Enhancing Multivariate Time Series Classifiers through Self-Attention and Relative Positioning Infusion"},{"paperId":"c7aa92b77f34d5603c8b0c7b09443c2afa430ed5","title":"Multilayer self‐attention residual network for code search"},{"paperId":"f2909fcd0a1c265097490ce43f5065ef6486310d","title":"Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters"},{"paperId":"6d51f4b220cb2c8321dc5f9755b7d66f10f1cad6","title":"RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL"},{"paperId":"03541e93629055228357814c4c59f87b6090fc23","title":"Single Motion Diffusion"},{"paperId":"f0e4a66a73cc2f8a8a1a4d9fb041bf9be19f97f6","title":"Predicting Synthetic Lethality in Human Cancers via Knowledge Graph Summarization"},{"paperId":"c159eff49a643f305d6bfbb39951d873f911b794","title":"Target detection algorithm based on multilayer attention mechanism-adaptive feature fusion network"},{"paperId":"7bd7539abdc3d696d75213d4011327218f79ce21","title":"Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames"},{"paperId":"7aaf4014eb09ce6dfb6b0adf6593f84ed37e08b4","title":"Augmented FCN: rethinking context modeling for semantic segmentation"},{"paperId":"d0b1e6e636236855417a7ca4d2b3cdb72feb9a5c","title":"PhysFormer++: Facial Video-Based Physiological Measurement with SlowFast Temporal Difference Transformer"},{"paperId":"1e8fcf495dbc386591fcbab75df75ac41a503859","title":"Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation"},{"paperId":"d30c21212e0e1165b2e3a05f9bc31ce8b49e5f0a","title":"Knowledge Distillation in Vision Transformers: A Critical Review"},{"paperId":"17b5d59d4b9fdd1d879f9ac3cca224ae2e0eb464","title":"Iterative convolutional enhancing self-attention Hawkes process with time relative position encoding"},{"paperId":"4ce987d4f8ae0f4680808c318980d42a82b9aa89","title":"Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers"},{"paperId":"e7a6c58afcb0ce07baa21c75a8a4fae0e3b75945","title":"Learning Neighbor User Intention on User-Item Interaction Graphs for Better Sequential Recommendation"},{"paperId":"90ebe1cc509cb80f7e6324f81226562f222f912c","title":"More than Syntaxes: Investigating Semantics to Zero-shot Cross-lingual Relation Extraction and Event Argument Role Labelling"},{"paperId":"7c3fd26cf7eb1a9e6b286c567bd0055b47638d07","title":"FPT: Fine-Grained Detection of Driver Distraction Based on the Feature Pyramid Vision Transformer"},{"paperId":"6fb8f8977101ddbb7fd747feb1ae3449f6b9f6e8","title":"HCTNet: A hybrid CNN-transformer network for breast ultrasound image segmentation"},{"paperId":"c341106a1b5490284cc36c44a1a16b815e76d91a","title":"WLiT: Windows and Linear Transformer for Video Action Recognition"},{"paperId":"75844016491add33e7452f8ded95bd1b62a04f21","title":"Relation-mining self-attention network for skeleton-based human action recognition"},{"paperId":"e18748951afdd39d451acf8fbd3133c4c6619322","title":"COVAD: Content-Oriented Video Anomaly Detection using a Self-Attention based Deep Learning Model"},{"paperId":"c4fcaf7fc7d9ff791f8e5a5f1b724940835a3caf","title":"Predicting Physiological Response in Heart Failure Management: A Graph Representation Learning Approach using Electronic Health Records"},{"paperId":"9529f886486713c27df1d32c58d615e3ad332d82","title":"A Survey on Low-Latency DNN-Based Speech Enhancement"},{"paperId":"a7f3d8fa8c05b4533e652e965593ffad99803bd8","title":"Variation-Aware Semantic Image Synthesis"},{"paperId":"ee224cb6ecd14515490504717ed15ff098229f48","title":"DeepTP: A Deep Learning Model for Thermophilic Protein Prediction"},{"paperId":"7195ed3c7f11220f29634cecb68b1d39db2e36d9","title":"Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness"},{"paperId":"0e84679cf0945a2868245ba2be68c90453e48f2e","title":"Screen Correspondence: Mapping Interchangeable Elements between UIs"},{"paperId":"2d33f48e9839e4d25fc2d75c5117e175d7e9a347","title":"Multi-task Label-wise Transformer for Chinese Named Entity Recognition"},{"paperId":"2129788d72f30343c3f082d8814bbb44c299b346","title":"LSTM-SN: complex text classifying with LSTM fusion social network"},{"paperId":"3985999efd7ff8505e412b6e73ad8a7dd2db1132","title":"PromptShots at the FinNLP-2022 ERAI Task: Pairwise Comparison and Unsupervised Ranking"},{"paperId":"97899e818cc83efba48a5e9892dfa0a6a6e1d274","title":"Enhancing the robustness of vision transformer defense against adversarial attacks based on squeeze-and-excitation module"},{"paperId":"5435ed7c26f0c250493f244acffb69dd929d116b","title":"Structured Case-based Reasoning for Inference-time Adaptation of Text-to-SQL parsers"},{"paperId":"86f3b14e3229be8e06060f08b6e65fd031efbc50","title":"Untied Positional Encodings for Efficient Transformer-Based Speech Recognition"},{"paperId":"10422b7bcf17eae9b04d4a020a591b3a61b45593","title":"Towards Autoformalization of Mathematics and Code Correctness: Experiments with Elementary Proofs"},{"paperId":"8623d66cafbea0b45c0935897fa090fa277ee210","title":"Teaching Old DB Neu(ral) Tricks: Learning Embeddings on Multi-tabular Databases"},{"paperId":"b6e80e715784aa7cdddf519ef91b130a17aa958f","title":"SADLN: Self-attention based deep learning network of integrating multi-omics data for cancer subtype recognition"},{"paperId":"87430e72a28d048221850e23297f9ed225c2c21c","title":"Modeling the Rhythm from Lyrics for Melody Generation of Pop Song"},{"paperId":"0261af3e2c668849a771b6f667c74a4339da5052","title":"MFCFlow: A Motion Feature Compensated Multi-Frame Recurrent Network for Optical Flow Estimation"},{"paperId":"9c082d9c217e0e1014da3b24ffe7569924724067","title":"Enhanced multihead self-attention block network for remote sensing image scene classification"},{"paperId":"6b9eadaa1cfc5bebc727be1228b6c9cb9195a75b","title":"Cross‐domain sentiment classification using decoding‐enhanced bidirectional encoder representations from transformers with disentangled attention"},{"paperId":"4f48df33374f5b5b7bd73776a668ada02fc6708a","title":"SSR-Net: A Spatial Structural Relation Network for Vehicle Re-identification"},{"paperId":"060cee8411181e8151ab1e3212b81528accd9b8b","title":"On Transforming Reinforcement Learning by Transformer: The Development Trajectory"},{"paperId":"9075eb79c8bcc130a9ba069b1dadb0c500bca36a","title":"Swin MAE: Masked Autoencoders for Small Datasets"},{"paperId":"8cac1a726a7fa1160ce86733e66ebba6c78b71eb","title":"Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal"},{"paperId":"e3d371bdc9ac9cecbfbbf03d20485efa17c81e36","title":"Optimizing Deep Transformers for Chinese-Thai Low-Resource Translation"},{"paperId":"55153dceb76a147f1080af72c7e502835208503e","title":"An Efficient and Intelligent Detection Method for Fabric Defects based on Improved YOLOv5"},{"paperId":"7ead0cae4e67f390b2eb0083117ea8ab90c53b47","title":"Landslide Susceptibility Mapping by Fusing Convolutional Neural Networks and Vision Transformer"},{"paperId":"5bdca463873f4f2dfc6897b6c5fd51e799bd76c7","title":"Generating music with sentiment using Transformer-GANs"},{"paperId":"9575afb5702bc33d7df14c48feeee5901ea00369","title":"A Length-Extrapolatable Transformer"},{"paperId":"e1d71a6958460b9a9e75d6c59dc7b10c8dbe198b","title":"Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments"},{"paperId":"90b58d561f0f2afc56b6c656e61ebbf78b238a71","title":"Multi-Level Attention-Based Domain Disentanglement for Bidirectional Cross-Domain Recommendation"},{"paperId":"f4847d928cdb071f0abc5683ca63d6bc38bcecb0","title":"Auto-Regressive Self-Attention Models for Diagnosis Prediction on Electronic Health Records"},{"paperId":"fe5f1760fceee44846998e0714f50df8b5e9f154","title":"LLU-Swin: Low-Light Image Enhancement with U-shaped Swin Transformer"},{"paperId":"df6e42e06d77ebc28b50492b1fd26b39c1bd4277","title":"Convolution-Enhanced Evolving Attention Networks"},{"paperId":"f7884dca33438307fd384d4a246665cff188faec","title":"Spatio-Temporal Graph-TCN Neural Network for Traffic Flow Prediction"},{"paperId":"9c2f756a1401e657aaedecc7832364af09e1e3ba","title":"Solar Filament Segmentation Based on AA-UNet"},{"paperId":"661e8d555c4424b5953f17434f2ba910bfcf3afe","title":"Efficient Long Sequence Modeling via State Space Augmented Transformer"},{"paperId":"ccd30015e71d413ab34f3d306f87da892fb3c64c","title":"An End-to-End Heart Rate Estimation Scheme Using Divided Space-Time Attention"},{"paperId":"410a958c4b51650e055c9a2c5bd9e9a7fa2162d4","title":"Bridging Graph Position Encodings for Transformers with Weighted Graph-Walking Automata"},{"paperId":"1a0975aeefaa2fc98059e3c63065c4294028c565","title":"CNN-transformer mixed model for object detection"},{"paperId":"6498252603503c4ceab80ec40c4e86a0943a94c1","title":"Dual-stream Self-attention Network for Image Captioning"},{"paperId":"f6d381941e2964872a283d5290a390126115ad03","title":"P-Transformer: Towards Better Document-to-Document Neural Machine Translation"},{"paperId":"4a43a773f0725c998c65f2d51ddd49d5919183f7","title":"Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning"},{"paperId":"e4555f6325b0401f6ec430414c03ad124e50c002","title":"Position Embedding Needs an Independent Layer Normalization"},{"paperId":"97b9c17847c36a34ae059eb19902dc9f801a4249","title":"Expanding Intra-class Difference and Boosting Frame-level Classification for Continuous Sign Language Recognition"},{"paperId":"bb1486013aedbbb4f9a960c83731fbb91a04b1f3","title":"Mitigation of Spatial Nonstationarity with Vision Transformers"},{"paperId":"1f0739e299d35a2f8089dd5570fd2fca9c066fbf","title":"Multi-source Interaction Network for TextVQA"},{"paperId":"ec0299281353f93875b481984c2f54347c3c6d2d","title":"Rec-AUNet: Attentive UNet for Reconstruction of ECG from BCG"},{"paperId":"690558178fa6829f1e08552b9636f9feebac93c1","title":"Boosting Neural Networks to Decompile Optimized Binaries"},{"paperId":"1c7bdad6bdcee8da7305646b60ab8776eeb43f85","title":"Human Health Activity Intelligence Based on mmWave Sensing and Attention Learning"},{"paperId":"aec9ae3c0e5a5784e171dbb7aa58c209de89e404","title":"A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling"},{"paperId":"376345572946f8710e265e10be270373587d4c4e","title":"Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning"},{"paperId":"934f03956a5092345ea941df915d79358ddd2581","title":"CLeBPI: Contrastive Learning for Bug Priority Inference"},{"paperId":"457f4e838790a04173b0cd4ede8852cbf0d8cefe","title":"An Optimized Method for Large-Scale Pre-Training in Symbolic Music"},{"paperId":"15a591ec89cf2548f405af6aee887074096ae435","title":"Video Prediction by Efficient Transformers"},{"paperId":"341beb3bc23d2aebb5a6c4eb2a44e4ff90936f0a","title":"ResFormer: Scaling ViTs with Multi-Resolution Training"},{"paperId":"704332d287bc78ac95fea3d90ec3945e9cec9ab3","title":"A survey on complex factual question answering"},{"paperId":"81d86d4270e20eb5e3e93637acc5b96c5e7d0bfd","title":"Improving the Quality of Machine Translation Using the Reverse Model"},{"paperId":"0833632548ef61df1aea98cd8f770f1209c8a964","title":"Visualization-Based Software Defect Prediction via Convolutional Neural Network with Global Self-Attention"},{"paperId":"b064def719fc5d8b5cadf8a3292f690c23806b21","title":"Motion In-Betweening via Two-Stage Transformers"},{"paperId":"7a20e80fdd7eb0acc30ee3a585c6e4580dbbff20","title":"Measurement of Investment activity in China based on Natural language processing technology"},{"paperId":"82774f03c39bf1232aa213e472e607ed49fb7abb","title":"Deepfake Detection with Spatio-Temporal Consistency and Attention"},{"paperId":"4125a258059f05dbbd393dd8e6d740a9272493b7","title":"Differentiable Short-Term Models for Efficient Online Learning and Prediction in Monophonic Music"},{"paperId":"2e935268b66e12dd310af5c715013147d454e560","title":"Mutual Exclusivity Training and Primitive Augmentation to Induce Compositionality"},{"paperId":"6787343183353807a2f896d8c29d082d611dba51","title":"Seformer: a long sequence time-series forecasting model based on binary position encoding and information transfer regularization"},{"paperId":"f1f78bbb3e146874501e3c52b56cff6abf731842","title":"Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges"},{"paperId":"5dfbca6d0d3d25e338482592213fa6e6e83737bb","title":"Special Vehicle Target Detection Based on Improved Yolov5 + Jetson NX"},{"paperId":"a49ec0f1e5f8673f2357ccfb3c0d1a4a64ff0e36","title":"Generating High Quality Titles in StackOverflow via Data Denoising Method"},{"paperId":"740c588d3983e1e46839c7d25560de7563efe4b7","title":"An Improved Method and Application of Recurrent Convolutional Neural Network With Self-attention Mechanism"},{"paperId":"d94e7aeab88d07f0b50b7866ceb5baa3c29be859","title":"NQE: N-ary Query Embedding for Complex Query Answering over Hyper-relational Knowledge Graphs"},{"paperId":"cee8a6e75d891145e6028f7562d84bb3727a69a0","title":"On the Typicality of Musical Sequences"},{"paperId":"38ec6fc80ccbf1dd8a7e66111ff37473f9edbad8","title":"A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding"},{"paperId":"8d6520112cf35d84cf680de38411ba84dfc4a4da","title":"Vision Transformer with Super Token Sampling"},{"paperId":"1c73635ec5636d0ac5b73cad545a54f131eefa00","title":"Application of Machine Learning Combined with Wireless Network in Design of Online Translation System"},{"paperId":"2838b70d09b9610abbc5fa3070ae1ca85b3a0cae","title":"A Unified Model for Tracking and Image-Video Detection Has More Power"},{"paperId":"0c0c5d607b0a7e90102c9e18ac5fce9e6df9ab9a","title":"Improved Cross-view Completion Pre-training for Stereo Matching"},{"paperId":"9bb4ac256a8edd931a202af2be9344771d8ce905","title":"Transformer Network with Self-Supervised Learning for Stenosis Detection in CT Angiography"},{"paperId":"313d893228532485752e6744c831fbe4e2c712da","title":"Hypergraph Transformer for Skeleton-based Action Recognition"},{"paperId":"0cf7d980b62555bbed101821742de9558bd7b0e0","title":"Parameter-Efficient Transformer with Hybrid Axial-Attention for Medical Image Segmentation"},{"paperId":"6782d501d8a1220436d7e1551b2c2c9bc73f126a","title":"ParCNetV2: Oversized Kernel with Enhanced Attention"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"8b22adf610de65da2e8832d2f29a9b8510004cb1","title":"Continuous Soft Pseudo-Labeling in ASR"},{"paperId":"43bab0d20e282e6d878c6279f2c1528981ed2a40","title":"Transformer Variational Wave Functions for Frustrated Quantum Spin Systems."},{"paperId":"4e06fa91a71690fddae3d4618e8ee25993d09dd1","title":"Cross-Attention is all you need: Real-Time Streaming Transformers for Personalised Speech Enhancement"},{"paperId":"e5bd79d935e63e6a70b6c5dd5f68ae7b170fbf6e","title":"VulRepair: a T5-based automated software vulnerability repair"},{"paperId":"7763db3e5cda340e63a818923e0e2849a6e80590","title":"Recurrent graph encoder for syntax-aware neural machine translation"},{"paperId":"2a5890d18f83e96634c5dbc927a0039394c8f394","title":"Improving Speech Prosody of Audiobook Text-to-Speech Synthesis with Acoustic and Textual Contexts"},{"paperId":"3e851661e1521c5353281cb6a86cf356aa5753a3","title":"Hybrid Pipeline for Building Arabic Tunisian Dialect-standard Arabic Neural Machine Translation Model from Scratch"},{"paperId":"c02aa9f62cc42630fa8ca60452664f5ee0fb9b00","title":"More Speaking or More Speakers?"},{"paperId":"344f93efa2d507d0920a443ffa33c8b26a61e4b8","title":"Classification of retinopathy of prematurity based on mixed attention"},{"paperId":"171e87462886d79656e6bab15714d766fbb185d4","title":"Deep-Learning Enabled Assessment of Neurocognitive Performance in Object Following in Mixed Reality"},{"paperId":"e9ea44f3d6ff674117e8c0337d95cfd64599ce5d","title":"Structured State Space Decoder for Speech Recognition and Synthesis"},{"paperId":"a8d8e5093c78a6664dcf7aa488febec1b118922a","title":"TransSizer: A Novel Transformer-Based Fast Gate Sizer"},{"paperId":"697452b24c3b884cff8f7dbbc9c8e62bc59e0fdf","title":"Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers"},{"paperId":"b6115e422e59c464db2d8257724c66827202b6c8","title":"Grafting Vision Transformers"},{"paperId":"c15d153a846103fca7c6a2a633d6605a9a701203","title":"Building Indonesian Dependency Parser Using Cross-lingual Transfer Learning"},{"paperId":"c12a38e4413afb3451b67a7a9d1760ce9248be25","title":"Meeting Record Generation Based on Hierarchical Decomposition Encoding"},{"paperId":"b6ed84e0e69fc87de61645c1a50bcc169c95b983","title":"Does Joint Training Really Help Cascaded Speech Translation?"},{"paperId":"6883cd3b16045551fba376610a834813b2a3140e","title":"Transformers over Directed Acyclic Graphs"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"a52a3aac68f4fd67b67ccf6839765403a101f681","title":"Adversarial Pretraining of Self-Supervised Deep Networks: Past, Present and Future"},{"paperId":"c524690e7b0a2ff6b2b7210d5c85e1fd0cf86781","title":"Revisiting Checkpoint Averaging for Neural Machine Translation"},{"paperId":"c10195a628f31315ec56c219e219b25d81bb2b7f","title":"Amos: An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale"},{"paperId":"bccbd500f2f8af6bdfb5ef3cf77c2b99d088ee4c","title":"LittleBird: Efficient Faster & Longer Transformer for Question Answering"},{"paperId":"cb36c3c85075ae1bcd48bfaea57b2212a46e383d","title":"Is Encoder-Decoder Redundant for Neural Machine Translation?"},{"paperId":"a6adef01397f9444272be46b2cfe86516ded4f8e","title":"Detecting Depression from Social Media Data as a Multiple-Instance Learning Task"},{"paperId":"1f8160b7e91b570e60e9cba2631cf0472cbc764c","title":"Continuous Pseudo-Labeling from the Start"},{"paperId":"6b0ea6a6869b5f4207a6a8672ba153b747975126","title":"TrajFormer: Efficient Trajectory Classification with Transformers"},{"paperId":"c85ed937562106a6c9016dbb42e2b48cc298cadc","title":"Efficient Learning with Pseudo Labels for Query Cost Estimation"},{"paperId":"0cfd0ce0c0ab737db8f717250bf0e62939dad444","title":"Disentangling Past-Future Modeling in Sequential Recommendation via Dual Networks"},{"paperId":"9fca53cdd498bd4a6806b4c76da525923f1787da","title":"Le-BEiT: A Local-Enhanced Self-Supervised Transformer for Semantic Segmentation of High Resolution Remote Sensing Images"},{"paperId":"3db61a38efc4cb4c0172065e07348b42a3fbd160","title":"Grid Synchronization using Machine Learning"},{"paperId":"8047495ec6e5cbf90e01364cc3a5a04d7895614c","title":"Local Embedding for Axial Attention"},{"paperId":"d5519f66e15297f9b13bfc49449ade8229f13193","title":"SAVE: Spatial-Attention Visual Exploration"},{"paperId":"0cd9fb453d71212f08937da7fe16b924f22bc536","title":"Learning to Jointly Transcribe and Subtitle for End-To-End Spontaneous Speech Recognition"},{"paperId":"687733531aa65ea28b8a389399d3cbce9c99cee8","title":"LSG Attention: Extrapolation of pretrained Transformers to long sequences"},{"paperId":"1f86bf1e334200ec0481349255559fbfe7a33caa","title":"MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"44fc5bc7cd0957e00159d588e433acfc3cbe7dbe","title":"Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries"},{"paperId":"18c129d202b5153ae5373a24d0deee2ded1ae7e0","title":"Percept U-Net: Percept Attention-based Convolutional Neural Network for Atrial Fibrillation Episode Localization"},{"paperId":"2919cfcfc4e48fe0a17ac18ef41949bdb1d50891","title":"Relational Attention: Generalizing Transformers for Graph-Structured Tasks"},{"paperId":"aca4df69152787da747b09a694ae226edf356676","title":"Designing Robust Transformers using Robust Kernel Density Estimation"},{"paperId":"7aa43fc01045d36c2babb3edd81b0147dfa8cb45","title":"Towards All Weather and Unobstructed Multi-Spectral Image Stitching: Algorithm and Benchmark"},{"paperId":"a0b4e5f73adb829f2b7c51eb373f0559c110e2ac","title":"Better Pre-Training by Reducing Representation Confusion"},{"paperId":"2c379f8106fabf2c3f0f1f2a4d4451cb7d664e6d","title":"Spatio-Temporal Attention-based Graph Convolution Networks for Traffic Prediction"},{"paperId":"e9ce0ca5f485e8ff4632693b76cdab3b64c34a84","title":"Latent Neural ODEs with Sparse Bayesian Multiple Shooting"},{"paperId":"628fb0736c5484f7a0006d0bb2d4a7a2ef9ae6b3","title":"SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training"},{"paperId":"03b7d2e942eafabd14b229f8962fa6e943053f75","title":"Melody Infilling with User-Provided Structural Context"},{"paperId":"43a37611194f9019dde2d1ef9c86b61e7dc555bb","title":"A Reverse Positional Encoding Multi-Head Attention-Based Neural Machine Translation Model for Arabic Dialects"},{"paperId":"61b9b5187e8c5ca0632d16d7c916a42784eca348","title":"Point Cloud Recognition with Position-to-Structure Attention Transformers"},{"paperId":"a8a2a8229f99c291bf71ec92b801a073854c52e2","title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models"},{"paperId":"4e1489d54bf802e588e08975a086108e3b6bf169","title":"Enhancing 3D-2D Representations for Convolution Occupancy Networks"},{"paperId":"fb5ed9d348ca6be8544f4bc10d19da2a7046fb35","title":"Extended context-based semantic communication system for text transmission"},{"paperId":"343e4883a3aa8ab7a1b5950f8f8fb8091a2a76be","title":"The encoding method of position embeddings in vision transformer"},{"paperId":"986d5cc495fcb02a598c6f2aa8750b0a610fc012","title":"nn-TransUNet: An Automatic Deep Learning Pipeline for Heart MRI Segmentation"},{"paperId":"504a2bca67b1b7cbd837f19c6f419ce12c37ffda","title":"Masked face recognition with convolutional visual self-attention network"},{"paperId":"6edada77cb8d008f9d026926a90ee71fe5063f78","title":"FGCNet: Fast Graph Convolution for Matching Features"},{"paperId":"f6e2ad0fe7fe6153a70a3e1cb1a612c27727bbc2","title":"Attention Constraint Mechanism through Auxiliary Attention"},{"paperId":"fd82a861a0bdb8693a5e3596516f1c0848a3d80a","title":"E-Branchformer: Branchformer with Enhanced Merging for Speech Recognition"},{"paperId":"15586fec836ffcb60eba81491e04c225e6914aac","title":"Protein structure generation via folding diffusion"},{"paperId":"562a3b8d6e9c5bd968907e828367397ad2dad28e","title":"SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data"},{"paperId":"9ae9e58cdd5975be4af3fac9a9e948adaf4bc4da","title":"Verifiable and Energy Efficient Medical Image Analysis with Quantised Self-attentive Deep Neural Networks"},{"paperId":"61d8390fffca540517e325dbfc1df27a1ecfbfe1","title":"Thermal Infrared Tracking Method Based on Efficient Global Information Perception"},{"paperId":"f882c90f6ec93f35ecdd2aed9d08673832d03cbb","title":"Improving Code Completion by Sequence Features and Structural Features"},{"paperId":"b056396085121d4c6c9678adde5ccd83e5c8aac1","title":"Taking a Respite from Representation Learning for Molecular Property Prediction"},{"paperId":"aee1142e10e9c702f6f96322500f9e8095000388","title":"S^2-Transformer for Mask-Aware Hyperspectral Image Reconstruction"},{"paperId":"607ab597edf96c0ed2b8b7f2b4fc25efcecef2f4","title":"RERG: Reinforced evidence reasoning with graph neural network for table-based fact verification"},{"paperId":"defeec6de4645db49820c6057682581ef16844be","title":"Genetic Algorithm-based Transformer Architecture Design for Neural Machine Translation"},{"paperId":"d513ace9bdd8ace3241491fcd563357ae0af49b7","title":"Automatic Classification of Software Bug Reports Based on LDA and Word2Vec"},{"paperId":"e901a9748feccb2eef7d9ff8959b060b94cb7d3a","title":"Beyond the Transformer: A Novel Polynomial Inherent Attention (PIA) Model and Its Great Impact on Neural Machine Translation"},{"paperId":"f49d9a6c4db99aaf12d4237044ca0770aac075ca","title":"Code samples summarization for knowledge exchange in developer community"},{"paperId":"28e7e17abff47b0236fe58de206038b7fb38cdbf","title":"ESPnet-ONNX: Bridging a Gap Between Research and Production"},{"paperId":"fb0f163627621f1879bacf917d93d749f07953d1","title":"Exploring vision transformer: classifying electron-microscopy pollen images with transformer"},{"paperId":"87297d4ae3aa78547c07b69ece13212338bffd41","title":"Dynamic Graph Message Passing Networks"},{"paperId":"c3e1bc1b9fb2d92c3b76b22ff666af42a75491f1","title":"WA-Transformer: Window Attention-based Transformer with Two-stage Strategy for Multi-task Audio Source Separation"},{"paperId":"1f5381823d48be4fa280d5cfbfd66512e258f580","title":"BiCAPT: Bidirectional Computer-Assisted Pronunciation Training with Normalizing Flows"},{"paperId":"d4d8db8bf134107e93b2d089a9d022e05066ce0c","title":"Self-Attention Based Time-Rating-Aware Context Recommender System"},{"paperId":"79b7f6965242af82405ad47647450f6684c0d121","title":"Parameter-Efficient Conformers via Sharing Sparsely-Gated Experts for End-to-End Speech Recognition"},{"paperId":"54e511310d40ae2ebb4448f92f27865df1b7bae1","title":"LogGD: Detecting Anomalies from System Logs with Graph Neural Networks"},{"paperId":"fd17b2b6fdd9a6d67c8534bf0a2507c99242f62d","title":"Graph-to-Text Generation with Dynamic Structure Pruning"},{"paperId":"7d51e0d810d7b4567c0f377860428b06ab3a6a14","title":"CAT-CPI: Combining CNN and transformer to learn compound image features for predicting compound-protein interactions"},{"paperId":"c2737d1165399f3826d14ab10e40d23aaee73edc","title":"Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention"},{"paperId":"9acc94aed6b6a6bf4ec5f4579999f175d40e3089","title":"Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation"},{"paperId":"5d3e28f9cc108202cf97a2aa80971dbe3d4f5b4a","title":"SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers"},{"paperId":"e420180e2b6da334ef2accf90b08087862cbff85","title":"TRQ3DNet: A 3D Quasi-Recurrent and Transformer Based Network for Hyperspectral Image Denoising"},{"paperId":"387f9bd2412c2b1154aa6c9a2156e1162cc5f2c9","title":"Semantic Representation Using Sub-Symbolic Knowledge in Commonsense Reasoning"},{"paperId":"34b43419f85f44751b28310d133d351c24dbead2","title":"Local-Aware Global Attention Network for Person Re-Identification"},{"paperId":"e7c835eeb56f28e043dbe5af866cd1e1f9b31b12","title":"An End-to-End Mutually Interactive Emotion–Cause Pair Extractor via Soft Sharing"},{"paperId":"3ea15bb1e2956c54b141b07e0cda933d3c94ded2","title":"DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation"},{"paperId":"5ce9641a5caf7fa5377edb68b449185a281eddce","title":"Structural Bias for Aspect Sentiment Triplet Extraction"},{"paperId":"35466623d3f0a8426e13d349d8260bfc83948d75","title":"A generalized-template-based graph neural network for accurate organic reactivity prediction"},{"paperId":"c2efc25fa1a2b0192b95dc92d9dccf90e602c74c","title":"Efficient Methods for Natural Language Processing: A Survey"},{"paperId":"e05aca5f3d24f250596fb7a7d8cc008f0478dfaf","title":"ELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer"},{"paperId":"cdc027a961da229e870be3ac41a429f4c165788d","title":"Modeling Spatial Trajectories Using Coarse-Grained Smartphone Logs"},{"paperId":"c8483055b1dbef7e23bf31f544fde4a2b1de417e","title":"Time-aware Self-Attention Meets Logic Reasoning in Recommender Systems"},{"paperId":"1aac692ca061feb846cf32cd61a1d422f89593a1","title":"A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions"},{"paperId":"6e7b5fca044ac77e1572591988f4be4b0f621358","title":"Progressive Transformer Machine for Natural Character Reenactment"},{"paperId":"623fe930267bdcbc50637298ea6a72969edd05a9","title":"K-Order Graph-oriented Transformer with GraAttention for 3D Pose and Shape Estimation"},{"paperId":"1ec5f71509c1fe981f751eb4b48b09ee62d64ca2","title":"Flat Multi-modal Interaction Transformer for Named Entity Recognition"},{"paperId":"39b1b104ff08aaea959a9a58dc8b4d158b4e9ab4","title":"Efficient Attention-free Video Shift Transformers"},{"paperId":"1f621d02b124f0e2d8aa2d71c831deb5ea0ee834","title":"PointTransformer: Encoding Human Local Features for Small Target Detection"},{"paperId":"a555364fdd7c8ae0d20d1d506501dcc253e0a16f","title":"ATLAS-MVSNet: Attention Layers for Feature Extraction and Cost Volume Regularization in Multi-View Stereo"},{"paperId":"025be76b0fbc593f74fff00b61b470bf2df40c23","title":"Deep Guided Context-aware Network for Anomaly Detection in Musculoskeletal Radiographs"},{"paperId":"2482e19a2e77d106deb7550144ff9d2cc910d5e2","title":"Accelerating Vision Transformer Training via a Patch Sampling Schedule"},{"paperId":"5c092c7bb0f9deecebb365dc79e52cb1e24614cc","title":"Position-aware Structure Learning for Graph Topology-imbalance by Relieving Under-reaching and Over-squashing"},{"paperId":"c6ea3671b2caef1562c60a8479a49ea60f91006a","title":"Bar transformer: a hierarchical model for learning long-term structure and generating impressive pop music"},{"paperId":"0f40c0957494c8d4bfdd6a9e2231a554b617fe28","title":"Learning to Rotate: Quaternion Transformer for Complicated Periodical Time Series Forecasting"},{"paperId":"2940f734dc3880545a82c5f1c354c20fcd2c7bcd","title":"TaxoTrans: Taxonomy-Guided Entity Translation"},{"paperId":"39ae1805a8d67bc833a104ba83edf7894ef968d8","title":"Non-stationary Time-aware Kernelized Attention for Temporal Event Prediction"},{"paperId":"a64cca16c7cb61d59afc39fef82ec774caffe5ea","title":"TGAN-AD: Transformer-Based GAN for Anomaly Detection of Time Series Data"},{"paperId":"42bc0824d8ca35105d181aaa0183654535325f55","title":"Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages"},{"paperId":"de016a6cd875fb79084b68424031cd5848d01726","title":"Txt2Img-MHN: Remote Sensing Image Generation from Text Using Modern Hopfield Networks"},{"paperId":"8c69cee64a08e2e74fbf737c001c01d2cf073e7e","title":"Bidirectional Transformer with absolute-position aware relative position encoding for encoding sentences"},{"paperId":"ba811d80e9f4173c19d1308da055750068a472b7","title":"Dynamic Adaptive and Adversarial Graph Convolutional Network for Traffic Forecasting"},{"paperId":"f12076b11b90e653c4a14f20646b13537db49cbb","title":"PointConvFormer: Revenge of the Point-based Convolution"},{"paperId":"076deb54a5d776cd21eabf2c40cdd839f53d6d77","title":"giMLPs: Gate with Inhibition Mechanism in MLPs"},{"paperId":"976bb85ba9e426f8faf0c85b37c96e2a9f95cc0d","title":"Time Series Forecasting of Motor Bearing Vibration Based on Informer"},{"paperId":"a60cb9c5ca4c4b437d4087f549ef12be12d4bdbd","title":"Frequency Stability Prediction of Power Systems Using Vision Transformer and Copula Entropy"},{"paperId":"114636918f6a99853420e6d1c975798774a3c14a","title":"End-to-end Modeling for Chinese Dialect Speech Recognition"},{"paperId":"ba3ea95532803d1a9947c877b751dc0590a49b99","title":"3D Point Cloud Feature Information Assisted Audio Source Separation"},{"paperId":"a6d0b96bdbb2267867826a15830c8a45faf852d6","title":"Global-Local Self-Distillation for Visual Representation Learning"},{"paperId":"6e4b798b39b42a6ecb738bf8e530f002064c52e8","title":"Integration of Multiple Time Embedding and GLU for Sequential Recommendation"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"437815d4f186b0a96f5464a6672c4b07eb8f18bf","title":"Learning a Dual-Mode Speech Recognition Model VIA Self-Pruning"},{"paperId":"23b6ea803e9b707edbc4aa286d2b5d9878cdba2a","title":"Innovations in Neural Data-to-text Generation: A Survey"},{"paperId":"e9d1a851d4f7950db360d12fe7f96647aaf547da","title":"Generalized Attention Mechanism and Relative Position for Transformer"},{"paperId":"833e5fe55853d0cb847bfc27b638123869f86c63","title":"Learning Object Placement via Dual-path Graph Completion"},{"paperId":"3b5a08ab2faa5b1826c66e25b5efe167940c5356","title":"Molormer: a lightweight self-attention-based method focused on spatial structure of molecular graph for drug-drug interactions prediction"},{"paperId":"aa16fe6dda0349acd2281183c7fff5f48140ead0","title":"Multi-head Cascaded Swin Transformers with Attention to k-space Sampling Pattern for Accelerated MRI Reconstruction"},{"paperId":"0a4d0f5a69336aee8e85a974dfdecdc9aa518ee2","title":"CMSBERT-CLR: Context-driven Modality Shifting BERT with Contrastive Learning for linguistic, visual, acoustic Representations"},{"paperId":"9a5e384e032337abbbedae06fb0867eb725ee6b7","title":"DGR: Decomposition Graph Reconstruction for Question Understanding"},{"paperId":"52147112360c0cb80adc0a491ff8d2de30a754fc","title":"Mobi-Trans: A Hybrid Network with Attention Mechanism for Myocardial Infarction Localization"},{"paperId":"4299353235595391e2b4f7298baffd00b5acf9d1","title":"LordBERT: Embedding Long Text by Segment Ordering with BERT"},{"paperId":"a7b9d34bf98a56f11a6d6513a7535f0b42784c22","title":"Boosting the Transformer with the BERT Supervision in Low-Resource Machine Translation"},{"paperId":"2797abe9e6a59a3fc5554f1dc49e3c9c0009e553","title":"Decision Making for Autonomous Driving Via Multimodal Transformer and Deep Reinforcement Learning*"},{"paperId":"b9607608ed65a53326d64f773055f0b7e6ac4f88","title":"CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising"},{"paperId":"4d80dae4039b4de6c1ba30227d5981fd1cbb4a1f","title":"Parameterization of Cross-token Relations with Relative Positional Encoding for Vision MLP"},{"paperId":"e7e5cec591285f5ab50a83a0312867ce63b2a7fb","title":"Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization"},{"paperId":"c6bf37c47eabc93b552130bd052fe8d3d90c8a27","title":"Position Prediction as an Effective Pretraining Strategy"},{"paperId":"c27c68597065dad70cb0b4f882cd861826bf28fa","title":"Automated Construction Contract Summarization Using Natural Language Processing and Deep Learning"},{"paperId":"eedf999f58b2fd888feb210e4ea5c5a6ce681880","title":"Transformer-Based Global Zenith Tropospheric Delay Forecasting Model"},{"paperId":"9692886ba8e2c9d8990b0505e9c67a696d9f28a7","title":"A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities"},{"paperId":"5f3c2a31fc84d13a72008f70106163bd92f2f9d0","title":"kMaX-DeepLab: k-means Mask Transformer"},{"paperId":"8bd623131b4a849a77e92aca6d9c113912b45646","title":"Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa"},{"paperId":"e28adeb4db46469df9f9bd653501871ddc5f4318","title":"MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization"},{"paperId":"0b7b72e5325416250e5dd224395f7b78aedac634","title":"Translation-Based Implicit Annotation Projection for Zero-Shot Cross-Lingual Event Argument Extraction"},{"paperId":"0b97eef60bccf79e76b7c50d5f11e44638b58bd1","title":"Distance Matters in Human-Object Interaction Detection"},{"paperId":"1ed91a88cfb55aa8e1b2a2145c7fff7c644bb284","title":"Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency"},{"paperId":"5046d276dbaa4c5245e8427791fe7f30b26eef8e","title":"Can we learn from developer mistakes? Learning to localize and repair real bugs from real bug fixes"},{"paperId":"3ee323d224391b610b18985237a633f64f55566a","title":"Relational local electroencephalography representations for sleep scoring"},{"paperId":"f16656c5a8702523c3340938a7040817736b83f2","title":"Unaligned Multimodal Sequences for Depression Assessment From Speech"},{"paperId":"1c3229a0a589467560f808b0234744181f051ea5","title":"Table2Graph: Transforming Tabular Data to Unified Weighted Graph"},{"paperId":"ffd2a9c326b68e3feed96855367647eb32f7b1da","title":"Guiding transformer to generate graph structure for AMR parsing"},{"paperId":"bccfcca7b28c4792bd59659c56e1d7843542dfe9","title":"Attention Biasing and Context Augmentation for Zero-Shot Control of Encoder-Decoder Transformers for Natural Language Generation"},{"paperId":"1c85c2a214b3e6a64c121e1bee8c5ca46005c6ab","title":"Capture Salient Historical Information: A Fast and Accurate Non-autoregressive Model for Multi-turn Spoken Language Understanding"},{"paperId":"3ac674d9ea88fe1fb9dcd4031e4f50c3ecaa3ab9","title":"SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech"},{"paperId":"765082d2cbc5511b2fa3fc2fa79809d90b2f2a32","title":"LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs"},{"paperId":"2e6028f8b156c8b344e1a68d15d88403a978c71d","title":"Learning Multiscale Transformer Models for Sequence Generation"},{"paperId":"1404b42b66d6f13af3aadf3d253ac531d260f38d","title":"VReBERT: A Simple and Flexible Transformer for Visual Relationship Detection"},{"paperId":"0fda2f1ada85dda45cbb3aa926620bbbd9c476b9","title":"SP-ViT: Learning 2D Spatial Priors for Vision Transformers"},{"paperId":"07e8fd26db7949b3d0b018f85ac360fec1e73492","title":"A Survey : Neural Networks for AMR-to-Text"},{"paperId":"6fb05e3483d05f79e59e5ce957708d8ab8932fdc","title":"Peripheral Vision Transformer"},{"paperId":"2fac281ccb4f52fe578e548aa62a75bf9e7fd2ad","title":"Unsupervised method for video action segmentation through spatio-temporal and positional-encoded embeddings"},{"paperId":"2a03b152e5758aac2d6a6aa3befbea11e59bd48c","title":"Code comment generation based on graph neural network enhanced transformer model for code understanding in open-source software ecosystems"},{"paperId":"31a84391e1b47fa15f3a521c43d62385a7757637","title":"MetaTPTrans: A Meta Learning Approach for Multilingual Code Representation Learning"},{"paperId":"619d53bcebcf72d77387bfef56be7d628e438c26","title":"The YiTrans Speech Translation System for IWSLT 2022 Offline Shared Task"},{"paperId":"4c4347e19392ce37ce2161d8bad1a6823c3b054b","title":"The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline Shared Task"},{"paperId":"9ea9e48950e311387c177b5195782c8c324ac119","title":"Position Labels for Self-Supervised Vision Transformer"},{"paperId":"014b2480e7f9487d571e9f5f56a6d6b27d1d706e","title":"Cortical activity during naturalistic music listening reflects short-range predictions based on long-term experience"},{"paperId":"eef43c8d4205d93258d9976379cffd768211506b","title":"Revisiting End-to-End Speech-to-Text Translation From Scratch"},{"paperId":"728fff6344f9ce65fafcbf3b9aea4f0eb908d44d","title":"How to Dissect a Muppet: The Structure of Transformer Embedding Spaces"},{"paperId":"2bc420d0f377c48fdaa7715715c655af4f43f526","title":"Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech"},{"paperId":"35941b8adb6cc3fec66292d355c504463a429e74","title":"EAANet: Efficient Attention Augmented Convolutional Networks"},{"paperId":"1cb5a1fce0b65b616e69cc5ffd4e43e03d259e97","title":"Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives"},{"paperId":"ca285c906eac9b1b31483d2bc3cddc52c90ac564","title":"KPGT: Knowledge-Guided Pre-training of Graph Transformer for Molecular Property Prediction"},{"paperId":"4c5aa3e450b7c626beb7a27f65f1a238b13e9606","title":"Voice activity detection using a local-global attention model"},{"paperId":"45bf4b6468a3fa643bc1e2f9b7cf049d3d45c2fc","title":"Graph Neural Networks with Precomputed Node Features"},{"paperId":"0dd163f1c10480128eeb25d77afec493c1c695eb","title":"Transformer with Fourier Integral Attentions"},{"paperId":"f5d292b97c0af02506c60c6615e1b58b0f4f421a","title":"Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection"},{"paperId":"31a9744bd5421b3fbbad2ab38ce33bb2f352c77a","title":"CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation"},{"paperId":"a1700c57759fe6482e972c30076d04529a08bf2d","title":"Preparing an endangered language for the digital age: The Case of Judeo-Spanish"},{"paperId":"66b7836001407120ad0000369af8b30c44788a33","title":"Transformer with Tree-order Encoding for Neural Program Generation"},{"paperId":"8e6ada0242dc9b3680ccf6ddeec9e7c3cd41766f","title":"Dynamically Relative Position Encoding-Based Transformer for Automatic Code Edit"},{"paperId":"746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a","title":"Your Transformer May Not be as Powerful as You Expect"},{"paperId":"e1461a3aa9f4bef86435fa2cc87b16d9f34d5ab0","title":"Do we really need temporal convolutions in action segmentation?"},{"paperId":"ed4e8f54074ac075148d29cf7650d0bff2ca95a8","title":"Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers"},{"paperId":"1bcd42583a7b4475d3b456678e7f3752acd9edd1","title":"FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?"},{"paperId":"805748eec6be59ae0cd92a48400a902b3b7ed8e6","title":"Flexible Diffusion Modeling of Long Videos"},{"paperId":"d6c5aab433d9871cabc01ffb1e5e1ea89141155b","title":"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"},{"paperId":"3978fd4769d528b47d0233c2d86dcc8b54b9de3f","title":"BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video"},{"paperId":"6e98a35c439c5c7387de6bffd96dea9b2943a548","title":"Trading Positional Complexity vs. Deepness in Coordinate Networks"},{"paperId":"38c8721a389324798b7d9628b80a43197d0cdc0b","title":"AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential Recommendation"},{"paperId":"004b97aea43f9f62cc49dec20f449abfbae28811","title":"Masked Autoencoders As Spatiotemporal Learners"},{"paperId":"77cb1ca1485f88f4061570dc999524da339863af","title":"Hero-Gang Neural Model For Named Entity Recognition"},{"paperId":"14793aa93920cb8f748776cc45c3895de6df5fbf","title":"RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL"},{"paperId":"9d03a164759bb5cc2fa6b575254b58f790ab6785","title":"Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction"},{"paperId":"137ec10dc0ca5f14deaa9c69ec6f6db3f63ac20d","title":"Sentence model based subword embeddings for a dialog system"},{"paperId":"99eb7caba122464e0b977df8456887a02d2bc051","title":"CSI-fingerprinting Indoor Localization via Attention-Augmented Residual Convolutional Neural Network"},{"paperId":"b4da9f3505e22d3e766ba21890285b822dc71599","title":"EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers"},{"paperId":"9b66c808f38f50d7f4e88e910caad015c23d118d","title":"Capturing Time Dynamics From Speech Using Neural Networks for Surgical Mask Detection"},{"paperId":"4990be2f9092f02df3560fca63d8aef26954bca6","title":"AST-Trans: Code Summarization with Efficient Tree-Structured Attention"},{"paperId":"a6dd1b1088a4ce34e0779cb0c6ae4547d0068b7e","title":"Detecting Loaded Trajectories for Hazardous Chemicals Transportation"},{"paperId":"4d90e08472949c936d6f1c962f4d8b038327f76b","title":"Recognition of Chinese Legal Elements Based on Transfer Learning and Semantic Relevance"},{"paperId":"9ae4666bf38e820292f8a889cb4a9fd796d2dba1","title":"DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation"},{"paperId":"3f200791d33165673740a3224a0afc5daf36f387","title":"Attention mechanism in neural networks: where it comes and where it goes"},{"paperId":"b9a701c90f3d3df27366f5b29a97f798eb940ac7","title":"ChapterBreak: A Challenge Dataset for Long-Range Language Models"},{"paperId":"c6eef27b7ecefe7b8efe15ba228e70137761da10","title":"SinTra: Learning an inspiration model from a single multi-track music segment"},{"paperId":"0aac6adb54813cdf16b321dbee76dea82d7c1d09","title":"Cross-stitched Multi-modal Encoders"},{"paperId":"2193d898e556ad2ac14f0ababf02f90e6fdfe663","title":"DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks"},{"paperId":"633851a323990270461b49e8068a2c9ca1cea530","title":"Dynamic Position Encoding for Transformers"},{"paperId":"a99cbe3d7b2e201491a2fa1f4ba57b1e80cca3a3","title":"LaMemo: Language Modeling with Look-Ahead Memory"},{"paperId":"215fd67a51240194b816c695ac46e0d6d1fdbdc8","title":"GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation"},{"paperId":"41481a39515fa843f7a582f6bb9b32a1c1f42c89","title":"A collection of deep learning-based feature-free approaches for characterizing single-objective continuous fitness landscapes"},{"paperId":"fd472735d69fe44870fe6aad0133eb3388eed527","title":"Accurate Portraits of Scientific Resources and Knowledge Service Components"},{"paperId":"01c93b745dac8d6d463fce3de4ac634c3048f065","title":"TANet: Thread-Aware Pretraining for Abstractive Conversational Summarization"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","title":"Video Diffusion Models"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"c04e3a0d4c3d5d734eadac8ca70dace565e34921","title":"LSTM-RASA Based Agri Farm Assistant for Farmers"},{"paperId":"fb3761d27765536b204191d2a8bca2898055cb95","title":"Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection"},{"paperId":"1984b0f7605e2884616466c92126269ad3a1e426","title":"A Complementary Joint Training Approach Using Unpaired Speech and Text for Low-Resource Automatic Speech Recognition"},{"paperId":"07b5755c833c240b1ee48629cf545442a16bd7d5","title":"Micro-Behavior Encoding for Session-based Recommendation"},{"paperId":"2ad12a7be5eaf339a98c4defd8669e11fe726acc","title":"MaxViT: Multi-Axis Vision Transformer"},{"paperId":"346adcf3ab9cbd06d816586ad30bd3112a5abd0f","title":"Dynamic Focus-aware Positional Queries for Semantic Segmentation"},{"paperId":"ae033080d452915e647ddb69b1189a70e2f2397d","title":"RayMVSNet: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo"},{"paperId":"652a892f36f576726a9fe6ea512e8cc540f5a144","title":"ELECTRIcity: An Efficient Transformer for Non-Intrusive Load Monitoring"},{"paperId":"c99a0aac9a7166ff7659c7b0c6c4a65b3c5642b7","title":"Pair-wise aspect and opinion terms extraction as graph parsing via a novel mutually-aware interaction mechanism"},{"paperId":"df1b3e55e8092a2aa42bb3eab8db778f2b94d66c","title":"The auto segmentation for cardiac structures using a dual‐input deep learning network based on vision saliency and transformer"},{"paperId":"e56b425d5264a30366986f875f2a754f7c27c9fc","title":"Early Detection of Alzheimer's Disease Using Bottleneck Transformers"},{"paperId":"49995802ad0d6ef327647868868458d7619d430d","title":"Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data"},{"paperId":"a2fc77f075f666b462d9350e7576f0ba9845c61b","title":"Transformer Language Models without Positional Encodings Still Learn Positional Information"},{"paperId":"35947a79360829331010011209927f8cb619f97a","title":"VPTR: Efficient Transformers for Video Prediction"},{"paperId":"de4fa71618e56128286a7b6f8302fa78eb155fff","title":"Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture"},{"paperId":"336672adf959e5eb4a2a189c976a2847a68f85c5","title":"ANNA”:\" Enhanced Language Representation for Question Answering"},{"paperId":"c52db31d0535ee38b7ca311350c6f490f0529924","title":"Federated Learning with Position-Aware Neurons"},{"paperId":"b90f8533cb6717121a741d60ac5f466afa71ee99","title":"Coordinate Transformer Network for Prediction of Pseudomonas Aeruginosa’s Drug Resistance"},{"paperId":"2a04c7d083eec666b2a1edcade876d9be9dcdfe8","title":"DPE-BoTNeT: Dual Position Encoding Bottleneck Transformer Network for Skin Lesion Classification"},{"paperId":"cc1a8af59affa863aede0dcd601ccab920d99548","title":"HELoC: Hierarchical Contrastive Learning of Source Code Representation"},{"paperId":"838a2297b94f7bad96c4f8370a5f58487f194f44","title":"Visual Abductive Reasoning"},{"paperId":"0b568a599c7aac6dae52e5a7412ad5a7429ac344","title":"Lane detection with position embedding"},{"paperId":"444e76c94a77b0ba6a401d5196b927b17251c610","title":"Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions"},{"paperId":"fa118487bf333d20573a9dc10b5809d74b696e02","title":"Speech Enhancement by Multiple Propagation through the Same Neural Network"},{"paperId":"002a219711654cf46a58b759954c58fb7c639fc6","title":"Scalable Video Object Segmentation with Identification Mechanism"},{"paperId":"dc998860b4b2562fb3a87e658ffcf9ef2a11438f","title":"Autotts: End-to-End Text-to-Speech Synthesis Through Differentiable Duration Modeling"},{"paperId":"2b7b7eb96fc30089d53e92ad97ac2db0035dc3cb","title":"Generalized Classification of Satellite Image Time Series with Thermal Positional Encoding"},{"paperId":"04a061d959ff3a777c266c8975ca1e36f05528ac","title":"ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation"},{"paperId":"36d649496b91ce22f380502553a488ccaf3027dd","title":"AAPFE: Aligned Assembly Pre-Training Function Embedding for Malware Analysis"},{"paperId":"e28ad66a8ee09278e00b8fa064b695eefca52d02","title":"FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction"},{"paperId":"d71915cf609dd0e785da481fcdab826c38611243","title":"HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing"},{"paperId":"93c1dffe2bae737da8f342fd749aa783df572a14","title":"XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding"},{"paperId":"a3c18ea5548acbc885836e8d0f202852db192268","title":"S^2SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers"},{"paperId":"acbe813244e07f32eb034d6c27547d772a995d1d","title":"Uncertainty Estimation for Language Reward Models"},{"paperId":"9f1b0e4c42a5a85d4c023030557ade4419f82ecf","title":"Scaling Up Your Kernels to 31×31: Revisiting Large Kernel Design in CNNs"},{"paperId":"0239063c7a7782ead73fe72e4b2d21c3988be23d","title":"Challenges of Neural Machine Translation for Short Texts"},{"paperId":"9aacdbc8b04fa63e6fe93f62a737a11c613f08fb","title":"Recent Advances in Neural Text Generation: A Task-Agnostic Survey"},{"paperId":"63de5aacac3c29f7a3bb17ec65e50229a6a179ba","title":"HDNet: High-resolution Dual-domain Learning for Spectral Compressive Imaging"},{"paperId":"4e9cd6be4a8fcad2ca562fcf41a1f882387a3167","title":"LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network"},{"paperId":"f8f3ba03d0350754fbb502192dc0a201a053024f","title":"A Unified Query-based Paradigm for Point Cloud Understanding"},{"paperId":"2e43501a14b831744999355c177321709659aff1","title":"TableFormer: Robust Transformer Modeling for Table-Text Encoding"},{"paperId":"83b455fd69b7cd1b9409c68fa262ae92335b8e61","title":"KG-SQL: Hybrid Knowledge-Guided Semantic Understanding for Text-to-SQL"},{"paperId":"189e36ba5b50a854eb45f612bcad51a621d7a7f1","title":"MS-Transformer: Introduce multiple structural priors into a unified transformer for encoding sentences"},{"paperId":"3921e7cd03c440c6d065a51b499c9f581d1d2f1f","title":"A Survey of Automatic Source Code Summarization"},{"paperId":"50af83ea20201b51014358534650213e6133650c","title":"FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks"},{"paperId":"7a9df88b2e10fd071d045cad1d4239116095ca99","title":"Machine Reading Comprehension-Enabled Public Service Information System: A Large-Scale Dataset and Neural Network Models"},{"paperId":"48df6b4c106bbb4735fcc9f1e2ac4e23bba10565","title":"SFINet: Shuffle–and–Fusion Interaction Networks for Wind Power Forecasting"},{"paperId":"5f104a804ed245c79847ad0593e8f86196d697b1","title":"Transformers in Time Series: A Survey"},{"paperId":"a996989466bb584602eb3027f308d78b2867893d","title":"Source Code Summarization with Structural Relative Position Guided Transformer"},{"paperId":"a7de7968bd9d4d97daaac74c3c1d466f3342ff45","title":"Entroformer: A Transformer-based Entropy Model for Learned Image Compression"},{"paperId":"10af057b73c206d5b999c200b4de4a7a67435dee","title":"Personalized Long-distance Fuel-efficient Route Recommendation Through Historical Trajectories Mining"},{"paperId":"2491a706049da76ab7fe17f74c6ea7f195502fdf","title":"Multi-Resolution Attention for Personalized Item Search"},{"paperId":"106ec729604861c1494f17fe7f277c130acef8dc","title":"Improving the Sample-Complexity of Deep Classification Networks with Invariant Integration"},{"paperId":"9d054fa2ce6b0b3160aa52712f44f7967057205b","title":"Corrupted Image Modeling for Self-Supervised Visual Pre-Training"},{"paperId":"ea0e4a9778e33b7f8e7b3246d63071330950995a","title":"Structure-Aware Transformer for Graph Representation Learning"},{"paperId":"d0194169a4787cf2977e5574746f5466ed6b96cc","title":"Curvature-Driven Deformable Convolutional Networks for End-To-End Object Detection"},{"paperId":"385dab6963b7e9deae1f32f68e53c846a2122a9b","title":"Self-attention-based time-variant neural networks for multi-step time series forecasting"},{"paperId":"77365b30336ac46d620d958dc4c108a159c02834","title":"WebFormer: The Web-page Transformer for Structure Information Extraction"},{"paperId":"2b34ee1b7f4cf5e7dac38d8e66849a02d96fade3","title":"Graph-based Neural Acceleration for Nonnegative Matrix Factorization"},{"paperId":"4c75564731f564e78cafc76e18739bbcf4fceeb2","title":"Knowledge Base Question Answering Based on Multi-head Attention Mechanism and Relative Position Coding"},{"paperId":"a81e30bbbf5422fbac75f6a017206ac97a508521","title":"Efficient Self-attention with Relative Position Encoding for Electric Power Load Forecasting"},{"paperId":"b92898a28bfad42a053726c2707cc05686cd332a","title":"GRPE: Relative Positional Encoding for Graph Transformer"},{"paperId":"7cf779d889dbf155e089289bab1495be2b186b11","title":"Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal Point Processes"},{"paperId":"83ebb3a6f8c81492456d024d5dfb9a9bc0221434","title":"Generative Cooperative Networks for Natural Language Generation"},{"paperId":"c817dbd4aa035b3e3d8d244f5e07c319f0e6fb06","title":"Position-Enhanced Multi-Head Self-Attention Based Bidirectional Gated Recurrent Unit for Aspect-Level Sentiment Classification"},{"paperId":"076d06d66886dedf3cb6e5dc31393dac38fef300","title":"Dual-Flattening Transformers through Decomposed Row and Column Queries for Semantic Segmentation"},{"paperId":"b51fa25bf1ce7fd18737a36b72e80f8e5808973e","title":"MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition"},{"paperId":"d10864946d1733444f0472acf1294f15b350e65e","title":"Continual Transformers: Redundancy-Free Attention for Online Inference"},{"paperId":"3e906906d475c73b6d8ce24ac5ebdac9979fd01b","title":"Video Transformers: A Survey"},{"paperId":"e55c6651a83e0565feda69276c25b7976b5a8fda","title":"Reverse-engineering information presentations: recovering hierarchical grouping from layouts of visual elements"},{"paperId":"49c3f85573a3204c5e66317289e4cecfed50f38a","title":"Assemble Foundation Models for Automatic Code Summarization"},{"paperId":"47b97fb6398122ef41344b7e5301a2389b1a94bb","title":"Feature Pyramid Multi-View Stereo Network Based on Self-Attention Mechanism"},{"paperId":"4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c","title":"Classification of Long Sequential Data using Circular Dilated Convolutional Neural Networks"},{"paperId":"063e410c2c52ccbaa22c62130ac2c58969bb3efa","title":"SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations"},{"paperId":"ab00d5e3183cd8d49f62b1424c2503fd1e6edaee","title":"PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture"},{"paperId":"c59a685133c8b0969fb8c5490e3c741b6e89541f","title":"Semi-MsST-GAN: A Semi-Supervised Segmentation Method for Corneal Ulcer Segmentation in Slit-Lamp Images"},{"paperId":"3d261c68c964fc2f9ba759e71fdaa9bf075d1c98","title":"D-former: a U-shaped Dilated Transformer for 3D medical image segmentation"},{"paperId":"d3ff0a2e58e29d652fd15cd86f0a0f53df8e8af8","title":"Transmorph: a transformer based morphological disambiguator for Turkish"},{"paperId":"ca1d71b52b3b81a7e1cca297f19d47f353cf3340","title":"Relation-Aware Graph Transformer for SQL-to-Text Generation"},{"paperId":"6449f3ea73692d3a8cac8c51cefe608aa4b22a02","title":"S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation"},{"paperId":"59a8dbbbf0002f1f497fddc4d01e92c0b9645c10","title":"Diformer: Directional Transformer for Neural Machine Translation"},{"paperId":"63b5ada6cbe9df123d18cd467b56fbcfbaf49acb","title":"PointSCNet: Point Cloud Structure and Correlation Learning Based on Space-Filling Curve-Guided Sampling"},{"paperId":"a1031489ac747006b0ecb1a1a6d74b2e76ada4d4","title":"Joint-training on Symbiosis Networks for Deep Nueral Machine Translation models"},{"paperId":"122ac5e90051fe41919884fe274a1cb66528f326","title":"Semantics-Recovering Decompilation through Neural Machine Translation"},{"paperId":"d81ec6393db9d67741b90915911180a52e42413d","title":"ViT-based VQ-VAE Generative Network for Accompaniment Generation"},{"paperId":"c44da47301bae65f687650997293ba1933ec9015","title":"Learning Positional Embeddings for Coordinate-MLPs"},{"paperId":"9eb3726604cd436010c8f19089598b97fb1bf8a3","title":"Improving Face-Based Age Estimation With Attention-Based Dynamic Patch Fusion"},{"paperId":"0366d4355cbce731054714a96dbd3bbdb7399a0d","title":"Protein-Ligand Binding Affinity Prediction Using Deep Learning"},{"paperId":"008a428e049003fe768068a0f1fa1416af5c4982","title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training"},{"paperId":"94c5ab1786266cf01d2a3f9dafd498b49d2b46a2","title":"Seizure prediction in scalp EEG based channel attention dual-input convolutional neural network"},{"paperId":"ae31edefdfd8b964625568f9c56d3f9c692b4ea4","title":"Developing A Deep Learning Natural Language Processing Algorithm For Automated Reporting Of Adverse Drug Reactions"},{"paperId":"9816ddd36859eb80e4d22e83f080f3e69ddc708d","title":"Towards More Efficient Insertion Transformer with Fractional Positional Encoding"},{"paperId":"79d9422ff0761c12f5b13b7199c08ee4e4fdfb29","title":"A Convolutional Neural Network Based on Self-Attention Mechanism for Molecular Property Prediction Using Molecular Hidden Fingerprints: An efficient molecular property prediction method"},{"paperId":"fdfa5490dd47e7dc8556bcfafe7a4f6ea07b4dbb","title":"UNITER-Based Situated Coreference Resolution with Rich Multimodal Input"},{"paperId":"9137efc758f80dd22bb56f82cca5c94f78a5db3e","title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"},{"paperId":"91a4cbae6553e975ddc3b2f6850ed725ff475307","title":"SwinTrack: A Simple and Strong Baseline for Transformer Tracking"},{"paperId":"494c1f745dbb7625e86e9a222c480e40949b8dad","title":"Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-Sentence Dependency Graph"},{"paperId":"2c11b720624dc669f78fd34557fe91b5b2097969","title":"Deep attentive style transfer for images with wavelet decomposition"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"2099da1395984292fe59f42e787027409da7b08c","title":"Leveraging contextual embeddings and self-attention neural networks with bi-attention for sentiment analysis"},{"paperId":"4c93661c3ad6fe4e3e092975afd9a451c49319a6","title":"KARL-Trans-NER: Knowledge Aware Representation Learning for Named Entity Recognition using Transformers"},{"paperId":"e73d87e7c19457b2e79878638ea44ee66196743c","title":"SANTM: Efficient Self-attention-driven Network for Text Matching"},{"paperId":"366766cbf762ba8f9d50fd77bfcf1e6956b275a4","title":"Dual-axial self-attention network for text classification"},{"paperId":"69634b1406115a4c787d611f2e024b56abbde291","title":"Dual-axial self-attention network for text classification"},{"paperId":"becb855b7cf635316ec634bdbb851bda13399fc0","title":"Deps-SAN: Neural Machine Translation with Dependency-Scaled Self-Attention Network"},{"paperId":"972706306f85b1bfb40c7d35c796ad5174eb0c9c","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"},{"paperId":"a782b1d91d90e3d3946cf019703162f7f9661cdf","title":"GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization"},{"paperId":"45f686be3b96302ede327645227134e1c304dbab","title":"Attention mechanisms in computer vision: A survey"},{"paperId":"c1793d88db202bd416cab2776224f17e4da7457a","title":"PESTO: Switching Point based Dynamic and Relative Positional Encoding for Code-Mixed Languages"},{"paperId":"9da405a29af85b16fff126db1e52158677ed8e05","title":"Remote Sensing Image Scene Classification Based on Global Self-Attention Module"},{"paperId":"333212e246fb65f7c9d43862021e78f007c48449","title":"A Survey of Visual Transformers"},{"paperId":"9418741ce0f9d1d0a00d7631adb17fdbfd06e4e5","title":"A Chinese Multi-type Complex Questions Answering Dataset over Wikidata"},{"paperId":"36d1aeff3f1e57f2f2bf3cd4f596d7797862bc86","title":"A Contextual Latent Space Model: Subsequence Modulation in Melodic Sequence"},{"paperId":"0a22b5df6c1c25b776d1015e9490974ed704c044","title":"ER-SQL: Learning enhanced representation for Text-to-SQL using table contents"},{"paperId":"8e488694a0f9675eab577d45e9bbee7d6982d741","title":"With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition"},{"paperId":"5ef4e013ce7bf0c19873c83ffd6898ce33ffd542","title":"AST-Transformer: Encoding Abstract Syntax Trees Efficiently for Code Summarization"},{"paperId":"23640b62ac800ecd1e6d57e5b765b45f21bfb6fb","title":"Pseudo-Labeling for Massively Multilingual Speech Recognition"},{"paperId":"2f05f579f6c48f43e6ced36d96eff9190e57bd40","title":"PatchFormer: An Efficient Point Transformer with Patch Attention"},{"paperId":"ff653651ddc57081f0a2b82ebcc2126905089182","title":"Two Heads are Better than One: Geometric-Latent Attention for Point Cloud Classification and Segmentation"},{"paperId":"c6481af1c5ebba68a8f59ade7d07eab1265d8e15","title":"Dispensed Transformer Network for Unsupervised Domain Adaptation"},{"paperId":"063eee315e864f0842d3074629dccc4bb36d19e7","title":"Discovering Non-monotonic Autoregressive Orderings with Variational Inference"},{"paperId":"a26d810b78fa39b23253ee072aeedac88c4c87c4","title":"Operation Diagnosis on Procedure Graph: The Task and Dataset"},{"paperId":"1dc2edea1b328c846bd48c59541f7b7c49b9d59e","title":"Cache-based GNN System for Dynamic Graphs"},{"paperId":"0eb024e6591f071f832be7a18657e9d6eb85fa1c","title":"Positional Mask Attention for Video Sequence Modeling"},{"paperId":"df473ca1b625f63335cd20524b16ddeb9c45e8b9","title":"Hierarchical Semantic Enhanced Directional Graph Network for Visual Commonsense Reasoning"},{"paperId":"eb36750a31af99baf22ed01d0b8c24d9ff550b40","title":"EAPT: Efficient Attention Pyramid Transformer for Image Processing"},{"paperId":"1df0f782b0d2264c455ba3a4332c26dbc014f2ee","title":"Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction"},{"paperId":"c29e21c58fc12024ef9b273b826050f536368071","title":"Image generation step by step: animation generation-image translation"},{"paperId":"b67ff4b6fca84d57f951a3ce1cc9829f6c902dce","title":"Multimodal Video Summarization via Time-Aware Transformers"},{"paperId":"46c8b4b133a359dec97a03d54b36de054384d5be","title":"Image Search with Text Feedback by Deep Hierarchical Attention Mutual Information Maximization"},{"paperId":"03da17fc6d1868efc0b91872f1823e8ff4612824","title":"Direction Relation Transformer for Image Captioning"},{"paperId":"42c248f36f8fe9b1cd530ca9b40ecc16fb29afbd","title":"Multi-View Stereo Network with attention thin volume"},{"paperId":"b34e7babf9443d792a765b65fd9f5dc3b9b3ddee","title":"RTJTN: Relational Triplet Joint Tagging Network for Joint Entity and Relation Extraction"},{"paperId":"1067c44e473b6998f89e13f0d4c0de730def43f0","title":"SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing"},{"paperId":"e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"},{"paperId":"fa945b5f34f053020b0c87c7340bf3b1838bd598","title":"Study of Positional Encoding Approaches for Audio Spectrogram Transformers"},{"paperId":"701f5eb3337c7ea3731168615886bba810aa1ff6","title":"A Speaker-aware Parallel Hierarchical Attentive Encoder-Decoder Model for Multi-turn Dialogue Generation"},{"paperId":"813f6e34feb3dc0346b6392d061af12ff186ba7e","title":"Learning Efficient Multi-Agent Cooperative Visual Exploration"},{"paperId":"c501d6f1eecf3b0fdae7d428e1829bb6607a6a37","title":"Relative Molecule Self-Attention Transformer"},{"paperId":"281fc1783a91481ebbd22e01f85e17ac67990a6f","title":"A Systematic Review of Deep Learning Approaches for Natural Language Processing in Battery Materials Domain"},{"paperId":"013e80651953899f656c63ba7e670dfd9630d26f","title":"Dual contextual module for neural machine translation"},{"paperId":"3356254badd0652bbb75515eae688517dc8d260f","title":"Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"68f14f333dad84ec35fc0eb9fcfd2c41fdc02596","title":"ATISS: Autoregressive Transformers for Indoor Scene Synthesis"},{"paperId":"12640af46eaf4c16125557b517a2d37fca70a82d","title":"Ripple Attention for Visual Perception with Sub-quadratic Complexity"},{"paperId":"87e879c2465b2414a58dd3a1184f8b346d48f3e7","title":"Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer"},{"paperId":"540ed994eb00b5279748d1f26d04371e3a67ec0d","title":"Molformer: Motif-Based Transformer on 3D Heterogeneous Molecular Graphs"},{"paperId":"7d34244c64fec4808503e21d648eaaefe5d85208","title":"SPaR.txt, a Cheap Shallow Parsing Approach for Regulatory Texts"},{"paperId":"7dbeb92e7c234b3887cc484ce406ad86b5743add","title":"Detecting Persuasive Atypicality by Modeling Contextual Compatibility"},{"paperId":"74fc2b3999897b0a09934224100212531335055a","title":"ResSaNet: A Hybrid Backbone of Residual Block and Self-Attention Module for Masked Face Recognition"},{"paperId":"8b2d357347b57217966863a36c95f4b52e6be3a6","title":"Gaze estimation via self-attention augmented convolutions"},{"paperId":"cdeb1f6188666809bcdeec2e3b916671fc012634","title":"GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation"},{"paperId":"98eafdb2a4d6242cc916f1278a0113b102de6391","title":"PortaSpeech: Portable and High-Quality Generative Text-to-Speech"},{"paperId":"232acef2483a26fe95c70b619f88fa0b82c1a105","title":"Multiplicative Position-aware Transformer Models for Language Understanding"},{"paperId":"27c4102b00a04681e5adbde4493006fd41628b35","title":"Modeling Dynamic Attributes for Next Basket Recommendation"},{"paperId":"11dcf910b4221e4c7a5440962371585cecf46502","title":"The NiuTrans Machine Translation Systems for WMT20"},{"paperId":"25dc7c6e808ffb097682941fe767d09709472480","title":"Underwater image enhancement via LBP-based attention residual network"},{"paperId":"8b6b9d426776c3c7c82f5dadee0e654d5088c14e","title":"Audio-Visual Speech Recognition is Worth $32\\times 32\\times 8$ Voxels"},{"paperId":"df6f3c607ae3956db722f76f3f81e672c3dfa803","title":"CodeQA: A Question Answering Dataset for Source Code Comprehension"},{"paperId":"4a8964ea0de47010fb458021b68fa3ef5c4b77b2","title":"Primer: Searching for Efficient Transformers for Language Modeling"},{"paperId":"0a35064caba35a80f21fb9798ada7abb1377ead9","title":"The NiuTrans System for the WMT21 Efficiency Task"},{"paperId":"8a3882fd35847d78997a8491f06ebbb3019d8775","title":"Generating Music Transition by Using a Transformer-Based Model"},{"paperId":"a299947b4d588dee748d0a3b3c6a4dec55c8b212","title":"A Two-Stage Short-Term Load Forecasting Method Using Long Short-Term Memory and Multilayer Perceptron"},{"paperId":"255c526f78bbfec35d4ed73b6dd8eacd9ef2c0b3","title":"RankNAS: Efficient Neural Architecture Search by Pairwise Ranking"},{"paperId":"64a0a4f357be12aaf30cc6e4964d1c3a9d927aac","title":"Sequence Length is a Domain: Length-based Overfitting in Transformer Models"},{"paperId":"9c5a5e932139621da37c93d48f8a8df40a9c61d4","title":"Dialogue State Tracking with a Language Model using Schema-Driven Prompting"},{"paperId":"586cafd248d01c1a1c67a57b3cef9f807173110c","title":"Performance-Efficiency Trade-Offs in Unsupervised Pre-Training for Speech Recognition"},{"paperId":"64522a5b3476e9f201f6a5b3e312ef0005c562f1","title":"SHAPE: Shifted Absolute Position Embedding for Transformers"},{"paperId":"bd7907735793f122c400d3afc4e73187dc57f376","title":"SPARQLing Database Queries from Intermediate Question Decompositions"},{"paperId":"32d7233aab1a5020f688333007f70bce3bf0a97a","title":"Leveraging Multi-Faceted User Preferences for Improving Click-Through Rate Predictions"},{"paperId":"33c831326bb47b2ba2031fd7213b6918d23eb01e","title":"The Impact of Positional Encodings on Multilingual Compression"},{"paperId":"23d11338be48471b3979b13eb172ec67fc22244b","title":"Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model"},{"paperId":"64f3a18921f7f3a384dca073cd6d2476b9af47f2","title":"Zero-Shot Dialogue State Tracking via Cross-Task Transfer"},{"paperId":"bb363c8c5bc1c473f0801c647c88d0c071792858","title":"PermuteFormer: Efficient Relative Position Encoding for Long Sequences"},{"paperId":"aec7e7143bfe082752f428fe01e4090dd7fc411c","title":"Ultra-high Resolution Image Segmentation via Locality-aware Context Fusion and Alternating Local Enhancement"},{"paperId":"e8f39c0bde3a3f624d2198576f5a0b99a87f862f","title":"Improving neural machine translation using gated state network and focal adaptive attention networtk"},{"paperId":"5b95ef4fea818f08897f0dddb56e5d3a8e5cb9d3","title":"ShopTalk: A System for Conversational Faceted Search"},{"paperId":"8e9d44502f5b6de37d5a7181e169a2899a91f7b3","title":"Self-Attention-Based Temporary Curiosity in Reinforcement Learning Exploration"},{"paperId":"51b5db5c679be0ce9a39a2ee21def42bca165efe","title":"Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning"},{"paperId":"678f0cdb6b8e59aa2ef6c409b97fecc529059703","title":"An Enhanced Visual Attention Siamese Network That Updates Template Features Online"},{"paperId":"1b45714103c276ebadb12cfe0a0d08f0667d98e5","title":"LinearSpeech: Parallel Text-to-Speech with Linear Complexity"},{"paperId":"053ac76f7b6e12f406eaf6e953b3bd5313fd69c2","title":"Ultra Fast Speech Separation Model with Teacher Student Learning"},{"paperId":"35e31785d64574f6f5a89be81ae934c616d3753a","title":"Lightweight Causal Transformer with Local Self-Attention for Real-Time Speech Enhancement"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"015b2fb3a12bd0134b91af272b29b62371ef9022","title":"Recurrent multiple shared layers in Depth for Neural Machine Translation"},{"paperId":"5882f98d17780c6c0b2638f57cd1764dd5267c18","title":"Deep Contrast Learning Approach for Address Semantic Matching"},{"paperId":"2eaf36dc40fd9a61f8f0fad14a6d3c9594b1c2be","title":"An Effective Non-Autoregressive Model for Spoken Language Understanding"},{"paperId":"e3d06054af531ee2f42270d43100b309c28546ef","title":"MUSIQ: Multi-scale Image Quality Transformer"},{"paperId":"564c7e51f930b208cf05f68a8c478d1195ccad05","title":"Learning Fair Face Representation With Progressive Cross Transformer"},{"paperId":"79678d2f10bddf14b2aedf3427f8a4c39908931f","title":"Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"2a52e0bb16638df4216cf63ea9708891741e565e","title":"Multi-Branch with Attention Network for Hand-Based Person Recognition"},{"paperId":"5019fc4c08177fb70045aa0ce1edd08099cbe1ec","title":"An end-to-end CNN with attentional mechanism applied to raw EEG in a BCI classification task"},{"paperId":"a5c41f188b0eb0acb444cb4899bf6af378ee9ede","title":"CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"},{"paperId":"17b4a75b432b9f58de143918608de9234a4da988","title":"Towards Continual Entity Learning in Language Models for Conversational Agents"},{"paperId":"5519272de5f4eb69986022a997fc013050416cc6","title":"Compute and Memory Efficient Universal Sound Source Separation"},{"paperId":"a9c214e846188adb645021cd7b1964b8ea1fef6f","title":"Rethinking and Improving Relative Position Encoding for Vision Transformer"},{"paperId":"ae30c7199df1991de6a90508d40c593bfee760e0","title":"CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation"},{"paperId":"4159a98819c10c2710f68d239fcc03bdb7ece472","title":"Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation"},{"paperId":"a514cc9796034bcc01450ac968b2f576fa35c70f","title":"Multi-Scale Local-Temporal Similarity Fusion for Continuous Sign Language Recognition"},{"paperId":"87e823d2cb58e741230c0fa3b83f3459c7e32241","title":"PiSLTRc: Position-Informed Sign Language Transformer With Content-Aware Convolution"},{"paperId":"fcf0cf8510e19fec46256c0c472665766fcdea7f","title":"Residual Tree Aggregation of Layers for Neural Machine Translation"},{"paperId":"2cac91a6022af7ba5fd6cb1b5ff6ebb815880bb4","title":"Text classification using improved bidirectional transformer"},{"paperId":"a4cc58555e2459168a8472b35d159151f6d90f30","title":"A Structural Transformer with Relative Positions in Trees for Code-to-Sequence Tasks"},{"paperId":"a5896974ae46a3232419e93b5f7522b92995003e","title":"Semantically Constrained Document-Level Chinese-Mongolian Neural Machine Translation"},{"paperId":"9058d322a09bfc0c93a070f87cac8fd840e63088","title":"From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers"},{"paperId":"485fb51d8e2e579d42a1f9be45b1806519a6ad8c","title":"DeepMutants: Training neural bug detectors with contextual mutations"},{"paperId":"0b036cd5dfc49d835d0c759c8ca31d89f2410e65","title":"CMT: Convolutional Neural Networks Meet Vision Transformers"},{"paperId":"1306680402070eecd37da6bb0d531017166c5f80","title":"The Piano Inpainting Application"},{"paperId":"00a92e580cb4801fc32f2ed8cdb215bfb9c1a575","title":"Conformer-based End-to-end Speech Recognition With Rotary Position Embedding"},{"paperId":"fc19e94109d4c2f05f3639a67327c708543def98","title":"Locally Enhanced Self-Attention: Combining Self-Attention and Convolution as Local and Context Terms"},{"paperId":"059783a60339601faabf8f9ec739fc2da89dff56","title":"Tool wear prediction based on multidomain feature fusion by attention-based depth-wise separable convolutional neural network in manufacturing"},{"paperId":"d3f51870f4da5dd9c2a08a55cfa8a380b8d49208","title":"Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation"},{"paperId":"3d6315f836c6ab33320df965c5504c95bd7c4a35","title":"ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data"},{"paperId":"fbac64d617914ab8dac0682639fbd6012faed771","title":"Rethinking Positional Encoding"},{"paperId":"8dfa41ada085940f3e4cb2af3eb7ba6fe1082a41","title":"The NiuTrans End-to-End Speech Translation System for IWSLT 2021 Offline Task"},{"paperId":"8c7aa01a5d57082474ed9188376eeea82608a3c0","title":"Investigation of Practical Aspects of Single Channel Speech Separation for ASR"},{"paperId":"fbc34e0bac913acf81bebfc1be2909465543f49b","title":"Relative Position Representation over Interaction Space for Natural Language Inference"},{"paperId":"1bed382373aed687c045bb65bc7541b16fc7a6be","title":"Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN"},{"paperId":"4cfaea33c6cd6e72d843e808cf7c69387e100e7b","title":"Exploiting Positional Information for Session-Based Recommendation"},{"paperId":"6d3558a70490ffa25393996a7c09cd439aaaede2","title":"Polarized Self-Attention: Towards High-quality Pixel-wise Regression"},{"paperId":"800cfb3d23115cdcd4d114234b65bbdf2080f798","title":"CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"},{"paperId":"a255b4674ecac416eb37d9441e1cda2fed147b0c","title":"StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR"},{"paperId":"d1919dc2e2fad6c32caa85da4a3a40372c1d9d33","title":"MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity"},{"paperId":"034869f2f55b01f240b30923983ea197ff9fc32c","title":"FastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis"},{"paperId":"cf444d60da03f6a4dbabeb44186b80de1a717db1","title":"Interpreting Depression From Question-Wise Long-Term Video Recording of SDS Evaluation"},{"paperId":"36de6b3b18ea763619bdca2d39035a8adf1582d2","title":"Language Models are Good Translators"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"a76d5f07caad631c196e109cc1052b85d45fb83d","title":"Learning Explainable Representations of Malware Behavior"},{"paperId":"0d508600d77d8a7e6a655cdb6d139779732f649f","title":"Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"},{"paperId":"c6dad3f9c9f29602b8c89585c23c73377ef00601","title":"Bootstrap Representation Learning for Segmentation on Medical Volumes and Sequences"},{"paperId":"0d46cbaf914da31a06ef2753e00b7f47e055e70d","title":"Probabilistic Attention for Interactive Segmentation"},{"paperId":"34ff62922161332cef417af9b470b77a7225a477","title":"Exemplars-Guided Empathetic Response Generation Controlled by the Elements of Human Communication"},{"paperId":"7cf872dbad1cd780a2ebf56f8bee76fb9497c018","title":"ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction"},{"paperId":"b99c61f6957c1b04ec1376b74f82dd1e83559695","title":"JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"df59d0098c1b2c1ee8995da802dd6b12d158c2b8","title":"Large-scale chemical language representations capture molecular structure and properties"},{"paperId":"f0993e68c76d940763ef7f8106db86cdc97b3a49","title":"Multi-head or Single-head? An Empirical Comparison for Transformer Training"},{"paperId":"e7473d3a44ad252b6bbd98c2f240498921fa4c87","title":"Learning to Combine Per-Example Solutions for Neural Program Synthesis"},{"paperId":"bcd53292bb2999a0e0a8aa7307f760a827d7f296","title":"Structure-Regularized Attention for Deformable Object Representation"},{"paperId":"92bf1c069747374fbc3efb55e7a916a3e2d736da","title":"Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"},{"paperId":"5d7d34abbc14739e40b53ec3c33a3c698a37e70e","title":"To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs"},{"paperId":"0a12ea86c9193bf2496d65d25d1f234c1fac923b","title":"Zero-Shot Controlled Generation with Encoder-Decoder Transformers"},{"paperId":"1dbb523a6555d6e0c5727620e2b57daaa5b79dc0","title":"Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models"},{"paperId":"a6337d9ebb0b7de84588806110157806f9c0383b","title":"GraphiT: Encoding Graph Structure in Transformers"},{"paperId":"de282eff05d89f9fb53d1585c1377c4f0499ee12","title":"Transformed CNNs: recasting pre-trained convolutional layers with self-attention"},{"paperId":"47ae807cd511b35e78a2cd4e198283dea6dafd41","title":"Do Transformers Really Perform Bad for Graph Representation?"},{"paperId":"9f4b69762ffb1ba42b573fd4ced996f3153e21c0","title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes"},{"paperId":"b50815251c948f00baedccaf5f56c281ffa7650f","title":"Staircase Attention for Recurrent Processing of Sequences"},{"paperId":"d8d2e574965fe733eb1416e03df2b5c2914fc530","title":"A Survey of Transformers"},{"paperId":"5ee4108f56e20398361d43420b5bee6223901d21","title":"Image super-resolution via channel attention and spatial attention"},{"paperId":"95f5bafba97beb9b4f8c1fe607f04ec28efab7f9","title":"Learning to Efficiently Sample from Diffusion Probabilistic Models"},{"paperId":"576c462dbc1f3d732b919ef1daac37a817123e52","title":"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias"},{"paperId":"2d98048c2d2fcd3f6b989d2a54003808906ab4b7","title":"Efficient Training of Visual Transformers with Small Datasets"},{"paperId":"31b14acb6caa45cd0a0f28a3cc9834819139611a","title":"Image super-resolution via channel attention and spatial attention"},{"paperId":"2835951fabf12804e17d5a525b2be2bee70e7910","title":"Uformer: A General U-Shaped Transformer for Image Restoration"},{"paperId":"7509c66a666e2e3f14bc8676b969b945ee6e136f","title":"CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"},{"paperId":"94576783bc73bf55a0091203a3d45a0a4665a1ae","title":"Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding"},{"paperId":"e7e626e379e67b8ae59689fa1c055ab83a5b2313","title":"Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction"},{"paperId":"27c09d07f1a7dba51889fef9ee8f925c87fb6b00","title":"X-volution: On the unification of convolution and self-attention"},{"paperId":"93892c5bbc489ee7bccb1b97eed92465013ea826","title":"Scalable Transformers for Neural Machine Translation"},{"paperId":"ab0ef8e705fa88f079a055842ba20b4f413a69db","title":"Associating Objects with Transformers for Video Object Segmentation"},{"paperId":"d8e7bad2681ce70277c900c77a22181d4b03d705","title":"The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"},{"paperId":"4dc2c552bfb0abcf15db9cfe795da9e97551ee42","title":"An Improved Model for Voicing Silent Speech"},{"paperId":"bc528b7af23199c660481ee8d452b900705c3c8f","title":"Complementary spatiotemporal network for video question answering"},{"paperId":"e4a3ef32779313f30e299eda9b2660943b5eeed1","title":"Complementary spatiotemporal network for video question answering"},{"paperId":"50db74aa7e662b640ccbf37788af62cd8af3e930","title":"LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations"},{"paperId":"806dc5c64d3a65f89e0f26ff9f51bb029c6908b2","title":"A Multi-Level Attention Model for Evidence-Based Fact Checking"},{"paperId":"f4b20bd6a7ec7c57f8afc2228bcfd00a7346197c","title":"Smart-Start Decoding for Neural Machine Translation"},{"paperId":"7af513e8a395b82628dbf627fe9269a44cea14ae","title":"Does Structure Matter? Encoding Documents for Machine Reading Comprehension"},{"paperId":"0e09c6b34b34a2ca3eb16e8ebfbe0c2472666206","title":"Enhancing Transformer with Horizontal and Vertical Guiding Mechanisms for Neural Language Modeling"},{"paperId":"1079b5459291ec73fc0a2d0692d38eaa97cb41a6","title":"StyTr2: Image Style Transfer with Transformers"},{"paperId":"8c4f89a9ac30cf94186916be1bfaa02dbfb3600d","title":"CoDesc: A Large Code–Description Parallel Dataset"},{"paperId":"685ca8238989d93a4c196abc6627bb960967d786","title":"Learning to Extend Program Graphs to Work-in-Progress Code"},{"paperId":"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages"},{"paperId":"50be8476315d8e520313f21dbb9908b47a91c3d1","title":"Explainable enterprise credit rating using deep feature crossing"},{"paperId":"08ffdec40291a2ccb5f8a6cc048b01247fb34b96","title":"Relative Positional Encoding for Transformers with Linear Complexity"},{"paperId":"9f863fcb229e762a95b8bb0e4a89d7aeeb3d8640","title":"Link Prediction on N-ary Relational Facts: A Graph-based Approach"},{"paperId":"f07c5c540233b22f0ca154c80c713e2aed3c9606","title":"Symbolic Music Generation with Transformer-GANs"},{"paperId":"a3196e65467b80f4755968923b382e40c02ccb51","title":"Two-Stream Convolution Augmented Transformer for Human Activity Recognition"},{"paperId":"52d01d9f71caf0d021fccb75b75b7d3dfc7460f5","title":"On Scalar Embedding of Relative Positions in Attention Models"},{"paperId":"d82a03951796a352dc4442387782c21d2b761480","title":"GTA: Graph Truncated Attention for Retrosynthesis"},{"paperId":"b8cee43a51c44f8f4448e78e41ecf081987707cf","title":"Towards Robust Vision Transformer"},{"paperId":"50a9ff8b1a3f49220baa0950bc4645ad6f88f013","title":"HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding"},{"paperId":"10d9c1f66742ac68ea8cd13f5c0a3968b3771c54","title":"GCN-MIF: Graph Convolutional Network with Multi-Information Fusion for Low-dose CT Denoising (preprint)"},{"paperId":"64a29bee2e1ad29547d590a3cc26274f4c537145","title":"Not All Memories are Created Equal: Learning to Forget by Expiring"},{"paperId":"95080f0ee0a2d4fcb9fd330bf4060870849a8b47","title":"Global Structure-Aware Drum Transcription Based on Self-Attention Mechanisms"},{"paperId":"fda805c6e85a03d10549acdc5489420ca8f3d405","title":"MuseMorphose: Full-Song and Fine-Grained Piano Music Style Transfer With One Transformer VAE"},{"paperId":"aecc84aa9ff29a99f44556689592cd0fff84cb87","title":"Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue StateTracking"},{"paperId":"6a3e13d7926a4aaa0ddcd3acc7c08e8d24c330e5","title":"ReadTwice: Reading Very Large Documents with Memories"},{"paperId":"61cce75554a6d1bb802f26758c3b0ba97de6918d","title":"Graph Attention Networks with Positional Embeddings"},{"paperId":"7f066a5fb06c0abe09ce67e9e420868d49ea47ad","title":"Duplex Sequence-to-Sequence Learning for Reversible Machine Translation"},{"paperId":"a83902f8b3aadfda633968a840ca1738bedef837","title":"Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs"},{"paperId":"5213e56878c0df2f6f8f59c1d412daf98de1d8bf","title":"COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 prediction"},{"paperId":"b18261a2b718cf29a87cb0237592a4725d5e0a6f","title":"AGMB-Transformer: Anatomy-Guided Multi-Branch Transformer Network for Automated Evaluation of Root Canal Therapy"},{"paperId":"01fe50c8023f15bce3628d4581f59bfe33bcdf16","title":"Incorporating Transformer and LSTM to Kalman Filter with EM algorithm for state estimation"},{"paperId":"a3c50f601a6b872751556a4324d29cd10db1704f","title":"GasHis-Transformer: A multi-scale visual transformer approach for gastric histopathological image detection"},{"paperId":"6dd0d7749691ea5f9d826fd7e8b3680d16b07427","title":"Score-Transformer: A Deep Learning Aid for Music Composition"},{"paperId":"32e6c2b2dda7b92d2425998e6e71a57c9bbaabec","title":"MVS2D: Efficient Multiview Stereo via Attention-Driven 2D Convolutions"},{"paperId":"2c9fdba6bf846e0986cbbf30d56b467d9e334333","title":"ConTNet: Why not use convolution and transformer at the same time?"},{"paperId":"52f79cb91dd3149960dd5d436d8746441fbffce4","title":"Optimizing small BERTs trained for German NER"},{"paperId":"1841d235095ba022f1463f0ac2436e29601829e4","title":"Neural Machine Translation for Harmonized System Codes prediction"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"ceb1a7e4bccb14b048e81e023b3219edca08862b","title":"Deep drug-target binding affinity prediction with multiple attention blocks"},{"paperId":"db46b0de44c5113c47f0ec5392eb91d0726497bf","title":"A Simple and Effective Positional Encoding for Transformers"},{"paperId":"1abd71fb70b8c3639af1d087cd179eaef8c718b0","title":"MIMO Self-attentive RNN Beamformer for Multi-speaker Speech Separation"},{"paperId":"eaa9fde5756852745e7e14383794ba75f797cc8b","title":"Question Decomposition with Dependency Graphs"},{"paperId":"286d2e0f3d882a37f486623c716d8a54a4a58fdc","title":"Dynamic Graph Neural Networks for Sequential Recommendation"},{"paperId":"999d78a048ae723be2787e25841863b725665e9c","title":"Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling"},{"paperId":"5b68522f58b61e7235b852677337ef3725075fd9","title":"Co-Scale Conv-Attentional Image Transformers"},{"paperId":"d27eac86c86a953a5b1ad13f7c7bc9d5fb127837","title":"Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN"},{"paperId":"5db0b92418dfe1f44a9440c682fae05b77bc6587","title":"Nonparametric analysis of inter‐individual relations using an attention‐based neural network"},{"paperId":"19be83b770b96905288fa47dd882665a4c64bb45","title":"Estimating articulatory movements in speech production with transformer networks"},{"paperId":"c114db5f1c38cbe6797bc74ef98072cac71f6cc6","title":"ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser"},{"paperId":"dcd0cb3efe3a21c9b5240a5c4e1f1fdd98200b7c","title":"Context-Self Contrastive Pretraining for Crop Type Semantic Segmentation"},{"paperId":"b43b69a6d303fe3c349d5ecf5920ff1dd47ec650","title":"Dual self-attention with co-attention networks for visual question answering"},{"paperId":"e12e837cb2e9baeaefdcab06fe1c75add8f46389","title":"Effective gene expression prediction from sequence by integrating long-range interactions"},{"paperId":"17a2834c6540703fd35edde4e5f7cc50da4ab41f","title":"MuSLCAT: Multi-Scale Multi-Level Convolutional Attention Transformer for Discriminative Music Modeling on Raw Waveforms"},{"paperId":"319d6c88152780f2835687100921393457668cc0","title":"ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation"},{"paperId":"5b81580712f6e16abd05824c162537674e99b095","title":"Learning Neural Representation of Camera Pose with Matrix Representation of Pose Shift via View Synthesis"},{"paperId":"003326a15fc4a8833785a47a741d7712474fa256","title":"LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference"},{"paperId":"44930df2a3186edb58c4d6f6e5ed828c5d6a0089","title":"Attention, please! A survey of neural attention models in deep learning"},{"paperId":"c478ad9eb6ffd16f3c63cfc942fd77081fbb6c3f","title":"TDJEE: A Document-Level Joint Model for Financial Event Extraction"},{"paperId":"92255f5be9f254057f809943398c156808ef47eb","title":"Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention"},{"paperId":"bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1","title":"A Practical Survey on Faster and Lighter Transformers"},{"paperId":"712bf9c7202b8dec3d06491a380bbac9c9600fbc","title":"Mask Attention Networks: Rethinking and Strengthen Transformer"},{"paperId":"6b9564d94aa4415b90561b430ac85b0ee11a5833","title":"NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction"},{"paperId":"91e8117e7ebc966bc76de2cb52ec717d2acdb1a4","title":"Scaling Local Self-Attention for Parameter Efficient Visual Backbones"},{"paperId":"05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14","title":"Language-Agnostic Representation Learning of Source Code from Structure and Context"},{"paperId":"c9963e89add5e02bf6e5b340586ee6406e087089","title":"API2Com: On the Improvement of Automatically Generated Code Comments Using API Documentations"},{"paperId":"5dc1e65fdad52b1847c49fae1dbc6a1ba8ac232f","title":"Neural Machine Translating from XML to RDF"},{"paperId":"cfb5eb84d6956de1207c18bf27137affbafb6141","title":"Question-relationship guided graph attention network for visual question answer"},{"paperId":"395221cd3ff22539f261ef1fc305fa3e928fca35","title":"Unsupervised Transfer Learning in Multilingual Neural Machine Translation with Cross-Lingual Word Embeddings"},{"paperId":"6a66cf7dd6d1f70e99018518e765917fb76491ad","title":"Variable-rate discrete representation learning"},{"paperId":"483b7857df6104d5c8899c61c64ab0397f87f7fa","title":"I2Net: Mining intra-video and inter-video attention for temporal action localization"},{"paperId":"41bc8ad57ae622a88de5611c4933a77a5cc5cdcc","title":"Symbolic Integration by Integrating Learning Models with Different Strengths and Weaknesses"},{"paperId":"e369e6a871eb9ca85cc41ff309631f6da2a79c2b","title":"A Survey on Document-level Neural Machine Translation"},{"paperId":"f77d9e0b14598df68d6cfb64c5dd70916ee29aea","title":"IOT: Instance-wise Layer Reordering for Transformer Structures"},{"paperId":"3bdc2e6b22b2b7567eb6fc02a4a4348737cb8526","title":"Advances in Deep Learning Methods for Visual Tracking: Literature Review and Fundamentals"},{"paperId":"20251a6c246d203d663a3c4253f8fc32ccb42ed6","title":"Compute and Memory Efficient Universal Sound Source Separation"},{"paperId":"b7d89dc055618b68a29d199457ee1fec18543dab","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"3cbce50dcd213b10190a9d27ddafa700752380e5","title":"An End-to-End Network for Emotion-Cause Pair Extraction"},{"paperId":"88a8061c17de49527c05e14b593671913fb32117","title":"Nested-block self-attention for robust radiotherapy planning segmentation"},{"paperId":"5776b9e8146bb7f6b3df1321cc7f121670699c90","title":"Iterative SE(3)-Transformers"},{"paperId":"daaa6d5a00adad83ca2e00820d9fe9e6d78f40c7","title":"Accelerating Transformer for Neural Machine Translation"},{"paperId":"2fd10e095b146f99da8cdc6ff58720e2e8fca36d","title":"When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute"},{"paperId":"79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb","title":"Do Transformer Modifications Transfer Across Implementations and Applications?"},{"paperId":"63812f583caac3ac32bbfb64f66ba69e57c1e90a","title":"Conditional Positional Encodings for Vision Transformers"},{"paperId":"7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview"},{"paperId":"1efb035c38baa3a3a282d3d8184679b2146b0671","title":"Few Shot Learning for Information Verification"},{"paperId":"1e5e8106700c8dbdfa036a5a9be5e61e06c0ed02","title":"Medical Transformer: Gated Axial-Attention for Medical Image Segmentation"},{"paperId":"07129116591119162bf873e7dafc9ab44aae323e","title":"Celltrack R-CNN: A Novel End-To-End Deep Neural Network For Cell Segmentation And Tracking In Microscopy Images"},{"paperId":"92a21e9ce3f702d0a2b0d619c4c974bdc8ff23cd","title":"Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction"},{"paperId":"0fe8b49369d70a2be473435a82b01544704b3c9f","title":"Evolving Attention with Residual Convolutions"},{"paperId":"36906613dcef29263afe711f128da1fc916cbbee","title":"Gaussian Kernelized Self-Attention for Long Sequence Data and its Application to CTC-Based Speech Recognition"},{"paperId":"cec7872b194aadf54140578b9be52939eb1112e9","title":"LambdaNetworks: Modeling Long-Range Interactions Without Attention"},{"paperId":"594db112d65891fbaf45b27f17d2f9de88ddcd82","title":"Revisiting Language Encoding in Learning Multilingual Representations"},{"paperId":"868d4d9c9d8afee78f21a0113cff762ff5eb4961","title":"Translational Equivariance in Kernelizable Attention"},{"paperId":"7095f65f655468951b7e1edb97db4ede9365064a","title":"A multi-omics-based serial deep learning approach to predict clinical outcomes of single-agent anti-PD-1/PD-L1 immunotherapy in advanced stage non-small-cell lung cancer."},{"paperId":"6e8f35c6d54acb14109c9b792a62609eac8a7b5e","title":"TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up"},{"paperId":"1f479fe113b0a4a5d6da08e0632395d3273fd3dd","title":"InsNet: An Efficient, Flexible, and Performant Insertion-based Text Generation Model"},{"paperId":"6f22b0da3447eebff003d0f6d4b9ed0864b85d2a","title":"Transformer Language Models with LSTM-Based Cross-Utterance Information Representation"},{"paperId":"1a73f486d35ad7c20e140f30ab7148c6804f8aaa","title":"Attentive Gaussian processes for probabilistic time-series generation"},{"paperId":"57c4035f49e6063dd9e8498418f05135d931235a","title":"On the application of BERT models for nanopore methylation detection"},{"paperId":"ca4b945ad7d109c3cbc2170a942ca3b0ecf6fcf5","title":"Wake Word Detection with Streaming Transformers"},{"paperId":"1dd30a5e56b39486369f27aa5ea8e3f282670c85","title":"NRTSI: Non-Recurrent Time Series Imputation"},{"paperId":"e175d23d33ccef7df511c1d299851ba763ca6ea4","title":"Structural attention network for graph"},{"paperId":"292d8fabc74dc5cbb42df821afb724535220fceb","title":"Syntax-Informed Self-Attention Network for Span-Based Joint Entity and Relation Extraction"},{"paperId":"a6ca91afe845ef5294c40c2029e0c1cba19ba40b","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"dc030407206fc26fda7df35c79f9a5a96419edb1","title":"Enquire One’s Parent and Child Before Decision: Fully Exploit Hierarchical Structure for Self-Supervised Taxonomy Expansion"},{"paperId":"16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78","title":"Bottleneck Transformers for Visual Recognition"},{"paperId":"f05126c1a792ea64a7af0c8c68b03bcddec5b297","title":"VisualMRC: Machine Reading Comprehension on Document Images"},{"paperId":"0477ef522fa3ec861a9ff5ba35c96cc08ba3697f","title":"Representations for Question Answering from Documents with Tables and Text"},{"paperId":"4f889a6b76fb77cbd9ed239ba9ba38748bceb89a","title":"An Investigation of Positional Encoding in Transformer-based End-to-end Speech Recognition"},{"paperId":"bf8241fff03fa98c8e57d858c125110a218c0a35","title":"Mitigating the Position Bias of Transformer Models in Passage Re-Ranking"},{"paperId":"1b0ed19bb9d9f07fe8eee0faa3877b3c871e964f","title":"To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph"},{"paperId":"1f465b77c7d774da1fa18827e197a6372bbaa63b","title":"A Zero Attentive Relevance Matching Networkfor Review Modeling in Recommendation System"},{"paperId":"e08eed9608382beea1febca49119c665fbabd031","title":"The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models"},{"paperId":"3a906b77fa218adc171fecb28bb81c24c14dcc7b","title":"Transformers in Vision: A Survey"},{"paperId":"7434fb6c79a50bfedd4ad1c872f9e4fb1f25571a","title":"ERNIE-Doc: A Retrospective Long-Document Modeling Transformer"},{"paperId":"0822f8d7e6a72a65e65f147d3a8d8fccd485da40","title":"Shortformer: Better Language Modeling using Shorter Inputs"},{"paperId":"5af128253aa1b8d21821ab10f2aa446b5143fabd","title":"A Chinese named entity recognition method combined with relative position information"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"cd02e0a094953077217e2e62f3557b36a365acff","title":"Optimizing Deeper Transformers on Small Datasets"},{"paperId":"0197abda042e6de87b5f716caa708a6a459f078c","title":"LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding"},{"paperId":"77f9b54da444f49b2436712abe932627f16cbf95","title":"Code Summarization with Structure-induced Transformer"},{"paperId":"72bc6534a83a8a8bc51c3d7185d7820a37728e98","title":"Multiple Structural Priors Guided Self Attention Network for Language Understanding"},{"paperId":"5d04a41b2b8ecf75aa6631abebe7a00bb25a3cdf","title":"Hybrid Interest Modeling for Long-tailed Users"},{"paperId":"a817d740f64dcc04734ece08e20e136ccff240e7","title":"Learning Light-Weight Translation Models from Deep Transformer"},{"paperId":"d40c77c010c8dbef6142903a02f2a73a85012d5d","title":"A Survey on Vision Transformer"},{"paperId":"0aa2a93e8c8cdae408c3d76149ab758ad4075ded","title":"Predicting Decisions in Language Based Persuasion Games"},{"paperId":"15e82f4d369c73320e83fa02e9cb782239838cab","title":"Attention-based Image Upsampling"},{"paperId":"95b0fc5bde87f7c4f513cf8532ca873823971d2f","title":"Portfolio Optimization with 2D Relative-Attentional Gated Transformer"},{"paperId":"f51bac51dc4f9f42942966b13f62df355ac9af1f","title":"Weighted Cluster-Range Loss and Criticality-Enhancement Loss for Speaker Recognition"},{"paperId":"4c5d4601a3a19c31da6588d2a34adfb161f68c0e","title":"Imitating Interactive Intelligence"},{"paperId":"37f4cecf9cf83cb6de2ffaecc864226eff1f9918","title":"Document Graph for Neural Machine Translation"},{"paperId":"bd6788cb13952d67cf668cc08f62d20ea090b646","title":"Enhancing Attention Models via Multi-head Collaboration"},{"paperId":"3a77f0699c036ee09ab5805213ca1cb5baa12a7a","title":"Investigating Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network"},{"paperId":"787119e3c3f819244c82b7d97779473773e60696","title":"MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"},{"paperId":"5a84bf724ee79c507e66e5dc564c98c76d553a53","title":"Porous Lattice Transformer Encoder for Chinese NER"},{"paperId":"cc12cea62725035d5428888117e0d58359c539e3","title":"Generalized Shortest-Paths Encoders for AMR-to-Text Generation"},{"paperId":"ac4ae352e2434d4a71c6a79bf5f93df5f600b058","title":"Increasing Learning Efficiency of Self-Attention Networks through Direct Position Interactions, Learnable Temperature, and Convoluted Attention"},{"paperId":"d5052e267f059e8ce78deb477f368b84636e6b3f","title":"Joint Aspect Extraction and Sentiment Analysis with Directional Graph Convolutional Networks"},{"paperId":"28bd12dfc7414b39023ff3cc9c17cb92d1a849c2","title":"ODIANLP’s Participation in WAT2020"},{"paperId":"855fbb2067978a33fe86f9e4e4488a9b87444a07","title":"Open-Ended Multi-Modal Relational Reasoning for Video Question Answering"},{"paperId":"076b78ac25f0f9c0de968ec1374cc791bb403e5f","title":"Improving Performance of a Resonant String-Based Pulsation Attenuator in Hydraulic Systems"},{"paperId":"d568d183750f3dfd338fd702cb188009457c4a5d","title":"A Hierarchical Self-attentive Convolution Network for Review Modeling in Recommendation Systems"},{"paperId":"8188ae74363ab55bf02e4204517314db0112795e","title":"Attention Aware Cost Volume Pyramid Based Multi-view Stereo Network for 3D Reconstruction"},{"paperId":"f556bdc969127ecd8c9f3faef20f200beff7826f","title":"A Light Transformer For Speech-To-Intent Applications"},{"paperId":"bb77ed615f4b9e5f55a54e1f69cb3f887667147c","title":"Deep learning pan‐specific model for interpretable MHC‐I peptide binding prediction with improved attention mechanism"},{"paperId":"54e6dc08b468b740bcdfdd83b2514be410994400","title":"Persuasive Dialogue Understanding: the Baselines and Negative Results"},{"paperId":"920294817417d9c2a4fbed86bd595450d58d60dc","title":"The Ubiqus English-Inuktitut System for WMT20"},{"paperId":"c2b4d96db34bd472e84c9234838cc4e808eb1ba9","title":"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language"},{"paperId":"ee15b0d8b7de63bdfc31532e866dfeb675442964","title":"s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis"},{"paperId":"1dda7bbd03435ab1b794516785c4363f4dc95c62","title":"Forecasting CPI inflation components with Hierarchical Recurrent Neural Networks"},{"paperId":"ce7fd7cd09b0fc82969d374b438d14bcaa9c6ee2","title":"Transformer Based Molecule Encoding for Property Prediction"},{"paperId":"4b5a931aea987e01a659a3bf67075d89130e7bbe","title":"I Know What You Asked: Graph Path Learning using AMR for Commonsense Reasoning"},{"paperId":"56bf3402b25e75faafff96f80fe43168a7e93b97","title":"Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations"},{"paperId":"6d4babd73cb98c452ff10d8bbc13d540fc1626b5","title":"Local-Aggregation Graph Networks"},{"paperId":"00b677e971ded11ac4a7da1b80ffda95b4f1ed78","title":"Pre-Training Transformers as Energy-Based Cloze Models"},{"paperId":"39a991a80df0d6586d838dab97d5e17040ad93be","title":"Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays"},{"paperId":"232b40980acb55afa89ec50dd9806a5e551f699b","title":"Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing"},{"paperId":"a45ee25414948e454bf14bf40d5d03dbaf42e9bd","title":"The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks"},{"paperId":"f7f0cbd289ee9f17aad546d78c14830c99e3d37d","title":"Bi-Directional Self-Attention with Relative Positional Encoding for Video Summarization"},{"paperId":"c565ab25591ade03f89277cfbfba507335f5356c","title":"An Empirical Exploration of Local Ordering Pre-training for Structured Prediction"},{"paperId":"3c661d139bf560716db7f4b1939faf087317b509","title":"Every Layer Counts: Multi-Layer Multi-Head Attention for Neural Machine Translation"},{"paperId":"bef4548a43fca8a7410734e4200157d50e257a29","title":"End-to-End ASR with Adaptive Span Self-Attention"},{"paperId":"d2405a1cb8b41701fa8177e1c2438c730051ac01","title":"Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model"},{"paperId":"11c2f3e323161b90d600fb9e0c56de06caea5dde","title":"Blind Deinterleaving of Signals in Time Series with Self-Attention Based Soft Min-Cost Flow Learning"},{"paperId":"5868a7bfe6a4590d332ca66b8097dbe5490c8a73","title":"SmBoP: Semi-autoregressive Bottom-up Semantic Parsing"},{"paperId":"f879bd75a10acbb4223d6feb9652ca3a8d35581f","title":"Don’t Shoot Butterfly with Rifles: Multi-Channel Continuous Speech Separation with Early Exit Transformer"},{"paperId":"556983a574e322fd7d01c6a8193b3785c97a8905","title":"A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code"},{"paperId":"131ee42e4839d153333e17f46facdb6806e98c73","title":"Developing Real-Time Streaming Transformer Transducer for Speech Recognition on Large-Scale Dataset"},{"paperId":"4d51dce0873d8df2aeeb4c4f2b000a9b2993eae4","title":"Stronger Transformers for Neural Multi-Hop Question Generation"},{"paperId":"a2462ff5546b45b1cc7cb50ffc50c7ddececca65","title":"DuoRAT: Towards Simpler Text-to-SQL Models"},{"paperId":"8f8701e17f1b14f5a0a69fa2b24e7d8515f6d711","title":"Multi-Unit Transformers for Neural Machine Translation"},{"paperId":"2a5c611d6a4cfc0ed66562bd754f1cbb5426f702","title":"Multi-Unit Transformer for Neural Machine Translation"},{"paperId":"1d00f609e8cbac2f7a67a04e13adf82a096ea689","title":"Predicting Chemical Properties using Self-Attention Multi-task Learning based on SMILES Representation"},{"paperId":"8452f4248eecc9396bd8146e3a8a8a68d1838bb8","title":"Personalized Flight Itinerary Ranking at Fliggy"},{"paperId":"91b84b91002dc9ff5ee8ef45854c6847ba7f8ce4","title":"DiDi’s Machine Translation System for WMT2020"},{"paperId":"2b2711b23dd2503933b2f02a41574fc72d21aabf","title":"Empirical study of transformers for source code"},{"paperId":"cf469e6fffbc5344572959a3f71d03b8037f284b","title":"DA-Transformer: Distance-aware Transformer"},{"paperId":"707ee137373a9f3e650a032f7627053aa59bea71","title":"Non-autoregressive Neural Machine Translation with Distortion Model"},{"paperId":"b2d849e2c5a87b812289db8ae555b20d299ca809","title":"Information Extraction from Swedish Medical Prescriptions with Sig-Transformer Encoder"},{"paperId":"4889ba5a8ae8b2169dd44d1d3a605bf9820bae8d","title":"What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding"},{"paperId":"2a0417e641233cd9aa77c58f00f944d3c5e84d62","title":"AutoQA: From Databases to Q&A Semantic Parsers with Only Synthetic Training Data"},{"paperId":"53af14897ccca4f8d6c465ee133e859f99ddd7e7","title":"Shallow-to-Deep Training for Neural Machine Translation"},{"paperId":"c05c39a01a418b2b32bbf9d543293a2665cd3d6b","title":"SSACBKM: An Integration Model for Biomedical Relationship Extraction"},{"paperId":"9af8f2bba51c76fe7fcc1eec5b57b5445747b5ad","title":"GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction"},{"paperId":"46e7383e6fb8da77479d0a828c7a24d924302169","title":"Stepwise Extractive Summarization and Planning with Structured Transformers"},{"paperId":"efb315c39ee035ef5fad8a66378b96b653f4a96d","title":"SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling"},{"paperId":"64b9be00f4eecd465b4e8e46e2ab7624d7eaeb2b","title":"Global Self-Attention Networks for Image Recognition"},{"paperId":"1be28ce9a1145c2cf4f78e6c494a4c15397fbac3","title":"SumGNN: multi-typed drug interaction prediction via efficient knowledge graph summarization"},{"paperId":"90d6719d1b898a0d35e2502ca0f3724eb1e7245d","title":"MIA-Prognosis: A Deep Learning Framework to Predict Therapy Response"},{"paperId":"7a5d1a7646ce1884ad76a0e177f956ae4d77c722","title":"Group Equivariant Stand-Alone Self-Attention For Vision"},{"paperId":"f8a77d8c43e0fab50aef6c05eafbe653039ac447","title":"Lexicon-Enhanced Transformer with Pointing for Domains Specific Generative Question Answering"},{"paperId":"3214fa2979603057aabd2c82e58c92da9d2fcf25","title":"Entity Relative Position Representation based Multi-head Selection for Joint Entity and Relation Extraction"},{"paperId":"6c46bd6de48214a1e81ebef369070713febaeced","title":"Generating Vietnamese Language Caption Automatically for Scene Images"},{"paperId":"f43bc763593e9b2fa48199274ce79eeaf4c7e238","title":"Information Extraction Method based on Dilated Convolution and Character-Enhanced Word Embedding"},{"paperId":"1f1ba4520e646ab27b2978a1d76f18c5e387ba7b","title":"MQTransformer: Multi-Horizon Forecasts with Context Dependent and Feedback-Aware Attention"},{"paperId":"cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca","title":"Learning to Represent Programs with Heterogeneous Graphs"},{"paperId":"84476fdf6ead3553f4493dff8e02308439d6222b","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"paperId":"28858688894da173dbdcc49899be6e22ea97bc63","title":"MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems"},{"paperId":"517902025f4de7b4c89540ab057516ad357c8214","title":"PURS: Personalized Unexpected Recommender System for Improving User Satisfaction"},{"paperId":"daa00c142fdf25aa454d8ca52e4d20f5d0605964","title":"Temporally Guided Music-to-Body-Movement Generation"},{"paperId":"0fe9f97efe4afd806b30be75a4b698ad2db93fc5","title":"Graph-to-Sequence Neural Machine Translation"},{"paperId":"8189b5580e8080d3d9c800d9855d2f72f867b3f5","title":"Building a Personally Identifiable Information Recognizer in a Privacy Preserved Manner Using Automated Annotation and Federated Learning"},{"paperId":"0e1610392cb0cd91b8261b7fbaed78178e304885","title":"Cascaded Semantic and Positional Self-Attention Network for Document Classification"},{"paperId":"24a151d2811e831992e840bd30f5de5f75aca72d","title":"Multi-domain sentiment analysis with mimicked and polarized word embeddings for human-robot interaction"},{"paperId":"d6d8b2bd23cb3feea0e37714564e8a849e105442","title":"Speech Emotion Recognition UsingConvolutional Neural Network and Long-Short TermMemory"},{"paperId":"6f3ad944bb56c6e930eac3d95cff1521ac03101e","title":"On estimating gaze by self-attention augmented convolutions"},{"paperId":"53aaff9d979084d76daf4048345e0ccf4bacb386","title":"TRU-NET: a deep learning approach to high resolution prediction of rainfall"},{"paperId":"5bc8f2fd72b3ffa0432a971c4097dc370f6d6222","title":"SoDA: Multi-Object Tracking with Soft Data Association"},{"paperId":"95210c74f28664d1481be29af411220d27cf63d7","title":"POP909: A Pop-song Dataset for Music Arrangement Generation"},{"paperId":"2abd377ae35740a28a38fde4f84aa6534e39ff37","title":"Continuous Speech Separation with Conformer"},{"paperId":"eb6309dd0e70fe68ba2c97629f9091eae3cbf8b4","title":"Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable End-to-End Speech Recognition"},{"paperId":"97eccd93834d48e1eb3063b6f51288e1421a54b2","title":"EXTENDED CONVOLUTIONAL NEURAL NETWORKS POST-TRAINED WITH FACTORED STATISTICAL MACHINE"},{"paperId":"1da75743ad30c9c9b23a534888be2dc99e52d087","title":"Select, Extract and Generate: Neural Keyphrase Generation with Syntactic Guidance"},{"paperId":"81c5c35ad1311fb1ebae00f6d87631021fc7d956","title":"Self-Attention and Dynamic Convolution Hybrid Model for Neural Machine Translation"},{"paperId":"41374b84f9bffda7862bb6103301017d3af2b2e4","title":"Integrating Bi-Dynamic Routing Capsule Network with Label-Constraint for Text classification"},{"paperId":"6181c5fb6b82a267dae129351cce8477a1fb463d","title":"Relation Aware Attention Model for Uncertainty Detection in Text"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"5fb52197928290d3020b2256ccab22d5bf93c366","title":"Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos"},{"paperId":"e7a4e1a5d933ca482ef215784085eaf19b8d5e16","title":"Spatially Aware Multimodal Transformers for TextVQA"},{"paperId":"43971a0a2593f660427e016032b983b52f8dd8eb","title":"Foley Music: Learning to Generate Music from Videos"},{"paperId":"566a026da7d2b1eec034017d67c6bb901f5acd85","title":"RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition"},{"paperId":"9a2b7ca4d98352f57785115862a81ff06666ef8a","title":"Self-Attentive Hawkes Process"},{"paperId":"20aec946cdc97135677afaa2373fe379cc3d33f7","title":"Transformer-XL Based Music Generation with Multiple Sequences of Time-valued Notes"},{"paperId":"1f1e38015e6f9d072c2b7bb2af1212d1c71b5b06","title":"Learning Graph Structure With A Finite-State Automaton Layer"},{"paperId":"9c965183931467448f0a6d24cf37050478ba26f2","title":"Learning Adversarial Transformer for Symbolic Music Generation"},{"paperId":"1a67761027bd89b078445bef7040ded5d20dd2c8","title":"Software Engineering Event Modeling using Relative Time in Temporal Knowledge Graphs"},{"paperId":"40f003d5784d069a4c901edc5438b11f75496d51","title":"LIT Team’s System Description for Japanese-Chinese Machine Translation Task in IWSLT 2020"},{"paperId":"d358f8939992a8c2c649338810aa5e66ed665568","title":"Polar Relative Positional Encoding for Video-Language Segmentation"},{"paperId":"8b2aae249e0a6961fb4790644561247620c40791","title":"CASIA’s System for IWSLT 2020 Open Domain Translation"},{"paperId":"7d8c9587ec051ed1612a686d93c6d1a92b7cbda4","title":"Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models"},{"paperId":"df6d1e461dc8245aa441eb87267ec5d2c28ad1d2","title":"Heterogeneous Graph Transformer for Graph-to-Sequence Learning"},{"paperId":"18f86146a095a30d57569197c04f807bb10064e8","title":"The NiuTrans System for WNGT 2020 Efficiency Task"},{"paperId":"a4886702f5c92f0d643a61578a58684d403e8f5e","title":"PA-GGAN: Session-Based Recommendation with Position-Aware Gated Graph Attention Network"},{"paperId":"ee0b27c32603a352ddebe27f98f89fc8d7795cb6","title":"TimeSAN: A Time-Modulated Self-Attentive Network for Next Point-of-Interest Recommendation"},{"paperId":"8256f48f759cf85044db251cc512f965834945b3","title":"Rethinking Positional Encoding in Language Pre-training"},{"paperId":"01db8a5e8d3d868517eb3b2b50aabcabf7b66d50","title":"Hybrid Models for Learning to Branch"},{"paperId":"5156381d63bb3e873533b08f203cb56c2d79b6c9","title":"Object-Centric Learning with Slot Attention"},{"paperId":"cf1eb488136995d76b5e64410fffa63ed7236702","title":"Differentiable Window for Dynamic Local Attention"},{"paperId":"82b5ee0ae468cd2e0b62df96f42b5b5480c75510","title":"Infinite attention: NNGP and NTK for deep attention networks"},{"paperId":"4e99406c10b61004826a0428634ad5c7b0f2f731","title":"SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks"},{"paperId":"6c5fb4717f2a402a6a6ba60b896b6ccdfa715981","title":"I-BERT: Inductive Generalization of Transformer to Arbitrary Context Lengths"},{"paperId":"659597b1699ba5b73da9a8628bf7e4ad9bebd242","title":"Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting"},{"paperId":"856b12bc3b99db662e93def08e49db73c1df5e79","title":"Modeling Graph Structure via Relative Position for Better Text Generation from Knowledge Graphs"},{"paperId":"75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1","title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling"},{"paperId":"fa22ef1881de15c75ad7e65936bdad7dad43ebad","title":"Syntax-based Transformer for Neural Machine Translation"},{"paperId":"59eccbe007ccf92172499db8420c12ea0933e24b","title":"Explicitly Modeled Attention Maps for Image Classification"},{"paperId":"575e4b980654773c40101659fe8d63ba325a2431","title":"Multi-head enhanced self-attention network for novelty detection"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"d457157eab0df47daca66d5c81321f0a9c41e30b","title":"Positional Context Aggregation Network for Remote Sensing Scene Classification"},{"paperId":"3c33cc56dfb1d53f568ec80ebaa133d9c91b5ea7","title":"BG-SAC: Entity relationship classification model based on Self-Attention supported Capsule Networks"},{"paperId":"5d8cbed235f6aed824e515885a5c8c04161c7761","title":"A Survey of the Model Transfer Approaches to Cross-Lingual Dependency Parsing"},{"paperId":"86d70b862b37402b42848c544813c7b920fc2e84","title":"Improving Self-Attention Networks With Sequential Relations"}],"references":[{"paperId":"33998aff64ce51df8dee45989cdca4b6b1329ec4","title":"Graph Attention Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"2cd8e8f510c89c7c18268e8ad51c061e459ad321","title":"A Decomposable Attention Model for Natural Language Inference"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e","title":"End-To-End Memory Networks"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"}],"id":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","summary":"This work presents an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements, on the WMT 2014 English-to-German and English- to-French translation tasks."},{"url":"https://www.semanticscholar.org/paper/cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting","venue":"International Conference on Wirtschaftsinformatik","year":2018,"referenceCount":22,"citationCount":37,"influentialCitationCount":0,"publicationDate":"14/09/2018","authors":"Chung-Chi Chen,Hen-Hsen Huang,Yow-Ting Shiue,Hsin-Hsi Chen","citations":[{"paperId":"6ad83dbd9a2da9607ece139c797694fd95738f62","title":"When Does Aggregating Multiple Skills with Multi-Task Learning Work? A Case Study in Financial NLP"},{"paperId":"b836382cf055651667738d320e5face5f4279d8e","title":"Evolving Social Media Background Representation with Frequency Weights and Co-Occurrence Graphs"},{"paperId":"251c3afbaafcc9b5178534be9109f644bfc5e912","title":"Enhancing Knowledge Bases with Quantity Facts"},{"paperId":"90e39ec249b42a23ee9847c45aa21b1fc3c6d22f","title":"An Overview of Financial Technology Innovation"},{"paperId":"fa118351f9088723499d89cc754c1caea4be16d2","title":"Twitter-aided decision making: a review of recent developments"},{"paperId":"320c1c6647a5b975c901347f71638c881888686b","title":"NQuAD: 70,000+ Questions for Machine Comprehension of the Numerals in Text"},{"paperId":"01f68682c88c23376dcbf194c846232cba5f28a6","title":"Distilling Numeral Information for Volatility Forecasting"},{"paperId":"40a1f266bb5ca853837355bfff272a55f0049c81","title":"Mining Numbers in Text: A Survey"},{"paperId":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"},{"paperId":"2e4aeff09548d160f91cb95b54cbc02ef53925b6","title":"Evaluating the Rationales of Amateur Investors"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"0772212420238b48233256a59912f8e6a31cde3d","title":"NumClaim: Investor's Fine-grained Claim Detection"},{"paperId":"a5ae9f992264908e51c7925280f42ee17a500858","title":"NLP in FinTech Applications: Past, Present and Future"},{"paperId":"c4295b53a1f6c4d609d6dfabd7e08d4b416b02e4","title":"Issues and Perspectives from 10,000 Annotated Financial Social Media Data"},{"paperId":"45ed7728a9b863d4c10efb53a8a4423ab7e67378","title":"Predictive Analytics on Emotional Data Mined from Digital Social Networks with a Focus on Financial Markets"},{"paperId":"0b078d4bbed8bcfd57fdab6da40ecd8a9601fc62","title":"Crowd View: Converting Investors' Opinions into Indicators"},{"paperId":"d84fa6b830ebb6188042d66aa850fe5ddcfbb2d4","title":"Next Cashtag Prediction on Social Trading Platforms with Auxiliary Tasks"},{"paperId":"2a0fc534ebf1b22b5851f5b1fb781dd707f7c530","title":"Numeral Attachment with Auxiliary Tasks"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"226144c37145e815d8bebfb8544fca9d69490348","title":"Final Report of the NTCIR-14 FinNum Task: Challenges and Current Status of Fine-Grained Numeral Understanding in Financial Social Media Data"},{"paperId":"ed7f9ecb033ea3f139bc97096caf406a8f0d91a9","title":"CrowdPT: Summarizing Crowd Opinions as Professional Analyst"},{"paperId":"5a6b6fbb1ab905d305223d1920205620f0580433","title":"MultiFin: A Dataset for Multilingual Financial NLP"},{"paperId":"4b470b68603b86582779fa33e5f034116c73685c","title":"Overview of the NTCIR-16 FinNum-3 Task: Investor’s and Manager’s Fine-grained Claim Detection"},{"paperId":"e722bcf6a706259dbae2950e385141a1ab7c4a3d","title":"WUST at NTCIR-16 FinNum-3 Task"},{"paperId":"2ecde642d9aa18d0b9b385ca76ab2520a66b5ace","title":"Financial Opinion Mining"},{"paperId":"7a318a00d61b1f063e954061e3bec35faf84b372","title":"Organizing Financial Opinions"},{"paperId":"3b458728091590afaa9a2b0d86444b5fa6c80275","title":"Sources and Corpora"},{"paperId":"dd97b0b041587fe73d4dbfc8a72e2cb615e85cb8","title":"Numerals in Financial Narratives"},{"paperId":"0cb1f68586f1d0e1f4a970b902b33f4cd5a6c7cd","title":"Is Domain Adaptation Worth Your Investment? Comparing BERT and FinBERT on Financial Tasks"},{"paperId":"fae743a3c57699793f59b81ac28b6a54015bcc82","title":"TMUNLP at the NTCIR-15 FinNum-2"},{"paperId":"e35a35e92e8b45c4826173dfb92440685189e025","title":"MIG at the NTCIR-15 FinNum-2 Task: Use the Transfer Learning and Feature Engineering for Numeral Attachment Task"},{"paperId":"a6b5266f340c36773f85a02673f35f381a1fc964","title":"Overview of the NTCIR-14 FinNum Task Fine-Grained Numeral Understanding in Financial Social Media Data"},{"paperId":"bb5f0975be40fc85907a3baadc66f16f73611679","title":"WUST at the NTCIR-14 FinNum Task"},{"paperId":"1d88b554dadf7a70dde8e5bbfe72393784a29ffb","title":"aiai at the NTCIR-14 FinNum Task : Financial Numeral Tweets Fine-Grained Classification Using Deep Word and Character Embedding-Based Attention Model"},{"paperId":"14314873db4e784d8dff288483ceccffa9ec0c2e","title":"ASNLU at the NTCIR-14 finnum task : Incorporating Knowledge into DNN for Financial Numeral Classification"},{"paperId":"21d95e881ab980b599a2086285204933d76149df","title":"FinNum Task : Enriched Sequence Labeling for Numeral Classification"},{"paperId":"97a223895354de39bd6ac1ba2cddc14946e9bb8e","title":"Overview of the NTCIR-15 FinNum-2 Task: Numeral Attachment in Financial Tweets"}],"references":[{"paperId":"dd0a60b3dbd35db5cac6680e4a5bf8efbfabb52e","title":"SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News"},{"paperId":"6642586d72329dbb78af964670876b8308f91c80","title":"Predicting Stock Market Behavior using Data Mining Technique and News Sentiment Analysis"},{"paperId":"055c6b7ca71dee209689264fec2d93cb97e69a79","title":"Learning to Generate Market Comments from Stock Prices"},{"paperId":"2366429e00310005da25612a3941fdaed1854fd1","title":"Temporal information extraction from clinical text"},{"paperId":"24158c9fc293c8a998ac552b1188404a877da292","title":"Neural Architectures for Named Entity Recognition"},{"paperId":"513167c08db5139162710aad9b2c217b344df2c4","title":"Numerical Relation Extraction with Minimal Supervision"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"70c621519bc02d7d2aaa79ca2320fe1af38b6f87","title":"News impact on stock price return via sentiment analysis"},{"paperId":"1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba","title":"Convolutional Neural Networks for Sentence Classification"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"1a8bd823c82bd68ab4fb83d13a730badbfe0726d","title":"The future of financial services"},{"paperId":"e498784edf2c02fe0b228479f88120f08b381cb6","title":"Twitter mood predicts the stock market"},{"paperId":"6c93b11ce414776553ed5cb9eb5d3cffa2a5fc6b","title":"Extraction and Approximation of Numerical Attributes from the Web"},{"paperId":"8d8536e5f330d9aca2e9ad608563b7a2cacd5f91","title":"Temporal Information Extraction"},{"paperId":"fa22ad98844764c014fd8e79cb1ed9e4caf8a9ee","title":"ISO-TimeML: An International Standard for Semantic Annotation"},{"paperId":"04b23f577c20d1a0e2a67aadda555f58e6d23d6e","title":"Support Vector Machines"},{"paperId":"24424f4050700dfa940851385d2e1ab7ba5d0cdc","title":"Extended Named Entity Ontology with Attribute Information"},{"paperId":"11b1cd032ebbc9d25ca661c5595ce32de614c90b","title":"Analyzing the Analysts: Career Concerns and Biased Earnings Forecasts"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"e23c34414e66118ecd9b08cf0cd4d016f59b0b85","title":"Bidirectional recurrent neural networks"},{"paperId":"7e7343a5608fff1c68c5259db0c77b9193f1546d","title":"The measurement of observer agreement for categorical data."},{"paperId":"e9c7307e1278c63e32af87ef0ae69a2cfbb6083d","title":"An Evaluation of Security Analysts' Forecasts"}],"id":"cabb0a468af8184e0e930841435b65679b580521","summary":"This work is the first attempt to understand numerals in financial social media data, and it provides the first comparison of fine-grained opinion of individual investors and analysts based on their forecast price."},{"url":"https://www.semanticscholar.org/paper/81b4920ad488affaee27389ff9540b7fea90a4ce","title":"“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding","venue":"Conference on Empirical Methods in Natural Language Processing","year":2019,"referenceCount":36,"citationCount":132,"influentialCitationCount":18,"publicationDate":"06/09/2019","authors":"Ben Zhou,Daniel Khashabi,Qiang Ning,D. Roth","citations":[{"paperId":"8d848ce7ab6ccde4854d3c658a169215ae483029","title":"An Overview Of Temporal Commonsense Reasoning and Acquisition"},{"paperId":"f0a7872fed856fe4f10f8fcdca60f5aa8ecfae12","title":"SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning"},{"paperId":"24a285b1e7ee9acfec9a82f7123563b62f532203","title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning"},{"paperId":"47a082afc342d94f031c5e92a606d09a0631cf28","title":"Mitigating Temporal Misalignment by Discarding Outdated Facts"},{"paperId":"05bfdd1c7280d2dcd794d1ba4b83dcaa0b8b5376","title":"Few-shot Unified Question Answering: Tuning Models or Prompts?"},{"paperId":"f576288b7f5ca4aff3a6492a9db1db5148b7616f","title":"Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?"},{"paperId":"fdc5534830c1cd6ff741c6f8f6789f23480793fd","title":"Prompting with Pseudo-Code Instructions"},{"paperId":"338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c","title":"STOAT: Structured Data to Analytical Text With Controls"},{"paperId":"6cec825e32b1790a69893a5b2506818241506217","title":"A Glimpse in ChatGPT Capabilities and its impact for AI research"},{"paperId":"f3f608d3296c85edb89db1ac625ee5ab9a7f801d","title":"Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering"},{"paperId":"367e0d715ef66c5aea8024e0a97cfa48e29d52c3","title":"Event Knowledge Incorporation with Posterior Regularization for Event-Centric Question Answering"},{"paperId":"4d7571441f507f39133209e8afa7ad088da2199c","title":"ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models"},{"paperId":"24ab6356b355b29a3770db56dd1f2200cdd987fa","title":"Salient Span Masking for Temporal Understanding"},{"paperId":"c73ff841b7e6821e7f18c087ff105d357364c2a5","title":"Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"9af3b6b3f8dcedbb02b88936c428e1cd02503a8a","title":"Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?"},{"paperId":"51e0f488565cdb2c75a7d0ea8ab0b3272718bf82","title":"Whats New? Identifying the Unfolding of New Events in Narratives"},{"paperId":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey"},{"paperId":"fd9c6baf8e485f13285777d8ec24da2359194461","title":"tieval: An Evaluation Framework for Temporal Information Extraction Systems"},{"paperId":"ee153a2c91d36b034dc86c945aee9859b79da812","title":"Test of Time: Instilling Video-Language Models with a Sense of Time"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"22d314c347a6c5c0bbf528a102455229ce0b36b5","title":"Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review"},{"paperId":"f3b41224f1153783a32e0d317cc107b23e92f6c4","title":"Effective Masked Language Modeling for Temporal Commonsense Reasoning"},{"paperId":"a630c70aed27b52f6d04d1e772b153c5a7b6f6fe","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"1d417bdd331912a458de920459f23fcc7f6e8699","title":"Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts"},{"paperId":"bb15f3727f827a3cb88b5d3ca48415c09b40a88f","title":"What Language Model to Train if You Have One Million GPU Hours?"},{"paperId":"1a5f48161df983a0e9485425495121201902433b","title":"DiscoSense: Commonsense Reasoning with Discourse Connectives"},{"paperId":"868f9bb603dfa8f6951787040fc6d62c909a15c2","title":"JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions"},{"paperId":"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","title":"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility"},{"paperId":"285d13bf3cbe6a8a0f164f584d84f8b74067271f","title":"Towards Faithful Model Explanation in NLP: A Survey"},{"paperId":"6923b2dd4f2a691496a6931c59150189cd496d76","title":"A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of Computational Approaches"},{"paperId":"2a89c7cf5f8e8014dbd743d0965ba83b1f67f137","title":"ILLUME: Rationalizing Vision-Language Models through Human Interactions"},{"paperId":"15bacb240e2598457af4ded3039b6988aa9706f0","title":"Few-shot Adaptation Works with UnpredicTable Data"},{"paperId":"4e6f63dc99c09560991cf89207f9fa12c739e711","title":"Can Language Models perform Abductive Commonsense Reasoning?"},{"paperId":"98f19ca97512361b12475b42b67a617de14d33a1","title":"Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic"},{"paperId":"e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP"},{"paperId":"1da214f8f265445b5997f5d677452819b334bdfb","title":"Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts"},{"paperId":"7890ece03cfb88e0620f8e791105569bd7128c76","title":"Housekeep: Tidying Virtual Households using Commonsense Reasoning"},{"paperId":"6e5944b759ef1c2ea2f694221cf54ef718ec95ee","title":"Reasoning about Procedures with Natural Language Processing: A Tutorial"},{"paperId":"078f4efd448822b0e25d3ee0aec842ced606a595","title":"Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts"},{"paperId":"81986b8a3d3fe6c5be06fc4527953fb514ad12e8","title":"Improving In-Context Few-Shot Learning via Self-Supervised Training"},{"paperId":"3a6a97a50695d43d95a015bbb554b2bc0d40394e","title":"Don’t Blame the Annotator: Bias Already Starts in the Annotation Instructions"},{"paperId":"659f3a959e59eb0e5382143417e7d9d2667d2fff","title":"Extracting seizure frequency from epilepsy clinic notes: a machine reading approach to natural language processing"},{"paperId":"97f456643712e9618edd7465676c62af3c8ae690","title":"A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models"},{"paperId":"7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"},{"paperId":"c3a454e50ec0610f1380d55b1988a5eb5d45207b","title":"Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization"},{"paperId":"47df3fd32d00220c85c2c51a571254fd99b2ecc7","title":"MetaICL: Learning to Learn In Context"},{"paperId":"d83c7aa12420d5d035f43d7737bfa6893e9e2c61","title":"Exploring Universal Intrinsic Task Subspace via Prompt Tuning"},{"paperId":"174a0e6da0dfb7f96d4a0a4076eed154c439e41a","title":"Probing Language Models for Understanding of Temporal Expressions"},{"paperId":"3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b","title":"Reframing Instructional Prompts to GPTk’s Language"},{"paperId":"2375e447e86f52a35655c3069bcb047e6655eeb8","title":"Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning"},{"paperId":"5f6a790722f18f0aa7419f8e0c6404c46acd99ba","title":"Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding"},{"paperId":"b1a71d8fb272016d618917a3fc392551fa941c81","title":"Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers"},{"paperId":"f977d79980ed2dfc7b2194fe680895e49b3b60a9","title":"From LSAT: The Progress and Challenges of Complex Reasoning"},{"paperId":"32c1767b956dec644f8bfc1f009fc5a1af47f0cc","title":"Discourse-Level Event Temporal Ordering with Uncertainty-Guided Graph Completion"},{"paperId":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"},{"paperId":"ac8d33e4c0a45e227a47353f3f26fbb231482dc1","title":"Time-Aware Language Models as Temporal Knowledge Bases"},{"paperId":"62953ca1252c9febe07c7007a10911726f37792d","title":"TIMEDIAL: Temporal Commonsense Reasoning in Dialog"},{"paperId":"6597d61bdb531051678c773526758a6dc113b9ce","title":"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences"},{"paperId":"b39f76381897ab744250673a058e5dd06c009813","title":"Event Time Extraction and Propagation via Graph Attention Networks"},{"paperId":"4ca39cf99747b8962fe37e7e025e284872df3425","title":"Comparing Test Sets with Item Response Theory"},{"paperId":"ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb","title":"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"},{"paperId":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions"},{"paperId":"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"},{"paperId":"c7f977f556d2060238fdc1286d057d46958afaf9","title":"ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning"},{"paperId":"2ef4be35f8424ea768aa2e1b44392b3eddbc780b","title":"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema"},{"paperId":"c5943bc4b22a63f595cb1c2823a449e03aad4787","title":"AR-LSAT: Investigating Analytical Reasoning of Text"},{"paperId":"7a9ef45ec4a6fe04c373715f455924b6c1c51ddd","title":"DAGN: Discourse-Aware Graph Network for Logical Reasoning"},{"paperId":"85084fbba9965bdf3446ede255041793f2aa4545","title":"Decomposing and Recomposing Event Structure"},{"paperId":"54bb329be4b557d38e0628b651e7074524d35be2","title":"LOME: Large Ontology Multilingual Extraction"},{"paperId":"d6c919ee0d51432496513ef4b6b2dbd128819779","title":"Microtask Detection"},{"paperId":"33b06c74eea3f400b6f5ef14ef163aef1db42d16","title":"Conditional Generation of Temporally-ordered Event Sequences"},{"paperId":"6eee69031d2e11aa03a5a8fcb219cff4562863be","title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning"},{"paperId":"9793b07ba09d9f2ac9cabd8117daa93bf3db4346","title":"DEER: A Data Efficient Language Model for Event Temporal Reasoning"},{"paperId":"d865c87f6ce4e0be29ae1e3780fae66b8034d04b","title":"TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation"},{"paperId":"f2143dd2a2c9c43d9d28462c1cd2a51f5f3db2c6","title":"Improving Event Duration Prediction via Time-aware Pre-training"},{"paperId":"a0e7d24f91da6a1be5f8856963e3b160d47ca7b7","title":"Towards Zero Shot Conditional Summarization with Adaptive Multi-task Fine-Tuning"},{"paperId":"3d2035edd4dd48e1e638279409e11bf689c461e1","title":"Temporal Reasoning in Natural Language Inference"},{"paperId":"9d436d25981ea61db728bd490f0d54376d08953e","title":"Temporal Reasoning on Implicit Events from Distant Supervision"},{"paperId":"6bd8c038fdcae6cc155f27bcca9acdf04c06c786","title":"Deriving Commonsense Inference Tasks from Interactive Fictions"},{"paperId":"f642ddb69f712e2b517cb6c6d9a062506991d1ed","title":"Commonsense Learning: An Indispensable Path towards Human-centric Multimedia"},{"paperId":"5dfc43bb697acf5eacf8b8a05d78dba8beb0dd42","title":"Paragraph-Level Commonsense Transformers with Recurrent Memory"},{"paperId":"65906e6027246ae9e4ecd18d6e019a24505c842e","title":"Aligning AI With Shared Human Values"},{"paperId":"45f952c21130d655090058e864c2772358a1de72","title":"Commonsense Reasoning for Natural Language Processing"},{"paperId":"5f4a70c0abeafa4ecff90739bd8efbe05ea3bac5","title":"Applying the T5 language model and duration units normalization to address temporal common sense understanding on the MCTACO dataset"},{"paperId":"d075301119b79e5b6b9306f9981d256191882981","title":"Adversarial Training for Commonsense Inference"},{"paperId":"07c1c2429b63fefdae41eb546c31b40de2a880f7","title":"INFOTABS: Inference on Tables as Semi-structured Data"},{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"30602e3382df3abedb5f225b55b7efce8580f74d","title":"ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data"},{"paperId":"5cc4fa603085753610f18c1429b383c62cbed043","title":"ForecastQA: Machine Comprehension of Temporal Text for Answering Forecasting Questions"},{"paperId":"36d6c8895bbc755964b8b2136c6fd6087a7af089","title":"TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions"},{"paperId":"9808d59113029d96f48a0376b1578dbab5427bb4","title":"Unsupervised Commonsense Question Answering with Self-Talk"},{"paperId":"35e6783307f82d1faa39be0653431305abec7271","title":"Evaluating Models’ Local Decision Boundaries via Contrast Sets"},{"paperId":"9fec5868542b4d9070306f1418d1d21666226e90","title":"Evaluating NLP Models via Contrast Sets"},{"paperId":"331a87762228ecca2eebf309b859189f033414ba","title":"NarrativeTime: Dense Temporal Annotation on a Timeline"},{"paperId":"287ed28f1e0f56ff56834580a1bf6720daeb6b2b","title":"Reasoning-Driven Question-Answering for Natural Language Understanding"},{"paperId":"4043a936960de8e149dc208178fe1bcb157c7fa4","title":"Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches"},{"paperId":"64ad3e1fbbc9a4bec2a414fb31b05ee9a62d50cb","title":"tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation"},{"paperId":"c059d251d8f0c2491892271eb40ea3cec4aea830","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue in Multiple Domains"},{"paperId":"a884bc8bc5b04e5ad096649856df5b7931fd3d23","title":"Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer"},{"paperId":"4df24ae528b1f6f5b7efdf3a0eeca47b36cb4475","title":"What’s New? Identifying the Unfolding of New Events in a Narrative"},{"paperId":"a5584d2d9b0de9e1692241d46d0c70942919cd60","title":"Answer-level Calibration for Free-form Multiple Choice Question Answering"},{"paperId":"624c6580cf5abd31db63b38596ffa3622c731b00","title":"Generating Temporally-ordered Event Sequences via Event Optimal Transport"},{"paperId":"a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b","title":"Good Night at 4 pm?! Time Expressions in Different Cultures"},{"paperId":"d29faea4f03cfe337a56dabce84b2d5ce7e473f5","title":"Attention-Focused Adversarial Training for Robust Temporal Reasoning"},{"paperId":"b672d2ec81c9395c312d57a27931864d07664592","title":"A Meta-framework for Spatiotemporal Quantity Extraction from Text"},{"paperId":"b0291bee1d532e8dc082753329d2579549100479","title":"Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts"},{"paperId":"7db7fb25a8753c9efc6b3722d178f94fcc1f82d3","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP"},{"paperId":"190d5823c9405a34ccdfcf6591a54d5a0bcf3f3c","title":"Improving Event Duration Question Answering by Leveraging Existing Temporal Information Extraction Data"},{"paperId":"82c6f5ffd4d2d43ae7684601f607eae26a759a5a","title":"Analytical Reasoning of Text"},{"paperId":"b3f1f0d6bdc9504a17747b9ae49136b3fe41051a","title":"ILLUME: Rationalizing Vision-Language Models by Interacting with their Jabber"},{"paperId":"efaee35df6cb22caa6988a26159b7959f42b14ee","title":"Are Visual-Linguistic Models Commonsense Knowledge Bases?"},{"paperId":"8b0b5b998d65add5c036b45c36afa3f9df96f4b0","title":"Toward Building a Language Model for Understanding Temporal Commonsense"},{"paperId":"b8768fbba4d356a1c49dcd8e4cd33aea0c6472bc","title":"How about Time? Probing a Multilingual Language Model for Temporal Relations"},{"paperId":"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","title":"20Q: Overlap-Free World Knowledge Benchmark for Language Models"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"655db2fd59e4112d0a5596e1695c5765457ca53c","title":"Extraction of Common-Sense Relations from Procedural Task Instructions using BERT"},{"paperId":"7b486a6eac4b46cf39e41c97b25ea22c5d27a883","title":"Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions"},{"paperId":"5d1e996bb64a0d082c85010b9c78036db4da02e0","title":"GCRC: A New Challenging MRC Dataset from Gaokao Chinese for Explainable Evaluation"},{"paperId":"e6d84cf9ae6efa10919bff765613e883a761db62","title":"Open Temporal Relation Extraction for Question Answering"},{"paperId":"8b383bd2eb054787d84636dab7b6ecf4a318c6c7","title":"Towards a Language Model for Temporal Commonsense Reasoning"},{"paperId":"21f018727ce0fa0f932188fa5abcc32426aba7b6","title":"On End-to-end Automatic Fact-checking Systems"},{"paperId":"9485076ed295c6997b8a3148e1f37156eff97813","title":"Research Statement: Climbing the Generality Ladder in NLP"},{"paperId":"65017016b35928cd0e2ab0037200d0f92619b120","title":"Low-resource Learning with Knowledge Graphs: A Comprehensive Survey"},{"paperId":"00dea030e47950e55d6497a5c8acf5053baebcbd","title":"ALICE++: Adversarial Training for Robust and Effective Temporal Reasoning"},{"paperId":"6bb369f874f49cd51415f216f1a3f635f2ca1eed","title":"ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations"},{"paperId":"177bc611389fac0e6239355a2d9eaa66f7aac53e","title":"Improving Unsupervised Commonsense Reasoning Using Knowledge-Enabled Natural Language Inference"},{"paperId":"23446cc15226c46eae9e4414598f7375eacb6ea1","title":"Back to Square One: Bias Detection, Training and Commonsense Disentanglement in the Winograd Schema"},{"paperId":"35115a7e16fb658a5f8e09eeb60a5f3204b24598","title":"Tell Me About Your Day: Designing a Conversational Agent for Time and Stress Management"},{"paperId":null,"title":"Later datasets increase in complexity and scale , incorporating reading comprehension"}],"references":[{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e20cf061851898e8477fea6f195cfe44f8ad7a86","title":"CogCompTime: A Tool for Understanding Time in Natural Language"},{"paperId":"711b1f7cc4e92d6f40c7813c6f0e1c2e179d48ad","title":"Commonsense for Generative Multi-Hop Question Answering Tasks"},{"paperId":"190eb1dff4c08fc12a085242b67c0442cfe5fc84","title":"Reasoning about Actions and State Changes by Injecting Commonsense Knowledge"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"bbad0b301561c9b44a43f2880b29f143dc7297ba","title":"Temporal Information Extraction by Predicting Relative Time-lines"},{"paperId":"34901b737b11aa51b688fa18c2eab47639d7b8c6","title":"Joint Reasoning for Temporal and Causal Relations"},{"paperId":"15d742935a5bb28e464048ed33ff18128c9b09e7","title":"MITRE at SemEval-2018 Task 11: Commonsense Reasoning without Commonsense Knowledge"},{"paperId":"99ad0533f84c110da2d0713d5798e6e14080b159","title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"},{"paperId":"82580fe4c429ac76f94c514c1ffc066844b13192","title":"Determining Event Durations: Models and Error Analysis"},{"paperId":"6c5eb60ac554f2210eb18124ca0f052806eec2b0","title":"SemEval-2018 Task 11: Machine Comprehension Using Commonsense Knowledge"},{"paperId":"2754380bf487a455112d9c5594914688f9205ac0","title":"Constructing Narrative Event Evolutionary Graph for Script Event Prediction"},{"paperId":"b7fbce48dfd734adbab95c669c290d9e4aaf3272","title":"Event2Mind: Commonsense Inference on Events, Intents, and Reactions"},{"paperId":"a6ced6341e8bf2d819cbc7de73b869752019afdd","title":"Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource"},{"paperId":"bff8ae9e28323d217b9ad5a7321e58f79607f557","title":"A Multi-Axis Annotation Scheme for Event Temporal Relations"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"a9e0fa94e59261f5905951e93d515af64c3fc7cb","title":"A Structured Learning Approach to Temporal Relation Extraction"},{"paperId":"c9f343b492c170c726f607c255ec6c7177dc5800","title":"Verb Physics: Relative Physical Knowledge of Actions and Objects"},{"paperId":"13f2792d63e933f8a9acda03d95a2d801d7c70f3","title":"Ordinal Common-sense Inference"},{"paperId":"83e7654d545fbbaaf2328df365a781fb67b841b4","title":"Enhanced LSTM for Natural Language Inference"},{"paperId":"dde70b2fe5ead57cea2556c52a975865e3bb4d30","title":"What Happens Next? Event Prediction Using a Compositional Neural Network Model"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cf3955f27c54f425c29c372af3aa19778abe2051","title":"Representations of commonsense knowledge"},{"paperId":"7b82383b09bedb765ab9c5c2153978035aacd830","title":"Context-dependent Semantic Parsing for Time Expressions"},{"paperId":"32a257bb8cbee2e6f833f7c7f6e9bcacde4919d7","title":"Extracting fine-grained durations for verbs from Twitter"},{"paperId":"407314c1b491511564c089d26e7e92c0c93af754","title":"Learning Temporal Information for States and Events"},{"paperId":"fb0b11046474b8f1c810f947f313c7c7229a988f","title":"SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"},{"paperId":"085730e9355193859e4172c729821aaedc71f4c2","title":"Using Query Patterns to Learn the Duration of Events"},{"paperId":"7716a474569d94a9dc0074a2cdfc3b32641b721e","title":"HeidelTime: High Quality Rule-Based Extraction and Normalization of Temporal Expressions"},{"paperId":"0c739b915d633cc3c162e4ef1e57b796c2dc2217","title":"VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations"},{"paperId":"bb0968ab69b8655d442f17cdfe317ce3bfebd789","title":"Can we derive general world knowledge from texts"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"0e9d94639ac964754f812019af91b7c519103b46","title":"Learning Scalar Adjective Intensity from Paraphrases"},{"paperId":"8eb9b2ea146e3a381a29b4a9314c36f61b71e367","title":"Extracting Commonsense Properties from Embeddings with Limited Human Guidance"},{"paperId":"fc090a68e45e0e6337136777d21c87b76a90ae72","title":"From TreeBank to PropBank"}],"id":"81b4920ad488affaee27389ff9540b7fea90a4ce","summary":"It is found that the best current methods used on MCTACO are still far behind human performance, by about 20%, and several directions for improvement are discussed."},{"url":"https://www.semanticscholar.org/paper/b626754a0fd7de12c87e88165b2484ac5d98212a","title":"Decoupling Strategy and Generation in Negotiation Dialogues","venue":"Conference on Empirical Methods in Natural Language Processing","year":2018,"referenceCount":33,"citationCount":100,"influentialCitationCount":26,"publicationDate":"29/08/2018","authors":"He He,Derek Chen,Anusha Balakrishnan,Percy Liang","citations":[{"paperId":"b34862afacf36e7011d40c67bb67c5ee9cf7da22","title":"DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI"},{"paperId":"80b4a44ab0303ace08afc1110381866461048b23","title":"Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey"},{"paperId":"67f23dec7687692660d8aa1315b9dbc8e1aacf22","title":"Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems"},{"paperId":"aa8e31dc2a6cba68bbbbce9c2e1495dc3141edc6","title":"Language of Bargaining"},{"paperId":"90ca394659153e24df9380b8c07bb3cc1dae67aa","title":"Inductive Bias for Emergent Communication in a Continuous Setting"},{"paperId":"2e01fdebbc780d3667ec3bf87a44927f0d9c188a","title":"Decision-Oriented Dialogue for Human-AI Collaboration"},{"paperId":"e52e880dc619f9c4ebef932b895d29c352b69bdc","title":"Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration"},{"paperId":"60cd53d03a5568d8563386cce3f46f330e8b5d18","title":"Towards Massively Multi-domain Multilingual Readability Assessment"},{"paperId":"9e0d3aba0c7f84b301c69c2e6a1d282f8d97de8c","title":"Towards Dialogue Systems with Agency in Human-AI Collaboration Tasks"},{"paperId":"c01bd362596abd2b9c6bcbed783a19e4a94a586c","title":"A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects"},{"paperId":"7db1dbdd2344a3d825bef8d129990400ab124744","title":"Graph-Grounded Goal Planning for Conversational Recommendation"},{"paperId":"4b5f214d07e34e34373aad06c101be1fc439ce9d","title":"MG-ShopDial: A Multi-Goal Conversational Dataset for e-Commerce"},{"paperId":"ffd53613590f434a08fbdbdf9c13ef067256f82e","title":"Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games"},{"paperId":"f59559bbc4640a63f5e3b9ede8df763acb82353f","title":"Towards the Scalable Evaluation of Cooperativeness in Language Models"},{"paperId":"1b07690864304b75c244efbff25bd9ebf4ec47b3","title":"Goal Driven Discovery of Distributional Differences via Language Descriptions"},{"paperId":"d318e0169f649656c71f02a1f84194a734fe1962","title":"Reward Design with Language Models"},{"paperId":"0974035826cd6d4be9c604a8679621c8621aff5f","title":"Let's Negotiate! A Survey of Negotiation Dialogue Systems"},{"paperId":"529548ce8485daac29dbead20c4c383d965ae1d6","title":"Improving Chess Commentaries by Combining Language Models with Symbolic Reasoning Engines"},{"paperId":"a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88","title":"Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks"},{"paperId":"4580b0e3ede1b7ad97fb742fa28cd3ebe3bed7a9","title":"Improving Policy Learning via Language Dynamics Distillation"},{"paperId":"b1708d989527a0c63649e223760da12b399da147","title":"Multimodal Persuasive Dialogue Corpus using Teleoperated Android"},{"paperId":"43e558c408fef659631573c1cbbf81f58decf5df","title":"Intelligent Negotiation Bot using Machine Learning Techniques"},{"paperId":"1bd3def5257e992cecbcd5381771d77a95cc8200","title":"Persuasion Strategies in Advertisements"},{"paperId":"42ffe1139f97ce3809c9140e2d92e837fe5eb1e7","title":"Structured and Natural Responses Co-generation for Conversational Search"},{"paperId":"ca1d78a0e30a554540ad5c1c7c84d87d780f9ade","title":"How to Ask for Donations? Learning User-Specific Persuasive Dialogue Policies through Online Interactions"},{"paperId":"680536428d734b28f23c86a7d4e53dd03144a752","title":"Do You Know My Emotion? Emotion-Aware Strategy Recognition towards a Persuasive Dialogue System"},{"paperId":"42b3982331aa3dbd233d6c45bf34577223e2ecc9","title":"Unsupervised Learning of Hierarchical Conversation Structure"},{"paperId":"a1948d9322c7a8d88168fdffac0800940da5e7e8","title":"Towards a Progression-Aware Autonomous Dialogue Agent"},{"paperId":"304c860cd33f6630fc2fb0d14b6f3ca62b0fa7dc","title":"Opponent Modeling in Negotiation Dialogues by Related Data Adaptation"},{"paperId":"88a9924ba302f9c5c8065d9e38798e9506c5b171","title":"CHAI: A CHatbot AI for Task-Oriented Dialogue with Offline Reinforcement Learning"},{"paperId":"81ebda8a734ff917681c6d73c179f6614b281818","title":"Context-Aware Language Modeling for Goal-Oriented Dialogue Systems"},{"paperId":"02b31611637db09856fa56f4e799d0f3b5674563","title":"Interacting with Non-Cooperative User: A New Paradigm for Proactive Dialogue Policy"},{"paperId":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"},{"paperId":"9a1c65b395974ee4c8f073f26d2a47b8492989e7","title":"Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue"},{"paperId":"b657861d3b96ff0e5a502ddc2cb865a5ea845f53","title":"Predicting Success of a Persuasion through Joint Modeling of Utterance Categorization"},{"paperId":"3451d237f29243d63c846e99f74558aa441d649c","title":"Improved Goal Oriented Dialogue via Utterance Generation and Look Ahead"},{"paperId":"5931c8ac145baf17cec9effc25c051049b7dfd4c","title":"Reference-Centric Models for Grounded Collaborative Dialogue"},{"paperId":"bab1b89c4b69b9661037114d45af68d26d3cdd70","title":"WeaSuL: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue"},{"paperId":"7f3fb456319181ee092b4e335302fb953523aaba","title":"Towards Emotion-Aware Agents For Negotiation Dialogues"},{"paperId":"c1361b0fe1140f02051eeb68d86f07993af9a6a9","title":"Dialogue Management for Interactive API Search"},{"paperId":"3517bed8f1fa9ccdd075cbc4ded1467b09e4c199","title":"A Wizard of Oz Study Simulating API Usage Dialogues With a Virtual Assistant"},{"paperId":"b89c361f37657fb4fab246e9d55b3c6f32e29713","title":"Targeted Data Acquisition for Evolving Negotiation Agents"},{"paperId":"0a21b9e31f329cc353cc8e5bda152faad3dc9be7","title":"DeliData: A dataset for deliberation in multi-party problem solving"},{"paperId":"2995b6175e6ad666c33273722f33a86d9a08e46b","title":"Semantic-Enhanced Explainable Finetuning for Open-Domain Dialogues"},{"paperId":"a5a853fbcca89ec308511527e1614fac701681e8","title":"Conversational AI Systems for Social Good: Opportunities and Challenges"},{"paperId":"98cb29c03a0882fde368db28ade214c57c3239d6","title":"Multi-agent deep reinforcement learning: a survey"},{"paperId":"8c0ec929aac2eb67d36f383f3175f22a1b285264","title":"Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems"},{"paperId":"37607828cb9b8ae5011bbcc8ecc2e159f719347c","title":"CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems"},{"paperId":"17fdf700dcd6e8621ef4d9aa4ae69d1ad7a2043e","title":"ResPer: Computationally Modelling Resisting Strategies in Persuasive Conversations"},{"paperId":"e77932feb60d4c91bcb3d862d2b30e4ec8fb7821","title":"A Neural Question Answering System for Basic Questions about Subroutines"},{"paperId":"fac8d660e9c0cef1da5d35aea35c572ed776e887","title":"Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration"},{"paperId":"aea24fe9653bcb9d654e53bdfc66b810d79d0a2c","title":"Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network"},{"paperId":"479f50023b4c4d95df7512ebf1d2723508821be5","title":"A Multi-Persona Chatbot for Hotline Counselor Training"},{"paperId":"314358d6e9969cb89da0ad0b6aa4f406294d3ff4","title":"Generalized Conditioned Dialogue Generation Based on Pre-trained Language Model"},{"paperId":"7bf03c12f74a83852fc831c468aa754e0008a6d7","title":"Improving Dialog Systems for Negotiation with Personality Modeling"},{"paperId":"3fa25dc99305113f1d28051094ebefd843f5281b","title":"Generating Strategic Dialogue for Negotiation with Theory of Mind"},{"paperId":"25ccaf7d0220423b58561970cb5e0bb5ebcf5b75","title":"Learning to Plan and Realize Separately for Open-Ended Dialogue Systems"},{"paperId":"abb59785aee6e5732c64ba7364fcbbb277e97bb6","title":"Preliminary Annotation Results: Bargaining Roles for Bilateral Dialogues"},{"paperId":"a4789304d4f5337d7c1e18dcc0f13049f2b5dbc5","title":"Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation"},{"paperId":"4fd961ec9a6af0c0d15e0a85471d0ff596342ac5","title":"Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System"},{"paperId":"de18584421e2192623dcc3e35cf4294d7d42e1aa","title":"GoChat: Goal-oriented Chatbots with Hierarchical Reinforcement Learning"},{"paperId":"bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d","title":"Experience Grounds Language"},{"paperId":"de094826a8d25994f103e207689d6eb1b595642e","title":"Dynamic Composition for Conversational Domain Exploration"},{"paperId":"716efab73e1916fdc2a23727b581d200271ed499","title":"BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes"},{"paperId":"c7ee2561a77673a2ba4eb83fa6f484f8ac72db7d","title":"Breakdown Detection in Negotiation Dialogues (Student Abstract)"},{"paperId":"4d0146679527de1208d801d253ab30f9de92bd39","title":"Modeling Dialogues with Hashcode Representations: A Nonparametric Approach"},{"paperId":"c4e8f111b4cb6cdfe0f718d0d81ca138c0e9464e","title":"Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition"},{"paperId":"333df5eed6cc5a0a511a37726a1652c731a33b56","title":"Recent advances and challenges in task-oriented dialog systems"},{"paperId":"3c0ad47e15c1ff9761aee446bb132d816973aedf","title":"Effects of Persuasive Dialogues: Testing Bot Identities and Inquiry Strategies"},{"paperId":"c9baad6c2d6c112e758e9d9a2f3287622c21a19b","title":"End-to-End Trainable Non-Collaborative Dialog System"},{"paperId":"8a9a798c56fc83858d7ace0352606d73aeaa204d","title":"Adversarial Language Games for Advanced Natural Language Intelligence"},{"paperId":"be2ce82730600d9b2eb2df9f2762f9d4beb6222d","title":"Executing Instructions in Situated Collaborative Interactions"},{"paperId":"7b622d7a991da4d2091d7bdb420177ca596f13a8","title":"Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History"},{"paperId":"e9055015b403098f60769a447134e30d5767d583","title":"MOSS: End-to-End Dialog System Framework with Modular Supervision"},{"paperId":"e7a7de829b11dce0a9da38ebe90371d786c00e87","title":"Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue"},{"paperId":"97398a7e0a46ffe89fb41fa3ff93a3da32febe57","title":"How to Build User Simulators to Train RL-based Dialog Systems"},{"paperId":"f60a65c9c287b44bd864abe2f87d49e44294ff5f","title":"A Dynamic Strategy Coach for Effective Negotiation"},{"paperId":"877ddea1117b0bfad645d42e3baeab165a659919","title":"Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog"},{"paperId":"3d52d429b4d83d096dd354e8470bf3655e8b67bc","title":"Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good"},{"paperId":"06236fa6fd407f4b629be41049c880a8569c20ab","title":"Target-Guided Open-Domain Conversation"},{"paperId":"e191b7eca9cf259abab3905dab1a54d1be091a7a","title":"Designing a Symbolic Intermediate Representation for Neural Surface Realization"},{"paperId":"88fbcfc01bab901aae084d734d82558d34105ea0","title":"Show, Price and Negotiate: A Hierarchical Attention Recurrent Visual Negotiator"},{"paperId":"590d2e1c66c9f11575df4ea9dc63d605b1c05e05","title":"Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models"},{"paperId":"e65594a323f979d3366b108fa30bddde5a9f8d90","title":"Modeling Psychotherapy Dialogues with Kernelized Hashcode Representations: A Nonparametric Information-Theoretic Approach."},{"paperId":"4ad44c9af940ff630f395624b932d778b5e05a74","title":"Dialogue Modeling Via Hash Functions: Applications to Psychotherapy"},{"paperId":"6abededed5b4eb1bdeba0fa53db5ae37526882ef","title":"Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond"},{"paperId":"ae4da8cb36e40fa167f565cbd4df3526f01322c7","title":"Conversational AI for Positive-sum Retailing under Falsehood Control"},{"paperId":"c44176020bc7034f5ea788cb8de7fcdda5f6a91d","title":"Social Influence Dialogue Systems: A Scoping Survey of the Efforts Towards Influence Capabilities of Dialogue Systems"},{"paperId":"d348561af0832b83652dfa31fed41f93a41ff9b0","title":"Persuasion Strategies in Advertisements: Dataset, Modeling, and Baselines"},{"paperId":"b6daae71b9fe9d8f3ad8b0d0fe55d203f92b19eb","title":"Persona or Context? Towards Building Context adaptive Personalized Persuasive Virtual Sales Assistant"},{"paperId":"fb367084876f836bcb33b710f0dd366af69513a1","title":"Dialogue Act-based Breakdown Detection in Negotiation Dialogues"},{"paperId":"e5dc7986565d4cc36364225a599e12549aa12a22","title":"Dialogue Context Encoder Structure Encoder Graph Encoding ( GAT ) Structure Encoder u 1 u 2 u 3 u 4 Graph Pooling Graph Pooling Graph Encoding ( GAT ) GCN-ASAPGCN-ASAP Utterance Embedding Utterance Generation"},{"paperId":"d0babb55df48d544f4765a29fba7fd6c854d03ba","title":"Phrase-Level Action Reinforcement Learning for Neural Dialog Response Generation"},{"paperId":"476dfdfae0cf07fdf4c7a3609763cddc1b48a6e7","title":"ORIENTED DIALOGUE SYSTEM"},{"paperId":"d6981d415ab968fb42f3ba145b2c389d015b24d5","title":"WeaSuL: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue"},{"paperId":"80d5684c1e7f5ae34b524ed25dd51f2aa2b613c9","title":"Learning Grounded Pragmatic Communication"},{"paperId":"cf952005c8de87a9cf11801a3201829eb66249cd","title":"IEC: Towards Interest-Eliciting Neural Conversational Agents"},{"paperId":"a0a0dbdfb7d88c8e38b9d8088ddf194195c2e6c7","title":"AUGMENTING NON-COLLABORATIVE DIALOG SYS-"},{"paperId":null,"title":"Seller Messages Buyer Product Description Listing Price Coach : Product Listing : Detect Strategies 2 1 2 3 5 Realize Strategies Select Optimal Strategies 4 3 Predict Strategies"}],"references":[{"paperId":"92ec8259e58f5107057e4e99fdcd03df6a08f9e7","title":"Emergent Communication through Negotiation"},{"paperId":"6a8dbea5e40831bd6e987c03b76487f45ac49599","title":"Deal or No Deal? End-to-End Learning of Negotiation Dialogues"},{"paperId":"77826df024f97583eb05700a28e11056a4aab848","title":"Latent Intention Dialogue Models"},{"paperId":"fcbedb12dcc3618df7a76012944c64fd62a18286","title":"Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings"},{"paperId":"bca1b5e690907843dff3b16d738268cd7db53430","title":"Evaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation Dialogue agents"},{"paperId":"58dfeb0bc41429393bf27ff882b7b679031f106c","title":"Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders"},{"paperId":"aea427bcea46c83021e62f4fb10178557510ca5d","title":"Latent Variable Dialogue Models and their Diversity"},{"paperId":"9c49763e37c20007ebbddbe405b02546705a2d83","title":"Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning"},{"paperId":"d261286b18d37aaf5ebf70a0325e43fb5357442d","title":"Training End-to-End Dialogue Systems with the Ubuntu Dialogue Corpus"},{"paperId":"176f1d608b918eec8dc4b75e7b6e0acaba84a447","title":"Adversarial Learning for Neural Dialogue Generation"},{"paperId":"fb32191ec07ba4d7badc76ca428c816995b5785a","title":"Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access"},{"paperId":"1298dae5751fb06184f6b067d1503bde8037bdb7","title":"Deep Reinforcement Learning for Dialogue Generation"},{"paperId":"f81be44000814e7bcb12ae04b4e2d9c01b6515b3","title":"Learning End-to-End Goal-Oriented Dialog"},{"paperId":"4d025da955a51350df56a54bbb59c1c1b7630e2b","title":"Discourse Structure and Dialogue Acts in Multiparty Dialogue: the STAC Corpus"},{"paperId":"4ba339bd571585fadb1fb1d14ef902b6784f574f","title":"The Dialog State Tracking Challenge Series: A Review"},{"paperId":"0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa","title":"A Network-based End-to-End Trainable Task-oriented Dialogue System"},{"paperId":"769372b8aacc19c94ca0f6bf2abfc14c85074b2f","title":"Strategic Dialogue Management via Deep Reinforcement Learning"},{"paperId":"feb4245aa7eaec9eaf225cc0d0676e41b9308a37","title":"Reinforcement Learning in Multi-Party Trading Dialog"},{"paperId":"5247a6e3a60ff0381355e66bfc313bf27512ae0c","title":"A Neural Network Approach to Context-Sensitive Generation of Conversational Responses"},{"paperId":"f30b6b28805513bb0a6eec8db69116a9b5538a81","title":"Toward Natural Turn-Taking in a Virtual Human Negotiation Agent"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"101d619b5911e9c2fda6f02365c593ae61617cb6","title":"Reinforcement Learning of Cooperative Persuasive Dialogue Policies using Framing"},{"paperId":"84b520a8d6de79f62bb095b565d077e95bfb6f5b","title":"POMDP-Based Statistical Spoken Dialog Systems: A Review"},{"paperId":"fe8b83874e8f0eca72d5f7531f4277a1aa13e038","title":"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"},{"paperId":"7a3db5fe8dc49d893851e4bc0ffa9d87944c8cea","title":"Multi-party, Multi-issue, Multi-strategy Negotiation for Multi-modal Virtual Agents"},{"paperId":"87b2f3ab89fc9231cfa6f8a17f4253de67c4c3d8","title":"Learning Mixed Initiative Dialog Strategies By Using Reinforcement Learning On Both Conversants"},{"paperId":"b2c340691a2ced929772c84eeb920f69c89aa5b7","title":"Negotiation games : applying game theory to bargaining and arbitration"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","title":"Simple statistical gradient-following algorithms for connectionist reinforcement learning"},{"paperId":"184ecd47d94d27711fe81c168b23096766a7b47a","title":"A Cultural Decision-Making Model for Negotiation based on Inverse Reinforcement Learning"},{"paperId":"2352b3a6e4adc7b010ee5de424079480e3afbfbf","title":"Goal-driven Answers in the Cards Dialogue Corpus"},{"paperId":"411654b5805de7eb554587d917b8bd4d01710323","title":"Modelling Strategic Conversation: model, annotation design and corpus"},{"paperId":"0fe2e9dea7df47bad029e2072e25ca1fddf80371","title":"Goal-Driven Answers in the CardsDialogue Corpus"},{"paperId":null,"title":"Learning noncooperative dialogue behaviours"}],"id":"b626754a0fd7de12c87e88165b2484ac5d98212a","summary":"A modular approach based on coarse dialogue acts (e.g., propose(price=50)) that decouples strategy and generation that can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy is proposed."},{"url":"https://www.semanticscholar.org/paper/d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving","venue":"ArXiv","year":2019,"referenceCount":28,"citationCount":40,"influentialCitationCount":1,"publicationDate":"25/09/2019","authors":"Imanol Schlag,P. Smolensky,Roland Fernandez,N. Jojic,J. Schmidhuber,Jianfeng Gao","citations":[{"paperId":"179237bc5fb46f34ef936b1552600bf3521c3c64","title":"Differentiable Tree Operations Promote Compositional Generalization"},{"paperId":"30977afbd4501249e1a320bd2e48581197914ab8","title":"A Short Survey of Systematic Generalization"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"42bc0824d8ca35105d181aaa0183654535325f55","title":"Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages"},{"paperId":"d9f65f6e3d13001309bf6a93f2c968f927c06149","title":"Artificial neural network modelling of the neural population code underlying mathematical operations"},{"paperId":"40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language"},{"paperId":"f2611a09cf0942170785ee3025cb511de3bdec2e","title":"Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems"},{"paperId":"1ee909a2a78d287cac549d5deea96f3cbd1c7092","title":"Enhancing Neural Mathematical Reasoning by Abductive Combination with Symbolic Library"},{"paperId":"2cffec40f1d7cfc81d498b8939493243bbcadebe","title":"Learning Symbolic Rules for Reasoning in Quasi-Natural Language"},{"paperId":"04db9b694280134f09af5fa787a306907edba29d","title":"How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN"},{"paperId":"2ec21ae2f7a72a1246482c915da7a8c077c8b959","title":"Generating Symbolic Reasoning Problems with Transformer GANs"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"d0dae92c4d37520ae20c072ec64fdb718874bfd0","title":"A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis"},{"paperId":"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","title":"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"},{"paperId":"45f7c0caf0f1f88a568279de52d7a8c29aa8e28f","title":"Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization"},{"paperId":"f3879fedf036175aefdb750c5527d184f038b932","title":"Compositional Processing Emerges in Neural Networks Solving Math Problems"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"58c74cec28f3416b9a1d308bb2d6519d21d53ab0","title":"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"},{"paperId":"6027247d7f4256c10bdc71b8584d5927e616fa37","title":"Logic Tensor Networks"},{"paperId":"d642868ce4325ebf3026c0aa0c497a079f112a8d","title":"On the Binding Problem in Artificial Neural Networks"},{"paperId":"c2b4d96db34bd472e84c9234838cc4e808eb1ba9","title":"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language"},{"paperId":"5e11e806d24dd80ecf0f91e7aacedbba8d9fd6fc","title":"Learning Associative Inference Using Fast Weight Memory"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"804a6d7c23335bbca6eec3b7d3c8366dcbe395a5","title":"Hopfield Networks is All You Need"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"9ebb28851b253817f9a0ea5ddc22b0fd9a934a2f","title":"Deep Learning--based Text Classification"},{"paperId":"7e9af8a6081dc00187bd4a6727751d1721bd7816","title":"Evaluating Logical Generalization in Graph Neural Networks"},{"paperId":"b32e631ae7779d93b9979c61c5b920a76342063e","title":"Teaching Temporal Logics to Neural Networks"},{"paperId":"5ade9c2fabc0fc526ea05445f8d0f92666266681","title":"Transformers Generalize to the Semantics of Logics"},{"paperId":"97d69e7e8c04714bf58dcbe5ae7454db69b657a7","title":"The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence"},{"paperId":"dc88d2bbcebd810d7c80ba281739908005b12235","title":"Neurocompositional computing in human and machine intelligence: A tutorial"},{"paperId":"904dcbe71ac8fa0cd96f92132a24dc8e3d00c821","title":"Causal Transformer for Estimating Counterfactual Outcomes"},{"paperId":"d129841cb2e30e25000dcd9edb83c880fc4babc1","title":"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention"},{"paperId":"8babeaffc8413747412765803a50d2c3adbbbbdd","title":"DEBERTA: DECODING-ENHANCED BERT"},{"paperId":"adc61e21eafecfbf6ebecc570f9f913659a2bfb2","title":"Deep Learning Based Text Classification: A Comprehensive Review"},{"paperId":"ef19c8b51ff4bc3026a3bf09b81a11f5b3fe04c7","title":"Inflectional paradigms as interacting systems"},{"paperId":"624d6a768e6f6d7e368ede0d22ee691d1aaf3a40","title":"T EACHING T EMPORAL L OGICS TO N EURAL N ET WORKS ∗"},{"paperId":"1de88fd6bdaac4afe6dd3b7ca80923e1dbff40e1","title":"Artificial General Intelligence: 13th International Conference, AGI 2020, St. Petersburg, Russia, September 16–19, 2020, Proceedings"},{"paperId":"caa202043b553cea0b099d591308e66db8a300ba","title":"Dreaming with ARC"},{"paperId":"f886a0dc2dda508f744bc9e1c3750b468ed2f65b","title":"Transformer Model for Mathematical Reasoning-CS 230 Final Report"},{"paperId":null,"title":"Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"}],"references":[{"paperId":"59a916cdc943f0282908e6f3fa0360f4c5fb78d0","title":"Stabilizing Transformers for Reinforcement Learning"},{"paperId":"830995ef17cc291c13f42dfd9f462137de1d2179","title":"Augmenting Self-attention with Persistent Memory"},{"paperId":"afd110eace912c2b273e64851c6b4df2658622eb","title":"Visualizing and Measuring the Geometry of BERT"},{"paperId":"165d51a547cd920e6ac55660ad5c404dcb9562ed","title":"Open Sesame: Getting inside BERT’s Linguistic Knowledge"},{"paperId":"67146c46304720c026a3c9bc324ffc2285df2c0b","title":"A Perspective on Objects and Systematic Generalization in Model-Based RL"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"f6fa98b0d1f6bff608a056acd7b82e2a9dd0a68e","title":"Learning to Reason with Third-Order Tensor Products"},{"paperId":"735708fb10c3420a1138b0d1c7a0923ff4de3c41","title":"A Simple Recurrent Unit with Reduced Tensor Product Representations"},{"paperId":"b5ecf634902b329aea683176c56aac35029da621","title":"Learning Distributed Representations of Symbolic Structure Using Binding and Unbinding Operations"},{"paperId":"6c7494a47cc5421a7b636c244e13586dc2dab007","title":"Systematic Generalization: What Is Required and Can It Be Learned?"},{"paperId":"fd4ae71916cf400bfd1490f275e91b154eb69160","title":"Relational recurrent neural networks"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"d4a9876f00fd9caf0358fd73e5572e53a47cda12","title":"Question-Answering with Grammatically-Interpretable Representations"},{"paperId":"dc620583dc06f7262707b4e2a9b7df91dbd384e0","title":"Deep Learning of Grammatically-Interpretable Representations Through Question-Answering"},{"paperId":"b92aa7024b87f50737b372e5df31ef091ab54e62","title":"Training Very Deep Networks"},{"paperId":"b71ac1e9fb49420d13e084ac67254a0bbd40f83f","title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":"9ca9f28676ad788d04ba24a51141a9a0a0df4d67","title":"A new model for learning in graph domains"},{"paperId":"c58dd287a476b4722c5b6b1316629e2874682219","title":"Learning task-dependent distributed representations by backpropagation through structure"},{"paperId":"61639af1a89c69094bcc0ed40fad752832b037c3","title":"Reducing the Ratio Between Learning Complexity and Number of Time Varying Variables in Fully Recurrent Nets"},{"paperId":"bc22e87a26d020215afe91c751e5bdaddd8e4922","title":"Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"},{"paperId":"24484e1105bd28acbf0184c94ac9833511328087","title":"Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems"},{"paperId":"3106e66537a0c8f53278e553bcb38f0b0992ec0e","title":"Distributed Representations"},{"paperId":"b7efb6b6f7e9ffa017e970a098665f76d4dfeca2","title":"Polynomial Theory of Complex Systems"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"e62f7643f616aaad65ffd47155a53bfa325e455d","title":"The Correlation Theory of Brain Function"}],"id":"d88f31a0091eee02c5a2aa2013914818cdef114e","summary":"The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems, and incorporates Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure."},{"url":"https://www.semanticscholar.org/paper/5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures","venue":"Transactions of the Association for Computational Linguistics","year":2019,"referenceCount":61,"citationCount":239,"influentialCitationCount":23,"publicationDate":"31/12/2019","authors":"Alon Talmor,Yanai Elazar,Yoav Goldberg,Jonathan Berant","citations":[{"paperId":"4323a09460c03cb31b3823ade9f75fba66d45316","title":"Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features"},{"paperId":"85d9151aa2efd0cbe822e403138cfe49f9536703","title":"SITTA: A Semantic Image-Text Alignment for Image Captioning"},{"paperId":"fa0c364b28f8ff721ac3aa2e82dca312024c19dd","title":"A BERT-based deontic logic learner"},{"paperId":"d89d8c93f62e66f8f22f0fbbddc688fb4017a15d","title":"ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling"},{"paperId":"2e403ad2cd02409e1fdc15839da0a3f89886a990","title":"MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text Classification"},{"paperId":"d2db40cf7de0eb23ae784dba292e15051f63a828","title":"Semantic HELM: An Interpretable Memory for Reinforcement Learning"},{"paperId":"9a1c481a25c609fe1dbb9b9d43faf892c3c71368","title":"Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks"},{"paperId":"56267168c5f1cea5049c0f1724e39166988d32b7","title":"Probing Physical Reasoning with Counter-Commonsense Context"},{"paperId":"070fe820f68ffd3ee79d80061930fa7c1f9afcbc","title":"Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models"},{"paperId":"84cde4abc973e3bb508f03506a7fa946222f4e6b","title":"Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization"},{"paperId":"2cfb1f44b34204213d789731871e599c756bdb83","title":"Exploring Large Language Models for Classical Philology"},{"paperId":"c82e95e282e85f649f901f16e3cbf434b582ba74","title":"Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks"},{"paperId":"99a062c83e4ec3574f6a93c7b551f3ef5e3635fa","title":"Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer"},{"paperId":"9c77b4343600a325f5c54a78b468cc183a36fbaa","title":"Harvesting Event Schemas from Large Language Models"},{"paperId":"44e7c477cdb3cd09ea7a6459eef1d9c07fced577","title":"ComputeGPT: A computational chat model for numerical problems"},{"paperId":"44772fe1c3fa422a3da7e25092db2544893d6bfb","title":"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming"},{"paperId":"39a936d4bb6145b50171401176920f6ec56de2f0","title":"Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning"},{"paperId":"502201bb6139d867a5f087a682ddd1465f23c798","title":"Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World"},{"paperId":"29a7644583d7042c4476af126f3fe0a372897abe","title":"LION: Implicit Vision Prompt Tuning"},{"paperId":"8018a68956d6751d7ea76110537d5a2e86ec05c4","title":"The Life Cycle of Knowledge in Big Language Models: A Survey"},{"paperId":"06396c7cd5d223a1776abf8811359ec7bc05d420","title":"Knowledge-Augmented Methods for Natural Language Processing"},{"paperId":"aad8b1ca56aef4a7512789102ce0cc3fc8b064e4","title":"Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views"},{"paperId":"992cff59f13f5944a74eb2e62a8cb11849204780","title":"KILM: Knowledge Injection into Encoder-Decoder Language Models"},{"paperId":"da345b189e4faaaa489f7319640868a37a3932a1","title":"Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?"},{"paperId":"2379d69c98548d8c291a53a9a932b5ea7911fbe5","title":"Probabilistic coherence, logical consistency, and Bayesian learning: Neural language models as epistemic agents"},{"paperId":"ae766548699f27e669932de14e1c0f47b2828536","title":"Prompting for Multimodal Hateful Meme Classification"},{"paperId":"c40aa04f1577e8f0f83208d1815e032d127b60a8","title":"Creating a Large Language Model of a Philosopher"},{"paperId":"a43425af7413e62c816a8fc007ab443465761829","title":"Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning"},{"paperId":"9a9e68d400069f023f7dc9b982226c95159a509d","title":"Dissociating language and thought in large language models: a cognitive perspective"},{"paperId":"9ac29a207d58496c730674af89b572243fed2616","title":"Knowing Knowledge: Epistemological Study of Knowledge in Transformers"},{"paperId":"b742233b83405d303f61b3378f70475e4efdf75b","title":"Analyzing Semantic Faithfulness of Language Models via Input Intervention on Conversational Question Answering"},{"paperId":"d651691e42d42423f0b13a0ee7dfc67087d87379","title":"'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers"},{"paperId":"8db5adadc0ab39f95a26a2eb6499d340d6c5ea21","title":"From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation"},{"paperId":"ad5573cb25fd403f7620332f363ae87327c69a49","title":"The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning"},{"paperId":"2f2dd927eebe91ce40ffc4bddc419f7337abbafb","title":"An Analogy based Approach for Solving Target Sense Verification"},{"paperId":"7e973c2a489ede4162eb3f7556c61104a68377ab","title":"Event knowledge in large language models: the gap between the impossible and the unlikely"},{"paperId":"17fc99b691d6fe1f4a84c774f01e86a2aae2c2df","title":"Demystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application"},{"paperId":"b4b37f87e0357f2e4cec70af67b2f088f6efce70","title":"A Survey of Knowledge-Enhanced Pre-trained Language Models"},{"paperId":"bcec7d17e68aceb91d020dd796ece075694f77c6","title":"COPEN: Probing Conceptual Knowledge in Pre-trained Language Models"},{"paperId":"6ae3e52ae55578c10722db3c2f898442f20e336c","title":"LMentry: A Language Model Benchmark of Elementary Language Tasks"},{"paperId":"1933a0ef47f8d2ba4a8277d702d522a06319302c","title":"IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"f663c1b574d0a56c2e1c8e7aee3d8caf90b52808","title":"LMPriors: Pre-Trained Language Models as Task-Specific Priors"},{"paperId":"72e3a1b4745d6f25b8cd186bd56fbad6e3a8464b","title":"Judgment aggregation, discursive dilemma and reflective equilibrium: Neural language models as self-improving doxastic agents"},{"paperId":"8f60454584a8d1f1d6f278198af5e0bd2d103067","title":"A Survey of Parameters Associated with the Quality of Benchmarks in NLP"},{"paperId":"f2dadc182d33f3ad74f95505e5b6611c3abafe02","title":"MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks"},{"paperId":"ad62d710f1854daf372680263f50a4e135e309f2","title":"CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models"},{"paperId":"53c20f7bf3fabc88e1403e00241eec009cc01ed8","title":"Measuring and Narrowing the Compositionality Gap in Language Models"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"ed99a2572fb5f4240aa6068e3bf274832e831306","title":"Recitation-Augmented Language Models"},{"paperId":"f2878e3a27950c674425ba896bfd1c2be7dd67e5","title":"Negation, Coordination, and Quantifiers in Contextualized Language Models"},{"paperId":"416e92370e15e25ec23547ef32e6c1702c1ca1fc","title":"CommunityLM: Probing Partisan Worldviews from Language Models"},{"paperId":"c8ff9fef927b6ccf242c20b3939a7442a0633ab3","title":"VIPHY: Probing \"Visible\" Physical Commonsense Knowledge"},{"paperId":"a6e39438f766a4df502d6922c7f253398af60c14","title":"Pre-Trained Language Models and Their Applications"},{"paperId":"aa3bae7def250ba247ea2c187ad1c1a99b3bf737","title":"Lost in Context? On the Sense-wise Variance of Contextualized Word Embeddings"},{"paperId":"94e5ca13e8c884509ccad233c77979923b677a3e","title":"UnCommonSense: Informative Negative Knowledge about Everyday Concepts"},{"paperId":"2796622dad6217f99508a3f241c3f9aeb24b374a","title":"Information Theory–based Compositional Distributional Semantics"},{"paperId":"5b5f9554b71321dc6df2518e1347354e4ac67c0d","title":"Pro-tuning: Unified Prompt Tuning for Vision Tasks"},{"paperId":"f3cf71c51b882fe3111d71c4bf104297d38197f8","title":"Inner Monologue: Embodied Reasoning through Planning with Language Models"},{"paperId":"f8d44802ac8190864c61c9aaf4a8b450261873ab","title":"An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics"},{"paperId":"9ecc1b8494328f7b2d1b58595b2c28eb768c2c7c","title":"longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks."},{"paperId":"3f5d3c57ccb3b9059eb8a94fa7af4d9f299c56c6","title":"Entity-Graph Enhanced Cross-Modal Pretraining for Instance-level Product Retrieval"},{"paperId":"e3dae33c5bdf397abdefd971ea34c48fb836dcc0","title":"GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models"},{"paperId":"3f3c01adbdd433d515c19ac8cf6c61c905f0061a","title":"History Compression via Language Models in Reinforcement Learning"},{"paperId":"a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7","title":"The Curious Case of Control"},{"paperId":"4e5f7cd537a1bbcd090f9887b1b59f39a3715dba","title":"Instruction Induction: From Few Examples to Natural Language Task Descriptions"},{"paperId":"a005a6160efa2fd055581b8222b41f71f966ea50","title":"Life after BERT: What do Other Muppets Understand about Language?"},{"paperId":"7f84d56fb8feb4e50cd6c3da3e3fd4ff6c4772cf","title":"ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models"},{"paperId":"c90a14b54ed56c975fda929418342cf373925e4e","title":"Entity-aware Transformers for Entity Search"},{"paperId":"cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","title":"A Review on Language Models as Knowledge Bases"},{"paperId":"706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding"},{"paperId":"c4e991c0c5c21608a5a21d31fd478ce7b7fb527d","title":"Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View"},{"paperId":"537d98e241975dc5c32d9372ae85134dffe45532","title":"DP-KB: Data Programming with Knowledge Bases Improves Transformer Fine Tuning for Answer Sentence Selection"},{"paperId":"3f4d11971f2c64be9125a7fe99c019588bbebf16","title":"Iteratively Prompt Pre-trained Language Models for Chain of Thought"},{"paperId":"50f7d69ddd7f9b34f5121607fbdcc57236d65b8c","title":"Can Pre-trained Language Models Interpret Similes as Smart as Human?"},{"paperId":"18dc24da603896db2264e9174116407a95c593d5","title":"“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction"},{"paperId":"7bebb48d34c219b119ca2d4ffc97d7fd4940c35c","title":"On the data requirements of probing"},{"paperId":"490002dbbeeda2c0c147538edec4afe7c956848b","title":"Do Transformers know symbolic rules, and would we know if they did?"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"926689bf5f21e82a24830a68a0dc879ac0783784","title":"Kformer: Knowledge Injection in Transformer Feed-Forward Layers"},{"paperId":"c3a454e50ec0610f1380d55b1988a5eb5d45207b","title":"Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization"},{"paperId":"0b483b550b21ec42d693fc04a372dbb10dd07019","title":"Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge"},{"paperId":"63f17017257063ee034c4082d93005dc4b25d42d","title":"Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability"},{"paperId":"f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884","title":"LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI"},{"paperId":"2cffec40f1d7cfc81d498b8939493243bbcadebe","title":"Learning Symbolic Rules for Reasoning in Quasi-Natural Language"},{"paperId":"a466d10b80dbdee3b130bef73ec62f3a89eb389b","title":"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples"},{"paperId":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","title":"Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey"},{"paperId":"d5784fd3ac7e06ec030abb8f7787faa9279c1a50","title":"Interpreting Deep Learning Models in Natural Language Processing: A Review"},{"paperId":"290867638c5ca520de5c48aa4336f196d426c226","title":"Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey"},{"paperId":"c5fbf9a62a91e3182f65e3746d3263387effa4a7","title":"The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail"},{"paperId":"c131a9eb834055a3b846c3d3816cd23a3434e528","title":"DeepA2: A Modular Framework for Deep Argument Analysis with Pretrained Neural Text2Text Language Models"},{"paperId":"bde10a2d3dfd138ea6a83f14c17ea3c3f9ac6db0","title":"A Survey of Knowledge Enhanced Pre-trained Models"},{"paperId":"e55391a9406245584b3e5b3225dad2e171b9a06b","title":"RuleBERT: Teaching Soft Rules to Pre-Trained Language Models"},{"paperId":"e113570afc29562bfde48f89fbb5efa624610573","title":"Distilling Relation Embeddings from Pretrained Language Models"},{"paperId":"1a40d4c7ee1044618c08e9f3cfee19eb2c12071d","title":"SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis"},{"paperId":"6cc9484612ab146c9fed9f7dce283c815af3cbc8","title":"Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids’ Representations"},{"paperId":"ceef266c59698999c9283a0cda852d8bc1ce27ea","title":"How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy"},{"paperId":"a418c0daa98a3639e1b1bd682c68644250259944","title":"Transformers in the loop: Polarity in neural models of language"},{"paperId":"299983121dec88d4cc8e4ea2aa06514787d8d878","title":"Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models"},{"paperId":"884c7aac3358a6f91887dd0d091759963764bedd","title":"A Bayesian Framework for Information-Theoretic Probing"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"26c60f8ffb0d66d8732d22af6f5b539e49f2a1e6","title":"Discover AI Knowledge to Preserve Cultural Heritage"},{"paperId":"e6f94081276a7a5e6aef34a080cb3d3a4b1b9c20","title":"Rethinking Why Intermediate-Task Fine-Tuning Works"},{"paperId":"9fabd0280e910cf4608dc27dc0268d5b8ed2d3ba","title":"SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation"},{"paperId":"ad0e6dfee9be05e2c9021779f2f995a3caeb3642","title":"How Can the [MASK] Know? The Sources and Limitations of Knowledge in BERT"},{"paperId":"2cf3cd3a7a08fc91eecab45e73299940c9c439dc","title":"Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills"},{"paperId":"966a38882be844dbf7e8b15478e1bdf3c75ef8a6","title":"A Closer Look at How Fine-tuning Changes BERT"},{"paperId":"ada0e2e476523714a5109c8bb19588140e2314e7","title":"Core Challenges in Embodied Vision-Language Planning"},{"paperId":"896c76bf99de9c1ba6fc3dfb695dfa66e73e1eb3","title":"Probing Pre-Trained Language Models for Disease Knowledge"},{"paperId":"7747ecbc26b1688e6cad1a6ce83914efa2a3c04c","title":"Prompting Contrastive Explanations for Commonsense Reasoning Tasks"},{"paperId":"5aab57cc0530560d82c74c055f664280619d7e81","title":"PROST: Physical Reasoning about Objects through Space and Time"},{"paperId":"d65a064eb837f838faf6ff67781b62450b92b159","title":"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification"},{"paperId":"f62acd332fd7a6f35b117ed4ffaf93b19483dcf7","title":"Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?"},{"paperId":"143310c074eb09d5e60adea4c42250dbe03bf9f2","title":"Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution"},{"paperId":"38cfcd4681cf85ab507ec0586c753182a4c8eecb","title":"John praised Mary because _he_? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs"},{"paperId":"84c8d939c765dd30574e6c7e6d0a3eb82db1c8fb","title":"ERNIE-NLI: Analyzing the Impact of Domain-Specific External Knowledge on Enhanced Representations for NLI"},{"paperId":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"},{"paperId":"bc5f9a01a0b9e3015c094ff92f5ff1863612a2b7","title":"What BERTs and GPTs know about your brand? Probing contextual language models for affect associations"},{"paperId":"489ffd70cb2afd550ab809bc90f5a766eb07aa80","title":"Inside ASCENT: Exploring a Deep Commonsense Knowledge Base and its Usage in Question Answering"},{"paperId":"12b336abe8b9889bfd2b82ff790e53603a899cbe","title":"Inspecting the concept knowledge graph encoded by modern language models"},{"paperId":"4f92221c7cedb1bd6212276e1c122dcac9860750","title":"Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates"},{"paperId":"408cc1103ab953a26c7071ffd9ce808469f77a01","title":"Understanding by Understanding Not: Modeling Negation in Language Models"},{"paperId":"befbef9f4e4c6269fa712294430ff916cf2fd51c","title":"Let’s Play Mono-Poly: BERT Can Reveal Words’ Polysemy Level and Partitionability into Senses"},{"paperId":"ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb","title":"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"},{"paperId":"db74b20baa82dbcaf641f82fb776e71e581f679b","title":"Novel Aficionados and Doppelgängers: a referential task for semantic representations of individual entities"},{"paperId":"1c0f8a6b8e6f4cc6fc81c4019fc07a2e5ec17107","title":"Probing for Bridging Inference in Transformer Language Models"},{"paperId":"b2816194217673d0b3be5db0b04666cb2f4cdbf5","title":"Modeling Age of Acquisition Norms Using Transformer Networks"},{"paperId":"0672f88d5dc762002b515ca4a0a9f101017fea35","title":"Probing Across Time: What Does RoBERTa Know and When?"},{"paperId":"2ef4be35f8424ea768aa2e1b44392b3eddbc780b","title":"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema"},{"paperId":"209f9bde2dee7cf1677801586562ffe56d435d38","title":"Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"},{"paperId":"c5943bc4b22a63f595cb1c2823a449e03aad4787","title":"AR-LSAT: Investigating Analytical Reasoning of Text"},{"paperId":"2e2ea29356e006fdbedb7592caf5090260f81f1e","title":"What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models"},{"paperId":"00c209ea764f709a5ce7d7fdc16c2352551bfa83","title":"DirectProbe: Studying Representations without Classifiers"},{"paperId":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","title":"Relational World Knowledge Representation in Contextual Language Models: A Review"},{"paperId":"8b652c4d7a8d5836925ce0fe28a91dc661778524","title":"What's the best place for an AI conference, Vancouver or ______: Why completing comparative questions is difficult"},{"paperId":"b265827019f420b44c79fd87be1cc6000329c762","title":"Exploring the Role of BERT Token Representations to Explain Sentence Probing Results"},{"paperId":"1a0d8dbd0252193abe9d64f72fc56cc1f05ed3eb","title":"CURIE: An Iterative Querying Approach for Reasoning About Situations"},{"paperId":"008b9fc834f5839a25febe150f3076d550ee442f","title":"Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2"},{"paperId":"f5d1dbdaa6c8a44f388f3f7fe538403baefc1252","title":"Large pre-trained language models contain human-like biases of what is right and wrong to do"},{"paperId":"54df9a3f12fa4810a94a7a5929d0cc7a672b13c4","title":"Information to Wisdom: Commonsense Knowledge Extraction and Compilation"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19","title":"Measuring and Improving Consistency in Pretrained Language Models"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"634e8fbeba53d45828846dd541ce0a0078c57b68","title":"Syntax-Enhanced Pre-trained Model"},{"paperId":"eb2fc03b8865b8e1b4cb933d917ea269ebe14584","title":"Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training"},{"paperId":"1d7f3297924a9dd90cfc0df522ebe9138c28b46f","title":"Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals"},{"paperId":"9ba6ad0de7dbe1a3b10c44106049adb96f87d483","title":"KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning"},{"paperId":"33422275fbb9958f55419620697faf531482699b","title":"How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering"},{"paperId":"9d1f9406ed676171d9975e27606c95633ca898b1","title":"On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT"},{"paperId":"d865c87f6ce4e0be29ae1e3780fae66b8034d04b","title":"TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation"},{"paperId":"6906c562e182ea22c086b2bb44ed9cec8602a220","title":"Classifier Probes May Just Learn from Linear Context Features"},{"paperId":"42595cb533b03ad44738102fb0c3cef3e4b5c27c","title":"Controlling the Imprint of Passivization and Negation in Contextualized Representations"},{"paperId":"c0c6f3ba310099b9a1ebe899f5f58bb04574df97","title":"Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"15bcf3b7aa6511a55d7066419453a6d5906b2db8","title":"It’s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT"},{"paperId":"14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9","title":"Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI"},{"paperId":"01400290c7db96c4d665d1c29519c42ba47401e0","title":"A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks"},{"paperId":"5b71bcf769e7efab90ceba56b9e4f898899538fe","title":"BERT Knows Punta Cana Is Not Just Beautiful, It’s Gorgeous: Ranking Scalar Adjectives with Contextualised Representations"},{"paperId":"1c6f94fb3d888167355afb580f04d55cd517ebc6","title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"},{"paperId":"b4013141cceb937c46ea5f84f8c06f6bf1215106","title":"PRover: Proof Generation for Interpretable Reasoning over Rules"},{"paperId":"4571feb26d903053661efd2f2f144e010902f458","title":"Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading"},{"paperId":"37bf0bf34603145246c3311df19e2afdf6e0270a","title":"JAKET: Joint Pre-training of Knowledge Graph and Language Understanding"},{"paperId":"fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7","title":"What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams"},{"paperId":"ee5fff85d3ec62698eddba162f054b7e73670b2a","title":"Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics"},{"paperId":"260cce438595c708433719a75c72889fefa5f731","title":"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA"},{"paperId":"a6a83754a0d1e9a8e41c1e9bbdbca32d3b9d1fd3","title":"It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"},{"paperId":"4d766ae60fbbaea4f6c4ca8ad7f14569ec3189d1","title":"Critical Thinking for Language Models"},{"paperId":"15aa86556be1579eadf8fbb0bc486f9427e681f0","title":"Evaluating representations by the complexity of learning low-loss predictors"},{"paperId":"64da659c0687762359226b4cf455520c78acd165","title":"Neural Language Generation: Formulation, Methods, and Evaluation"},{"paperId":"cc4db47a416aba22d1073e59a6867b6997928b7c","title":"What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"a3aa1562e72aecaa06bcec4613cc3d020f4b8e49","title":"Pre-trained Language Models as Symbolic Reasoners over Knowledge?"},{"paperId":"6f2b90ee5a0feea87264148c25a874f84bae20a0","title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?"},{"paperId":"79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88","title":"Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge"},{"paperId":"c8b00d4706fc8979a9c5f410addccbcfe1c0d894","title":"When Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions"},{"paperId":"26cfb57a9722599b361858d454ec816420723e36","title":"Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models"},{"paperId":"7a5d33bccf1f7fa6d00c070f6a6ed22467267368","title":"Commonsense Evidence Generation and Injection in Reading Comprehension"},{"paperId":"011869f932f89d047ce2bd36d73a95cc04888193","title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms"},{"paperId":"3c2f6c2cf7c9b121a0e46a660846bb0a5dad0ee8","title":"DQI: Measuring Data Quality in NLP"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"688e324770a8c3db0af25968bd15e2cc33b9a83d","title":"Can BERT Reason? Logically Equivalent Probes for Evaluating the Inference Capabilities of Language Models"},{"paperId":"567469dc08fbb702df9f525637e9a3fc43bb1fbb","title":"Probing Contextual Language Models for Common Ground with Visual Representations"},{"paperId":"467ac47b2e01ce6ae74e8d70561ca0f8f66c7b8c","title":"Probing Text Models for Common Ground with Visual Representations"},{"paperId":"2297c559ac3509b3ab456229f7032b2a0fbf23c1","title":"Pretraining on Non-linguistic Structure as a Tool for Analyzing Learning Bias in Language Models"},{"paperId":"c30b457fdfb0623b87379de79ffaa570a7f3bb48","title":"Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation"},{"paperId":"0ebb1d1fbf488fba8c18a5a6057a6ccd9e87510f","title":"Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models"},{"paperId":"196c558ce126f5f7d66df4ea52e2442848fc65be","title":"Asking without Telling: Exploring Latent Ontologies in Contextual Representations"},{"paperId":"1e391e60081dcb148cc2732e1f8aa26655fef31e","title":"Explaining Question Answering Models through Text Generation"},{"paperId":"885e52a5b1a4fb36fae0e3fe31e9d4da21e03b41","title":"On the Existence of Tacit Assumptions in Contextualized Language Models"},{"paperId":"774319233a107a29622003a115aa6c79f4a7b37f","title":"Probing Neural Language Models for Human Tacit Assumptions"},{"paperId":"f4b585c9a79dfce0807b445a09036ea0f9cbcdce","title":"Information-Theoretic Probing with Minimum Description Length"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"c44120f765fc43994c5cfb4e12e4f62999efeae6","title":"How Context Affects Language Models' Factual Predictions"},{"paperId":"15ad2b27c5248e7d1db5456794ca1ca8a8198f5d","title":"Transformers as Soft Reasoners over Language"},{"paperId":"80376bdec5f534be78ba82821f540590ebce5559","title":"How Much Knowledge Can You Pack into the Parameters of a Language Model?"},{"paperId":"4f03e69963b9649950ba29ae864a0de8c14f1f86","title":"K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"},{"paperId":"8ae9a17c87a4518b513e860683a0ef7824be994d","title":"Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"},{"paperId":"5a9001cdccdb8b1de227a45eccc503d32d1a2464","title":"What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge"},{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"04dd5ec6dcb2470adfb7d9398eb92e39530fd85e","title":"Textual Pre-Trained Models for Gender Identification Across Community Question-Answering Members"},{"paperId":"4b8969091ab33583035b0fd6c0c0b6755f088338","title":"You are what you’re for: Essentialist categorization in large language models"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"483ef564512eaa8bdb261f8f498b22925de354a9","title":"Eliciting Affective Events from Language Models by Multiple View Co-prompting"},{"paperId":"fc80b1324187d5f7aa8466a7cb94d0d1640dfc2e","title":"Can Neural Networks Learn Implicit Logic from Physical Reasoning?"},{"paperId":"414536eafa990570530433b2f112ad343c66b4b0","title":"Deep Learning Enabled Consumer Research for Product Development"},{"paperId":"cd54a839ba12e7417e321b4407788dc426ebaefc","title":"Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering"},{"paperId":"fed460648303afa32247e493847e4dc73dc1a5b3","title":"Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?"},{"paperId":"41237c1350eb2371363af543a6c6a30db9d09197","title":"AnaLog: Testing Analytical and Deductive Logic Learnability in Language Models"},{"paperId":"2aba5bba16dac5cd62683bab9de5d6faaaed0de1","title":"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach"},{"paperId":"b71c245e093568d8c95aa889f968ce72b18e3d8b","title":"The First Workshop on Commonsense Representation and Reasoning May 27 , 2022"},{"paperId":"82c6f5ffd4d2d43ae7684601f607eae26a759a5a","title":"Analytical Reasoning of Text"},{"paperId":"0ad58485bb1b0ff3cdcf0d2a087ce8c40de529e8","title":"Where Does Linguistic Information Emerge in Neural Language Models? Measuring Gains and Contributions across Layers"},{"paperId":"af7503c8afb957edbf6f196f3821a8cd79bfadf6","title":"Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation"},{"paperId":"9efd60f5aa52c6f147bbdacd2f40da1716fb70d2","title":"Keynote Talk: What kinds of questions have we been asking? A taxonomy for QA/RC benchmarks"},{"paperId":"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","title":"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning"},{"paperId":"5c8cb5d39aa8531fcb5962821ac404e4a0e109d1","title":"Is It Smaller Than a Tennis Ball? Language Models Play the Game of Twenty Questions"},{"paperId":"1bbbe12b060f54a97a4e81fd23cbb7932ff9de53","title":"Language Models have a Moral Dimension"},{"paperId":"291a00d8433fecd2dd10f7f13b62dae8ce500043","title":"Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models"},{"paperId":"b7cd1d84baaa3a65be4216e39086e034b8432845","title":"Exploring the Use of Neural Transformers for Psycholinguistics"},{"paperId":"1db86e01300e2f30fd08b46e63ea11656cb6dcf5","title":"TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models"},{"paperId":"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","title":"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations"},{"paperId":"1841cf23c65ff2f27f21ba0d2268c3445f20332f","title":"Few-Shot Text Generation with Natural Language Instructions"},{"paperId":"1b94ebedacda0c21a4b8a40a5a40afcea4cc719a","title":"When Combating Hype, Proceed with Caution"},{"paperId":"07f6f423950210e7eecc3affc2499a9b59d346df","title":"Test Harder than You Train: Probing with Extrapolation Splits"},{"paperId":"2af476f7c2a7c040fc9ab7750bf41a84f66aa947","title":"Knowledge Based Multilingual Language Model"},{"paperId":"593284010379e02f6ab57e7208b5511185ce8c0e","title":"Probing Pre-trained Language Models for Semantic Attributes and their Values"},{"paperId":"61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886","title":"Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI"},{"paperId":"42fffebf899440941faf558dfef001c85c5c442d","title":"BERT, XLNet or RoBERTa: The Best Transfer Learning Model to Detect Clickbaits"},{"paperId":"23446cc15226c46eae9e4414598f7375eacb6ea1","title":"Back to Square One: Bias Detection, Training and Commonsense Disentanglement in the Winograd Schema"},{"paperId":"db88987a25e036a9fd8b69f8575e0a75d63b260b","title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"},{"paperId":"170ec3ac79b81f21ac35247b7f8e73991a14ebac","title":"Can RoBERTa Reason? A Systematic Approach to Probe Logical Reasoning in Language Models"},{"paperId":"5414be4432fcfe1e83932127ee2463d107e5b61c","title":"A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"},{"paperId":"189b518d70ad34c8de6f613bf3bd5051077608bc","title":"Probing Language Models for Common Ground with Visual Representations"},{"paperId":"adc79a6040c49f048cf4ed81fbc441fef661d5ed","title":"End-to-end Dialog Systems with Numerical Slot Filling"},{"paperId":null,"title":"Preprint 1 Masked entity prediction : Earth → Q 2 : Earth Earth"},{"paperId":null,"title":"Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"}],"references":[{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"68c1bf884f0fc0e86641466a1f1fa67e79f16a17","title":"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"},{"paperId":"83f54abd65634f34e68b30c6fff2a57de7a34f37","title":"Negated LAMA: Birds cannot fly"},{"paperId":"c2c165dd615d4fc31d4fef4b4acbcab1a1655983","title":"On Making Reading Comprehension More Comprehensive"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"4c9a8caf940627126aaa9bd3ac813d07065c86a0","title":"Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets"},{"paperId":"8e914370a043fd626e172d55816952bc477bd582","title":"Question Answering is a Format; When is it Useful?"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"681fbcd98acf20df3355eff3585994bd1f9008b7","title":"Probing Natural Language Inference Models through Semantic Fragments"},{"paperId":"199ff73d2f728e997f860b62a2322823d3e3d9e8","title":"Designing and Interpreting Probes with Control Tasks"},{"paperId":"0e4cd6bae6ac1017e7b1b9bd644375aee65b8372","title":"Show Your Work: Improved Reporting of Experimental Results"},{"paperId":"3cd331c997e90f737810aad6fcce4d993315189f","title":"Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"67c6e842ceafda5b31626c773b380f9e97423cd2","title":"Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts"},{"paperId":"a0e49f65b6847437f262c59d0d399255101d0b75","title":"What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"afd110eace912c2b273e64851c6b4df2658622eb","title":"Visualizing and Measuring the Geometry of BERT"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"165d51a547cd920e6ac55660ad5c404dcb9562ed","title":"Open Sesame: Getting inside BERT’s Linguistic Knowledge"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1","title":"Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"},{"paperId":"e46a16c0bdfeded39977a9f356cbea148f10bed9","title":"Knowledge of animal appearance among sighted and blind adults"},{"paperId":"e2587eddd57bc4ba286d91b27c185083f16f40ee","title":"What do you learn from context? Probing for sentence structure in contextualized word representations"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"a5ec883e080d858b5df60f1b5d711d514459b1e4","title":"Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition"},{"paperId":"19281b9ecdb5c07a93423a506627ab9d9b0cf039","title":"Learning and Evaluating General Linguistic Intelligence"},{"paperId":"efeab0dcdb4c1cce5e537e57745d84774be99b9a","title":"Assessing BERT's Syntactic Abilities"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"ac11062f1f368d97f4c826c317bf50dcc13fdb59","title":"Dissecting Contextual Word Embeddings: Architecture and Representation"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"1536e8958697c5364f68b2e2448905dbbeb3a0ca","title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"c423c2e5f3a0a931f113da53c184542a0fc5a1db","title":"The Description Length of Deep Learning models"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"7d5cf22c70484fe217936c66741fb73b2a278bde","title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents"},{"paperId":"15bce8864a15c20cf483e18099f3601b94673565","title":"Premise Selection for Theorem Proving by Deep Graph Embedding"},{"paperId":"c9f343b492c170c726f607c255ec6c7177dc5800","title":"Verb Physics: Relative Physical Knowledge of Actions and Objects"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"3aa52436575cf6768a0a1a476601825f6a62e58f","title":"Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"},{"paperId":"83e7654d545fbbaaf2328df365a781fb67b841b4","title":"Enhanced LSTM for Natural Language Inference"},{"paperId":"e44da7d8c71edcc6e575fa7faadd5e75785a7901","title":"Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks"},{"paperId":"53d43ccc593bf44e9aa52e3971df1b9dd396e30d","title":"Probing for semantic evidence of composition by means of simple classification tasks"},{"paperId":"1c3884e43bba39c50e6172c403c057659ef3ce83","title":"Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects"},{"paperId":"8a4257052ead8da5773afeff3aa6b6291ca20f60","title":"Building a shared world: mapping distributional to model-theoretic semantic spaces"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"629d8fd5e4483b36cf35803b0de3941df2c168c3","title":"Wikidata"},{"paperId":"cceb698cbbb828537f2f195fb70b6fdc586d3327","title":"Reporting bias and knowledge acquisition"},{"paperId":"53a139116437f716b17e60b0b8775708ea597ff5","title":"Formal semantics of Natural Language: Adverbs of quantification"},{"paperId":"d87ceda3042f781c341ac17109d1e94a717f5f60","title":"WordNet : an electronic lexical database"},{"paperId":"8862c1760476de1cf859e2a59998231e18e33317","title":"Generalized quantifiers and natural language"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":"8eb9b2ea146e3a381a29b4a9314c36f61b71e367","title":"Extracting Commonsense Properties from Embeddings with Limited Human Guidance"},{"paperId":"b3c41f269f77726e3f480915af017c4527d15f4f","title":"Donald Davidson's truth-theoretic semantics"},{"paperId":null,"title":"Hannaneh Hajishirzi, Alon Talmor, and Sewon Min"},{"paperId":null,"title":"Semisupervised sequence learning"}],"id":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","summary":"This work proposes eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition, and findings can help future work on designing new datasets, models, and objective functions for pre-training."},{"url":"https://www.semanticscholar.org/paper/e081dfa64e5ed12fe15fc885ec82ef1f35ab9830","title":"iNALU: Improved Neural Arithmetic Logic Unit","venue":"Frontiers in Artificial Intelligence","year":2020,"referenceCount":24,"citationCount":10,"influentialCitationCount":0,"publicationDate":"17/03/2020","authors":"Daniel Schlör,Markus Ring,A. Hotho","citations":[{"paperId":"26e07b6ac9b0f04396e2e7f7ca518b6fbae67ce7","title":"Improving the Robustness of Neural Multiplication Units with Reversible Stochasticity"},{"paperId":"9025d12ef12d248b529dcca2e5c8b63f89776821","title":"Deep Learning and Symbolic Regression for Discovering Parametric Equations"},{"paperId":"d6c6f61725cd10636821797f8b4aec0efc51ad98","title":"From Analog to Digital Computing: Is Homo sapiens’ Brain on Its Way to Become a Turing Machine?"},{"paperId":"0ce6a798f8222ed8f221326ca566311c648cb4dc","title":"Learning Division with Neural Arithmetic Logic Modules"},{"paperId":"def7c67c3688bc459fbef3ce50a466a0cbf411a9","title":"A Primer for Neural Arithmetic Logic Modules"},{"paperId":"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","title":"Neural Power Units"},{"paperId":"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","title":"Neural Status Registers"},{"paperId":"c36277b67814e0a522e786a38e1768612b0e63f2","title":"Exploring the Learning Mechanisms of Neural Division Modules"},{"paperId":"33149f835f391119d287cc2c6b009464e7d14fe4","title":"On the Abilities of Mathematical Extrapolation with Implicit Models"},{"paperId":"3653d82b9814dd5e64328282004a077b52c2a99e","title":"Financial Fraud Detection with Improved Neural Arithmetic Logic Units"}],"references":[{"paperId":"112ac68ddb0f021517dd465e89918fa52755cc35","title":"Measuring Arithmetic Extrapolation Performance"},{"paperId":"8b85b7eedea2d6790e450c10f296255ac4152d32","title":"Flow-based Network Traffic Generation using Generative Adversarial Networks"},{"paperId":"6f69e19348870552a7ab92d038c8b8d753fe6b60","title":"Neural Arithmetic Expression Calculator"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"da895fa70d45ba06efae6cb128e885ab49c5e977","title":"Microscopy cell counting and detection with fully convolutional regression networks"},{"paperId":"acd87843a451d18b4dc6474ddce1ae946429eaf1","title":"Wasserstein Generative Adversarial Networks"},{"paperId":"618784f1c811dd3df9a0fb95de9868575b38f7a4","title":"Improving the Neural GPU Architecture for Algorithm Learning"},{"paperId":"3a1d2057543cdf2a771f7421c8deab280a04779d","title":"Paysim: a financial mobile money simulator for fraud detection"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"b59d91e0699d4e1896a15bae13fd180bdaf77ea5","title":"Neural Programmer-Interpreters"},{"paperId":"5b791cd374c7109693aaddee2c12d659ae4e3ec0","title":"Grid Long Short-Term Memory"},{"paperId":"b0e52226bc29275698f35283060a97689b373490","title":"Cross-scene crowd counting via deep convolutional neural networks"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"eabba0b46f4325d28d6e02f4ebbf70f40a1263a5","title":"An empirical comparison of botnet detection methods"},{"paperId":"b71ac1e9fb49420d13e084ac67254a0bbd40f83f","title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":"d9665992ee36699b8ae4a2e2294552cd4be9003a","title":"Statistical fraud detection: A review"},{"paperId":"6775ba237f3c3d077ba3639d96cb307ce2bce260","title":"Microscopy cell counting with fully convolutional regression networks"},{"paperId":null,"title":"Microscopy cell counting"},{"paperId":null,"title":"NALU (m) E0.2, 0"},{"paperId":null,"title":"NALU (v) U(-5, 5), (-10, -5) (14) iNALU (sw) E0.2, 0.5 (15) iNALU (sw) E0.8, 0.5 (16) iNALU (sw) N(-2, 4)"},{"paperId":null,"title":"Learning to execute. arXiv [Preprint"},{"paperId":null,"title":"Cross-scene crowd"}],"id":"e081dfa64e5ed12fe15fc885ec82ef1f35ab9830","summary":"An in-depth analysis reveals practical shortcomings by design, such as the inability to multiply or divide negative input values or training stability issues for deeper networks, and proposes an improved model architecture that solves stability issues and outperforms the original NALU model in means of arithmetic precision and convergence."},{"url":"https://www.semanticscholar.org/paper/716efab73e1916fdc2a23727b581d200271ed499","title":"BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes","venue":"ArXiv","year":2020,"referenceCount":15,"citationCount":3,"influentialCitationCount":0,"publicationDate":"06/04/2020","authors":"Kushal Chawla,Gale M. Lucas,J. Gratch,Jonathan May","citations":[{"paperId":"67f23dec7687692660d8aa1315b9dbc8e1aacf22","title":"Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"04b748f940147bdd84a9047ad70ed1e9985d9770","title":"An Improvisational Approach to Acquire Social Interactions"}],"references":[{"paperId":"63748e59f4e106cbda6b65939b77589f40e48fcb","title":"Text Summarization with Pretrained Encoders"},{"paperId":"5a9b1a72c7ef52b756ce022176f1496279e9482c","title":"Intelligent Tutoring System for Negotiation Skills Training"},{"paperId":"b626754a0fd7de12c87e88165b2484ac5d98212a","title":"Decoupling Strategy and Generation in Negotiation Dialogues"},{"paperId":"112d8c462be072de121c13a523535f94f0256383","title":"How People Negotiate? From the Analysis of a Dialogue Corpus to a Dialogue System"},{"paperId":"bb669de2fce407df2f5cb2f8c51dedee3f467e04","title":"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6a8dbea5e40831bd6e987c03b76487f45ac49599","title":"Deal or No Deal? End-to-End Learning of Negotiation Dialogues"},{"paperId":"4d025da955a51350df56a54bbb59c1c1b7630e2b","title":"Discourse Structure and Dialogue Acts in Multiparty Dialogue: the STAC Corpus"},{"paperId":"34b4c12146e6aab1309bfe91a86ff3fa76d1677a","title":"Negotiation as a Challenge Problem for Virtual Humans"},{"paperId":"b8a6025630a0972f8a8bbf28376040951d109cfc","title":"GENIUS: AN INTEGRATED ENVIRONMENT FOR SUPPORTING THE DESIGN OF GENERIC AUTOMATED NEGOTIATORS"},{"paperId":"cb826a3899752b796f14df1c50378c64954a6b0a","title":"Statistical Significance Tests for Machine Translation Evaluation"},{"paperId":"395171da2b9c0617f2cc7adba9bbf13706effd9b","title":"Negotiation behavior when cultures collide: the United States and Japan."},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"bde21379dab0ccb52c96d1fb649fff4ed11f3e92","title":"IAMhaggler: A Negotiation Agent for Complex Environments"}],"id":"716efab73e1916fdc2a23727b581d200271ed499","summary":"This model is able to predict a negotiation's outcome within 10% for more than 70% of the cases, and suggests that rather than just being a way to realize a negotiation, natural language should be incorporated in the negotiation planning as well."},{"url":"https://www.semanticscholar.org/paper/4599f96dd1a2584e00d342953fc7e1361ffd6e1f","title":"Neural Status Registers","venue":"ArXiv","year":2020,"referenceCount":56,"citationCount":5,"influentialCitationCount":1,"publicationDate":"15/04/2020","authors":"Lukas Faber,Roger Wattenhofer","citations":[{"paperId":"4578747d52aa1b3537612287352a803a7b17e999","title":"Asynchronous Neural Networks for Learning in Graphs"},{"paperId":"def7c67c3688bc459fbef3ce50a466a0cbf411a9","title":"A Primer for Neural Arithmetic Logic Modules"},{"paperId":"30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0","title":"MC-LSTM: Mass-Conserving LSTM"},{"paperId":"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","title":"Neural Power Units"},{"paperId":"c36277b67814e0a522e786a38e1768612b0e63f2","title":"Exploring the Learning Mechanisms of Neural Division Modules"}],"references":[{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"def7c67c3688bc459fbef3ce50a466a0cbf411a9","title":"A Primer for Neural Arithmetic Logic Modules"},{"paperId":"2c0a266f9cb88bb914c138ece0deaab8cf528f78","title":"Neural Sequence-to-grid Module for Learning Symbolic Rules"},{"paperId":"6b989b8327db3a7212141c59c1569f0219775058","title":"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks"},{"paperId":"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","title":"Neural Power Units"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"e081dfa64e5ed12fe15fc885ec82ef1f35ab9830","title":"iNALU: Improved Neural Arithmetic Logic Unit"},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"a4a1a70b7cd477de254f88662787406dcd40d8bb","title":"Neural Execution of Graph Algorithms"},{"paperId":"112ac68ddb0f021517dd465e89918fa52755cc35","title":"Measuring Arithmetic Extrapolation Performance"},{"paperId":"da6ba9f19581bd7e28bc280c53385a1327eb09dd","title":"LSTM Networks Can Perform Dynamic Counting"},{"paperId":"271b31b41782d11c5176a260f0ecdf2611b21e77","title":"What Can Neural Networks Reason About?"},{"paperId":"640a08c0bf0f69cd659bae3fcee7ad359d8eee7c","title":"Neural Stored-program Memory"},{"paperId":"3a6447361b20c249f5306ae17dee43f645430e31","title":"Neural Logic Machines"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"f9717d29840f4d8f1cc19d1b1e80c5d12ec40608","title":"A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"},{"paperId":"e53fd0c9a04df127af80b2699f5f8f23a8a3e2af","title":"On Evaluating the Generalization of LSTM Models in Formal Languages"},{"paperId":"6f69e19348870552a7ab92d038c8b8d753fe6b60","title":"Neural Arithmetic Expression Calculator"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"acc43abe319bca7652a91f7d4ca6187049fb82e4","title":"Measuring abstract reasoning in neural networks"},{"paperId":"660f08f5aa355c19a851d848e0c1321c5c32c1ab","title":"Learning Equations for Extrapolation and Control"},{"paperId":"19b7769dab4e6092aa4b7eeb8aa078a7b725c9b4","title":"Relational inductive biases, deep learning, and graph networks"},{"paperId":"06354570d5f6be803d4a79bf59ecbb097bca8755","title":"On the Practical Computational Power of Finite Precision RNNs for Language Recognition"},{"paperId":"21937ecd9d66567184b83eca3d3e09eb4e6fbd60","title":"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"},{"paperId":"4df7bbe3ca7806f39a490c99f17867a0ac299bc3","title":"Learning Explanatory Rules from Noisy Data"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"0c253bb9aee9aa1ae7909700eda845bd3124197f","title":"Neural Program Meta-Induction"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6b024162f81e8ff7aa34c3a43d601a912d012c78","title":"Making Neural Programming Architectures Generalize via Recursion"},{"paperId":"e24cdf73b3e7e590c2fe5ecac9ae8aa983801367","title":"Neural Message Passing for Quantum Chemistry"},{"paperId":"618784f1c811dd3df9a0fb95de9868575b38f7a4","title":"Improving the Neural GPU Architecture for Algorithm Learning"},{"paperId":"efc2ee2c7a9ec5e798720c29c54512407efe06d0","title":"Neural Functional Programming"},{"paperId":"15064b7e47b43a68b6661bc7c3eaaad207493191","title":"Neural Program Lattices"},{"paperId":"3058d4a4fa6ebaac30e0279c6b8c30e5f969d315","title":"Extensions and Limitations of the Neural GPU"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"cdc90e091e64b796f5069bda7bbfdc5a04bd6365","title":"Extrapolation and learning equations"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"49e24b1fd415e20839c09b5dd68a01f61d1681d9","title":"Learning Simple Algorithms from Examples"},{"paperId":"b59d91e0699d4e1896a15bae13fd180bdaf77ea5","title":"Neural Programmer-Interpreters"},{"paperId":"e837b79de602c69395498c1fbbe39bbb4e6f75ad","title":"Learning to Transduce with Unbounded Memory"},{"paperId":"5259755f9c100e220ffaa7e08439c5d34be7757a","title":"Reinforcement Learning Neural Turing Machines - Revised"},{"paperId":"4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e","title":"End-To-End Memory Networks"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"c3823aacea60bc1f2cabb9283144690a3d015db5","title":"Neural Turing Machines"},{"paperId":"71ae756c75ac89e2d731c9c79649562b5768ff39","title":"Memory Networks"},{"paperId":"e15cf50aa89fee8535703b9f9512fca5bfc43327","title":"Going deeper with convolutions"},{"paperId":"86ee1835a56722b76564119437070782fc90eb19","title":"Generative Adversarial Nets"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"b71ac1e9fb49420d13e084ac67254a0bbd40f83f","title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"8331c6613c175e98fb2226523e8aabfb36fb2408","title":"Towards Scale-Invariant Graph-related Problem Solving by Iterative Homogeneous GNNs"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Zero-shot learningthe good, the bad and the ugly"},{"paperId":null,"title":"Learning XOR: exploring the space of a classic problem"}],"id":"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","summary":"The Neural Status Register is introduced, inspired by physical Status Registers, and at the heart of the NSR are arithmetic comparisons between inputs that allow end-to-end differentiation and learns such comparisons reliably."},{"url":"https://www.semanticscholar.org/paper/45e5d7637a585a87d967a4a357d17c5d89aecea2","title":"A Formal Hierarchy of RNN Architectures","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":28,"citationCount":39,"influentialCitationCount":1,"publicationDate":"01/04/2020","authors":"William Cooper Merrill,Gail Weiss,Yoav Goldberg,Roy Schwartz,Noah A. Smith,Eran Yahav","citations":[{"paperId":"3b909916f57e0493a45ecef3fd6dbdfd78211222","title":"DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification"},{"paperId":"8133ad3008aebcc734708f05580a21a0b948ed60","title":"Exploring the Promise and Limits of Real-Time Recurrent Learning"},{"paperId":"94c1da0e6bb1d9792986fa4badeb20af14737ebc","title":"Modeling rapid language learning by distilling Bayesian priors into artificial neural networks"},{"paperId":"e81602166f323d4eb8376bdc7b57c0744aac820f","title":"Empirical Analysis of the Inductive Bias of Recurrent Neural Networks by Discrete Fourier Transform of Output Sequences"},{"paperId":"221de85e2054d9f1762881fa6852f1061b53e399","title":"Modelling Concurrency Bugs Using Machine Learning"},{"paperId":"eb87c18dd5bfcc1106086a4c89653ecd02d71f33","title":"Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks"},{"paperId":"bab6f0823c3ca64f8c5c9f56f2467c27aabc80f6","title":"Exploring the Long-Term Generalization of Counting Behavior in RNNs"},{"paperId":"fb6d75a4f3b1af2058f59957116c178a47b56f05","title":"Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions"},{"paperId":"9bd14955135aa1bf3927c6011232702d25394352","title":"Interaction of lexical strata in hybrid compound words through gradient phonotactics"},{"paperId":"c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617","title":"Neural Networks and the Chomsky Hierarchy"},{"paperId":"bab33a1a464bc237bfeccfaba9bd155d2b90d186","title":"Implicit n-grams Induced by Recurrence"},{"paperId":"0c7b82cef44cbdca4cbefe17a8ba7ca47fbffea9","title":"Exploring a diverse world of effector domains and amyloid signaling motifs in fungal NLR proteins"},{"paperId":"3fa611d27986ba9983df8c4013114b995c1793cc","title":"Extracting Finite Automata from RNNs Using State Merging"},{"paperId":"648069f71d7ab9d4c9a4c44a782e1b5d75179fd9","title":"Stronger Separation of Analog Neuron Hierarchy by Deterministic Context-Free Languages"},{"paperId":"33382bb7352b4aabd33a8d2285f5b3ba73861208","title":"Training dynamics of neural language models"},{"paperId":"0735fb79bf34698c1df4461a05ed51c232c412e4","title":"Thinking Like Transformers"},{"paperId":"56f46451a8790a919c7048f869ee9aaac7ae2fd7","title":"Extracting Weighted Automata for Approximate Minimization in Language Modelling"},{"paperId":"26cf06dc782a7c484c5c2463b1c9c7482536c4e4","title":"Learning and Generalization in RNNs"},{"paperId":"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages"},{"paperId":"fdc1815b83a59c0667ac75c8696febf5f1fd59e8","title":"Lexical strata and phonotactic perplexity minimization"},{"paperId":"6a3aa7acaacd72fecb33e32d1c524883a45a18b7","title":"Formal Language Theory Meets Modern NLP"},{"paperId":"39928e0f5fd2d65aa92ea8fd03108b5d1f25f6f6","title":"Stronger Separation of Analog Neuron Hierarchy by Deterministic Context-Free Languages"},{"paperId":"2b0e5a699095184144023cce674e9c88994598a6","title":"Connecting Weighted Automata, Tensor Networks and Recurrent Neural Networks through Spectral Learning"},{"paperId":"f10a04a77fd1cd719792de374a60f3fd03f6b944","title":"Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent"},{"paperId":"9f13b84e05a0c6c717b06038ddbfd9af55b51806","title":"Parameter Norm Growth During Training of Transformers"},{"paperId":"d06e84ac9e912b415719f0e7f3163d59e0a329cd","title":"RNNs Can Generate Bounded Hierarchical Languages with Optimal Memory"},{"paperId":"52d87b59a1fa6561b90d5ea56f21180a9b1a505c","title":"How Can Self-Attention Networks Recognize Dyck-n Languages?"},{"paperId":"93a640dfdac5950bde902db9db31e3502e7fffe2","title":"Distillation of Weighted Automata from Recurrent Neural Networks using a Spectral Approach"},{"paperId":"0def0a8dfccd1e70bf0f0f8b1a2217252fa3f952","title":"On the Ability of Self-Attention Networks to Recognize Counter Languages"},{"paperId":"10c86505de83647c7b4157595ab10f64e97c94ef","title":"On the Ability and Limitations of Transformers to Recognize Formal Languages"},{"paperId":"032399b7fc693a9fc12bb26d6be8c02d77dd397a","title":"What they do when in doubt: a study of inductive biases in seq2seq learners"},{"paperId":"75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1","title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling"},{"paperId":"ae807218c494fd4fb74690d3a48d18cdfbb44b1d","title":"Provably Stable Interpretable Encodings of Context Free Grammars in RNNs with a Differentiable Stack"},{"paperId":"299546bc706ee776aea9d58cb21c729ee4499bd2","title":"A provably stable neural network Turing Machine"},{"paperId":"0d317ae9d1cc285cfa90f44e0afbdb34b8b24d41","title":"Analog neuron hierarchy"},{"paperId":"1862ff140e0169c6b367dc3fd9e7c714dae38026","title":"N EURAL N ETWORKS AND THE C HOMSKY H IERARCHY"},{"paperId":"1781517a91be8248dd0febad65211a1b6614e199","title":"On the Power of Saturated Transformers: A View from Circuit Complexity"},{"paperId":"5bd9af165c7fe18aa6235df5032155befa522454","title":"Deep Sentiment Analysis: A Case Study on Stemmed Turkish Twitter Data"},{"paperId":"7e1c4d2f0b276bd30656f7577444c48210ab54ad","title":"Investigating Backpropagation Alternatives when Learning to Dynamically Count with Recurrent Neural Networks"}],"references":[{"paperId":"d15e715d99a52f1c8a1ae6ec0ca853b13e04bbc7","title":"On the Linguistic Capacity of Real-Time Counter Automata"},{"paperId":"a28ee1bea680c745636d7e09218fffda5d544ffe","title":"Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages"},{"paperId":"f0471ad6daecabbd2521c6cede46007a02260771","title":"RNN Architecture Learning with Sparse Regularization"},{"paperId":"b3564be8b79f25585acb035f3deaf4ae93c26d8f","title":"Theoretical Limitations of Self-Attention in Neural Sequence Models"},{"paperId":"da6ba9f19581bd7e28bc280c53385a1327eb09dd","title":"LSTM Networks Can Perform Dynamic Counting"},{"paperId":"a1b35b15a548819cc133e3e0e4cf9b01af80e35d","title":"Sequential Neural Networks as Automata"},{"paperId":"a56ebc39b8c527774be705cccdcb5f66c7302e0c","title":"Rational Recurrences"},{"paperId":"6bf2f1dc081b319ed55f2e185278c7ebfacd9e45","title":"Bridging CNNs, RNNs, and Weighted Finite-State Machines"},{"paperId":"06354570d5f6be803d4a79bf59ecbb097bca8755","title":"On the Practical Computational Power of Finite Precision RNNs for Language Recognition"},{"paperId":"93b4cc549a1bc4bc112189da36c318193d05d806","title":"AllenNLP: A Deep Semantic Natural Language Processing Platform"},{"paperId":"31b26b31f28988ebcfe7ff356e7fda7e17f1558c","title":"Recurrent Neural Networks as Weighted Language Recognizers"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2d876ed1dd2c58058d7197b734a8e4d349b8f231","title":"Quasi-Recurrent Neural Networks"},{"paperId":"1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba","title":"Convolutional Neural Networks for Sentence Classification"},{"paperId":"c71325ea1e3391a688c70024c855b1b142854a2c","title":"Spectral learning of weighted automata"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"e72d20e5b6e21a2013d4b0d62001a194d86a210e","title":"Context-Free Recognition with Weighted Automata"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"93aaf126443643fb0835df896ab07b523f2c9613","title":"Analog computation via neural networks"},{"paperId":"17594df98c222217a11510dd454ba52a5a737378","title":"On the computational power of neural nets"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"},{"paperId":"c49838c33a6d190dea1981a04f54ee3443ac607b","title":"Realizations by Stochastic Finite Automata"},{"paperId":"31b1beb7a33feee7853fe195e5bc554d581154c6","title":"Counter machines and counter languages"},{"paperId":"19c278e06339c5a3e7dd94403ea70b7e919579bb","title":"Turing Machines with Restricted Memory Access"},{"paperId":"6a4d840d64865ad887a4717a5b769e094a050c73","title":"The Handbook of Brain Theory and Neural Networks"},{"paperId":"cb19a063ea91e716a2c84d7bd3ce8935a645ef8d","title":"Rational and Recognisable Power Series"},{"paperId":null,"title":"2017) into the state expressiveness hierarchy. We consider a single-head self attention encoder"},{"paperId":null,"title":"Matrices de Hankel"}],"id":"45e5d7637a585a87d967a4a357d17c5d89aecea2","summary":"It is hypothesized that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy, and empirical results to support this conjecture are provided."},{"url":"https://www.semanticscholar.org/paper/8a5590978930070f50c5f9fbf61f67e5d95794f0","title":"Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning","venue":"AAAI Conference on Artificial Intelligence","year":2020,"referenceCount":74,"citationCount":17,"influentialCitationCount":2,"publicationDate":"03/04/2020","authors":"Wenhe Zhang,Chi Zhang,Yixin Zhu,Song-Chun Zhu","citations":[{"paperId":"223886da4fd58a88930b033d7132cf59a038fbb4","title":"MEWL: Few-shot multimodal word learning with referential uncertainty"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"228000eb469b6be8a3d9876b141a493277d21574","title":"Are Deep Neural Networks SMARTer than Second Graders?"},{"paperId":"4c5108dddde056f430fd745eaeb661d54aa8a156","title":"Geoclidean: Few-Shot Generalization in Euclidean Geometry"},{"paperId":"6c8c67321204866bf7695805fbfa0a593d1af78f","title":"Geoclidean: Few-Shot Generalization in Euclidean Geometry"},{"paperId":"3c554b11d6c21799bb1687d5db916a3f963da640","title":"DETR++: Taming Your Multi-Scale Detection Transformer"},{"paperId":"02eae58e5ff7edb2f7cbb334e81c3af6b2768b59","title":"A Review of Emerging Research Directions in Abstract Visual Reasoning"},{"paperId":"59cb81d7763d8de23d9b4144c16287cb6808da1d","title":"Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices"},{"paperId":"03da1b6759f70c10e49b33a0ee914cb893d6f949","title":"Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning"},{"paperId":"40b065eb3aa5c5a54962aee78ebe30943beaabb1","title":"Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images"},{"paperId":"316c5d697f7caf92b419213e929a6063afaf253c","title":"ACRE: Abstract Causal REasoning Beyond Covariation"},{"paperId":"721379fe5d859f13442102a5681cf58952b5af14","title":"Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution"},{"paperId":"b7d89dc055618b68a29d199457ee1fec18543dab","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"edaafec59b651d503ac7c8a86f8e2335273e1f7a","title":"Learning by Fixing: Solving Math Word Problems with Weak Supervision"},{"paperId":"862bca372c0c7475f9e5eb4951b9309f39cd4ace","title":"Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense"},{"paperId":"c0255e528a73fe42e730c905227821327fef96df","title":"Geoclidean: Few-Shot Generalization in Euclidean Geometry"},{"paperId":"4d1b18ee2d6093fc977df09d62b2a0e40b62a698","title":"A HINT from Arithmetic: On Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"e764dee4e50db01d77976e8f313fc092fc0eba85","title":"GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning"}],"references":[{"paperId":"0187141e52f6d5cd0cf812ced648960e8f610abe","title":"Learning Perceptual Inference by Contrasting"},{"paperId":"70336ddb0082a43cf6180a321cb0c0683ac6d59e","title":"Theory-based Causal Transfer: Integrating Instance-level Induction and Abstract-level Structure Learning"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"d0bfd3cb732471a0843a39d2d047caf60a844466","title":"RAVEN: A Dataset for Relational and Analogical Visual REasoNing"},{"paperId":"0d16b0fbe1e2bf6ef02aea2e058f2e13c3a83fa2","title":"Learning to Make Analogies by Contrasting Abstract Relational Structure"},{"paperId":"acc43abe319bca7652a91f7d4ca6187049fb82e4","title":"Measuring abstract reasoning in neural networks"},{"paperId":"b36a5bb1707bb9c70025294b3a310138aae8327a","title":"Automatic differentiation in PyTorch"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"007112213ece771be72cbecfd59f048209facabd","title":"A simple neural network module for relational reasoning"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"40d71f60c70527ba6007a0458c2004fa45753a3c","title":"Associations of non-symbolic and symbolic numerical magnitude processing with mathematical competence: a meta-analysis."},{"paperId":"ba84b8f16efc032945cd8ed960c121a9bcd064d5","title":"Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"815c84ab906e43f3e6322f2ca3fd5e1360c64285","title":"Human-level concept learning through probabilistic program induction"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"f9c990b1b5724e50e5632b94fdb7484ece8a6ce7","title":"Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"946dffe06446c38a397fafb8d54e48c764196712","title":"A Source Book Of Gestalt Psychology"},{"paperId":"a4ffb87c11a7d1a52777a99817e18640272606a4","title":"Flexible and adaptive use of strategies and representations in mathematics education"},{"paperId":"8b0401a2c013b07f8daea62cff00df1dc1c0ec70","title":"Solving Geometric Analogy Problems Through Two-Stage Analogical Mapping"},{"paperId":"6b34f82bc2a1f5a802bb47e8e8e176fa7cfa595d","title":"Representation of number in the brain."},{"paperId":"f75c694f31ae66fe9b5f4ae0345b3cc8a7cff1a6","title":"Oxford English Dictionary."},{"paperId":"9fc2448d9e38dd6a25fd236f8a78818f2ea758a5","title":"Exploring the microstructure of children's central conceptual structures in the domain of number."},{"paperId":"573ed869823a4f6c0730a8d24e6307cdfcf81517","title":"The ABC of cardinal and ordinal number representations"},{"paperId":"8c560202ceda574c65fcef53ba6a22c2e6640384","title":"The Organization of Learning"},{"paperId":"09699eae4b5f9ba6eb022c7381cc84a2e00c0ee7","title":"Using Measures of Number Sense to Screen for Difficulties in Mathematics: Preliminary Findings"},{"paperId":"09962320700f0ff1c71d353d406793531b6fc807","title":"Visual Analogy in Problem Solving"},{"paperId":"37e4180aaf2e4762cbdc9dd04e714643d86c219f","title":"Culture and systems of thought: holistic versus analytic cognition."},{"paperId":"b82e21162f76da0e082282a298e7519f9b0bdd81","title":"Constraint relaxation and chunk decomposition in insight problem solving"},{"paperId":"5385a494e90c6fc01195bf44d680c6e01cf687b5","title":"A System for Relational Reasoning in Human Prefrontal Cortex"},{"paperId":"1050141af39d7c64303e60b608db1cec23db528d","title":"Bayesian Modeling of Human Concept Learning"},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"249b235ef1fdc4de22032b60e6f12ed10adba0de","title":"Ordering of the numerosities 1 to 9 by monkeys."},{"paperId":"94518df3c15d4c88ec16cc4cc9f301895bca1af1","title":"Abstract representations of numbers in the animal and human brain"},{"paperId":"7e879518ec31255bc75c7f5a1586df9f39b231e6","title":"Brain mechanisms of quantity are similar in 5-year-old children and adults."},{"paperId":"6d9dc49325c8dd4edbf854c2a2eb174a52d9b126","title":"Addition and subtraction by human infants"},{"paperId":"eb45b073b52040d05aa9b462415cb881595b85fc","title":"Concept formation knowledge and experience in unsupervised learning"},{"paperId":"40e2df90808e9a38b342bff588b2583e66059b44","title":"THE NICOMACHEAN ETHICS"},{"paperId":"617b1261af3be2a9e72adc343e82c9edc9aedad3","title":"Numerical competence in animals: Definitional issues, current evidence, and a new research agenda"},{"paperId":"93151da5382d95b2b70e415d814b585871117988","title":"Perception of numbers by human infants."},{"paperId":"4978e96a066ab4e9189f0d117ef16378953d92e5","title":"Theory of fluid and crystallized intelligence: A critical experiment."},{"paperId":"06f2e16b9e19268d714b956af49649b2e8c79fc3","title":"Decomposing Human Causal Learning: Bottom-up Associative Learning and Top-down Schema Reasoning"},{"paperId":"49830f605c316fec8e9e365e7e64adeea47e89d8","title":"Modeling Visual Problem Solving as Analogical Reasoning"},{"paperId":"7606136f5c67d8806cdc8750770db1a8c5073f10","title":"A Structure-Mapping Model of Raven's Progressive Matrices"},{"paperId":"d880bd87dea6c66de8c3326d2b2416947f15e6b8","title":"How much does number matter to a monkey (Macaca mulatta)?"},{"paperId":"cf54074bebbb4f9d148d2e6068b7f5e22a87e2b5","title":"AND CASE"},{"paperId":"0939cd35617482641d6e5e4f7619e68601e8048a","title":"Numbers, language, and the human mind"},{"paperId":"161889c6dfaadfda26a3834342354b52198ab433","title":"Psychological models for the development of mathematical understanding: rational numbers and functions"},{"paperId":"7847fa2a5a86ab30068d9d6b1757a96c386ae43c","title":"Visual Analogy: Consciousness as the Art of Connecting"},{"paperId":"a64dddf69180e744f62a22d9be07736ad4b2d7d9","title":"Assessment of a problem-centered second-grade mathematics project."},{"paperId":"8aa8e60994006f803fd25f86f039e5a5b1e2542c","title":"Animal cognition: the representation of space, time and number."},{"paperId":"5ef6f2547e8a004a2549a1cfee26906879fe0025","title":"Oxford English dictionary"},{"paperId":"318cddbeb64fc022a482dfcb316f732af99e6d47","title":"Manual for Raven's progressive matrices and vocabulary scales"},{"paperId":"43a0503e38b4c47a5b794c04439299bd78310189","title":"Laws of organization in perceptual forms."},{"paperId":null,"title":"Mental tests used in genetic studies: The performance of related individuals on tests mainly educative and mainly reproductive"},{"paperId":null,"title":"Ordering of the numerosities"},{"paperId":null,"title":"Visualisation and the development of number sense with kindergarten children. Children's number learning: A research monograph of MERGA/AAMT Adelaide: Australian Association of Mathematics teachers"},{"paperId":null,"title":"Visualisation and the development of number sense with kindergarten children"},{"paperId":null,"title":"Ii. exploring the microstructure of children's central conceptual structures in the domain of number. Monographs of the Society for research in"},{"paperId":null,"title":"Automatic differentiation in pytorch . In ICLR . [ Raven and Court 1998 ]"},{"paperId":null,"title":"The Nicomachean ethics . AJ Valpy . [ Temple and Posner 1998 ]"},{"paperId":null,"title":"and Weiner"},{"paperId":null,"title":"and Mumford"},{"paperId":null,"title":"and Chang"},{"paperId":null,"title":"Oxford english dictionary . Dictionary"},{"paperId":null,"title":"Ii. exploring the microstructure of children's central conceptual structures in the domain of number. Monographs of the Society for research in"},{"paperId":null,"title":"Visual analogy in problem solving . In IJCAI . [ Davis and Pérusse 1988 ]"},{"paperId":null,"title":"and Ba"},{"paperId":null,"title":"and Pérusse"},{"paperId":null,"title":"A stochastic grammar of images"},{"paperId":null,"title":"and Nieder"},{"paperId":null,"title":"Oxford pyschologists Press. [Raven 1936] Raven, J. C. 1936. Mental tests used in genetic studies: The performance of related individuals on tests mainly educative and mainly reproductive"},{"paperId":null,"title":"and Forbus"}],"id":"8a5590978930070f50c5f9fbf61f67e5d95794f0","summary":"A dataset, Machine Number Sense (MNS), consisting of visual arithmetic problems automatically generated using a grammar model—And-Or Graph (AOG), called for attention in fusing the classic search-based algorithms with modern neural networks to discover the essential number concepts in future research."},{"url":"https://www.semanticscholar.org/paper/36d6c8895bbc755964b8b2136c6fd6087a7af089","title":"TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":50,"citationCount":69,"influentialCitationCount":5,"publicationDate":"01/05/2020","authors":"Qiang Ning,Hao Wu,Rujun Han,Nanyun Peng,Matt Gardner,D. Roth","citations":[{"paperId":"8d848ce7ab6ccde4854d3c658a169215ae483029","title":"An Overview Of Temporal Commonsense Reasoning and Acquisition"},{"paperId":"d48b543b010784d3956cd34120599be845733a42","title":"Chinese Event Temporal Relation Extraction on Multi-Dimensional Attention"},{"paperId":"11daaaedd317ae23c7de7df506572d9155017ae3","title":"Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models"},{"paperId":"5d77cd554909a12e8ae6660b24e5903074c56ba5","title":"Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning"},{"paperId":"367e0d715ef66c5aea8024e0a97cfa48e29d52c3","title":"Event Knowledge Incorporation with Posterior Regularization for Event-Centric Question Answering"},{"paperId":"1cbf13d558911fb79663473cfe482bafa71d1e77","title":"A-CAP: Anticipation Captioning with Commonsense Knowledge"},{"paperId":"1358a3f7bb9be34692db189fc5b968822feac3d3","title":"PIPER: A logic-driven deep contrastive optimization pipeline for event temporal reasoning"},{"paperId":"293583961efdf22c0905cc04465aa88ea96826ec","title":"Causal schema induction for knowledge discovery"},{"paperId":"cb304313e5ccddd370c6a0803cc49bf2c84c2a67","title":"TKGQA Dataset: Using Question Answering to Guide and Validate the Evolution of Temporal Knowledge Graph"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"63cb0939c285f4fa7e61117528ac66a697be4303","title":"Event Temporal Relation Extraction with Bayesian Translational Model"},{"paperId":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey"},{"paperId":"fd9c6baf8e485f13285777d8ec24da2359194461","title":"tieval: An Evaluation Framework for Temporal Information Extraction Systems"},{"paperId":"ee153a2c91d36b034dc86c945aee9859b79da812","title":"Test of Time: Instilling Video-Language Models with a Sense of Time"},{"paperId":"649dfa75f6a8ab4c4180e0a2e0be577c49000b70","title":"Generic Temporal Reasoning with Differential Analysis and Explanation"},{"paperId":"1d417bdd331912a458de920459f23fcc7f6e8699","title":"Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts"},{"paperId":"0b0fb5b834d16400b922110daeb02b764fb9ef6f","title":"Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation"},{"paperId":"98f19ca97512361b12475b42b67a617de14d33a1","title":"Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic"},{"paperId":"750448e5d852ec0a2e4f7f809f16a1470b2b479b","title":"StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models"},{"paperId":"078f4efd448822b0e25d3ee0aec842ced606a595","title":"Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts"},{"paperId":"cf047a55569834900045f4c82e08fb1b97d95f61","title":"A Survey of Machine Narrative Reading Comprehension Assessments"},{"paperId":"7fe912158bb91365286edb4c828452275a26d9ff","title":"Temporal Relation Extraction with Joint Semantic and Syntactic Attention"},{"paperId":"e4169d63c99b7348ddd7e35293ce6ddeb22a9dcc","title":"A survey of methods for revealing and overcoming weaknesses of data-driven Natural Language Understanding"},{"paperId":"3352644e4ee2f5bbccf34a74d1cc8b919397d823","title":"What Makes Reading Comprehension Questions Difficult?"},{"paperId":"d3115b64a5c9d0e75e9acbf5c45fc778f99f4059","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding"},{"paperId":"79ec50cd1320697819fd44c4ac6570c48a312349","title":"Faking Fake News for Real Fake News Detection: Propaganda-Loaded Training Data Generation"},{"paperId":"3469127ffee73eca466bd86e4bfdfa8e8c71107f","title":"Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs"},{"paperId":"50ec3d960ac458573a1e4a1556420c5e96d58609","title":"Distantly-Supervised Evidence Retrieval Enables Question Answering without Evidence Annotation"},{"paperId":"174a0e6da0dfb7f96d4a0a4076eed154c439e41a","title":"Probing Language Models for Understanding of Temporal Expressions"},{"paperId":"9f9667c06c94cc854eb55f16cfad2b9db7da49cd","title":"Salience-Aware Event Chain Modeling for Narrative Understanding"},{"paperId":"8c4e9f559367fbbf731e97f66cad5559373863b0","title":"Extracting Event Temporal Relations via Hyperbolic Geometry"},{"paperId":"278351ba38c46b61337e5a810747a37e54185bf9","title":"A Dataset for Answering Time-Sensitive Questions"},{"paperId":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"},{"paperId":"ac8d33e4c0a45e227a47353f3f26fbb231482dc1","title":"Time-Aware Language Models as Temporal Knowledge Bases"},{"paperId":"62953ca1252c9febe07c7007a10911726f37792d","title":"TIMEDIAL: Temporal Commonsense Reasoning in Dialog"},{"paperId":"47fe46e561e3b270407b7bffa176e35291f165a7","title":"Question Answering Over Temporal Knowledge Graphs"},{"paperId":"6597d61bdb531051678c773526758a6dc113b9ce","title":"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences"},{"paperId":"80e015b4edbe72cb10af0bd2cb065bba163d6e0d","title":"“Nice Try, Kiddo”: Investigating Ad Hominems in Dialogue Responses"},{"paperId":"7d9a3b94f78827952b078c664b0da1c02e1c2ee3","title":"What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?"},{"paperId":"163c6d3f6c65239099f1d7b84a42266b69a14a43","title":"Extracting Temporal Event Relation with Syntax-guided Graph Transformer"},{"paperId":"c7f977f556d2060238fdc1286d057d46958afaf9","title":"ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning"},{"paperId":"de74100ac5a16f9169b8a7fe98dd05647b0e8777","title":"SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning"},{"paperId":"5b0c4820eca8c5749860b932ab437ff110edc652","title":"Grid Search Hyperparameter Benchmarking of BERT, ALBERT, and LongFormer on DuoRC"},{"paperId":"d6c919ee0d51432496513ef4b6b2dbd128819779","title":"Microtask Detection"},{"paperId":"33b06c74eea3f400b6f5ef14ef163aef1db42d16","title":"Conditional Generation of Temporally-ordered Event Sequences"},{"paperId":"6eee69031d2e11aa03a5a8fcb219cff4562863be","title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning"},{"paperId":"9793b07ba09d9f2ac9cabd8117daa93bf3db4346","title":"DEER: A Data Efficient Language Model for Event Temporal Reasoning"},{"paperId":"3d2035edd4dd48e1e638279409e11bf689c461e1","title":"Temporal Reasoning in Natural Language Inference"},{"paperId":"9d436d25981ea61db728bd490f0d54376d08953e","title":"Temporal Reasoning on Implicit Events from Distant Supervision"},{"paperId":"09be2777ff6974e27c5f04c0fdf1edeaabe56e30","title":"\"Nice Try, Kiddo\": Ad Hominems in Dialogue Systems"},{"paperId":"778560c0da279b82b0da91ba43a2a023b9c6711f","title":"Neural Language Modeling for Contextualized Temporal Graph Generation"},{"paperId":"a0c52a5cca698636b5a516b24f824d23f506f6e8","title":"Easy, Reproducible and Quality-Controlled Data Collection with CROWDAQ"},{"paperId":"30602e3382df3abedb5f225b55b7efce8580f74d","title":"ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data"},{"paperId":"337ac06d80da63166ac91461e68edab6e4f7efa7","title":"Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding"},{"paperId":"b672d2ec81c9395c312d57a27931864d07664592","title":"A Meta-framework for Spatiotemporal Quantity Extraction from Text"},{"paperId":"867915b9587c046ce8e4b71ab4dee2a1d8bf0b48","title":"DocTime: A Document-level Temporal Dependency Graph Parser"},{"paperId":"586cc1f02f41a7d702bd441e28fc94b9833e81d4","title":"Improving Event Temporal Relation Classification via Auxiliary Label-Aware Contrastive Learning"},{"paperId":"aa027ca4a3e71001283471cb88becd2c3b2a3ad5","title":"DCT-Centered Temporal Relation Extraction"},{"paperId":"8b0b5b998d65add5c036b45c36afa3f9df96f4b0","title":"Toward Building a Language Model for Understanding Temporal Commonsense"},{"paperId":"b8768fbba4d356a1c49dcd8e4cd33aea0c6472bc","title":"How about Time? Probing a Multilingual Language Model for Temporal Relations"},{"paperId":"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","title":"20Q: Overlap-Free World Knowledge Benchmark for Language Models"},{"paperId":"e6d84cf9ae6efa10919bff765613e883a761db62","title":"Open Temporal Relation Extraction for Question Answering"},{"paperId":"965840bb0a8803b78c169f4b387e9e14cc80fffc","title":"Extracting Temporal Event Relation with Syntactic-Guided Temporal Graph Transformer"},{"paperId":"6bb369f874f49cd51415f216f1a3f635f2ca1eed","title":"ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations"},{"paperId":"f214512bee16dbed2ca60fd867bb6ba3c6c27669","title":"Benchmarking Machine Reading Comprehension: A Psychological Perspective"},{"paperId":"ce9e010c5cb2323dfed3af687b12875f01c759bc","title":"TIMERS: Document-level Temporal Relation Extraction"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"43003de1bdf12e14e917c98807ad0ab244caa923","title":"Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation"}],"references":[{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"9fec5868542b4d9070306f1418d1d21666226e90","title":"Evaluating NLP Models via Contrast Sets"},{"paperId":"35e6783307f82d1faa39be0653431305abec7271","title":"Evaluating Models’ Local Decision Boundaries via Contrast Sets"},{"paperId":"c2c165dd615d4fc31d4fef4b4acbcab1a1655983","title":"On Making Reading Comprehension More Comprehensive"},{"paperId":"8e914370a043fd626e172d55816952bc477bd582","title":"Question Answering is a Format; When is it Useful?"},{"paperId":"81b4920ad488affaee27389ff9540b7fea90a4ce","title":"“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding"},{"paperId":"483a47868d797ba998ba2eb107fa24d3f817220a","title":"QuASE: Question-Answer Driven Sentence Encoding"},{"paperId":"e03b5bc5edeb44d4b47d225c0c26ac54088fe528","title":"An Improved Neural Baseline for Temporal Relation Extraction"},{"paperId":"0abcbdf40f872e6baf1c082811d4ae93df787698","title":"Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets"},{"paperId":"50dbda1cb9196f88914679fd34f4e44b2955434d","title":"Reasoning Over Paragraph Effects in Situations"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"bbad0b301561c9b44a43f2880b29f143dc7297ba","title":"Temporal Information Extraction by Predicting Relative Time-lines"},{"paperId":"34901b737b11aa51b688fa18c2eab47639d7b8c6","title":"Joint Reasoning for Temporal and Causal Relations"},{"paperId":"bbb3a49edf69a1909c0cf453858b451ef23fcbaf","title":"Context-Aware Neural Model for Temporal Information Extraction"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"82580fe4c429ac76f94c514c1ffc066844b13192","title":"Determining Event Durations: Models and Error Analysis"},{"paperId":"907659d4744e2796d81fe2ce65d7235d79826f66","title":"SemEval 2018 Task 6: Parsing Time Normalizations"},{"paperId":"bff8ae9e28323d217b9ad5a7321e58f79607f557","title":"A Multi-Axis Annotation Scheme for Event Temporal Relations"},{"paperId":"a9e0fa94e59261f5905951e93d515af64c3fc7cb","title":"A Structured Learning Approach to Temporal Relation Extraction"},{"paperId":"b05e8fac145817a276f922ac8d65644625448838","title":"SemEval-2017 Task 12: Clinical TempEval"},{"paperId":"2a8e11f6b8d90de4a06a7bf29065184e2627acff","title":"Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths"},{"paperId":"b67a64da079ce493c8d24e6212a6534490464bb9","title":"Neural Architecture for Temporal Relation Extraction: A Bi-LSTM Approach for Detecting Narrative Containers"},{"paperId":"fa025e5d117929361bcf798437957762eb5bb6d4","title":"Zero-Shot Relation Extraction via Reading Comprehension"},{"paperId":"7f1b0539bec01e52c4ffa5543c5ed55880dead55","title":"Neural Temporal Relation Extraction"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"04041a04ede58a464c843f95608e813a7686f6eb","title":"CaTeRS: Causal and Temporal Relation Scheme for Semantic Annotation of Event Structures"},{"paperId":"a2fc4a8882a03f7cb4ccc3711592005c1e4af2b9","title":"SemEval-2016 Task 12: Clinical TempEval"},{"paperId":"7daf69424feafdce1c896ff19f9a08a5b31ad5d8","title":"Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language"},{"paperId":"9c34bf7bdb5a4247772c5c6d4982aa4d41e67be9","title":"Event Nugget Annotation: Processes and Issues"},{"paperId":"ece7f0706a75a0558f57178e674fb1e29abda031","title":"SemEval-2015 Task 5: QA TempEval - Evaluating Temporal Information Understanding with Question Answering"},{"paperId":"06d22d068996a25df4e463410c2f36deded36512","title":"SemEval-2015 Task 4: TimeLine: Cross-Document Event Ordering"},{"paperId":"df4dcc3224e74f34e7c6bf7717ad76ec3b71a9b9","title":"Dense Event Ordering with a Multi-Pass Architecture"},{"paperId":"341da28f0a153c260239cffc2a35caeb3f74725f","title":"Income and Poverty in the United States: 2013"},{"paperId":"1f78e47c67c0ed3e5e3293337fa8c240dafa8b52","title":"An Annotation Framework for Dense Event Ordering"},{"paperId":"226f51c22f2852cbb456588d083acf1b52071951","title":"Temporal Annotation in the Clinical Domain"},{"paperId":"0d9d8be5ee0c1cda47beafea0ef0b14722cbd908","title":"SemEval-2013 Task 1: TempEval-3: Evaluating Time Expressions, Events, and Temporal Relations"},{"paperId":"39c230241d51b1435472115aaa8c62b94ab9927d","title":"Joint Inference for Event Timeline Construction"},{"paperId":"807aac46a06742026a365c978a8b5ff180f3d5cb","title":"SemEval-2010 Task 13: TempEval-2"},{"paperId":"c23dd3ef19c54b458cf104f7f7fa0920c25f89c2","title":"Timelines from Text: Identification of Syntactic Temporal Relations"},{"paperId":"ce428501603cc8f904038a08bc1897cdc589f098","title":"SemEval-2007 Task 15: TempEval Temporal Relation Identification"},{"paperId":"d36afe59ad1b706e020f55b54740bc9cddf25dcd","title":"Towards a General Theory of Action and Time"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3838387ea8dd1bb8c2306be5a63c1c120075c5a2","title":"Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning"},{"paperId":"7dce3c780c85c41ff9dfe7511962ecddb2291ab8","title":"Structured Learning for Temporal Relation Extraction from Clinical Records"},{"paperId":"1b4e435c0b62a09265e7c334508e8c16139302d0","title":"Representations of Time Expressions for Temporal Relation Extraction with Convolutional Neural Networks"},{"paperId":"1a210410493fbc052f0b7a54e7bc89cee20e8d28","title":"Crowdsourcing Question-Answer Meaning Representations"},{"paperId":"40322efad1be8b03541348fd56cfa1d37e279075","title":"Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation"},{"paperId":null,"title":"The TIMEBANK corpus"},{"paperId":null,"title":"Question answering is a format"},{"paperId":null,"title":"TimeLine: Cross-document event ordering"},{"paperId":null,"title":"The TIMEBANK corpus"},{"paperId":"3a9b136ca1ab91592df36f148ef16095f74d009e","title":"The ACE 2005 ( ACE 05 ) Evaluation Plan Evaluation of the Detection and Recognition of ACE Entities , Values , Temporal Expressions , Relations , and Events 1"}],"id":"36d6c8895bbc755964b8b2136c6fd6087a7af089","summary":"TORQUE is introduced, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships, and results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance."},{"url":"https://www.semanticscholar.org/paper/016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":32,"citationCount":106,"influentialCitationCount":11,"publicationDate":"02/05/2020","authors":"Bill Yuchen Lin,Seyeon Lee,Rahul Khanna,Xiang Ren","citations":[{"paperId":"9815f586f329a098df83ca872799880bd5cb1a15","title":"Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes"},{"paperId":"88b4ed17e8881b4e420627f3650ddab181bb632b","title":"The Web Can Be Your Oyster for Improving Large Language Models"},{"paperId":"0b315e6d4d800e04caf2f587312ce163e748d10c","title":"Numeric Magnitude Comparison Effects in Large Language Models"},{"paperId":"a57f578f6ad68cddd4fe80b1e313e890664e2b0d","title":"Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey"},{"paperId":"dc6d91a8ee0d4fc22764bf254f90176e8eeed86f","title":"Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements"},{"paperId":"3270c3054e6a14c5ee483f690ccda7367dd9a556","title":"CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population"},{"paperId":"be8f769f135b0ffcf829594b9421788781f38008","title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts"},{"paperId":"4d7571441f507f39133209e8afa7ad088da2199c","title":"ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models"},{"paperId":"24bcdce51edb8e1174fbabd072a0c07bf7362d57","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"8018a68956d6751d7ea76110537d5a2e86ec05c4","title":"The Life Cycle of Knowledge in Big Language Models: A Survey"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"ce2e318d33b0e44fe371de981f072527abf2f6fb","title":"Class Cardinality Comparison as a Fermi Problem"},{"paperId":"06396c7cd5d223a1776abf8811359ec7bc05d420","title":"Knowledge-Augmented Methods for Natural Language Processing"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"9af3b6b3f8dcedbb02b88936c428e1cd02503a8a","title":"Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?"},{"paperId":"17a8b5e6fef1f69979d57021a8f30a5159e152c7","title":"Commonsense Reasoning for Conversational AI: A Survey of the State of the Art"},{"paperId":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey"},{"paperId":"83c2bb56f58d4ce63adb2faf073fc35c3515cda8","title":"Understanding Finetuning for Factual Knowledge Extraction from Language Models"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"71471561137956d6aeec39173db460a28d86c11b","title":"Analogical Math Word Problems Solving with Enhanced Problem-Solution Association"},{"paperId":"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","title":"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data"},{"paperId":"03c4ecc2796ecde6a93562fdd149cea10157f805","title":"Visually Grounded Commonsense Knowledge Acquisition"},{"paperId":"1d4d3bdbc70ab400d206ae209546f267f85817c4","title":"SocioProbe: What, When, and Where Language Models Learn about Sociodemographics"},{"paperId":"6681f0a0cc6ddaa70cdea109b941c47538caaa27","title":"Judicial knowledge-enhanced magnitude-aware reasoning for numerical legal judgment prediction"},{"paperId":"0cf5c61f367e3937dc07be634a43952a50b589a4","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"5137977bcbc4030a8c888cc5998aa047764535b0","title":"Improving Imbalanced Text Classification with Dynamic Curriculum Learning"},{"paperId":"d130837a5601f4f82380736897d07f3008de1fa4","title":"Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","title":"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility"},{"paperId":"e02dce6ee032a13b1653f69b034a35676e7d4dc2","title":"Can language representation models think in bets?"},{"paperId":"3b622664d44a57280d3a189fa6475e56b96f1add","title":"CIKQA: Learning Commonsense Inference with a Unified Knowledge-in-the-loop QA Paradigm"},{"paperId":"b47a3f7bcf540adb6fd97869c51449888d3160bb","title":"Can Language Models Be Specific? How?"},{"paperId":"2591c66c6006c9c275a3dc7108a487934bc1c06f","title":"Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering"},{"paperId":"15bacb240e2598457af4ded3039b6988aa9706f0","title":"Few-shot Adaptation Works with UnpredicTable Data"},{"paperId":"e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP"},{"paperId":"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","title":"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"},{"paperId":"011095a0082e5e301f9bf30267b193c1c9e7e370","title":"Perturbation Augmentation for Fairer NLP"},{"paperId":"1da214f8f265445b5997f5d677452819b334bdfb","title":"Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts"},{"paperId":"e3dae33c5bdf397abdefd971ea34c48fb836dcc0","title":"GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models"},{"paperId":"d48b29889241551e1ee6622fa78c3fa4159255dd","title":"Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning"},{"paperId":"39f0f28848990f74eeb9019f579c6ebcc8ef3ea1","title":"TransTab: Learning Transferable Tabular Transformers Across Tables"},{"paperId":"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence"},{"paperId":"251c3afbaafcc9b5178534be9109f644bfc5e912","title":"Enhancing Knowledge Bases with Quantity Facts"},{"paperId":"591c54203cb06602d302230de1d6a60d753a4d8e","title":"What If: Generating Code to Answer Simulation Questions in Chemistry Texts"},{"paperId":"cf2453589ee98561c599694fce3220aa79e5c56b","title":"Probing Script Knowledge from Pre-Trained Models"},{"paperId":"682ee9c06f3eff3e3708b4d0419dc85ecf9c6c87","title":"What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured"},{"paperId":"706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding"},{"paperId":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"},{"paperId":"50f7d69ddd7f9b34f5121607fbdcc57236d65b8c","title":"Can Pre-trained Language Models Interpret Similes as Smart as Human?"},{"paperId":"8c62277dada489904a63de4dd87336c27c68fb5e","title":"Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models"},{"paperId":"7bebb48d34c219b119ca2d4ffc97d7fd4940c35c","title":"On the data requirements of probing"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"4497f3ed54ac520b50ffa05df04b37a59d4c1265","title":"Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET"},{"paperId":"7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"},{"paperId":"1ea78b1684371de0e859edf759e5e659152d31e3","title":"What do Large Language Models Learn about Scripts?"},{"paperId":"e02a757617c2c42eb62889cc4d4aee3765928303","title":"The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"47df3fd32d00220c85c2c51a571254fd99b2ecc7","title":"MetaICL: Learning to Learn In Context"},{"paperId":"12a763cb52f650710900790ca0bc43e5d5b88be6","title":"Generated Knowledge Prompting for Commonsense Reasoning"},{"paperId":"1360dc6d21ccbf2c23701c9ddd7c6ab8d04e4531","title":"The World of an Octopus: How Reporting Bias Influences a Language Model’s Perception of Color"},{"paperId":"d83c7aa12420d5d035f43d7737bfa6893e9e2c61","title":"Exploring Universal Intrinsic Task Subspace via Prompt Tuning"},{"paperId":"b15469d0ab3dc3a9dec037d761817b3fe546bed6","title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey"},{"paperId":"18ae4f4ccce2602fc09d5cf1ddcdfd9eaf416958","title":"Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP)"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0c4b2ae22f08425563a69527a3433516fc9737a1","title":"CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge"},{"paperId":"f640ef82eef63f16e9e732409e72aec638a53a3d","title":"Accurate, yet inconsistent? Consistency Analysis on Language Understanding Models"},{"paperId":"e248b22b7107ab057a0ea42a4dd075a5a4b2df26","title":"How to Query Language Models?"},{"paperId":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"},{"paperId":"40a1f266bb5ca853837355bfff272a55f0049c81","title":"Mining Numbers in Text: A Survey"},{"paperId":"e337ed6543c2e6e7e51c312c7d998798fc79fdde","title":"Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases"},{"paperId":"c6fd846b9b8f9eb0a492d6d6242fffce987c4580","title":"Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning"},{"paperId":"62953ca1252c9febe07c7007a10911726f37792d","title":"TIMEDIAL: Temporal Commonsense Reasoning in Dialog"},{"paperId":"5aab57cc0530560d82c74c055f664280619d7e81","title":"PROST: Physical Reasoning about Objects through Space and Time"},{"paperId":"6597d61bdb531051678c773526758a6dc113b9ce","title":"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences"},{"paperId":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"},{"paperId":"f80b837a211f71c6647c5755d4569559f0c2c0f7","title":"Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization"},{"paperId":"ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb","title":"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"},{"paperId":"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"},{"paperId":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","title":"Relational World Knowledge Representation in Contextual Language Models: A Review"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"9f620ad41b4e506e777c0665681b839c89cd682a","title":"Dimensions of Commonsense Knowledge"},{"paperId":"71fab1ce3c66998ba681ab378484be77690327a9","title":"RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"011869f932f89d047ce2bd36d73a95cc04888193","title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms"},{"paperId":"64ad3e1fbbc9a4bec2a414fb31b05ee9a62d50cb","title":"tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation"},{"paperId":"60c11f02982bcfe1f8be25c87c82606aeef9758b","title":"Probing Power by Prompting: Harnessing Pre-trained Language Models for Power Connotation Framing"},{"paperId":"a884bc8bc5b04e5ad096649856df5b7931fd3d23","title":"Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer"},{"paperId":"1491a27a67cfd209445580764179c982e7870b7c","title":"SELECTION-INFERENCE: EXPLOITING LARGE LAN-"},{"paperId":"ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","title":"Numerical Correlation in Text"},{"paperId":"da20a595aee606dec221bd1c5757274b17f39c66","title":"Probing Measuring Skills in Pre-trained Language Models"},{"paperId":"7db7fb25a8753c9efc6b3722d178f94fcc1f82d3","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP"},{"paperId":"2a83a92b08e0f3873d07162c73c67e533321112e","title":"Aligning Generative Language Models with Human Values"},{"paperId":"31662a7beb1acbdda51ecb6d8ec419fb9887a817","title":"Topicalization in Language Models: A Case Study on Japanese"},{"paperId":"a41b2d46b462b4af3dfada86c355a07814ae05db","title":"Is Self-Attention Powerful to Learn Code Syntax and Semantics?"},{"paperId":"1e336827a740f0e4a3def14d261e55d1bda26d83","title":"Probing Representations of Numbers in Vision and Language Models"},{"paperId":"2b8c6c2ae778184f41db3467ed2cde5342aae676","title":"RiddleSense: Answering Riddle Questions as Commonsense Reasoning"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"884c0b6db564208d99cadf2548f0aa96dee5f859","title":"Commonsense Reasoning with Implicit Knowledge in Natural Language"},{"paperId":"080df61ee1c15ff3c8e5d0d82d60bfd80e372e38","title":"Probing Toxic Content in Large Pre-Trained Language Models"},{"paperId":"02d4e18bc3677be373ff3f31c4a5702cb4e1f520","title":"Pre-trained Language Models in Biomedical Domain: A Survey fromMultiscale Perspective"},{"paperId":"94271aa76b145af14be87f2c8ee8b9075f17a349","title":"Do Language Embeddings capture Scales?"},{"paperId":null,"title":"Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"},{"paperId":null,"title":"Later datasets increase in complexity and scale , incorporating reading comprehension"}],"references":[{"paperId":"35ca61aee1d58ccb641eec3804ffd96fdb968cb7","title":"GenericsKB: A Knowledge Base of Generic Statements"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"f67fcbb1aec92ae293998ddfd904f61a31bef334","title":"Inducing Relational Knowledge from BERT"},{"paperId":"521b4e26df0f1cf5763dece14cbb218df152dc59","title":"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering"},{"paperId":"727c9d3846ebd80a9138d0e6c9e995d9afc1d312","title":"CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning"},{"paperId":"41094beddc498680d807e99e07efa41fec5d6724","title":"How Pre-trained Word Representations Capture Commonsense Physical Comparisons"},{"paperId":"136396e988dbbdaca433096e931b22732c3d95dd","title":"Text2Math: End-to-end Parsing Text into Math Expressions"},{"paperId":"bd2895717effe3cc52e77fbee3da8117cf1c01e1","title":"Measuring Numerical Common Sense: Is A Word Embedding Approach Effective?"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"710d183174844da5b7f392667f3cc25d2b098dde","title":"KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"f98e135986414cccf29aec593d547c0656e4d82c","title":"Commonsense Knowledge Mining from Pretrained Models"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"95a251513853c6032bdecebd4b74e15795662986","title":"What Does BERT Look at? An Analysis of BERT’s Attention"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796","title":"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"8cb592fa5e30e6fa5abe7041767768964f1f8cf4","title":"Do Language Models Have Common Sense"},{"paperId":"d7b6753a2d4a2b286c396854063bde3a91b75535","title":"A Simple Method for Commonsense Reasoning"},{"paperId":"c9f343b492c170c726f607c255ec6c7177dc5800","title":"Verb Physics: Relative Physical Knowledge of Actions and Objects"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"45a23651bcc5a6cc993d722e71b0d301a6dc9dee","title":"Open Mind Common Sense: Knowledge Acquisition from the General Public"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"Evaluating commonsense in pretrained language models"}],"id":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","summary":"Investigating whether and to what extent one can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process finds that this may not work for numerical Commonsense knowledge."},{"url":"https://www.semanticscholar.org/paper/72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks","venue":"ArXiv","year":2020,"referenceCount":45,"citationCount":10,"influentialCitationCount":1,"publicationDate":"18/05/2020","authors":"Swaroop Mishra,Arindam Mitra,Neeraj Varshney,Bhavdeep Singh Sachdeva,Chitta Baral","citations":[{"paperId":"fb30166c218bef3597b0d9789ad340defc3989ca","title":"In-BoXBART: Get Instructions into Biomedical Multi-Task Learning"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"},{"paperId":"fb1af83fcf237692768a30a4f9eb0ab61c500c14","title":"Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems"},{"paperId":"dd75f713420cd8eb3e0b3ce870b2680cd93b39fd","title":"Sequence to General Tree: Knowledge-Guided Geometry Word Problem Solving"},{"paperId":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"884c0b6db564208d99cadf2548f0aa96dee5f859","title":"Commonsense Reasoning with Implicit Knowledge in Natural Language"},{"paperId":null,"title":"Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"}],"references":[{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"8ea4a76fc781d9dcbf792e47435092d485d60a56","title":"ORB: An Open Reading Benchmark for Comprehensive Evaluation of Machine Reading Comprehension"},{"paperId":"04f4e55e14150b7c48b0287ba77c7443df76ed45","title":"PIQA: Reasoning about Physical Commonsense in Natural Language"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"6bc831932127ed850b539a1789a9b26f05fccf7d","title":"A Simple and Effective Model for Answering Multi-span Questions"},{"paperId":"efa23afbed4c95bb416b72e1bd477f6c27471baf","title":"Exploring ways to incorporate additional knowledge to improve Natural Language Commonsense Question Answering"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a4804394c362e7129312ae20fc094e60f56db3b0","title":"Careful Selection of Knowledge to Solve Open Book Question Answering"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"51004bc6461a572e1189a0e3b32b441155d760ce","title":"QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships"},{"paperId":"9784fbf77295860b2e412137b86356d70b25e3c0","title":"The Natural Language Decathlon: Multitask Learning as Question Answering"},{"paperId":"8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6","title":"Hypothesis Only Baselines in Natural Language Inference"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"8b7336f5dd13a45d4aab38428b4a88ce507ea310","title":"Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving"},{"paperId":"7b6f61e6067d3fe520e01b19e2941aa122fcd564","title":"Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"390808617b277987999b6e8e0aa5742aab54a6a0","title":"My Computer Is an Honor Student - but How Intelligent Is It? Standardized Tests as a Measure of AI"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"abb33d75dc297993fcc3fb75e0f4498f413eb4f6","title":"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"},{"paperId":"818b92bfad6e11d849bae552be60111579d91e91","title":"Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge!"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"128cb6b891aee1b5df099acb48e2efecfcff689f","title":"The Winograd Schema Challenge"},{"paperId":"75f9f2743951edd71369372d9cd6416cc822f063","title":"Number as a cognitive technology: Evidence from Pirahã language and cognition"},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"243e2140a14022531461c5f169ebbe14930efccc","title":"Two Cheers for Rebooting AI: Building Artificial Intelligence We Can Trust"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"4a98b6353d8492ed6ebe71c3a7df5f866f457173","title":"What’s Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"Honnibal and Montani, 2017] Matthew Honnibal and Ines Montani. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing"},{"paperId":null,"title":"spacy 2: Natural language understanding with bloom embeddings"},{"paperId":"cc9afe82dd74486598023dcad37d4c8b810f15ac","title":"Tag-based Multi-Span Extraction in Reading Comprehension"}],"id":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","summary":"This work introduces NUMBERGAME, a multifaceted benchmark to evaluate model performance across numerical reasoning tasks of eight diverse formats, and takes forward the recent progress in generic system development, demonstrating the scope of under-explored tasks."},{"url":"https://www.semanticscholar.org/paper/c3d487ca75f26adadd94ba7af4b57aed8d661ff3","title":"Automatic Knowledge Acquisition for Object-Oriented Expert Systems","venue":"ArXiv","year":2020,"referenceCount":9,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/05/2020","authors":"J. Colloc,D. Boulanger","citations":[{"paperId":"edd712436dbecd05161fdbe1caf96fd9f1739564","title":"Fvsoomm a Fuzzy Vectorial Space Model and Method of Personality, Cognitive Dissonance and Emotion in Decision Making"}],"references":[{"paperId":"92fc0736065d35fd089b86d60938930e5d18b7ab","title":"Dynamic Derivation of Personalized Views"},{"paperId":"8d7197d958b8d89a566e2c7469c0aea6461ec841","title":"Representational axes and temporal cooperative processes"},{"paperId":"61ed2ab133e48f34c59c14b4cc362cfa0f363392","title":"A Step Towards Unification of Syntactic and Statistical Pattern Recognition"},{"paperId":"5c0f93b5e4f64d833fa5aab15f8642b78f2dfb9f","title":"DETECTING HETEROGENEITY IN A MULTIDATABASE ENVIRONMENT THROUGH AN O-O MODEL"},{"paperId":null,"title":"Creating Expert-Systems for Business Industry"},{"paperId":null,"title":"Mécanismes de classification pour la recherche d'informations dans une base d'objets"},{"paperId":null,"title":"Object Oriented Model and Method Applied to Build an Expert System for Intoxication Diagnoses"},{"paperId":null,"title":"Automatic knowledge acquisition for object oriented expert systems AVIGNON'93"},{"paperId":null,"title":"Automatic knowledge acquisition for object oriented expert systems AVIGNON'93"}],"id":"c3d487ca75f26adadd94ba7af4b57aed8d661ff3","summary":"An Object Oriented Model for building Expert Systems is described and original algorithms which deal with total and partial structural similitude of objects to facilitate knowledge acquisition are proposed."},{"url":"https://www.semanticscholar.org/paper/0bfd4ed399054eae26c3cdaabc0aed80ca95e125","title":"Neural Power Units","venue":"Neural Information Processing Systems","year":2020,"referenceCount":33,"citationCount":7,"influentialCitationCount":3,"publicationDate":"02/06/2020","authors":"Niklas Heim,T. Pevný,V. Šmídl","citations":[{"paperId":"26e07b6ac9b0f04396e2e7f7ca518b6fbae67ce7","title":"Improving the Robustness of Neural Multiplication Units with Reversible Stochasticity"},{"paperId":"4578747d52aa1b3537612287352a803a7b17e999","title":"Asynchronous Neural Networks for Learning in Graphs"},{"paperId":"0ce6a798f8222ed8f221326ca566311c648cb4dc","title":"Learning Division with Neural Arithmetic Logic Modules"},{"paperId":"def7c67c3688bc459fbef3ce50a466a0cbf411a9","title":"A Primer for Neural Arithmetic Logic Modules"},{"paperId":"30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0","title":"MC-LSTM: Mass-Conserving LSTM"},{"paperId":"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","title":"Neural Status Registers"},{"paperId":"c36277b67814e0a522e786a38e1768612b0e63f2","title":"Exploring the Learning Mechanisms of Neural Division Modules"}],"references":[{"paperId":"4e181e7f1067ee2b24db5f21e6c4cc0b4875edd2","title":"Fractional SIR epidemiological models"},{"paperId":"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","title":"Neural Status Registers"},{"paperId":"e081dfa64e5ed12fe15fc885ec82ef1f35ab9830","title":"iNALU: Improved Neural Arithmetic Logic Unit"},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"696b388ee6221c6dbcfd647a06883b2bfee773d9","title":"Universal Differential Equations for Scientific Machine Learning"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"112ac68ddb0f021517dd465e89918fa52755cc35","title":"Measuring Arithmetic Extrapolation Performance"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"3c9961153493370500020c81527b3548c96f81e0","title":"Data-driven discovery of coordinates and governing equations"},{"paperId":"bc00ff34ec7772080c7039b17f7069a2f7df0889","title":"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead"},{"paperId":"e53fd0c9a04df127af80b2699f5f8f23a8a3e2af","title":"On Evaluating the Generalization of LSTM Models in Formal Languages"},{"paperId":"86ccab92ea6bd1f1cfae8d6d1cca3ea71e89bdc7","title":"Fashionable Modelling with Flux"},{"paperId":"6f69e19348870552a7ab92d038c8b8d753fe6b60","title":"Neural Arithmetic Expression Calculator"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"449310e3538b08b43227d660227dfd2875c3c3c1","title":"Neural Ordinary Differential Equations"},{"paperId":"0c10a2cafb715f5c7b1810f89dd2eb3fadc5a38e","title":"Finding numbers in the brain"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"34473be212082c64ee6c944758988fd64c9c9ef4","title":"DifferentialEquations.jl – A Performant and Feature-Rich Ecosystem for\n Solving Differential Equations in Julia"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"a2499dd426c46c645ee805d7594b6687547c72d4","title":"Neural Random Access Machines"},{"paperId":"5b791cd374c7109693aaddee2c12d659ae4e3ec0","title":"Grid Long Short-Term Memory"},{"paperId":"c3823aacea60bc1f2cabb9283144690a3d015db5","title":"Neural Turing Machines"},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"801257f13663c493851d55366b431fad0cc801a9","title":"A contribution to the mathematical theory of epidemics"},{"paperId":null,"title":"Measuring Arithmetic Extrapolation Performance. arXiv:1910.01888 [cs], November 2019"},{"paperId":null,"title":"The Mythos of Model Interpretability. arXiv:1606.03490 [cs, stat], March 2017"},{"paperId":null,"title":"Fractional SIR Epidemiological Models. preprint, Epidemiology, April 2020"},{"paperId":null,"title":"Neural Random-Access Machines. arXiv:1511.06392 [cs], February 2016"},{"paperId":null,"title":"Deep Learning for Symbolic Mathematics. arXiv:1912.01412 [cs], December 2019"},{"paperId":null,"title":"Measuring arithmetic extrapolation performance. CoRR"},{"paperId":null,"title":"Neural GPUs Learn Algorithms. arXiv:1511.08228 [cs], March 2016"},{"paperId":null,"title":"Neural Status Registers. arXiv:2004.07085 [cs, stat], April 2020"},{"paperId":null,"title":"Neural Ordinary Differential Equations. arXiv:1806.07366 [cs, stat], December 2019"}],"id":"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","summary":"The Neural Power Unit (NPU) is introduced that operates on the full domain of real numbers and is capable of learning arbitrary power functions in a single layer and fixes the shortcomings of existing arithmetic units and extends their expressivity."},{"url":"https://www.semanticscholar.org/paper/63857190aaf5aab1d94b54bb257b7b03b8cb5a50","title":"GMAT: Global Memory Augmentation for Transformers","venue":"ArXiv","year":2020,"referenceCount":30,"citationCount":32,"influentialCitationCount":3,"publicationDate":"05/06/2020","authors":"Ankit Gupta,Jonathan Berant","citations":[{"paperId":"531614f12de74645b6ce116f7af2b150af0acbbb","title":"Focus Your Attention (with Adaptive IIR Filters)"},{"paperId":"23005cd763b0ad723390f07c4a82693d58657919","title":"FIT: Far-reaching Interleaved Transformers"},{"paperId":"594d8e1696619f3cebb7c6bffdad8e0a5592f006","title":"Scaling Transformer to 1M tokens and beyond with RMT"},{"paperId":"5735e49e501c8e51e9be4079592e46e047747b03","title":"Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"},{"paperId":"980d1c3bf9d1a3c0ce68567e0efc1a72f203f12c","title":"Global memory transformer for processing long documents"},{"paperId":"ce750b62c1b090770aedd949fb289b96b03eedad","title":"SeDR: Segment Representation Learning for Long Documents Dense Retrieval"},{"paperId":"429803c4d7a88fccff5b24f6a9aaea8aabf7da81","title":"Simplified State Space Layers for Sequence Modeling"},{"paperId":"732e3faec4e5be4d144256f2c379b9dc49f0b227","title":"Efficient Long-Text Understanding with Short-Text Models"},{"paperId":"cfc5d3032c0c370892eb42687162afbb75455778","title":"Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes"},{"paperId":"a8cf0f7a20f886acfb332071c2daaf58ba86a5ca","title":"Recurrent Memory Transformer"},{"paperId":"5f3c2a31fc84d13a72008f70106163bd92f2f9d0","title":"kMaX-DeepLab: k-means Mask Transformer"},{"paperId":"eaef083b9d661f42cc0d89d9d8156218f33a91d9","title":"Long Range Language Modeling via Gated State Spaces"},{"paperId":"31a9744bd5421b3fbbad2ab38ce33bb2f352c77a","title":"CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation"},{"paperId":"b9a701c90f3d3df27366f5b29a97f798eb940ac7","title":"ChapterBreak: A Challenge Dataset for Long-Range Language Models"},{"paperId":"71e15a9a52dcafca57bff5f310b95e2c7d0cfc87","title":"Diagonal State Spaces are as Effective as Structured State Spaces"},{"paperId":"2f5a60cce19c6e5ca5a527f713d3b1545d64f0ef","title":"Socialformer: Social Network Inspired Long Document Modeling for Document Ranking"},{"paperId":"dbd684b259d6d7383ad4bb354ed53a0d12d38b63","title":"Leveraging Multi-view Inter-passage Interactions for Neural Document Ranking"},{"paperId":"73d64ecbe3e846394444dab6c5e89ba33e5daa49","title":"Memory transformer with hierarchical attention for long document processing"},{"paperId":"2ca44146b2ee3859d0a9f9e04e4bec0983e3f57b","title":"Interpretable Self-supervised Multi-task Learning for COVID-19 Information Retrieval and Extraction"},{"paperId":"c1a4278f969acfc6682a924e31b95e1ade9703ee","title":"Memory-efficient Transformers via Top-k Attention"},{"paperId":"af679d69fcc1d0fcf0f039aba937853bcb50a8de","title":"Luna: Linear Unified Nested Attention"},{"paperId":"69285c9040c1356272752499b7e1e53ef25ac008","title":"Beyond Paragraphs: NLP for Long Sequences"},{"paperId":"a4bd6a22dcdc740a9ff20af48fe2d828f0190b17","title":"Value-aware Approximate Attention"},{"paperId":"787119e3c3f819244c82b7d97779473773e60696","title":"MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"35f773098092189a42f26c9282c52afde277ec87","title":"GATED STATE SPACES"},{"paperId":"5deef7fc161cbdc884aff15b9810f8a432c1489a","title":"k-means Mask Transformer"},{"paperId":"48ad536d00742a31eb8c6408c5d7ad96e654fe7a","title":"Receptive Field Alignment Enables Transformer Length Extrapolation"},{"paperId":"a8a048b862cf84a78e0a3f2e8ca8979b7c70ddb7","title":"An explainability analysis of a sentiment prediction task using a transformer-based attention filter"},{"paperId":"9b1434c63cec05b12500bc70e09fa6861e03a80d","title":"Topic Sentence Named Entity Recognition: A New Task with Its Dataset and Benchmarks"},{"paperId":"d828ec273669d7c3769d1f66ec37edfbc5de15f3","title":"TRANSFORMER-QL: A STEP TOWARDS MAKING TRANSFORMER NETWORK QUADRATICALLY LARGE"},{"paperId":null,"title":"L ONG R ANGE L ANGUAGE M ODELING VIA G ATED S TATE S PACES"}],"references":[{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"657329c633709dd1ac34a30d57341b186b1a47c2","title":"Efficient Content-Based Sparse Attention with Routing Transformers"},{"paperId":"80376bdec5f534be78ba82821f540590ebce5559","title":"How Much Knowledge Can You Pack into the Parameters of a Language Model?"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"f51497f463566581874c941353dd9d80069c5b77","title":"Compressive Transformers for Long-Range Sequence Modelling"},{"paperId":"2e14e84ccec924ed770b58108ad1d9de6f0ca295","title":"BP-Transformer: Modelling Long-Range Context via Binary Partitioning"},{"paperId":"2cf3bd0cc1382f35384e259d99e4f9744eeaed28","title":"Blockwise Self-Attention for Long Document Understanding"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"d6dccb5d71fbb6f5765f89633ba3a8e6809a720d","title":"Stand-Alone Self-Attention in Vision Models"},{"paperId":"95f2aedef1453e5d88af9d5fddb79e0e56223cb0","title":"Compositional Questions Do Not Necessitate Multi-hop Reasoning"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"fb507ada871d1e8c29e376dbf7b7879689aa89f9","title":"Music Transformer: Generating Music with Long-Term Structure"},{"paperId":"807370352540f171685998aec7a5701f7110f147","title":"Understanding Back-Translation at Scale"},{"paperId":"1db9bd18681b96473f3c82b21edc9240b44dc329","title":"Image Transformer"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a","title":"The NarrativeQA Reading Comprehension Challenge"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}],"id":"63857190aaf5aab1d94b54bb257b7b03b8cb5a50","summary":"This work proposes to augment sparse Transformer blocks with a dense attention-based $\\textit{global memory}$ of length $M$ ($\\ll L$) which provides an aggregate global view of the entire input sequence to each position, and empirically shows that this method leads to substantial improvement on a range of tasks."},{"url":"https://www.semanticscholar.org/paper/8256f48f759cf85044db251cc512f965834945b3","title":"Rethinking Positional Encoding in Language Pre-training","venue":"International Conference on Learning Representations","year":2020,"referenceCount":34,"citationCount":170,"influentialCitationCount":22,"publicationDate":"28/06/2020","authors":"Guolin Ke,Di He,Tie-Yan Liu","citations":[{"paperId":"88695b5bb6462872ce1dd946cff00dd6ebabf2d9","title":"Scaling TransNormer to 175 Billion Parameters"},{"paperId":"8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8","title":"Linearized Relative Positional Encoding"},{"paperId":"db7145cb5f000fa39cec8d839c9ac15d897aa46b","title":"SPDER: Semiperiodic Damping-Enabled Object Representation"},{"paperId":"2d7b6175cd81ac745923b01aeae0f67d525b5eee","title":"Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset"},{"paperId":"01cc6983e74bcb047c9581305e0c231fde6b65a0","title":"Relational Temporal Graph Reasoning for Dual-task Dialogue Language Understanding"},{"paperId":"345ba0a4fad9c762d87ef0268c3c047f774e9b5c","title":"SSIN: Self-Supervised Learning for Rainfall Spatial Interpolation"},{"paperId":"08e5d8d1a9d481204b4caff58490755f59b0e88f","title":"Monotonic Location Attention for Length Generalization"},{"paperId":"9da4d1a93a0fd58e323592ab1061dabcdf4373f1","title":"What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"ab69a66fef4c4f489dbb6099c2d6e34fdb3c2ed5","title":"Spa-L Transformer: Sparse-self attention model of Long short-term memory positional encoding based on long text classification"},{"paperId":"9fe29c834afbe1848d9df713ae6e0ca3bd053605","title":"Probing the Role of Positional Information in Vision-Language Models"},{"paperId":"240bc60c98c9b860c27c6f962992618a6775cab1","title":"Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts"},{"paperId":"08ec444e77c43b318aa3999b32553f68f8a402fc","title":"Pre-training Language Model as a Multi-perspective Course Learner"},{"paperId":"c0f5a27f26c93a5109d8a7103ac3536344cd164a","title":"Rethinking the Encoding of Satellite Image Time Series"},{"paperId":"f20982cea07674eb779de507f09c26f65d445719","title":"HST-MRF: Heterogeneous Swin Transformer with Multi-Receptive Field for Medical Image Segmentation"},{"paperId":"6845815e09e1ea2807cabb4e907915ba96afa4b4","title":"EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation"},{"paperId":"0c0aabb2660797c6093d335935748f8db91c8b6b","title":"AutoMLP: Automated MLP for Sequential Recommendations"},{"paperId":"62fc770746f6a283fbc5cfc32275e23bfef82eb7","title":"Target-Aware Tracking with Long-term Context Attention"},{"paperId":"77786eda28e5f0b25d682c27846334d53daf43e4","title":"Complex-Valued Relative Positional Encodings for Transformer"},{"paperId":"55cd2d0a8f26c4dc458303f937af2b6fb8f8b693","title":"PolyFormer: Referring Image Segmentation as Sequential Polygon Generation"},{"paperId":"9e449d611bec60ff2e3faaec38dce3b79abac44e","title":"Enhancing Multivariate Time Series Classifiers through Self-Attention and Relative Positioning Infusion"},{"paperId":"7e96a8bc938b47cf805383ef8c079cd852bd64ba","title":"Representation Deficiency in Masked Language Modeling"},{"paperId":"d30c21212e0e1165b2e3a05f9bc31ce8b49e5f0a","title":"Knowledge Distillation in Vision Transformers: A Critical Review"},{"paperId":"1420b9ff9c7ecfc8c8b5fdce4517e6fc51cebf92","title":"Ankh ☥: Optimized Protein Language Model Unlocks General-Purpose Modelling"},{"paperId":"7389b6ebbf36f4d869a02e305e2ef52ad2c92264","title":"Applications of transformer-based language models in bioinformatics: a survey"},{"paperId":"86f3b14e3229be8e06060f08b6e65fd031efbc50","title":"Untied Positional Encodings for Efficient Transformer-Based Speech Recognition"},{"paperId":"4b308ba40e67b0b4b25c6fde17195d5a456a2f41","title":"Cramming: Training a Language Model on a Single GPU in One Day"},{"paperId":"f6d381941e2964872a283d5290a390126115ad03","title":"P-Transformer: Towards Better Document-to-Document Neural Machine Translation"},{"paperId":"bb1486013aedbbb4f9a960c83731fbb91a04b1f3","title":"Mitigation of Spatial Nonstationarity with Vision Transformers"},{"paperId":"0f107a8247983e494789ffd81663708dfbe483e6","title":"You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model"},{"paperId":"8e2930ac8ae9a758b367513cccb7d562f354afea","title":"Who Says Elephants Can’t Run: Bringing Large Scale MoE Models into Cloud Scale Production"},{"paperId":"e4679aaf696fcdd7fc0aab8b4b3af095d512b68f","title":"Pure Transformer with Integrated Experts for Scene Text Recognition"},{"paperId":"3ba812282de79a909c3c6074940e91f322b51db2","title":"AutoAttention: Automatic Field Pair Selection for Attention in User Behavior Modeling"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"e82e3f4347674b75c432cb80604d38ee630d4bf6","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"240300b1da360f22bf0b82c6817eacebba6deed4","title":"What Makes Convolutional Models Great on Long Sequence Modeling?"},{"paperId":"a0b4e5f73adb829f2b7c51eb373f0559c110e2ac","title":"Better Pre-Training by Reducing Representation Confusion"},{"paperId":"03b7d2e942eafabd14b229f8962fa6e943053f75","title":"Melody Infilling with User-Provided Structural Context"},{"paperId":"d4d07180764fc30cf31261ddd072175a4daee10b","title":"A Dual-Attention Learning Network with Word and Sentence Embedding for Medical Visual Question Answering"},{"paperId":"3eedf6e694a8d58c5faff3ac47c6a20daf05e48d","title":"Exploring Heterogeneous Feature Representation for Document Layout Understanding"},{"paperId":"e3486e1c1a3cac6ef2653ea409c86b1ee6a42ebd","title":"Husformer: A Multi-Modal Transformer for Multi-Modal Human State Recognition"},{"paperId":"70e91e16eb321067d9402710e14a40cf28311f73","title":"Mega: Moving Average Equipped Gated Attention"},{"paperId":"d6bf7e75662363c306c4c791d9ab1b4dd6a0b6f5","title":"Hyperspectral Image Classification via Spectral Pooling and Hybrid Transformer"},{"paperId":"1dd5beb70fe2a4ace695d3fc7f1fb0c53f757d87","title":"Pre-Training a Graph Recurrent Network for Language Representation"},{"paperId":"52e4f9232bdd5d298e49c75eb84dcd5c1445843e","title":"Global Positional Self-Attention for Skeleton-Based Action Recognition"},{"paperId":"369a564359d467eb3878d01f9f36e79703aa0374","title":"Learning Program Representations with a Tree-Structured Transformer"},{"paperId":"acd082f28074e047792691c5236819bb50132a7a","title":"Improving Micro-video Recommendation by Controlling Position Bias"},{"paperId":"076deb54a5d776cd21eabf2c40cdd839f53d6d77","title":"giMLPs: Gate with Inhibition Mechanism in MLPs"},{"paperId":"e9d1a851d4f7950db360d12fe7f96647aaf547da","title":"Generalized Attention Mechanism and Relative Position for Transformer"},{"paperId":"4299353235595391e2b4f7298baffd00b5acf9d1","title":"LordBERT: Embedding Long Text by Segment Ordering with BERT"},{"paperId":"4d80dae4039b4de6c1ba30227d5981fd1cbb4a1f","title":"Parameterization of Cross-token Relations with Relative Positional Encoding for Vision MLP"},{"paperId":"9692886ba8e2c9d8990b0505e9c67a696d9f28a7","title":"A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities"},{"paperId":"bc67a1d69d53bb6b5a4efa86721122f09532c97e","title":"Progressive Self-Attention Network with Unsymmetrical Positional Encoding for Sequential Recommendation"},{"paperId":"e28adeb4db46469df9f9bd653501871ddc5f4318","title":"MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization"},{"paperId":"8d47a9950a44b0d2f1172620966c4737908ad542","title":"R^2VOS: Robust Referring Video Object Segmentation via Relational Multimodal Cycle Consistency"},{"paperId":"b0c4438723c9193a4ab850f6cbb72bc5ffca3983","title":"Improving session-based recommendation with contrastive learning"},{"paperId":"b09420b30fc093d63fb2ee1aac26c71da81da437","title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"},{"paperId":"31a84391e1b47fa15f3a521c43d62385a7757637","title":"MetaTPTrans: A Meta Learning Approach for Multilingual Code Representation Learning"},{"paperId":"8e6ada0242dc9b3680ccf6ddeec9e7c3cd41766f","title":"Dynamically Relative Position Encoding-Based Transformer for Automatic Code Edit"},{"paperId":"746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a","title":"Your Transformer May Not be as Powerful as You Expect"},{"paperId":"e1461a3aa9f4bef86435fa2cc87b16d9f34d5ab0","title":"Do we really need temporal convolutions in action segmentation?"},{"paperId":"7a93df4da51f45e2af79856d417c77ca697016ab","title":"Neural Additive Models for Nowcasting"},{"paperId":"d6c5aab433d9871cabc01ffb1e5e1ea89141155b","title":"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"},{"paperId":"6e98a35c439c5c7387de6bffd96dea9b2943a548","title":"Trading Positional Complexity vs. Deepness in Coordinate Networks"},{"paperId":"9d03a164759bb5cc2fa6b575254b58f790ab6785","title":"Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction"},{"paperId":"7f9290803a6df981cc460894487868226e2131f3","title":"RM-Transformer: A Transformer-based Model for Mandarin Speech Recognition"},{"paperId":"b0773e4edee5a5f2bfc11ce793a7011a922ce743","title":"Spatial-Temporal Interval Aware Sequential POI Recommendation"},{"paperId":"501eb473c1ca97e66f7a0ff6379a00756d869d6c","title":"Decoupled Side Information Fusion for Sequential Recommendation"},{"paperId":"2193d898e556ad2ac14f0ababf02f90e6fdfe663","title":"DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks"},{"paperId":"633851a323990270461b49e8068a2c9ca1cea530","title":"Dynamic Position Encoding for Transformers"},{"paperId":"682ca4de7fc91ed44a86ce7762d74f58791f63e7","title":"3D Shuffle-Mixer: An Efficient Context-Aware Vision Learner of Transformer-MLP Paradigm for Dense Prediction in Medical Volume"},{"paperId":"b6ec1e8f18185b4b3d46201359a440404575460c","title":"METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals"},{"paperId":"02f36289e227595dd2786b32c3975ce7e61dff48","title":"Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators"},{"paperId":"838a2297b94f7bad96c4f8370a5f58487f194f44","title":"Visual Abductive Reasoning"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"4e9cd6be4a8fcad2ca562fcf41a1f882387a3167","title":"LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network"},{"paperId":"60c07da8f233dd3af214d4d5fbd2582a165797d8","title":"Probabilistic deep learning model as a tool for supporting the fast simulation of a thermal-hydraulic code"},{"paperId":"50af83ea20201b51014358534650213e6133650c","title":"FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks"},{"paperId":"5f104a804ed245c79847ad0593e8f86196d697b1","title":"Transformers in Time Series: A Survey"},{"paperId":"12809bcb734beafeb47876f42e7b438e27fe99fe","title":"General-purpose, long-context autoregressive modeling with Perceiver AR"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"c72a8f823f9e2d4c9820df5f78e3f6c4922910b6","title":"Improving Sample Efficiency of Value Based Models Using Attention and Vision Transformers"},{"paperId":"3b342959401b9401144d2239a2eb171edbe0bf06","title":"Rewiring with Positional Encodings for Graph Neural Networks"},{"paperId":"5fb29bd2eb585be9c7e2f8a483cc5fe8255d77c2","title":"SoK: Vehicle Orientation Representations for Deep Rotation Estimation"},{"paperId":"91a4cbae6553e975ddc3b2f6850ed725ff475307","title":"SwinTrack: A Simple and Strong Baseline for Transformer Tracking"},{"paperId":"be0fbb810583930c071d0b9b2c5187fe260783f5","title":"Swin Transformer V2: Scaling Up Capacity and Resolution"},{"paperId":"28a65397488d826e0063df6352f00e6a660d0a2f","title":"Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer"},{"paperId":"63d70dba02c34e465f36fd8b123390efe7aa67e0","title":"Can Vision Transformers Perform Convolution?"},{"paperId":"2945348e0843b8a414cd54fe2588a45430249355","title":"Position-Augmented Transformers with Entity-Aligned Mesh for TextVQA"},{"paperId":"c501d6f1eecf3b0fdae7d428e1829bb6607a6a37","title":"Relative Molecule Self-Attention Transformer"},{"paperId":"87e879c2465b2414a58dd3a1184f8b346d48f3e7","title":"Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer"},{"paperId":"232acef2483a26fe95c70b619f88fa0b82c1a105","title":"Multiplicative Position-aware Transformer Models for Language Understanding"},{"paperId":"33c831326bb47b2ba2031fd7213b6918d23eb01e","title":"The Impact of Positional Encodings on Multilingual Compression"},{"paperId":"aec7e7143bfe082752f428fe01e4090dd7fc411c","title":"Ultra-high Resolution Image Segmentation via Locality-aware Context Fusion and Alternating Local Enhancement"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"51b5db5c679be0ce9a39a2ee21def42bca165efe","title":"Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning"},{"paperId":"3c8146b0c77af1e3177c6497fcac87046578f81b","title":"Conditional DETR for Fast Training Convergence"},{"paperId":"79678d2f10bddf14b2aedf3427f8a4c39908931f","title":"Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding"},{"paperId":"87e823d2cb58e741230c0fa3b83f3459c7e32241","title":"PiSLTRc: Position-Informed Sign Language Transformer With Content-Aware Convolution"},{"paperId":"d3f51870f4da5dd9c2a08a55cfa8a380b8d49208","title":"Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation"},{"paperId":"e38dc0554dd89745bb17039a4d4ee9d714cf77f1","title":"SpectralFormer: Rethinking Hyperspectral Image Classification With Transformers"},{"paperId":"fbac64d617914ab8dac0682639fbd6012faed771","title":"Rethinking Positional Encoding"},{"paperId":"0d508600d77d8a7e6a655cdb6d139779732f649f","title":"Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"},{"paperId":"c6dad3f9c9f29602b8c89585c23c73377ef00601","title":"Bootstrap Representation Learning for Segmentation on Medical Volumes and Sequences"},{"paperId":"df59d0098c1b2c1ee8995da802dd6b12d158c2b8","title":"Large-scale chemical language representations capture molecular structure and properties"},{"paperId":"1dbb523a6555d6e0c5727620e2b57daaa5b79dc0","title":"Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models"},{"paperId":"47ae807cd511b35e78a2cd4e198283dea6dafd41","title":"Do Transformers Really Perform Bad for Graph Representation?"},{"paperId":"d8d2e574965fe733eb1416e03df2b5c2914fc530","title":"A Survey of Transformers"},{"paperId":"7509c66a666e2e3f14bc8676b969b945ee6e136f","title":"CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"},{"paperId":"d8e7bad2681ce70277c900c77a22181d4b03d705","title":"The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"},{"paperId":"07e987364bf0be1949e379f976f8dea675977337","title":"MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens"},{"paperId":"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages"},{"paperId":"08ffdec40291a2ccb5f8a6cc048b01247fb34b96","title":"Relative Positional Encoding for Transformers with Linear Complexity"},{"paperId":"52d01d9f71caf0d021fccb75b75b7d3dfc7460f5","title":"On Scalar Embedding of Relative Positions in Attention Models"},{"paperId":"b8cee43a51c44f8f4448e78e41ecf081987707cf","title":"Towards Robust Vision Transformer"},{"paperId":"fda805c6e85a03d10549acdc5489420ca8f3d405","title":"MuseMorphose: Full-Song and Fine-Grained Piano Music Style Transfer With One Transformer VAE"},{"paperId":"1c0752fb3e9ab5c9392f196225075422f26b5110","title":"How could Neural Networks understand Programs?"},{"paperId":"f9a61d89209da81df0b232f788c64cbf18adaba5","title":"Predicting functional effect of missense variants using graph attention neural networks"},{"paperId":"d76bf20d059359f9e05b342e56cfc7785af71bf4","title":"LANA: Towards Personalized Deep Knowledge Tracing Through Distinguishable Interactive Sequences"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"36938a8ccb323cdd45c304d4461a29aee3ebc28a","title":"SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization"},{"paperId":"db46b0de44c5113c47f0ec5392eb91d0726497bf","title":"A Simple and Effective Positional Encoding for Transformers"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview"},{"paperId":"1e9ff5f2e9aa3e6c01bc89c81ba16442f1b5938d","title":"Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder"},{"paperId":"19537be34dbadbcaa4fffcf028a8ada5095b1b5c","title":"COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining"},{"paperId":"594db112d65891fbaf45b27f17d2f9de88ddcd82","title":"Revisiting Language Encoding in Learning Multilingual Representations"},{"paperId":"eb0931c39904a40c6cb4aa35c9b21d5e3b7dc856","title":"Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs"},{"paperId":"0822f8d7e6a72a65e65f147d3a8d8fccd485da40","title":"Shortformer: Better Language Modeling using Shorter Inputs"},{"paperId":"5d76c2591334f56dc9155568f793b013df0f6613","title":"Focusing More on Conflicts with Mis-Predictions Helps Language Pre-Training"},{"paperId":"eb2fc03b8865b8e1b4cb933d917ea269ebe14584","title":"Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training"},{"paperId":"ac4ae352e2434d4a71c6a79bf5f93df5f600b058","title":"Increasing Learning Efficiency of Self-Attention Networks through Direct Position Interactions, Learnable Temperature, and Convoluted Attention"},{"paperId":"829580d6fc73fa601c4982e2b1b6832f2796270b","title":"Positional Artefacts Propagate Through Masked Language Model Embeddings"},{"paperId":"0971ad49ed7f063375d7a21bdfc2fab347d68ba8","title":"Transformer-based Arabic Dialect Identification"},{"paperId":"6482dbb26e6441c0cfdaa43c52b868751d16d10b","title":"Contextual BERT: Conditioning the Language Model Using a Global State"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"fc6b203739c6a3de38e3af9eed652e2797248b07","title":"U NI -M OL : A U NIVERSAL 3D M OLECULAR R EPRESENTATION L EARNING F RAMEWORK"},{"paperId":"2b0d96dccd07ebe8feb90951fe90d3aa81741097","title":"Understand and Modularize Generator Optimization in ELECTRA-style Pretraining"},{"paperId":"9c9d49995c929db2bc928151123d82b2e575c30b","title":"C ROSSFORMER : T RANSFORMER U TILIZING C ROSS - D IMENSION D EPENDENCY FOR M ULTIVARIATE T IME S ERIES F ORECASTING"},{"paperId":"5cdb940cb0e8158bc5bc5aabcee84ffeee0c30fe","title":"Improve Transformer Pre-Training with Decoupled Directional Relative Position Encoding and Representation Differentiations"},{"paperId":"20333c34f892c8e0c2f4e6c37295a8b43ef35c02","title":"Rethinking Positional Encoding in Tree Transformer for Code Representation"},{"paperId":"2a98fd84fd7a1eadbeb4513ad322d13286ecb9ac","title":"Graph Hawkes Transformer for Extrapolated Reasoning on Temporal Knowledge Graphs"},{"paperId":"6410348a7b9da61ea5ca2979aaebb164d4b87b01","title":"Going “Deeper”: Structured Sememe Prediction via Transformer with Tree Attention"},{"paperId":"3884307cb95329275755baaf99600e7431be695d","title":"Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation"},{"paperId":"7865ae1c4d7ba0e3ba48aef54270c3fdf61ec11d","title":"Symbolic Data Augmentation for Assisted Neural Reasoning"},{"paperId":"08013183713f427258d36332b1bee9d6ce37cf30","title":"S TRUCTURE -A WARE T RANSFORMER P OLICY FOR I NHOMOGENEOUS M ULTI -T ASK R EINFORCEMENT L EARNING"},{"paperId":"5cea5af1822e7407c8393bd7c5d0208af26ea57e","title":"Priming Ancient Korean Neural Machine Translation"},{"paperId":"26ce5dd8221efc307fceef111e003bdb0ffe8ed5","title":"Positional Attention Guided Transformer-like Architecture for Visual Question Answering"},{"paperId":"7adba2d365188a4514d695b9bc5e9901016c3f05","title":"CSiT: A Multiscale Vision Transformer for Hyperspectral Image Classification"},{"paperId":"97fcb73b687593059d5dd054183093a2bfe46cc1","title":"Hourglass Attention Network for Image Inpainting"},{"paperId":"6aded7d6f93fd93c5632b627b6520830a3999ad4","title":"Molformer: Large Scale Chemical Language Representations Capture Molecular Structure and Properties"},{"paperId":"a1ad15d2333cf9d9b55bbc97a3aacd244a8b9fdf","title":"Demystifying the Better Performance of Position Encoding Variants for Transformer"},{"paperId":"8835d7e27472e6041b640c869ce6b94021f8c851","title":"Do Large Scale Molecular Language Representations Capture Important Structural Information?"},{"paperId":"f5a2ce064ea0efc3d7f26acd91041824c3254cd8","title":"NRTSI: Non-Recurrent Time Series Imputation for Irregularly-sampled Data"},{"paperId":"dc3ee859157d163c7dd0ba497d8736a802fe59a0","title":"Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder"},{"paperId":"89bc3b21c15c0656643918488ea25af939d0881a","title":"MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with Just One Transformer VAE"},{"paperId":"976a609cf540d1ded373b872d34779f7164d840a","title":"Rethinking the Design Principles of Robust Vision Transformer"},{"paperId":"dc35daba3fb34b2e6a5b12530badb7b799262bbf","title":"On Position Embeddings in BERT"},{"paperId":"79a2ac8ec0c4d3bfc1762d330e3446af5f8f1922","title":"S2T: Learning Semantic-Spatial-Temporal Representation for Robust Active Object Tracking"},{"paperId":"2a405483796dfedf5d95483aa8880c57626e0e9f","title":"Integrating Tree Path in Transformer for Code Representation"},{"paperId":"8d6b1929b92211ad0eb3e14b2c2b41789ccf053a","title":"Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models"},{"paperId":"5be217c678916543884c354654263f27c0a6bd9f","title":"Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder"},{"paperId":"c687acfeaafb86ab045f9fcbb6e2ff691827a0c2","title":"On Position Embeddings in BERT O N P OSITION E MBEDDINGS IN BERT"},{"paperId":"a28861bdef17182df3b64fc99e9359e2e23e7672","title":"Transformer with Syntactic Position Encoding for Machine Translation"},{"paperId":null,"title":"Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"}],"references":[{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"e8984c6e6c24aab26c332728a5fff616dfb3adbb","title":"Learning to Encode Position for Transformer with Continuous Dynamical Model"},{"paperId":"f64e1d6bc13aae99aab5449fc9ae742a9ba7761e","title":"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"},{"paperId":"748629cb0b8e5a5708e1c6605f71b36eb525a3ce","title":"On Layer Normalization in the Transformer Architecture"},{"paperId":"72aa8d7f6f3a36d5e208cd1f41e90613ed6aee75","title":"Encoding word order in complex embeddings"},{"paperId":"37a23c43ddf09ea97b82b38e2827a2229cfae545","title":"Novel positional encodings to enable tree-based transformers"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"95a251513853c6032bdecebd4b74e15795662986","title":"What Does BERT Look at? An Analysis of BERT’s Attention"},{"paperId":"5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88","title":"Efficient Training of BERT by Progressively Stacking"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"b36a5bb1707bb9c70025294b3a310138aae8327a","title":"Automatic differentiation in PyTorch"},{"paperId":"2e10560579f2bdeae0143141f26bd9f0a195b4b7","title":"Mixed Precision Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5feb32a73dd1bd9e13f84a7b3344497a5545106b","title":"FastText.zip: Compressing text classification models"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"79d1b330f0ef51f63ecb9b291dd5a05de5a858c0","title":"Toeplitz and Circulant Matrices: A Review"},{"paperId":"0c2d8f329f6809a90168dbfea2f11c7b0c3abc06","title":"Speech and Language: Advances in Basic Research and Practice"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"99e8d34817ae10d7304521e89c5fbf908b9d856b","title":"Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding"},{"paperId":"8f424d35b3e5af10b2e492f073b3347f1f5176f8","title":"Quantitative Linguistik / Quantitative Linguistics - Ein internationales Handbuch / An International Handbook"}],"id":"8256f48f759cf85044db251cc512f965834945b3","summary":"This work investigates the problems in the previous formulations and proposes a new positional encoding method for BERT called Transformer with Untied Positional Encoding (TUPE), which can achieve a higher score than baselines while only using 30% pre-training computational costs."},{"url":"https://www.semanticscholar.org/paper/79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88","title":"Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge","venue":"Neural Information Processing Systems","year":2020,"referenceCount":44,"citationCount":99,"influentialCitationCount":10,"publicationDate":"11/06/2020","authors":"Alon Talmor,Oyvind Tafjord,Peter Clark,Yoav Goldberg,Jonathan Berant","citations":[{"paperId":"42b850269b3619978f65d8d78a3c7e8640b99984","title":"KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding"},{"paperId":"7a6a298efb965ce9a351a3212f6f536e94dbbb03","title":"Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step"},{"paperId":"d7f1fe57b694b125bef35c4f0d30becba14b6c09","title":"Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond"},{"paperId":"f021aebbc4c8d38f55470ad11bfb1a2c59b788a7","title":"BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information"},{"paperId":"fcf36a99a8eddc4a29471507d4f3ab31580baa6c","title":"Causal interventions expose implicit situation models for commonsense language understanding"},{"paperId":"015f7ef29bebe85b705efead4c737e252baa3da7","title":"Large Language Models Are Not Abstract Reasoners"},{"paperId":"60c452cec0d2f607c7610bf28dcedc227c2a2431","title":"Beyond Labels: Empowering Human with Natural Language Explanations through a Novel Active-Learning Architecture"},{"paperId":"704cff9a00c7ea3358ac2710d5b213fe09c7f570","title":"Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification"},{"paperId":"44772fe1c3fa422a3da7e25092db2544893d6bfb","title":"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming"},{"paperId":"74b05bba46db21e589a2cc0f916f81069b0368ef","title":"Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"},{"paperId":"c83e93b519acec9f9588dfb43a0d97fbc1222267","title":"Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems"},{"paperId":"c715914c388fa64dd8686cd8755e5adfebbf2388","title":"REFINER: Reasoning Feedback on Intermediate Representations"},{"paperId":"0b16563cf7eb568b41374fdaf49d916f5c02fa9c","title":"Explicit Planning Helps Language Models in Logical Reasoning"},{"paperId":"5eab810cc5d90de1c52127d1a5824f0817f46c30","title":"Nature Language Reasoning, A Survey"},{"paperId":"63d0e5a8f195b1453006781d4d8a4eb7262652d9","title":"Logical Reasoning over Natural Language as Knowledge Representation: A Survey"},{"paperId":"d39db76fb80c1f74455a0f6432c2242b727389da","title":"Self-planning Code Generation with Large Language Model"},{"paperId":"2ae807688d5bae0a7331992793be066b93d7655f","title":"Does Deep Learning Learn to Abstract? A Systematic Probing Framework"},{"paperId":"9a9e68d400069f023f7dc9b982226c95159a509d","title":"Dissociating language and thought in large language models: a cognitive perspective"},{"paperId":"490d8006851b1562cfd9ec1f057471f2868289d1","title":"Rethinking with Retrieval: Faithful Large Language Model Inference"},{"paperId":"c7a4946eb49bc6a9c01eaa79e84a35316595bd5a","title":"Language Models as Inductive Reasoners"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"30977afbd4501249e1a320bd2e48581197914ab8","title":"A Short Survey of Systematic Generalization"},{"paperId":"8ee376114a43432399554be39a79c1a2b6c65d51","title":"Can Transformers Reason in Fragments of Natural Language?"},{"paperId":"61ddf932488405ab1c7b275460d2b3c5dfa274a0","title":"Fixing Model Bugs with Natural Language Patches"},{"paperId":"798abf86efae9e37b9b6a694ef87b6c1dbaab263","title":"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models"},{"paperId":"d400a649f0f0a3de22b89a268f48aff2dcb06a09","title":"Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"66f333c51e2bfa25380069f66500b491218da9c3","title":"Can Pretrained Language Models (Yet) Reason Deductively?"},{"paperId":"6fb0b072c4fcdc0c78218bfd1b181fd562f07cd2","title":"COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models"},{"paperId":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment"},{"paperId":"767c1338c001fdadb5bf4eed530fc78ede49e9ae","title":"Dynamic Generation of Grounded Logical Explanations in a Neuro-Symbolic Expert System"},{"paperId":"5581bf85386737bd3378eec68189759a05280bea","title":"FOLIO: Natural Language Reasoning with First-Order Logic"},{"paperId":"cb11d43d90aa5da2c0a8a682efc52e204dc3e0e0","title":"Reasoning over Logically Interacted Conditions for Question Answering"},{"paperId":"5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e","title":"On the Paradox of Learning to Reason from Data"},{"paperId":"20b12e0a6c5e3fb42cc389be6d806dc2fafe523b","title":"Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models"},{"paperId":"4b516216d7d150a081fd74993bddf36b6b22c118","title":"Chain of Thought Imitation with Procedure Cloning"},{"paperId":"e1943cbf4817605a1f988fe5fd785f6b707ca233","title":"METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation"},{"paperId":"0abcd69d87723b93bfc2074167418a4cf81a892e","title":"Logically Consistent Adversarial Attacks for Soft Theorem Provers"},{"paperId":"cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","title":"A Review on Language Models as Knowledge Bases"},{"paperId":"3f4d11971f2c64be9125a7fe99c019588bbebf16","title":"Iteratively Prompt Pre-trained Language Models for Chain of Thought"},{"paperId":"242eaa55cce5daad200850dd10a788a0f960cdd8","title":"Logical Reasoning for Task Oriented Dialogue Systems"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"e5aa2a1e36a2c68fa4aa59afdb8b6e1c419f547c","title":"Natural Language Deduction through Search over Statement Compositions"},{"paperId":"f76c965153398cd8513ef95eaa32196c4cae3f86","title":"Memory-assisted prompt editing to improve GPT-3 after deployment"},{"paperId":"bd44f34b47c8a4b6947695853fc2814ac69664a6","title":"Datasheet for the Pile"},{"paperId":"0b483b550b21ec42d693fc04a372dbb10dd07019","title":"Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge"},{"paperId":"188aa594c294f01fd18e0ff43eeb0d8430b39b90","title":"Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback"},{"paperId":"07d5bba7d2bc511c88eb143a926d3c297298ad15","title":"Interscript: A dataset for interactive learning of scripts through error feedback"},{"paperId":"f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884","title":"LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI"},{"paperId":"4a247cbfca9dcf91e2da24e6d2d84601a9041a8f","title":"Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs"},{"paperId":"a466d10b80dbdee3b130bef73ec62f3a89eb389b","title":"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples"},{"paperId":"3ecc1bb0171e7540410fbd454fed473b8b6fa437","title":"On Semantic Cognition, Inductive Generalization, and Language Models"},{"paperId":"ed02d4a08a50d23709b7ce5df9878a8bc34ac12c","title":"GNN is a Counter? Revisiting GNN for Question Answering"},{"paperId":"ecad7a322ede6de0013b46dc64429eed4c43e8af","title":"BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief"},{"paperId":"6ed7f8d8673fbf380c45cefa30138ff2b77d1d1b","title":"Abstraction, Reasoning and Deep Learning: A Study of the \"Look and Say\" Sequence"},{"paperId":"e55391a9406245584b3e5b3225dad2e171b9a06b","title":"RuleBERT: Teaching Soft Rules to Pre-Trained Language Models"},{"paperId":"d65a064eb837f838faf6ff67781b62450b92b159","title":"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification"},{"paperId":"f5ca46585818771e64ee9449c930748fbee35cba","title":"Improving Neural Model Performance through Natural Language Feedback on Their Explanations"},{"paperId":"911b7539e964782670e555930b291de16fa971c5","title":"Flexible Generation of Natural Language Deductions"},{"paperId":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","title":"Relational World Knowledge Representation in Contextual Language Models: A Review"},{"paperId":"f06ca547e5eaf0fd118f184b9ddd1cf7cd5c29e0","title":"Updater-Extractor Architecture for Inductive World State Representations"},{"paperId":"4ea544c849aee3e4f99e3d835ab3ea9ed685a5b9","title":"KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"008b9fc834f5839a25febe150f3076d550ee442f","title":"Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2"},{"paperId":"e0c4a2116ec0f3dc48457627bbd8bfe1f0026479","title":"Overcoming Poor Word Embeddings with Word Definitions"},{"paperId":"dcb67a471a3c6f782cf237ced1da28d3da23553c","title":"Concepts, Properties and an Approach for Compositional Generalization"},{"paperId":"5b6c582d51266be9aa7e32bfdc20891e5231eca4","title":"When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data"},{"paperId":"87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1","title":"ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language"},{"paperId":"18e5fb8cec55a75b288a499c57d77ede541dc049","title":"Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering"},{"paperId":"2db020e3398c06e3a22f12d8caffe76b0d9d1dda","title":"Knowledge-driven Self-supervision for Zero-shot Commonsense Question Answering"},{"paperId":"4bda9767c11a97a3d1a83577cb8ec94f16ceccb5","title":"A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English"},{"paperId":"d877020a6808f0d4baded6d6cd92d09bbc46bde2","title":"Constructing Taxonomies from Pretrained Language Models"},{"paperId":"bea36d43c6c0cbcb197231aa70ca095e331a967b","title":"Inducing Taxonomic Knowledge from Pretrained Transformers"},{"paperId":"58ba42ffc34dd24d6a77fe58c8973b3533c369cd","title":"Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views"},{"paperId":"6414281e2efb42601aa7bb5eac8759a774896968","title":"Do PLMs Know and Understand Ontological Knowledge?"},{"paperId":"f727f928e7e179307d8d4a1da2387393f2bd7915","title":"Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models"},{"paperId":"8fb90b215d3533cbb280e251d551b178c578080d","title":"MEERQAT-IRIT at SemEval-2023 Task 2: Leveraging Contextualized Tag Descriptors for Multilingual Named Entity Recognition"},{"paperId":"23f4cfc5695e13ebe32606475421d77a3999d2cb","title":"S CENARIO - BASED Q UESTION A NSWERING WITH I NTERACTING C ONTEXTUAL P ROPERTIES"},{"paperId":"2aba5bba16dac5cd62683bab9de5d6faaaed0de1","title":"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach"},{"paperId":"10de447404ce566f08e6a3c1444739fb8261f009","title":"COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models"},{"paperId":"41a4482bb47de60fdc44492fa633701e425cb501","title":"SpARC: Sparsity Activation Regularization for Consistency"},{"paperId":"36589346063ff26506330451976280011273b935","title":"Towards Teachable Reasoning Systems"},{"paperId":"63cd10c4ca6733a8894d55cf1343636fa816cf7c","title":"Analyzing the Contribution of Commonsense Knowledge Sources for Why-Question Answering"},{"paperId":"c4487e6690173618bfec98751ce3590204e981ee","title":"Logical Reasoning with Span Predictions: Span-level Logical Atoms for Interpretable and Robust NLI Models"},{"paperId":"fa46f4ddd5c6e793a47c61db9c1ecde7ea1c82bc","title":"Dynamic Generation of Interpretable Inference Rules in a Neuro-Symbolic Expert System"},{"paperId":"fed460648303afa32247e493847e4dc73dc1a5b3","title":"Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?"},{"paperId":"2f21201ac9fcb88a72c56471402388ec2fc365a8","title":"Inferring Implicit Relations in Complex Questions with Language Models"},{"paperId":"41f44979cf1cd3f4cbd615dc130bc33721f5281b","title":"MemPrompt: Memory-assisted Prompt Editing with User Feedback"},{"paperId":"7bb907e754942b832bacf7889ba1d6bd72945ca0","title":"Using Commonsense Knowledge to Answer Why-Questions"},{"paperId":"5de29c60071d65f0c00ab584b0738dcc56a1b158","title":"The First Workshop on Learning with Natural Language Supervision"},{"paperId":"bf9097371d7a88f45bbdc380680e5b2c98ffd793","title":"Identifying Physical Object Use in Sentences"},{"paperId":"f074ec057ae40bf8ea330bfa0df166a93e459439","title":"Flexible Operations for Natural Language Deduction"},{"paperId":"1b806d23d6e1daee1a7fa8df12b009e5c64bee59","title":"Are we there yet? Exploring clinical domain knowledge of BERT models"},{"paperId":"884c0b6db564208d99cadf2548f0aa96dee5f859","title":"Commonsense Reasoning with Implicit Knowledge in Natural Language"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"2af476f7c2a7c040fc9ab7750bf41a84f66aa947","title":"Knowledge Based Multilingual Language Model"},{"paperId":"f0ba384a13adfd4280ed10ff9974b787193c2531","title":"Improving scripts with a memory of natural feedback"},{"paperId":"751f55beac45cec14b0aff6174ed0139afb54b08","title":"Modelling Symbolic Knowledge Using Neural Representations"}],"references":[{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"15ad2b27c5248e7d1db5456794ca1ca8a8198f5d","title":"Transformers as Soft Reasoners over Language"},{"paperId":"c44120f765fc43994c5cfb4e12e4f62999efeae6","title":"How Context Affects Language Models' Factual Predictions"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"5a9001cdccdb8b1de227a45eccc503d32d1a2464","title":"What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"521b4e26df0f1cf5763dece14cbb218df152dc59","title":"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering"},{"paperId":"c2c165dd615d4fc31d4fef4b4acbcab1a1655983","title":"On Making Reading Comprehension More Comprehensive"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"681fbcd98acf20df3355eff3585994bd1f9008b7","title":"Probing Natural Language Inference Models through Semantic Fragments"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"401dc39c2c8c910253d47980cfa3b4d2f7790d9b","title":"WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"},{"paperId":"95f2aedef1453e5d88af9d5fddb79e0e56223cb0","title":"Compositional Questions Do Not Necessitate Multi-hop Reasoning"},{"paperId":"0707f1e3791a6805bf4542605245cf4cdee3b9e0","title":"Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA"},{"paperId":"636904d91d9dd1a641a595d9578ba7640f35aa74","title":"MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension"},{"paperId":"df2ce576eca0b3db177c83bc6cf0f9fe2c7714f0","title":"Dynamically Fused Graph Network for Multi-hop Reasoning"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"53e50a313ddfd222d958469edb6742f19458ec74","title":"Modeling Semantic Plausibility by Injecting World Knowledge"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"fbba8629ff9633ca57be1f2209d53d9bcfc2273c","title":"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference"},{"paperId":"7d5cf22c70484fe217936c66741fb73b2a278bde","title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"83e7654d545fbbaaf2328df365a781fb67b841b4","title":"Enhanced LSTM for Natural Language Inference"},{"paperId":"2cd8e8f510c89c7c18268e8ad51c061e459ad321","title":"A Decomposable Attention Model for Natural Language Inference"},{"paperId":"a9de2fc7ae3230652f27fd95e7175da8cf44f075","title":"Harnessing Deep Neural Networks with Logic Rules"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","title":"Wikidata: a free collaborative knowledgebase"},{"paperId":"e9cb15132afd04ae93ba693044fd73f6746f58c4","title":"Recognizing Textual Entailment: Models and Applications"},{"paperId":"cf8f2ca0c2d618104bc8724a6effc509088f16c4","title":"Never-Ending Learning"},{"paperId":"c9c17279a2a6e564219cf573a74c6675cebb62e6","title":"True Knowledge: Open-Domain Question Answering Using Structured Knowledge and Inference"},{"paperId":"d87ceda3042f781c341ac17109d1e94a717f5f60","title":"WordNet : an electronic lexical database"},{"paperId":"563e0a5ab5e384a4afc7ae71bada34bd709498cd","title":"Interactive Transfer of Expertise: Acquisition of New Inference Rules"},{"paperId":"494aedf82da4755badc1fe74e4d21cf5fc029e9d","title":"Programs with common sense"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"}],"id":"79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88","summary":"This work provides a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements, and demonstrates that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting."},{"url":"https://www.semanticscholar.org/paper/0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models","venue":"BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","year":2020,"referenceCount":9,"citationCount":12,"influentialCitationCount":0,"publicationDate":"13/10/2020","authors":"Devin J. Johnson,Denise Mak,Drew Barker,Lexi Loessberg-Zahl","citations":[{"paperId":"96c87efd6eef934979a2a2b083b17520ddf96e3f","title":"Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models"},{"paperId":"763f47b73d0273a93a52a8ee8a63cf19009df6e1","title":"Injecting the BM25 Score as Text Improves BERT-Based Re-rankers"},{"paperId":"1ec4ae64ded99846bcd8a1a3b16fe2875f55c7c1","title":"Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"03d73f3073ac0d73d60d5567fc1ed558367c8279","title":"Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"8fe49d025e8b83816f7169daa74becaac6184f9e","title":"Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":null,"title":"Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"}],"references":[{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"a0e49f65b6847437f262c59d0d399255101d0b75","title":"What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"53d43ccc593bf44e9aa52e3971df1b9dd396e30d","title":"Probing for semantic evidence of composition by means of simple classification tasks"},{"paperId":"70ba4f5adfbb88e88b0dbfb60f3c9dfb97bf9940","title":"Preschool Origins of Cross-National Differences in Mathematical Competence: The Role of Number-Naming Systems"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}],"id":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","summary":"Novel multilingual probing tasks tested on DistilBERT, XLM, and BERT find evidence that the information encoded in these pretrained models’ embeddings is sufficient for grammaticality judgments but generally not for value comparisons."},{"url":"https://www.semanticscholar.org/paper/51c62d63c6204deecb24a1d3f9ea8e0a42d23817","title":"Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces","venue":"International Conference on Learning Representations","year":2020,"referenceCount":31,"citationCount":10,"influentialCitationCount":0,"publicationDate":"27/08/2020","authors":"Yatin Nandwani,Deepanshu Jindal,Mausam,Parag Singla","citations":[{"paperId":"fdb0f24fdf31b703921e4044ea1f55d82074b56f","title":"Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs"},{"paperId":"bcb51ffb6849317c36f7d197a0e54bcb631f4cca","title":"Scalable Coupling of Deep Learning with Logical Reasoning"},{"paperId":"4fb5641502fc47b0a54c06a0e79fd3192aea00d6","title":"Unsupervised Learning for Combinatorial Optimization Needs Meta-Learning"},{"paperId":"587450871bcc3f9211d2291037ed306e4c881988","title":"A Solver-Free Framework for Scalable Learning in Neural ILP Architectures"},{"paperId":"52f6003584f8b423abb0a8f2b890ae0eac660889","title":"Unsupervised Learning for Combinatorial Optimization with Principled Objective Relaxation"},{"paperId":"1cdcf5f527a887cb935eaf2a0109b40d2fc90fdb","title":"Conditional set generation using Seq2seq models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6294b2966523a264e99cf90dffaf31e40874c6cb","title":"Neural Models for Output-Space Invariance in Combinatorial Problems"},{"paperId":"0e282a0725ff485bd97a344668fe667027d26c93","title":"End-to-End Neuro-Symbolic Architecture for Image-to-Image Reasoning Tasks"},{"paperId":"bea00213becb832b2dd3ec6692d14145db168e8d","title":"Unsupervised Learning for Combinatorial Optimization with Principled Objective Design"}],"references":[{"paperId":"3cb4239741e5b7496b60ee7af24c08b4341af0a2","title":"Structured Prediction with Partial Labelling through the Infimum Loss"},{"paperId":"cb4571fa905abb70868d0bb9d4681f0a612c2d0f","title":"Differentiable Reasoning on Large Knowledge Bases and Natural Language"},{"paperId":"918fea871683bb51f7ca97540c767b067ef9e3c1","title":"Partial Label Learning via Label Enhancement"},{"paperId":"d3850595d3ae7c73e9488054c9b437f75511b569","title":"SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver"},{"paperId":"3a6447361b20c249f5306ae17dee43f645430e31","title":"Neural Logic Machines"},{"paperId":"0b86e5561f0de00047b96a1bb42a3e6379ed58c2","title":"Partial Label Learning with Self-Guided Retraining"},{"paperId":"fd4ae71916cf400bfd1490f275e91b154eb69160","title":"Relational recurrent neural networks"},{"paperId":"bbe56733113671455a9fee900bfca1a6d7f247bd","title":"Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning"},{"paperId":"b3dae9529f3caeeec9cc6872e94aa690418acb22","title":"Reinforcement Learning for Relation Classification From Noisy Data"},{"paperId":"de3b9eb697feed3d097e3f671afe395f48c1ab76","title":"Stochastic Video Generation with a Learned Prior"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"34273979fd2a62fd7b49ee6d14a925864ff94e74","title":"Recurrent Relational Networks"},{"paperId":"bfdb693df3a04fa9645233c444ccd8ec16c6c477","title":"Prediction Under Uncertainty with Error-Encoding Networks"},{"paperId":"4df7bbe3ca7806f39a490c99f17867a0ac299bc3","title":"Learning Explanatory Rules from Noisy Data"},{"paperId":"032274e57f7d8b456bd255fe76b909b2c1d7458e","title":"A Deep Reinforced Model for Abstractive Summarization"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"1bc49abe5145055f1fa259bd4e700b1eb6b7f08d","title":"SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents"},{"paperId":"62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e","title":"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge"},{"paperId":"bf55591e09b58ea9ce8d66110d6d3000ee804bdd","title":"Image Captioning with Semantic Attention"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"6694e7012d6ebc2b19a0e8c90c81b0bde21bc961","title":"There Is No 16-Clue Sudoku: Solving the Sudoku Minimum Number of Clues Problem via Hitting Set Enumeration"},{"paperId":"442f09ddb5bb7ba4e824c0795e37cad754967208","title":"Learning from Partial Labels"},{"paperId":"a6ccfe1ac31444fb5a0d32b58182e0fb1b17c0e4","title":"Multi-Label Classification: An Overview"},{"paperId":"cf376cd3d516e68dfbec2f08e552aa97d88e975b","title":"Zchaff2004: An Efficient SAT Solver"},{"paperId":"822f1ed9a76a57cc19d8fda7745365b97130b97a","title":"Injecting Logical Background Knowledge into Embeddings for Relation Extraction"},{"paperId":"89c3aed3e1219985c4a832f09adcd1df7096bf60","title":"Learning with Multiple Labels"},{"paperId":"754b96e00671c74de0add9df1ef60dcf21160483","title":"Local search strategies for satisfiability testing"},{"paperId":null,"title":"Can convolutional neural networks crack sudoku puzzles?"},{"paperId":null,"title":"2018] 11 as the prediction network for this task. RRN uses a message passing based"},{"paperId":null,"title":"Minimum sudoku"}],"id":"51c62d63c6204deecb24a1d3f9ea8e0a42d23817","summary":"This paper formally defines the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku, and proposes an RL based approach to jointly train the selection module with the prediction network."},{"url":"https://www.semanticscholar.org/paper/a20712b1b9779ee43ce143a19b3f67f0cacbbf57","title":"Neural Databases","venue":"ArXiv","year":2020,"referenceCount":59,"citationCount":7,"influentialCitationCount":0,"publicationDate":"14/10/2020","authors":"James Thorne,Majid Yazdani,Marzieh Saeidi,F. Silvestri,Sebastian Riedel,A. Halevy","citations":[{"paperId":"5c828011a508611df4d58cced9cc48d049dc4eb9","title":"A Data Source for Reasoning Embodied Agents"},{"paperId":"f49feb36a148fa0c5be0f2a2353583367e7cf4bd","title":"QuTE: Answering Quantity Queries from Web Tables"},{"paperId":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","title":"Relational World Knowledge Representation in Contextual Language Models: A Review"},{"paperId":"73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19","title":"Measuring and Improving Consistency in Pretrained Language Models"},{"paperId":"d51bec1426494f23de57b2c4184c7e26583eda53","title":"Relational Pretrained Transformers towards Democratizing Data Preparation [Vision]"}],"references":[{"paperId":"0b09448f7543453cc066416f547292dc1e4471f6","title":"KILT: a Benchmark for Knowledge Intensive Language Tasks"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"0544af8d6b6b6e9ce14be9c6425e520a638be380","title":"Photon: A Robust Cross-Domain Text-to-SQL System"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"512f34906ddaefe885af2e5eec9b2b3b50ffd377","title":"Deep entity matching with pre-trained language models"},{"paperId":"15ad2b27c5248e7d1db5456794ca1ca8a8198f5d","title":"Transformers as Soft Reasoners over Language"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"71c908529b12ef6ee8d735127a63d48b1fc5c43c","title":"Break It Down: A Question Understanding Benchmark"},{"paperId":"cb4571fa905abb70868d0bb9d4681f0a612c2d0f","title":"Differentiable Reasoning on Large Knowledge Bases and Natural Language"},{"paperId":"521b4e26df0f1cf5763dece14cbb218df152dc59","title":"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"4d031258a66076187001b4d6182345198624d872","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"d3cacb4806886eb2fe59c90d4b6f822c24ff1822","title":"Visualizing and Understanding the Effectiveness of BERT"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"b9372e972997c5056bb79c70526230baed2e372b","title":"Multi-hop Reading Comprehension through Question Decomposition and Rescoring"},{"paperId":"e2587eddd57bc4ba286d91b27c185083f16f40ee","title":"What do you learn from context? Probing for sentence structure in contextualized word representations"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"8ebbc0f5bde3146ce2a17b81b9591febc190dce7","title":"BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning"},{"paperId":"cd832f7081ab7b83240140c4e5e58b4fb1f8e0e6","title":"Interpretation of Natural Language Rules in Conversational Machine Reading"},{"paperId":"ac11062f1f368d97f4c826c317bf50dcc13fdb59","title":"Dissecting Contextual Word Embeddings: Architecture and Representation"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"c0a29cb35c2965930566d6a407da043e18431eaa","title":"Deep Learning for Entity Matching: A Design Space Exploration"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"b1d24e8e08435b7c52335485a0d635abf9bc604c","title":"FEVER: a Large-scale Dataset for Fact Extraction and VERification"},{"paperId":"0539535989147bc7033f4a34931c7b8e17f1c650","title":"The Case for Learned Index Structures"},{"paperId":"7d5cf22c70484fe217936c66741fb73b2a278bde","title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5889e9afbcc3935867f9ae16fe46c71b9f2b071f","title":"End-to-end Differentiable Proving"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"4c41104e871bccbd56494350a71d77a7f1da5bb0","title":"Understanding Neural Networks through Representation Erasure"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"5091316bb1c6db6c6a813f4391911a5c311fdfe0","title":"\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","title":"Neural Module Networks"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"0fadc80c26e879d22cc3660783934c0e3158a167","title":"DB-IR integration using tight-coupling in the Odysseus DBMS"},{"paperId":"4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e","title":"End-To-End Memory Networks"},{"paperId":"abb33d75dc297993fcc3fb75e0f4498f413eb4f6","title":"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","title":"Wikidata: a free collaborative knowledgebase"},{"paperId":"89de826b17bb3a8009c20b7573f85a184827e7fd","title":"Constructing an Interactive Natural Language Interface for Relational Databases"},{"paperId":"564257469fa44cdb57e4272f85253efb9acfd69d","title":"MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"672b3d3976aa106c72b845db7734cd8f194cc261","title":"ODYS: an approach to building a massively-parallel search engine using a DB-IR tightly-integrated parallel DBMS for higher-level functionality"},{"paperId":"6304131a1fb60b9883f1469072eeda8a0bd441f5","title":"DB&IR: both sides now"},{"paperId":"52e40e9466178129a15e6584f31d89d31800308f","title":"Report on the DB/IR panel at SIGMOD 2005"},{"paperId":"1df699c7da1b948bffd449ac44809017ee4206e0","title":"Natural language interfaces to databases – an introduction"},{"paperId":"03b7e51c52084ac1db5118342a00b5fbcfc587aa","title":"Q-learning"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"8d43a96e12a07b53014997f050005e09a62b7cef","title":"Crossing the Structure Chasm"},{"paperId":"c5f96d281b436ad4f1e0a3c37cbbcda4c5b17eea","title":"Models for Integrated Information Retrieval and Database Systems"},{"paperId":null,"title":"Neural turing machines. CoRR, abs/1410"},{"paperId":"5d6cbe6f9f02219b4f5624a0af15429c5dbed2f5","title":"Xeggora: Exploiting Immune-to-Evidence Symmetries with Full Aggregation in Statistical Relational Models (Extended Abstract)"}],"id":"a20712b1b9779ee43ce143a19b3f67f0cacbbf57","summary":"This paper describes NeuralDB, a database system with no pre-defined schema, in which updates and queries are given in natural language, and describes an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators."},{"url":"https://www.semanticscholar.org/paper/2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks","venue":"ArXiv","year":2021,"referenceCount":46,"citationCount":41,"influentialCitationCount":8,"publicationDate":"25/02/2021","authors":"Rodrigo Nogueira,Zhiying Jiang,Jimmy J. Li","citations":[{"paperId":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers"},{"paperId":"b43383f10634f7e610f22badd4f42c93e5dcb947","title":"Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI"},{"paperId":"3e7a0dc5795dc108c78993bcf3624fc626a9f9cf","title":"Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks"},{"paperId":"a2ef15fd7e2b3e618359370078863cbac9baec04","title":"Exposing Attention Glitches with Flip-Flop Language Modeling"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering"},{"paperId":"857d4589b62085e900805fd5432f496e8fc07bd9","title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"887138f4c365b9d1325de41a522d27bec34e0d7e","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","title":"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)"},{"paperId":"49e46e615747f258517248de5a736814fada17ee","title":"What is my math transformer doing? - Three results on interpretability and generalization"},{"paperId":"e82e3f4347674b75c432cb80604d38ee630d4bf6","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"8e5ca53f7633450e2756950c234c5f9d04b5c9f2","title":"One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"2a7ae3e98357569c41424dacd60c62d3df78a0db","title":"Limitations of Language Models in Arithmetic and Symbolic Induction"},{"paperId":"40f73969cd3415e9c1b03796cbb5a50c79ebd448","title":"SALSA: Attacking Lattice Cryptography with Transformers"},{"paperId":"b09420b30fc093d63fb2ee1aac26c71da81da437","title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"},{"paperId":"d2018d1b0f69ca6805aa18a22be95cab9b5c44c7","title":"On Neural Architecture Inductive Biases for Relational Tasks"},{"paperId":"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence"},{"paperId":"edd80013e8b9fcba9231cf99884f32e5236ff329","title":"AA-TransUNet: Attention Augmented TransUNet For Nowcasting Tasks"},{"paperId":"45ece6f3b0a319dba60c20b3013b5161dd49c58b","title":"Linear algebra with transformers"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"4f451ba06c4c9effd6c4ac0bae222495501a6200","title":"Innovations in Neural Data-to-text Generation"},{"paperId":"58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","title":"JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection"},{"paperId":"c27f3904490b8e5f9f39fe2b36722090c189e916","title":"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF"},{"paperId":"33149f835f391119d287cc2c6b009464e7d14fe4","title":"On the Abilities of Mathematical Extrapolation with Implicit Models"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"}],"references":[{"paperId":"4383a975c09b72ba2f1a77cd779bb6965dbfb2fb","title":"Scaling Laws for Transfer"},{"paperId":"02791e807dc9a91f854a1f3d5f6005122a546109","title":"Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"45cdda3f96ae32927c7b073ebbedf46f5e0fbdc5","title":"Enhancing the Numeracy of Word Embeddings: A Linear Algebraic Perspective"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"84476fdf6ead3553f4493dff8e02308439d6222b","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"8256f48f759cf85044db251cc512f965834945b3","title":"Rethinking Positional Encoding in Language Pre-training"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"ac67ec5b985a30239926c3362fa45a9a03f017af","title":"Learning to Generate Correct Numeric Values in News Headlines"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"d5fcad8a3b183642fcf609519a4dbbda9c3541ff","title":"Learning Numeral Embeddings"},{"paperId":"72aa8d7f6f3a36d5e208cd1f41e90613ed6aee75","title":"Encoding word order in complex embeddings"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"136396e988dbbdaca433096e931b22732c3d95dd","title":"Text2Math: End-to-end Parsing Text into Math Expressions"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"5435f997d98754f68492334eeb87d027047e60cb","title":"Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3058d4a4fa6ebaac30e0279c6b8c30e5f969d315","title":"Extensions and Limitations of the Neural GPU"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"5b791cd374c7109693aaddee2c12d659ae4e3ec0","title":"Grid Long Short-Term Memory"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"111fd833a4ae576cfdbb27d87d2f8fc0640af355","title":"Learning internal representations by error propagation"},{"paperId":"5414be4432fcfe1e83932127ee2463d107e5b61c","title":"A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}],"id":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","summary":"It is found that how a number is represented in its surface form has a strong influence on the model’s accuracy, and this result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement."},{"url":"https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":59,"citationCount":158,"influentialCitationCount":20,"publicationDate":"05/03/2021","authors":"Dan Hendrycks,Collin Burns,Saurav Kadavath,Akul Arora,Steven Basart,Eric Tang,D. Song,J. Steinhardt","citations":[{"paperId":"3254960e8f6eed6b16533406fe84a93d657d9bc7","title":"SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning"},{"paperId":"4b4ba6a02148c9d6f78e95d8e0d927104c3e91a7","title":"Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias"},{"paperId":"98032f95e274db30570727fb7196c15e325fb35a","title":"Three Bricks to Consolidate Watermarks for Large Language Models"},{"paperId":"c29dbfbc17fa190b787a2662d49f08a38c8bd166","title":"ARB: Advanced Reasoning Benchmark for Large Language Models"},{"paperId":"4993258852711c4e3d0011325ac3db680eae84f4","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"},{"paperId":"e4604c35cd07a524e63f7a86a0326740c60a66ec","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"6c0a3927005c8cde1e9df52e7aee686b59ca5bd0","title":"Beyond the Obvious: Evaluating the Reasoning Ability In Real-life Scenarios of Language Models on Life Scapes Reasoning Benchmark~(LSR-Benchmark)"},{"paperId":"50f9f33b284b7363fbd9b9d2da4939b989a1c7cd","title":"Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models"},{"paperId":"71fea6d80866a8fd29c466e092a5c700287f1193","title":"A Survey on Evaluation of Large Language Models"},{"paperId":"fae57797d357bfa3b39b220336d1a2e8deba5318","title":"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving"},{"paperId":"2ed07f3574fde65772b625153e853d716c2a6e14","title":"CMMLU: Measuring massive multitask language understanding in Chinese"},{"paperId":"97d9d728f924c1f6cc085844136a481cac07c4b0","title":"The BEA 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues"},{"paperId":"a4929de687f3c6937dabbf733258af635781d3c4","title":"StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code"},{"paperId":"86b37cc52a41ce0cae618202bb746dd7802edc77","title":"Deductive Verification of Chain-of-Thought Reasoning"},{"paperId":"71faef5970a36cf3b0a4ef02cb6a9ed616d0299b","title":"Argument and explanation"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","title":"An Empirical Study on Challenging Math Problem Solving with GPT-4"},{"paperId":"d8244cf970b9019cfb7f813275eba10d17c47166","title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks"},{"paperId":"be8db99310602d66bba64bcf41a572c45816fbfc","title":"Let's Verify Step by Step"},{"paperId":"2e01fdebbc780d3667ec3bf87a44927f0d9c188a","title":"Decision-Oriented Dialogue for Human-AI Collaboration"},{"paperId":"d3060876d9ad4e4e50e1c88a8c04186df00f24e2","title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets"},{"paperId":"ea75117f34b168a20f2a4309ac2eb685ca6b1436","title":"Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance"},{"paperId":"b8fe9d7b5762f7c9c3789cff2bdbe968ff0f0ed6","title":"Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving"},{"paperId":"be88dfeac13da517c4bf11784e87bc2ac0c0c084","title":"Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models"},{"paperId":"95ca67ba607d7859ee8eec457f4b59b115d69bf5","title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models"},{"paperId":"d9685ec1d83988d5b3b26a5576b5aec5cc7713b8","title":"Out-of-Distribution Generalization in Text Classification: Past, Present, and Future"},{"paperId":"ba704774f194938b04b1e2be40b1d111a4ca08e1","title":"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation"},{"paperId":"aafa168f9756f42c4ff707f6577cdd2eccc62b12","title":"RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"1563cff13dfa53e571641d108f2fec6b2bf77767","title":"VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models"},{"paperId":"1237e6213d9a426fdfdb1c9dc33d15f4a660f56f","title":"OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models"},{"paperId":"0b315e6d4d800e04caf2f587312ce163e748d10c","title":"Numeric Magnitude Comparison Effects in Large Language Models"},{"paperId":"eccee350691708972370b7a12c2a78ad3bddd159","title":"PaLM 2 Technical Report"},{"paperId":"236c7dafea3df7ecffb5f18ec780d12f2f27d4b0","title":"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"},{"paperId":"785bb49af762efd64d841e52aa82c708341a7c43","title":"Code Execution with Pre-trained Language Models"},{"paperId":"61aa5bfa47bc500690640e5a6fec4cdc82328c14","title":"Algebra Error Classification with Large Language Models"},{"paperId":"62176de125738e3b95850d1227bac81fd646b78e","title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models"},{"paperId":"d1bbd1c9ae17fcf532fa373f80e4718693b39525","title":"LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering"},{"paperId":"69bef4ab1018cc956a77c3ccdcaa57b124ab9fcc","title":"ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness"},{"paperId":"27c16cca907aa43397cc226a182b73b396c5cf66","title":"Inducing anxiety in large language models increases exploration and bias"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"e36e5df3dde73c4e3606cdd4498cfc304a29bf5c","title":"Learning to Program with Natural Language"},{"paperId":"1d9b7628e5775cdf979de01c2d0bca4f8ed37970","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"57100e39d0413ee585b381ba9ab366e8a6cf2866","title":"Solving Math Word Problems by Combining Language Models With Symbolic Solvers"},{"paperId":"a4f4e5f5c1410c2256bcfeed31426b01defc33a1","title":"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"},{"paperId":"dca6c3927ade6481a1ae080f5c24decbfeced1be","title":"Boosted Prompt Ensembles for Large Language Models"},{"paperId":"033275ccc2c7c5c38592ae893da0b5923cf90717","title":"Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases"},{"paperId":"24bcdce51edb8e1174fbabd072a0c07bf7362d57","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"fc3fe217de5d8de1a5814daf94de52e0d941cf21","title":"Mind meets machine: Unravelling GPT-4's cognitive psychology"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"9b5de3f83649c347b2b2b9fdecb142fbca293f57","title":"Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference"},{"paperId":"9f8ac6ee3760ab202e492c733362e5bfc6763934","title":"Baldur: Whole-Proof Generation and Repair with Large Language Models"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"34e97ba23d856b1ebf99b5080c3f04c0342a3954","title":"Safety without alignment"},{"paperId":"272afc28d03890160b1f2808cc551c962ea9138c","title":"ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics"},{"paperId":"0cf694b8f85ab2e11d45595de211a15cfbadcd22","title":"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks"},{"paperId":"d2170504c4ad9403bea118ae8debdfda95978546","title":"The Wisdom of Hindsight Makes Language Models Better Instruction Followers"},{"paperId":"cd0988714ea326642d2b1bb18753e187fec71e42","title":"A Categorical Archive of ChatGPT Failures"},{"paperId":"3f2cb353c7528efafb847309ab1e1e95245740a4","title":"Mathematical Capabilities of ChatGPT"},{"paperId":"10422b7bcf17eae9b04d4a020a591b3a61b45593","title":"Towards Autoformalization of Mathematics and Code Correctness: Experiments with Elementary Proofs"},{"paperId":"18fa8810f5c5f0e9d3785f21be1415897eea08c6","title":"Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers"},{"paperId":"e965e93e76a9e6c4e4863d145b5c007b540d575d","title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"},{"paperId":"7cbc7aa08b96de770d9ce5c90d01e75e9df2caee","title":"Language models are better than humans at next-token prediction"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"03fb95e6be583ca954c3d00812a9e9a40f118e51","title":"LAMBADA: Backward Chaining for Automated Reasoning in Natural Language"},{"paperId":"887138f4c365b9d1325de41a522d27bec34e0d7e","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"95c11cc5820ba32c60d5f2671f6567b9914a4978","title":"ALERT: Adapting Language Models to Reasoning Tasks"},{"paperId":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers"},{"paperId":"e3bdf0e84ca06ad456dcd2b073cc72fe81c5b46e","title":"ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning"},{"paperId":"7d5175db1b99552491063d2d9581b0b51e1d2932","title":"Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety"},{"paperId":"0e1f80a3f52c9051026656f00e31c0b9c1428d7a","title":"Can language models automate data wrangling?"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","title":"Galactica: A Large Language Model for Science"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","title":"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)"},{"paperId":"0cf5c61f367e3937dc07be634a43952a50b589a4","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"4634362d75606287955260ef1788171286efbeaa","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"711d5e8ddbb840ad31a9ffa3d38590603ba69a92","title":"Prompting GPT-3 To Be Reliable"},{"paperId":"e02dce6ee032a13b1653f69b034a35676e7d4dc2","title":"Can language representation models think in bets?"},{"paperId":"6d5555348f453bac901c5b57e8a4eeb3074b4071","title":"Learning to Reason With Relational Abstractions"},{"paperId":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment"},{"paperId":"2fe6060ced80c1c245a718e6188b6516207bf0a8","title":"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions"},{"paperId":"465dfa563e7d5ddef794f90ad3b0b19cf46d91a7","title":"Compute Like Humans: Interpretable Step-by-step Symbolic Computation with Deep Neural Network"},{"paperId":"53b3b15e8cbe2baf82cd0de3709e0a1e6f677415","title":"Limits of an AI program for solving college math problems"},{"paperId":"0a9df881784009cbb8efcd037d82ae222440aade","title":"An Interpretability Evaluation Benchmark for Pre-trained Language Models"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"041edc8b14bdd0e5627377956fd0e6c6c011146a","title":"Machine Learning Model Sizes and the Parameter Gap"},{"paperId":"6059e073b11b96af7566efddd1d4ee0e25046c54","title":"Forecasting Future World Events with Neural Networks"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"e6ecdbbceff06cc1e667e3261596fd0fa6b32c4b","title":"How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild"},{"paperId":"23af448f60ddbad785402055e96366b65464fa8f","title":"Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation"},{"paperId":"70cb319c2339d5adee742912a6f3ef88ae462bdd","title":"Unveiling Transformers with LEGO: a synthetic reasoning task"},{"paperId":"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","title":"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"},{"paperId":"c28e95a06dfcf13fc65a1cac83722f53e34f12a5","title":"Autoformalization with Large Language Models"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"354bf043179e3e9f05df73e3f04517e53c326d1f","title":"TALM: Tool Augmented Language Models"},{"paperId":"d072b46a0504ac023d5035d8ec0c7876151245c4","title":"Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models"},{"paperId":"c9f48406851954cb098911eccb4124ea5f966675","title":"A Survey on Multi-hop Question Answering and Generation"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"004d6ce718b0edb5b999f26710c5ae80b04bc900","title":"GPT-based Open-Ended Knowledge Tracing"},{"paperId":"04ff95e0edc3759fc5d18a1b929b3ccf79b032b2","title":"Deconstructing Distributions: A Pointwise Framework of Learning"},{"paperId":"916a06a6d51aa93de27aac2f3e14faed08dd6706","title":"Formal Mathematics Statement Curriculum Learning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"739715750d8eacbf04f16e36770089afad358cfa","title":"Towards More Robust Natural Language Understanding"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"f729e16ee6765e2fd76772cbf9d3dd17b4d25438","title":"Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"3ceca3cc39cd90515ccd73afe6539cc953762140","title":"How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI"},{"paperId":"21f455ca0bbf9a355611bc0593dd1cf8a8d32584","title":"The Computational Complexity of Finding Arithmetic Expressions With and Without Parentheses"},{"paperId":"320c1c6647a5b975c901347f71638c881888686b","title":"NQuAD: 70,000+ Questions for Machine Comprehension of the Numerals in Text"},{"paperId":"0cc771ca7859b395728c428b2a707007c4d69416","title":"Pretrained Language Models are Symbolic Mathematics Solvers too!"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"7ba98b00a224094c09676090f5d6d69498f5b299","title":"MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics"},{"paperId":"9140afbf83ac5fa07b3d4672e2329012812ca0fc","title":"Systematic human learning and generalization from a brief tutorial with explanatory feedback"},{"paperId":"d2824febb0c0c1fbfedc35e1a4d0532017603753","title":"Solving Machine Learning Problems"},{"paperId":"a2a97c944a5a987435cbe8249693b63a3a2c2745","title":"Effect of pre-training scale on intra- and inter-domain, full and few-shot transfer learning for natural and X-Ray chest images"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"4cc1fb128fa3abf6f90d567744767e8fd6315e1d","title":"NaturalProofs: Mathematical Theorem Proving in Natural Language"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"6a1b25f7a67395ad1e676027322913acbb0a0635","title":"CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review"},{"paperId":"2f714ebe6feea773f5ca474d2db101c43ec36a63","title":"ALERT: Adapt Language Models to Reasoning Tasks"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"3457222ca38472a41c0c76346afbdf58bf68a28f","title":"Scalable and Explainable Automated Scoring for Open-Ended Constructed Response Math Word Problems"},{"paperId":"2fc3a62d07fa644f46507d77090a56d2ba9b3829","title":"The ADAIO System at the BEA-2023 Shared Task: Shared Task Generating AI Teacher Responses in Educational Dialogues"},{"paperId":"46b294941c397699fde0ee7e7fc441f6a755f671","title":"Machine learning for formal proof synthesis Several approaches propose to combine machine learning with modern interactive theorem provers"},{"paperId":"874200f7c5062f32ccefc5e37796bd8cae6b8972","title":"Evaluating Language-Model Agents on Realistic Autonomous Tasks"},{"paperId":"dc4da699b547a38e7f28ae6b39a779338b701131","title":"Structural information in mathematical formulas for exercise difficulty prediction: a comparison of NLP representations"},{"paperId":"c8e727c4e2bbdbdfbc77610215e8c9e9b09ce63b","title":"Transformer-Encoder and Decoder Models for Questions on Math"},{"paperId":"e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","title":"Solving Math Word Problems with Process-based and Outcome-based Feedback"},{"paperId":"d1bcb86bab0906bd631c4eb5ab81ce342b5927d0","title":"Open-Ended Knowledge Tracing"},{"paperId":"15789db234a9338e57aff9709868dcbd290727d3","title":"Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network"},{"paperId":"15e767aa26a14455da95a2b2f11e3d59f2c250f6","title":"Autoformalization for Neural Theorem Proving"},{"paperId":"cc8417aa578016203cb52efc63592bba64b08bb3","title":"FinMath: Injecting a Tree-structured Solver for Question Answering over Financial Reports"},{"paperId":"c27f3904490b8e5f9f39fe2b36722090c189e916","title":"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF"},{"paperId":"8eabf511ce56c56b99dd79ac3e9935624313eaac","title":"Analyzing the Effects of Annotator Gender across NLP Tasks"},{"paperId":"9529ef8c65a148ab0542ff3b7e9b81d73ac3d8b2","title":"Hybrid Tokenization and Datasets for Solving Mathematics and Science Problems Using Transformers"},{"paperId":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-ended Knowledge Tracing for Computer Science Education"},{"paperId":"8a4dce5735a101ff8f64c2b676afb8c24950a5d8","title":"Zero-shot Mathematical Problem Solving via Generative Pre-trained Transformers"},{"paperId":"bfda912457074fc7e1ccc10a250d4664e8efd2ec","title":"Analyzing Korean Math Word Problem Data Classification Difficulty Level Using the KoEPT Model"},{"paperId":"5eadc68b7a6a049eacd5449a8047405a3f6d3893","title":"Extracting Operator Trees from Model Embeddings"},{"paperId":"d9c05c32b7935dc8f7a048f79c2ce2f45558ddc8","title":"ProofNet: A Benchmark for Autoformalizing and Formally Proving Undergraduate-Level Mathematics Problems"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"f1362000a1561924a3a07d7b9ab3d8cc3fd4e96d","title":"Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images"},{"paperId":"096cffc20d3bc89e8bc337607435a67e79d888d9","title":"Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images"},{"paperId":"4bbafce8de5301222f236784dfa23217636963e4","title":"Overleaf Example"},{"paperId":"4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc","title":"Learning Methods for Solving Astronomy Course Problems"},{"paperId":"cfe14dc505f4e3fd4574b9d1b4fafdc64583cc03","title":"Modern Approaches in Natural Language Processing"},{"paperId":"4befd752d21a6231a9d930b1946177bd4cba30cb","title":"Mathematical Word Problem Solution Evaluation via Data Preprocessing Approach"}],"references":[{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"4383a975c09b72ba2f1a77cd779bb6965dbfb2fb","title":"Scaling Laws for Transfer"},{"paperId":"58c74cec28f3416b9a1d308bb2d6519d21d53ab0","title":"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","title":"Measuring Massive Multitask Language Understanding"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"da97bd6d2d0a2f11bb011b9925585e086010cff0","title":"A Promising Path Towards Autoformalization and General Artificial Intelligence"},{"paperId":"00974f862a2cbef54a87ca47413c90884bddd5c1","title":"LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"53ebd9e0ef2a6a088a5f977f331e434de256ad26","title":"Modelling High-Level Mathematical Reasoning in Mechanised Declarative Proofs"},{"paperId":"45bb43cdc35324fea4350ed335c500d4a5fd6ef5","title":"Mathematical Reasoning via Self-supervised Skip-tree Training"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"e02b0fc9255d58cae698121a5935bd9793c1883c","title":"MathZero, The Classification Problem, and Set-Theoretic Type Theory"},{"paperId":"bd49e66af9755e6138967eba6aeb37d8190d2b4f","title":"ExpBERT: Representation Engineering with Natural Language Explanations"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"36efa56d011bc3dc84c2499bd0dfcdcba55cfefb","title":"Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"9ef2e09a9e16e176e19c3fdc3b6ee22c5d3f3c97","title":"HOList: An Environment for Machine Learning of Higher-Order Theorem Proving (extended version)"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"87c425f23bcac2f082968abda64a971f91522f73","title":"GamePad: A Learning Environment for Theorem Proving"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"6ff2a434578ff2746b9283e45abf296887f48a2d","title":"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"8388f1be26329fa45e5807e968a641ce170ea078","title":"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"687684749814cbe54672d3b53ea08df6c66382bb","title":"Beyond the Turing Test"},{"paperId":"8e8ec502208f29ee9f78ded19226578e027ecd16","title":"Universal Intelligence: A Definition of Machine Intelligence"},{"paperId":null,"title":"Language models are few-shot"},{"paperId":null,"title":"2020) introduce BART, which has a bidirectional encoder and unidirectional decoder"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Attention is all you"},{"paperId":"395cc693f69e61bb5212f77015560570a97a1b20","title":"Solve it."},{"paperId":"3ac7cae59622f1f91833dbd0d9a8313f21b97614","title":"The Unreasonable Effectiveness of Mathematics in the Natural Sciences"},{"paperId":"6e51d8e92729366721de4b886ce3c26b124c9e58","title":"A Formal Definition of Intelligence Based on an Intensional Variant of Algorithmic Complexity"},{"paperId":null,"title":"integration using completing the square; integration using long division; integration using trigonometric identities"},{"paperId":null,"title":"integration with partial fractions; intercepts from an equation; interpret quadratic models; interval of convergence; inverse of a 3x3 matrix; inverses of functions"},{"paperId":null,"title":"hopital's rule: 0/0; l'hopital's rule: ∞/∞; lagrange error bound"},{"paperId":null,"title":"relative minima and maxima; remainder theorem; remainder theorem and factors"},{"paperId":null,"title":"-digit divisors); divide with remainders (2-digit by 1-digit); dividing complex numbers"},{"paperId":null,"title":"multiplying polynomials; multiplying polynomials 0.5; multiplying positive and negative fractions"},{"paperId":null,"title":"probability with permutations and combinations; problems involving definite integrals (algebraic)"},{"paperId":null,"title":"Khan Academy Modules (4/4): properties of exponents (rational exponents); proportion word problems; pythagorean identities; pythagorean theorem"},{"paperId":null,"title":"integration by parts; integration by parts: definite integrals"},{"paperId":null,"title":"multiplying rational numbers; multivariable chain rule; multivariable chain rule intro; negative exponents"},{"paperId":null,"title":"parametric curve arc length; parametric equations differentiation; parametric velocity and speed"},{"paperId":null,"title":"special right triangles; square and cube challenge; square roots of perfect squares"},{"paperId":null,"title":"Khan Academy Modules (3/4): integrate and differentiate power series"},{"paperId":null,"title":"systems of equations with substitution; systems of equations word problems; tangents to polar curves"},{"paperId":null,"title":"number of solutions of quadratic equations; one step equations; one step equations with multiplication"}],"id":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","summary":"This work introduces MATH, a new dataset of 12, 500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations, and shows that accuracy remains relatively low, even with enormous Transformer models."},{"url":"https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":29,"citationCount":184,"influentialCitationCount":31,"publicationDate":"11/03/2021","authors":"Arkil Patel,S. Bhattamishra,Navin Goyal","citations":[{"paperId":"1e2eba005ccd8ab7a668a525c5b43245853bdaf1","title":"Reasoning in Large Language Models Through Symbolic Math Word Problems"},{"paperId":"9cf08276b8b08a77368f2e2743c4e6b2c8bfe2ab","title":"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"},{"paperId":"c29dbfbc17fa190b787a2662d49f08a38c8bd166","title":"ARB: Advanced Reasoning Benchmark for Large Language Models"},{"paperId":"4745c942a397aa830a0da37fda14556baa20b604","title":"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"},{"paperId":"3bd83ff979f3c0e9470f23c360a18333593dc5a1","title":"GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution"},{"paperId":"c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36","title":"MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning"},{"paperId":"e4604c35cd07a524e63f7a86a0326740c60a66ec","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"a79330d1069a86b7f8bf6f9fe0ef01d7b1379a91","title":"Chain of Thought Prompting Elicits Knowledge Augmentation"},{"paperId":"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","title":"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements"},{"paperId":"8f7297454d7f44365b9bcda5ebb9439a43daf5e6","title":"Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs"},{"paperId":"c64c780427ee486568c919c3b7c2a314dfc89ecf","title":"DiversiGATE: A Comprehensive Framework for Reliable Large Language Models"},{"paperId":"ce51db215a08e83124b6845115984e488fc08eb3","title":"Solving Math Word Problems Following Logically Consistent Template"},{"paperId":"0ba49945649b40f205503dba3443e2bf550c7115","title":"Learning by Analogy: Diverse Questions Generation in Math Word Problem"},{"paperId":"ec9af8d8973fa4d302ccd53e5e0236651201351c","title":"Your Prompt is My Command: On Assessing the Human-Centred Generality of Multimodal Models"},{"paperId":"9efa81ec4954b0859c47dad8f42edfaf8bced69b","title":"Boosting Language Models Reasoning with Chain-of-Knowledge Prompting"},{"paperId":"4713dc19179cdd9083e47067fa9504751f8759c6","title":"Human-in-the-Loop through Chain-of-Thought"},{"paperId":"30ea884a5b8e4cffe18ebfef6c033e5c37b79eeb","title":"World Models for Math Story Problems"},{"paperId":"2d338cdd12091814dec11155d3f6f848d7bab4d8","title":"Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models"},{"paperId":"69b8a50b232206b935dd4c45a5397a27b9d0e03e","title":"The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"d8244cf970b9019cfb7f813275eba10d17c47166","title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks"},{"paperId":"8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","title":"An Empirical Study on Challenging Math Problem Solving with GPT-4"},{"paperId":"d75d11d2c89c01cd284383546ae057cb827dc272","title":"Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning"},{"paperId":"04270591b6006a83b0a8970ef80bcbfc26a835d9","title":"Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models"},{"paperId":"6289de84a02f0c27734f295ada565603ac958948","title":"Tab-CoT: Zero-shot Tabular Chain of Thought"},{"paperId":"24a285b1e7ee9acfec9a82f7123563b62f532203","title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning"},{"paperId":"4cde406e8c6abe893c8f05e702d0c143c0c88ad1","title":"Neural Machine Translation for Mathematical Formulae"},{"paperId":"700f9c8a76d7af0fc550d119aa1d1164a496055e","title":"Mixture of Prompt Experts for Generalizable and Interpretable Question Answering"},{"paperId":"4c4c0f7a455980d4e19bfe875bc10df55983c5e7","title":"Discriminator-Guided Multi-step Reasoning with Language Models"},{"paperId":"dbfd154190667087ed1cd6c7f75a81858c2f397e","title":"Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models"},{"paperId":"6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3","title":"Automatic Model Selection with Large Language Models for Reasoning"},{"paperId":"ba704774f194938b04b1e2be40b1d111a4ca08e1","title":"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation"},{"paperId":"bb8f7fbec020675d269ccfa0e6e603f02b664c0d","title":"PaD: Program-aided Distillation Specializes Large Models in Reasoning"},{"paperId":"9a9b1e2968302eb882870537d4af6e2c722dfd1a","title":"Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement"},{"paperId":"7919cb1a1dcf70ed7803c43a71d43dba696ef149","title":"Making Language Models Better Tool Learners with Execution Feedback"},{"paperId":"073e6b91adc25c656d85002e3cb059e4530db20b","title":"Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation"},{"paperId":"5acb662fe3ee67a24f90ed0c5e1b33eb518ca533","title":"SpokenWOZ: A Large-Scale Speech-Text Dataset for Spoken Task-Oriented Dialogue in Multiple Domains"},{"paperId":"4313b009cd30a469400d36b9fb20267620293d04","title":"A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"f576288b7f5ca4aff3a6492a9db1db5148b7616f","title":"Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?"},{"paperId":"1f07d5bf633fdaaf7dcb73397688cf256c6d709f","title":"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"},{"paperId":"661ef7301c3c399130d3d8673098dd27f5696130","title":"Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs"},{"paperId":"bbd9a93671ce487b7799efa32508ecb631d978ac","title":"SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs"},{"paperId":"531702f6af619b531586dd87a32519234d9c9ec7","title":"Can Language Models Solve Graph Problems in Natural Language?"},{"paperId":"a39f960b28a627d554712a457efa5d3e3b7f8406","title":"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning"},{"paperId":"62176de125738e3b95850d1227bac81fd646b78e","title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models"},{"paperId":"1ced16c3861cd4a9b88eee7330df1917f55b3a00","title":"NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports"},{"paperId":"f11ea0fe307ce40fee1f18dfc3eef946a1c7a769","title":"SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data"},{"paperId":"aad167be3c902388ea625da4117fcae4325b8b7d","title":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"},{"paperId":"0061d6c1aa6c6032120974e8939ee11f5eed8813","title":"Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding"},{"paperId":"fce42753155280051ac64817404b4e1d3be6ebaa","title":"MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks"},{"paperId":"a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6","title":"Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"7a6d33ebb514a39dd2abcf4cf4e3baab6a38a343","title":"Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models"},{"paperId":"1d9b7628e5775cdf979de01c2d0bca4f8ed37970","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"90dd829f3d64dda19092b6e26909803bea5c37c1","title":"From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning"},{"paperId":"dca6c3927ade6481a1ae080f5c24decbfeced1be","title":"Boosted Prompt Ensembles for Large Language Models"},{"paperId":"0d502a1e300336ae628f5c8b99ee4d3766c8f60b","title":"Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"},{"paperId":"44b17e06fc7fbc87dd30e4a08f41507764f97e3e","title":"When do you need Chain-of-Thought Prompting for ChatGPT?"},{"paperId":"c715914c388fa64dd8686cd8755e5adfebbf2388","title":"REFINER: Reasoning Feedback on Intermediate Representations"},{"paperId":"c2cd18f9d1f73ae516ee264bffcbb34d10eba8b6","title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"},{"paperId":"bebede105aa69a81045bf79682272ee4cfb61475","title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks"},{"paperId":"1d29334cfbe9a1a943082058876f0c22d44c62fd","title":"A Survey of Large Language Models"},{"paperId":"9a75e23639bfcc3a51da57a3b682a984d1d8ac0b","title":"Language Models can Solve Computer Tasks"},{"paperId":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"},{"paperId":"0d42221038c05cee8443c5b5af838505ee137dc3","title":"ART: Automatic multi-step reasoning and tool-use for large language models"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"c9a9735216915e9afa0fc97b02b57148a0491bdd","title":"NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions"},{"paperId":"e05483a41e8002e7024d39457e55a3fe533f5835","title":"How Many Demonstrations Do You Need for In-context Learning?"},{"paperId":"2ebd5df74980a37370b0bcdf16deff958289c041","title":"Foundation Models for Decision Making: Problems, Methods, and Opportunities"},{"paperId":"b626560f19f815808a289ef5c24a17c57320da70","title":"MathPrompter: Mathematical Reasoning using Large Language Models"},{"paperId":"fdacdbc6a00eeb42efe7f81848b0bc09be5ca997","title":"Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!"},{"paperId":"1462a0e5b7db47301bb0995db56426e1f4a0ac7d","title":"Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers"},{"paperId":"1358f90705b05cdb20ebe6799b02196205e7e9f0","title":"Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data"},{"paperId":"3fc3460c4554a28e489a0ea6ef067b79b7d301d9","title":"Active Prompting with Chain-of-Thought for Large Language Models"},{"paperId":"85996f9fc312777f487dd51bf9e96bb3704c2fb7","title":"On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)"},{"paperId":"0cc9d031ca2f85c1412d5eab9449416c47b8cacd","title":"Learning by Applying: A General Framework for Mathematical Reasoning via Enhancing Explicit Knowledge Learning"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey"},{"paperId":"873a581320d928249609d3c07229d5af182a379c","title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"},{"paperId":"40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a","title":"Techniques to Improve Neural Math Word Problem Solvers"},{"paperId":"cd0988714ea326642d2b1bb18753e187fec71e42","title":"A Categorical Archive of ChatGPT Failures"},{"paperId":"69619a2a47faee7a29ec596db13172e2a42ff921","title":"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models"},{"paperId":"3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context"},{"paperId":"f2b0017ddd77fa38760a18145e63553105a1a236","title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"},{"paperId":"ea0688f9e7dfb0d3c2249486af65209c25809544","title":"Faithful Chain-of-Thought Reasoning"},{"paperId":"edc9bf11c4810a77f00ccb96130ff67ee578391e","title":"ThoughtSource: A central hub for large language model reasoning data"},{"paperId":"1338b3771c27090dee722cc5b351ace179ebae76","title":"Mathematics, word problems, common sense, and artificial intelligence"},{"paperId":"e659fa1e79a2a151be331125c14339988542aac3","title":"Batch Prompting: Efficient Inference with Large Language Model APIs"},{"paperId":"9a9e68d400069f023f7dc9b982226c95159a509d","title":"Dissociating language and thought in large language models: a cognitive perspective"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","title":"Large Language Models Are Reasoning Teachers"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"887138f4c365b9d1325de41a522d27bec34e0d7e","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"7715ba5e75f5256e1061c7473afe61bb0dbb9065","title":"Large Language Models are Better Reasoners with Self-Verification"},{"paperId":"f0a136313ce7908a400dfb19ef6f9b9cc4a7b964","title":"Comparative Analysis of Problem Representation Learning in Math Word Problem Solving"},{"paperId":"5485c60dc4192cf118c4834d05da7b35a88e3928","title":"A Comparative Analysis of Math Word Problem Solving on Characterized Datasets"},{"paperId":"126a4776ff8315fd506766cb8f3c722cf746ad9e","title":"Teaching Small Language Models to Reason"},{"paperId":"2c08e45a4489937d6c38704d8884e127d8628ffe","title":"Explaining Math Word Problem Solvers"},{"paperId":"8fd462f6248d5e3f1b6602697c09489086b5655f","title":"Distilling Reasoning Capabilities into Smaller Language Models"},{"paperId":"5d10a5fcc943afab50bce6293eaaaf3d22815868","title":"Chaining Simultaneous Thoughts for Numerical Reasoning"},{"paperId":"3a3100001bdca4b094345263fd2aafebd5e93555","title":"Textual Enhanced Contrastive Learning for Solving Math Word Problems"},{"paperId":"e6745fb621481ccb0ed53c267a37292e499c1b42","title":"Automatic Generation of Socratic Subquestions for Teaching Math Word Problems"},{"paperId":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"6681f0a0cc6ddaa70cdea109b941c47538caaa27","title":"Judicial knowledge-enhanced magnitude-aware reasoning for numerical legal judgment prediction"},{"paperId":"538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct"},{"paperId":"456de7821c97806d3d74559cb51d19e692883b79","title":"Self-consistent Reasoning For Solving Math Word Problems"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"b6b42f59ca3e0c5147b159f7c1fe2169219d5263","title":"Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem"},{"paperId":"3fa70115248377c3d1517c9f978791a296fbc1dd","title":"Large Language Models Can Self-Improve"},{"paperId":"6157e46e5be13225c86c5918bada3930565a97dc","title":"ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler"},{"paperId":"2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","title":"Mind's Eye: Grounded Language Model Reasoning through Simulation"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"2fe6060ced80c1c245a718e6188b6516207bf0a8","title":"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions"},{"paperId":"f557f3a32d309373e7d31bb93ca1b80b4a6e39e7","title":"Symbolic Math Reasoning with Language Models"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"60f208b19bb63d82fda5759897677f92d4e1e2fc","title":"Improving Compositional Generalization in Math Word Problem Solving"},{"paperId":"74cc3d340039c67bdabaef090d1386fe2c5376ca","title":"CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning"},{"paperId":"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","title":"Language models show human-like content effects on reasoning"},{"paperId":"2a0c444d14c4758ec37c32b20551f6048541f035","title":"Expression Syntax Information Bottleneck for Math Word Problems"},{"paperId":"6d147d1b7a283252052cda28a98ee6cc6379f932","title":"Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)"},{"paperId":"e826ac71dad8c4ce36d82fb7add43e3d306bb7e1","title":"Making Large Language Models Better Reasoners with Step-Aware Verifier"},{"paperId":"3ce8c07349d91bb3f022a211be36e98eef0e1046","title":"MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems"},{"paperId":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers"},{"paperId":"99752e255a866484291866a5ff5cf94e96d6bdc4","title":"Is a Question Decomposition Unit All We Need?"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e","title":"On the Paradox of Learning to Reason from Data"},{"paperId":"4feb4f49c5837cbb1c9c8e61d4e8056cafb102e6","title":"Unbiased Math Word Problems Benchmark for Mitigating Solving Bias"},{"paperId":"a560cb91061edac97053c9485f44eed176c1ed99","title":"LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning"},{"paperId":"b21670e8061a06ab97e7d6052c9345a326e84ff8","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"0232a71bd47b6410dea4b00559acb93755f65fff","title":"Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers"},{"paperId":"c9f48406851954cb098911eccb4124ea5f966675","title":"A Survey on Multi-hop Question Answering and Generation"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"4333161031b842949ef75d60d982438ca52d8ecc","title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"03488f1a193066b5ea8b9b800e119f07df5c1d9e","title":"Reasoning Like Program Executors"},{"paperId":"2ff977905770df1bb75b69cbbeb8e242c5ac46b7","title":"Improving a Graph-to-Tree Model for Solving Math Word Problems"},{"paperId":"64fef156c226b9fc812d80eb119fb074cf575279","title":"Evaluating State-of-the-Art Visual Question Answering Models Ability to Answer Complex Counting Questions"},{"paperId":"f729e16ee6765e2fd76772cbf9d3dd17b4d25438","title":"Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems"},{"paperId":"39df1a17da84f02bfcb8751de8965b798653a5ee","title":"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems"},{"paperId":"927efd299cffcfca3716efefcc904331b70c153e","title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset"},{"paperId":"68900bf4b9cc820f4b47608871eafcac65a33917","title":"Adversarial Examples for Evaluating Math Word Problem Solvers"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"c0f407e76b02da687524484435482b869b7f947c","title":"Evaluating and Enhancing the Robustness of Math Word Problem Solvers"},{"paperId":"f71234e3405d668b26d070d83274530a26caa2c6","title":"Toward Automatic Tutoring of Math Word Problems in Intelligent Tutoring Systems"},{"paperId":"f2abfc521e170d4ca0317bbd2ca122157eb9bbcd","title":"Making Language Models Better Reasoners with Step-Aware Verifier"},{"paperId":"751a94c3f6ce75858c2023f0fdffa3572d535026","title":"Improving Mathematics Tutoring With A Code Scratchpad"},{"paperId":"a0eb6078463a8fd524bfa092429a6ab595845ffa","title":"It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations"},{"paperId":"cd059ebe0e3fa86062d50cb063c9c10d39f8f336","title":"Integrating Heterogeneous Graphs Using Graph Transformer Encoder for Solving Math Word Problems"},{"paperId":"cd6e3ede9f799db018897473e69cad9b02229940","title":"It Takes One to Tango but More Make Trouble? The Number of Demonstrations Needed for In-Context Learning"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"56e403ba5941cf4053a52fb752426e4a9b0255fd","title":"Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Dataset Augmented by ChatGPT"},{"paperId":"c059d251d8f0c2491892271eb40ea3cec4aea830","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue in Multiple Domains"},{"paperId":"520711a1e93e6c4221f2a7c97c27a508379e8e37","title":"Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts"},{"paperId":"322c9e4ef9dc7724935ba818c0ff38d2c3d11483","title":"Bridging the Resource Gap: Exploring the Efficacy of English and Multilingual LLMs for Swedish"},{"paperId":null,"title":"MIND’S EYE: GROUNDED LANGUAGE MODEL REA-"},{"paperId":"5b2d98622d793afacc6113028e4586d8ae0adfb2","title":"Evaluating the Robustness of Deep learning Models for NLP"},{"paperId":"d2b9af575082b8321859464b0dc14c626b2fd263","title":"UL2: U NIFYING L ANGUAGE L EARNING P ARADIGMS"},{"paperId":"9c46ade7da15032256b6cfe137ead958c416a14d","title":"DESCRIPT : Audio / Video ummarizer"},{"paperId":"064422a3713b96acc173d6bbcdfc6b6b15e3f5b7","title":": Leveraging Collective Human Intelligence to Study Large Language Models"},{"paperId":"760561c57f68044e2f1d089088df1da6c627b09a","title":"On the Advance of Making Language Models Better Reasoners"},{"paperId":"909003934f2e68527c5638fae1b60821eaf438f1","title":"HAWP: a Dataset for Hindi Arithmetic Word Problem Solving"},{"paperId":"f02e8f1c9b5ab12ddfb1977570f9f5445a99a973","title":"Large Language Models are reasoners with Self-Verification"},{"paperId":"4aca69be58a271b1be45ec7ebb3586569cec50b0","title":"ArMATH: a Dataset for Solving Arabic Math Word Problems"},{"paperId":"8d282bf295d655e38b63ddbc7cdc94b188df8bb2","title":"G ENERATING S EQUENCES BY L EARNING TO [S ELF -]C ORRECT"},{"paperId":"11eff7418c2e4171467aa48899d6cd3771c1e6e3","title":"Noun-MWP: Math Word Problems Meet Noun Answers"},{"paperId":"f645347e06b430d32f916a3c673acfd348e23058","title":"Towards Interpretable Math Word Problem Solving with Grounded Linguistic Logic Reasoning"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"15e2520e7f3bd96ba9d9d6e967771b34e448a9f1","title":"Compositional Generalization and Neuro-Symbolic Architectures"},{"paperId":"3eae64c02e448e656305dde5d496f30db423683d","title":"MATHion: Solving Math Word Problems with Logically Consistent Problems"},{"paperId":"37cc867e3893f407b572113705ecd0f6efc8b9fc","title":"Improving Equation Set Problems with Label Augmentation"},{"paperId":"4befd752d21a6231a9d930b1946177bd4cba30cb","title":"Mathematical Word Problem Solution Evaluation via Data Preprocessing Approach"},{"paperId":null,"title":"Later datasets increase in complexity and scale , incorporating reading comprehension"}],"references":[{"paperId":"f9c07ed1d2113c858b38861790af6a26310b8465","title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"},{"paperId":"91bad6519095404998f4ce23592547b409cdb60a","title":"Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"33ec7eb2168e37e3007d1059aa96b9a63254b4da","title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"},{"paperId":"d0cda85c030711aaa5383c80d5928a4d22f8d3bf","title":"How Can We Accelerate Progress Towards Human-like Linguistic Generalization?"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"ce177672b00ddf46e4906157a7e997ca9338b8b9","title":"Attention is not not Explanation"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"c20584e878efbc4d716710b9aa2f4954c4014638","title":"Learning Fine-Grained Expressions to Solve Math Word Problems"},{"paperId":"7220d7ec31f6dc6ad7030f601743b5392513a2d9","title":"Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks"},{"paperId":"624527d8b92bc6d423784113ded0b9fd639add00","title":"Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":null,"title":"Hyperparameters Scratch RoBERTa Scratch RoBERTa Scratch RoBERTa Scratch RoBERTa Embedding Size"}],"id":"13c4e5a6122f3fa2663f63e49537091da6532f35","summary":"It is shown that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs, and models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy."},{"url":"https://www.semanticscholar.org/paper/d898662fb0519f00f5ccc87f06294fa7322715b4","title":"LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training","venue":"ArXiv","year":2022,"referenceCount":17,"citationCount":2,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Hongwei Han,Jialiang Xu,Mengyuan Zhou,Yijia Shao,Shi Han,Dongmei Zhang","citations":[{"paperId":"fce42753155280051ac64817404b4e1d3be6ebaa","title":"MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks"},{"paperId":"2ebfa7db6e0899041fff28b9f7fd212581d4f7e8","title":"One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data"}],"references":[{"paperId":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"1b055049c568be70d6a762679cdb93f630d5d6e6","title":"Tabular Transformers for Modeling Multivariate Time Series"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","title":"Pre-trained models for natural language processing: A survey"},{"paperId":"ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96","title":"TabFact: A Large-scale Dataset for Table-based Fact Verification"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"10ffbbcf23923ffef2b1ae78b516ec4329c78727","title":"Time2Vec: Learning a Vector Representation of Time"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Methods for numeracypreserving word embeddings"}],"id":"d898662fb0519f00f5ccc87f06294fa7322715b4","summary":"The LUNA framework is proposed which improves the numerical reasoning and calculation capabilities of transformer-based language models and bridges the gap between number and vocabulary embeddings with number pre-training and model distillation."},{"url":"https://www.semanticscholar.org/paper/28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":68,"citationCount":59,"influentialCitationCount":3,"publicationDate":"24/03/2021","authors":"Avijit Thawani,J. Pujara,Pedro A. Szekely,Filip Ilievski","citations":[{"paperId":"1364e3c277e5833299f73b618c021b38510207cd","title":"ML-LJP: Multi-Law Aware Legal Judgment Prediction"},{"paperId":"be3a74f9f6010889d049060eab2a4b09eb48bbfb","title":"The Value of Numbers in Clinical Text Classification"},{"paperId":"1f9079a7408d08ae6cd0dd74ebfaa728216b3626","title":"Towards Sentence Level Inference Attack Against Pre-trained Language Models"},{"paperId":"0b315e6d4d800e04caf2f587312ce163e748d10c","title":"Numeric Magnitude Comparison Effects in Large Language Models"},{"paperId":"9e1ba67d5f443a8bd42a8b856534f50c429baf11","title":"Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency"},{"paperId":"857d4589b62085e900805fd5432f496e8fc07bd9","title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"},{"paperId":"fce42753155280051ac64817404b4e1d3be6ebaa","title":"MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"be8f769f135b0ffcf829594b9421788781f38008","title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"032962bfc563e2caf986d3b3746f3217303be1b0","title":"Extracting Victim Counts from Text"},{"paperId":"579ebd42633908a72eea4ff2d153b766973e4b84","title":"Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking"},{"paperId":"763f47b73d0273a93a52a8ee8a63cf19009df6e1","title":"Injecting the BM25 Score as Text Improves BERT-Based Re-rankers"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"0e97b13624bc15e7f30c82402252eca4d1a8eeba","title":"LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training"},{"paperId":"1f84b6075ffa2d4ed685f31668ea1324bfba4fe5","title":"A Survey on Medical Document Summarization"},{"paperId":"611fb68240299adf0b19450b9972c55fc63df483","title":"T2G-Former: Organizing Tabular Features into Relation Graphs Promotes Heterogeneous Feature Interaction"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"6681f0a0cc6ddaa70cdea109b941c47538caaa27","title":"Judicial knowledge-enhanced magnitude-aware reasoning for numerical legal judgment prediction"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"5efab88c0cdb11c795fa8f44a5d31b40e2a1c261","title":"What do Large Language Models Learn beyond Language?"},{"paperId":"87a31b729295a3357949683276a2625288fdd0f0","title":"PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance"},{"paperId":"d7104dab8e5379f9566108857e3bd9dc832286df","title":"Developing a general-purpose clinical language inference model from a large corpus of clinical notes"},{"paperId":"8e5ca53f7633450e2756950c234c5f9d04b5c9f2","title":"One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"1c29ae78cbaf67f8c3a9132ba97ebc771a176cd1","title":"Multi-modal Siamese Network for Entity Alignment"},{"paperId":"23b6ea803e9b707edbc4aa286d2b5d9878cdba2a","title":"Innovations in Neural Data-to-text Generation: A Survey"},{"paperId":"77fd5f28660f7f4cc71d07711e2acfe74723d268","title":"Ad astra or astray: Exploring linguistic knowledge of multilingual BERT through NLI task"},{"paperId":"d422d79726461806c74ec4bb9a89e3408d6c4a75","title":"OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression"},{"paperId":"8ffa52bb533c95e5bdc832b30b738f372fc8dbff","title":"Arithmetic-Based Pretraining -- Improving Numeracy of Pretrained Language Models"},{"paperId":"b8eafd80fe009c4ca35dc26ee58a189bb66b89c2","title":"mGPT: Few-Shot Learners Go Multilingual"},{"paperId":"c11b6653a4af4ab030ff91d3183f090694115b07","title":"FiNER: Financial Numeric Entity Recognition for XBRL Tagging"},{"paperId":"7f2435d170e40132312113c6cbc1ef4a1e7eb254","title":"Modelling the semantics of text in complex document layouts using graph transformer networks"},{"paperId":"3c9ba25baca64151af4e9d50c7947de28eb2a599","title":"Survey of Hallucination in Natural Language Generation"},{"paperId":"49f4b4ca86e574c7ec688cfd45d2e17ff079c313","title":"Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"93adb9609268ba308eb9d42843761f4dcd20c849","title":"Cat2Type: Wikipedia Category Embeddings for Entity Typing in Knowledge Graphs"},{"paperId":null,"title":"Truth-Conditional Captioning of Time Series Data"},{"paperId":"3d4781c86e5b49124e307b4ad6ef2fceeeee92f2","title":"Truth-Conditional Captions for Time Series Data"},{"paperId":"5d9ac727b4a4e1044b3ba464df3be66eb8568127","title":"Numerical reasoning in machine reading comprehension tasks: are we there yet?"},{"paperId":"b350be3836c3d183464642815b26b061f24e8314","title":"Learning Mathematical Properties of Integers"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"a1364257028332760208827cd0c7af08d91e058b","title":"HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation"},{"paperId":"f8e0f7babe7cd858aecfafb03375bf578fe161f7","title":"MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving"},{"paperId":"2cf3cd3a7a08fc91eecab45e73299940c9c439dc","title":"Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"5903227db7b76161c3d24b94ba9ad454367c6c24","title":"ITTC at SemEval 2023-Task 7: Document Retrieval and Sentence Similarity for Evidence Retrieval in Clinical Trial Data"},{"paperId":"8f90451a627c0b5d0cdff5f6e11b4cdf2fe008db","title":"Improving the Numerical Reasoning Skills of Pretrained Language Models"},{"paperId":"58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","title":"JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection"},{"paperId":"a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b","title":"Good Night at 4 pm?! Time Expressions in Different Cultures"},{"paperId":"4f451ba06c4c9effd6c4ac0bae222495501a6200","title":"Innovations in Neural Data-to-text Generation"},{"paperId":"c97bcf9e605938c2bef54f12b2156c678f1a2373","title":"Introducing Semantic Information for Numerical Attribute Prediction over Knowledge Graphs"},{"paperId":"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","title":"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning"},{"paperId":"1e336827a740f0e4a3def14d261e55d1bda26d83","title":"Probing Representations of Numbers in Vision and Language Models"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":"f38325f21d7c3a5175893874951540f5bd200b9e","title":"Learning Numeracy: A Simple Yet Effective Number Embedding Approach Using Knowledge Graph"},{"paperId":null,"title":"Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"}],"references":[{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"55596cdc41004c50505800aa427ee9d7d2a8c1e0","title":"Learning Numeral Embedding"},{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"0772212420238b48233256a59912f8e6a31cde3d","title":"NumClaim: Investor's Fine-grained Claim Detection"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"251a27e00b9838c8c315f0152cb52762db766aee","title":"Visual sense of number vs. sense of magnitude in humans and machines"},{"paperId":"75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1","title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling"},{"paperId":"79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88","title":"Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks"},{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"716efab73e1916fdc2a23727b581d200271ed499","title":"BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"41094beddc498680d807e99e07efa41fec5d6724","title":"How Pre-trained Word Representations Capture Commonsense Physical Comparisons"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"007bc04c97f9f8bcec0487699e197315418f22e7","title":"From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the Aristo Project"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"da6ba9f19581bd7e28bc280c53385a1327eb09dd","title":"LSTM Networks Can Perform Dynamic Counting"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"5f277d2605a272c321615373254097939aae4fac","title":"“When Numbers Matter!”: Detecting Sarcasm in Numerical Portions of Text"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1","title":"Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"51004bc6461a572e1189a0e3b32b441155d760ce","title":"QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships"},{"paperId":"cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"ef89a1f27d1b1bfd451d4482a6de2dcd3d4a1318","title":"Grounding Gradable Adjectives through Crowdsourcing"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"c9f343b492c170c726f607c255ec6c7177dc5800","title":"Verb Physics: Relative Physical Knowledge of Actions and Objects"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"63e39cdf1ad884da6bc69096bb3413b5b1100559","title":"Using the Output Embedding to Improve Language Models"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"b8e6e8fd64d71f7c89b1aa9cf1fa073451f6d5e9","title":"“Was It Good? It Was Provocative.” Learning the Meaning of Scalar Adjectives"},{"paperId":"0ee09a9f7c586b703650f9a189668366f619b9e2","title":"A Fact-aligned Corpus of Numerical Expressions"},{"paperId":"91fa08ca6f7d39489d53af1deb87e9f7219f2dba","title":"The Biber Cognitive Estimation Test."},{"paperId":"d35195324e24e496083245caa6b806bcdea35daa","title":"Core systems of number"},{"paperId":"24d34fb890b3c13a1019342faf129e25b6419a14","title":"Variability signatures distinguish verbal from nonverbal counting for both large and small numbers"},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"68c03788224000794d5491ab459be0b2a2c38677","title":"WordNet: A Lexical Database for English"},{"paperId":"1ce38ff6a6801a0a7e0ec1fbd24503d7a2142fbf","title":"The self-organizing map"},{"paperId":"6c52dcad9514eba4049bada022308044140e5417","title":"Children's understanding of counting"},{"paperId":"09437cba3abec8d7befd5358c0049281eb2afd69","title":"Dataset for Evaluation of Mathematical Reasoning Abilities in Russian"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"da4fc448a81ca207609e78bced9a02a10f75866e","title":"SemEval-2018 Task 5: Counting Events and Participants in the Long Tail"},{"paperId":"6ddc5a5009e6b2a146e103e7d1dac283d3741fa3","title":"Core systems in human cognition."},{"paperId":"3182aa468641a3aa4f1b26a853aed45a48099853","title":"EFFECTIVE TEACHERS OF NUMERACY"},{"paperId":null,"title":"he was 16 yrs old’ entails the hypothesis ‘he was a teenager"},{"paperId":null,"title":"English gigaword. Linguistic Data Consortium"},{"paperId":null,"title":"Contextual datetime language model adaptation for speech recognition"}],"id":"28a5a53dafacebad8a7c47773079caeffb9a5baa","summary":"This work synthesizes best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation."},{"url":"https://www.semanticscholar.org/paper/d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning","venue":"ArXiv","year":2022,"referenceCount":230,"citationCount":16,"influentialCitationCount":1,"publicationDate":"20/12/2022","authors":"Pan Lu,Liang Qiu,Wenhao Yu,S. Welleck,Kai-Wei Chang","citations":[{"paperId":"4993258852711c4e3d0011325ac3db680eae84f4","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"},{"paperId":"fae57797d357bfa3b39b220336d1a2e8deba5318","title":"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"5ffd2ded3d828a34aed5c0076834e14c634256a2","title":"Don't Trust GPT When Your Question Is Not In English"},{"paperId":"ba704774f194938b04b1e2be40b1d111a4ca08e1","title":"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"95ca67ba607d7859ee8eec457f4b59b115d69bf5","title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models"},{"paperId":"0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","title":"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs"},{"paperId":"be9ec68e494c9717b1d6cc32269fc30216b8f252","title":"Answering Questions by Meta-Reasoning over Multiple Chains of Thought"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"1d29334cfbe9a1a943082058876f0c22d44c62fd","title":"A Survey of Large Language Models"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"5938abafd61881f6b23a2ba318d2d3d0327402c0","title":"A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram"},{"paperId":"3f2cb353c7528efafb847309ab1e1e95245740a4","title":"Mathematical Capabilities of ChatGPT"},{"paperId":"887138f4c365b9d1325de41a522d27bec34e0d7e","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"}],"references":[{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"72fce949725b20428e5f56247fef5c6bd1ce6154","title":"UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression"},{"paperId":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"75f7e9e2b59fb640ef9d1dff94097175daf46c4d","title":"Large Language Models Struggle to Learn Long-Tail Knowledge"},{"paperId":"538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct"},{"paperId":"a630c70aed27b52f6d04d1e772b153c5a7b6f6fe","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"4634362d75606287955260ef1788171286efbeaa","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"d96997265f8146e93b4c9350f19d55e46d1317f0","title":"ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering"},{"paperId":"07955e96cbd778d0ae2a68f09d073b866dd84c2a","title":"Decomposed Prompting: A Modular Approach for Solving Complex Tasks"},{"paperId":"c88cafa3e980765a64febe369ceb7c2aa7261d2a","title":"Complexity-Based Prompting for Multi-Step Reasoning"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"b2542a738b75ee9b7ce1a13d8b78f9095d212412","title":"Generate rather than Retrieve: Large Language Models are Strong Context Generators"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"d4f42cf69b91f08e86acc36b6f9f47bd1aac4a2a","title":"Insights into Pre-training via Simpler Synthetic Tasks"},{"paperId":"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","title":"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"},{"paperId":"3464bf8d2fa50e8b5ddd07cfb10790bbfe6dfa2f","title":"A Survey in Mathematical Language Processing"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"c28e95a06dfcf13fc65a1cac83722f53e34f12a5","title":"Autoformalization with Large Language Models"},{"paperId":"50b0c6ee2b3d53ba5af69d6c00b5d60888a9026f","title":"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"65b4b25272c50dc376f5c018338931bfd349e532","title":"HyperTree Proof Search for Neural Theorem Proving"},{"paperId":"c2d574f7c6a9e3bafe396ecb4ab639179d6fd92c","title":"Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"f08cf3957d6f4ca66296cfd361d30fea08bccf65","title":"PGDP5K: A Diagram Parsing Dataset for Plane Geometry Problems"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"4333161031b842949ef75d60d982438ca52d8ecc","title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"},{"paperId":"b2e8906d0c2999e3b6cbb5bfd7dc687f1a0a751c","title":"Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"916a06a6d51aa93de27aac2f3e14faed08dd6706","title":"Formal Mathematics Statement Curriculum Learning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"f9838a3be5c94bb2674a0e224de349b50e18f3c4","title":"Learning To Retrieve Prompts for In-Context Learning"},{"paperId":"3af37400f1f9a4f4f211c4a472e18963edc2b34f","title":"ValueNet: A New Dataset for Human Value Driven Dialogue System"},{"paperId":"ab15163d8c57fa5706d399c8fb62e489d57b22b7","title":"Injecting Numerical Reasoning Skills into Knowledge Base Question Answering Models"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"45ece6f3b0a319dba60c20b3013b5161dd49c58b","title":"Linear algebra with transformers"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"3ceca3cc39cd90515ccd73afe6539cc953762140","title":"How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI"},{"paperId":"9bcf3b43f2323a194036cc52c6878a9b1dc7e058","title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning"},{"paperId":"39df1a17da84f02bfcb8751de8965b798653a5ee","title":"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems"},{"paperId":"a7353543dd7cc2dc8396e8e8d2a1b60e9d38985c","title":"Improving Fractal Pre-training"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"927efd299cffcfca3716efefcc904331b70c153e","title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset"},{"paperId":"3d33c71af053b42c14ad8d476c9df9cf6dfc1e16","title":"Does Pretraining for Summarization Require Knowledge Transfer?"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"7ba98b00a224094c09676090f5d6d69498f5b299","title":"MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"102ebe229df18c8733ea1b8def56cd79996e2178","title":"A Survey of Human-in-the-loop for Machine Learning"},{"paperId":"f8e0f7babe7cd858aecfafb03375bf578fe161f7","title":"MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving"},{"paperId":"2406cf39805c70264c4226b7325a09b506c70921","title":"TAPEX: Table Pre-training via Learning a Neural SQL Executor"},{"paperId":"5cfdc75bcd204c94eb979c5b17fdb86d33b3c184","title":"Solving arithmetic word problems by scoring equations with recursive neural networks"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7654dbd372d8b65e730e3bd477ff9fec96c16dc5","title":"Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"291133a657498920451481d3bf784ebbafda8d6e","title":"GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning"},{"paperId":"4ae6b5ce971c58c1280bc971a1879e6d547c5f8c","title":"A Bottom-Up DAG Structure Extraction Model for Math Word Problems"},{"paperId":"680b0467861a70be41c31e4f2415fe5e2958fbc0","title":"HMS: A Hierarchical Solver with Dependency-Enhanced Understanding for Math Word Problem"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"fb1c90806fc5ec72987f58110aa255edbce6620d","title":"Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning"},{"paperId":"593499b654360101682edec1dd711fa7c09f6971","title":"IsarStep: a Benchmark for High-level Mathematical Reasoning"},{"paperId":"0adec918885dff698acf359988ed79a543157f80","title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"},{"paperId":"4cc1fb128fa3abf6f90d567744767e8fd6315e1d","title":"NaturalProofs: Mathematical Theorem Proving in Natural Language"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"065112180cd381ffc018780cf8fc0a14ae2580b1","title":"Proof Artifact Co-training for Theorem Proving with Language Models"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"a6ca91afe845ef5294c40c2029e0c1cba19ba40b","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"58c74cec28f3416b9a1d308bb2d6519d21d53ab0","title":"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"593ca71119fb6ee560926d9b304bde095267432d","title":"SMART: A Situation Model for Algebra Story Problems via Attributed Grammar"},{"paperId":"edaafec59b651d503ac7c8a86f8e2335273e1f7a","title":"Learning by Fixing: Solving Math Word Problems with Weak Supervision"},{"paperId":"53953a19e2fdf9c5ad5ff445c07ce36cc70f551b","title":"Solving Math Word Problems with Multi-Encoders and Multi-Decoders"},{"paperId":"24ed85ad966823868c1694a19385d01c6ad71008","title":"A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving"},{"paperId":"3938fe72ccfe4fe92387258874cb1cbe66194d4f","title":"Point to the Expression: Solving Algebraic Word Problems Using the Expression-Pointer Transformer Model"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"f9c07ed1d2113c858b38861790af6a26310b8465","title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"eb2f1480eca6195fe8dbab5de8bbac8749e9be58","title":"RODA: Reverse Operation Based Data Augmentation for Solving Math Word Problems"},{"paperId":"44507bde6e9caf60b41c60d703e7972b520d48a6","title":"Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems"},{"paperId":"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","title":"Measuring Massive Multitask Language Understanding"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"ea1f95989f808f409a3cd29b128000c04036c224","title":"Retrieval Augmented Language Model Pre-Training"},{"paperId":"016863a86189c4e8ccecf9a36c4406c439a8a84c","title":"INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"33ba3001cdbc1c1f59fefe5368300245c05560c5","title":"Teacher-Student Networks with Multiple Decoders for Solving Math Word Problem"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"cdcdc7ab1f5b6e86146b5c0224cba7d8cd35142c","title":"What Does BERT with Vision Look At?"},{"paperId":"ab38bbb36ba38047c5bb556694d148225971957f","title":"Premise Selection in Natural Language Mathematical Texts"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"97f08c1ae8ca5ddf5948c66bfbbc0546ac154807","title":"Pretrained Transformers Improve Out-of-Distribution Robustness"},{"paperId":"750bf158dd5060dfac0c6e1734654060df1f6374","title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem"},{"paperId":"52cb05d721688cb766c6e282e9d55c3b8e3dc0cf","title":"TaPas: Weakly Supervised Table Parsing via Pre-training"},{"paperId":"8a5590978930070f50c5f9fbf61f67e5d95794f0","title":"Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"1180c1d2b2fad2527e98c0065319ea8150a3b888","title":"Natural Language Premise Selection: Finding Supporting Statements for Mathematical Text"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","title":"Pre-trained models for natural language processing: A survey"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"388e2fcdcefbe0834e153ab2a0be127092f9674d","title":"DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"9df88155d6b65e14e06f8dcd851d31b7753659b1","title":"The lean mathematical library"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"81b4920ad488affaee27389ff9540b7fea90a4ce","title":"“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding"},{"paperId":"007bc04c97f9f8bcec0487699e197315418f22e7","title":"From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the Aristo Project"},{"paperId":"a6bfa815dd02f94c95083eaaf11873d8fb5849cd","title":"Solving Math Word Problems with Double-Decoder Transformer"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"9ff3afa6577d5469cd68c4991476f426efdf6bca","title":"Template-Based Math Word Problem Solvers with Recursive Neural Networks"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"80c4185d5311841cbbcbb9f1d3555b61081ca5d8","title":"HOList: An Environment for Machine Learning of Higher Order Logic Theorem Proving"},{"paperId":"a596f03145285cd05a6ca57a4e25418b23b24976","title":"Learning to Prove Theorems via Interacting with Proof Assistants"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"6d6c91485aa0527195c256997be54c188f7640ad","title":"Survey on Mathematical Word Problem Solving Using Natural Language Processing"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"6ff68b34a5f78bdd14437fe5a79aebbc42c26467","title":"DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"e9b13731027418ed38103d1dfc8a70f6881bc684","title":"Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering"},{"paperId":"51004bc6461a572e1189a0e3b32b441155d760ce","title":"QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships"},{"paperId":"6605bba6e0caabda06b090d67698a5683eba4dfa","title":"Translating a Math Word Problem to an Expression Tree"},{"paperId":"f67ede490780f939103d4cc4e9c6866b83ee59b3","title":"Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"caeb950e503872a903e18a3b259424e3cc3c6006","title":"Neural Math Word Problem Solver with Reinforcement Learning"},{"paperId":"de95601d9e3b20ec51aa33e1f27b1880d2c44ef2","title":"CBAM: Convolutional Block Attention Module"},{"paperId":"87c425f23bcac2f082968abda64a971f91522f73","title":"GamePad: A Learning Environment for Theorem Proving"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"a396effc8987cc0938a4057d9d9008ff2a5452b6","title":"Data-Driven Methods for Solving Algebra Word Problems"},{"paperId":"835c6b524b90b1639aba28742f7161137ddf4397","title":"MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning"},{"paperId":"b1b88c6d1f05913be18750ff00e06edd5e1f49ef","title":"TacticToe: Learning to Prove with Tactics"},{"paperId":"7289a240c9425bc7cad87b3b835e5f0cac22f488","title":"DVQA: Understanding Data Visualizations via Question Answering"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"55ca9fe4ae98904bfe026d22dcf1420ff9c0dd86","title":"FigureQA: An Annotated Figure Dataset for Visual Reasoning"},{"paperId":"7cfa5c97164129ce3630511f639040d28db1d4b7","title":"FiLM: Visual Reasoning with a General Conditioning Layer"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"81f466a535cdec4957989999f9ca381bc4fe14e9","title":"From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems"},{"paperId":"c20584e878efbc4d716710b9aa2f4954c4014638","title":"Learning Fine-Grained Expressions to Solve Math Word Problems"},{"paperId":"7220d7ec31f6dc6ad7030f601743b5392513a2d9","title":"Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"38bdca75587ed41bb7e1e6673fe5118aa25efe89","title":"A Survey of Question Answering for Math and Science Problem"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"f1318d15d7d8ab626b92d0c70dbdc5b5d37e223f","title":"HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving"},{"paperId":"e77e00286a63a32dafb9629cd79f6a77bddb1941","title":"Deep Network Guided Proof Search"},{"paperId":"8b7336f5dd13a45d4aab38428b4a88ce507ea310","title":"Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving"},{"paperId":"3d3b4ec7789f634e0752d50484dad7d2ea2460d5","title":"Dialogue Learning With Human-In-The-Loop"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"ba84b8f16efc032945cd8ed960c121a9bcd064d5","title":"Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems"},{"paperId":"2ab3c53b08d4180c8642207ee09738f8df193b92","title":"Holophrasm: a neural Automated Theorem Prover for higher-order logic"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"f8909e8b359c72bc0e3257dadcc48a8787bca74b","title":"DeepMath - Deep Sequence Models for Premise Selection"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"0da2fc91eb560f4245d8e7b6dff9edd0a5727dff","title":"DRAW: A Challenging and Diverse Algebra Word Problem Set"},{"paperId":"6a9533730bc2070b222b056df9bf0ee16ba7a509","title":"Four Decades of Mizar"},{"paperId":"acfe603d8a90ee3d4187e426b26c4d7bf394b4c8","title":"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning"},{"paperId":"c87dccf7c21e67679389f23f86f039cd96720c3f","title":"Solving Geometry Problems: Combining Text and Diagram Interpretation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"2a441a46e228ed0ea2251a4e61be6c7025b45766","title":"The Lean Theorem Prover (System Description)"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"32de44f01a96d4473d21099d15e25bc2b9f08e2f","title":"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"9f9d16e6cf929c66be59fd56d9870196c19410c1","title":"Distributed Asynchronous Online Learning for Natural Language Processing"},{"paperId":"226966243877f186d346be01047cf71cee1b5ec4","title":"Online EM for Unsupervised Models"},{"paperId":"1bc38b2c99f58a9199771c124248eaad2ab82671","title":"MPTP 0.2: Design, Implementation, and Initial Experiments"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"8a61ce3920931123d08dce297350c50ecb292d66","title":"Automated generation of readable proofs with geometric invariants"},{"paperId":"7ee54cd7419f1f90912508fdbdc37fdd47dc0947","title":"Isabelle: A Generic Theorem Prover"},{"paperId":"38a5fc91e0366bbd5121154aeb498a1a83573dd5","title":"Basic principles of mechanical theorem proving in elementary geometries"},{"paperId":"9fc77941297522cc420ce9292193dd04ed2ed1af","title":"Natural Language Input for a Computer Problem Solving System"},{"paperId":"ea6ed67711f11043a9f2616b60193169c9267486","title":"Empirical explorations of the geometry theorem machine"},{"paperId":"7b202896ca3151fabb4c4197c329b530491c2b3e","title":"Empirical explorations of the logic theory machine: a case study in heuristic"},{"paperId":"ecb6cad427818163b27bff2241edd1c8a7eb5946","title":"An Augmented Benchmark Dataset for Geometric Question Answering through Dual Parallel Text Encoding"},{"paperId":"760561c57f68044e2f1d089088df1da6c627b09a","title":"On the Advance of Making Language Models Better Reasoners"},{"paperId":null,"title":"Learning to understand plane geometry diagram"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"4aca69be58a271b1be45ec7ebb3586569cec50b0","title":"ArMATH: a Dataset for Solving Arabic Math Word Problems"},{"paperId":"1420d4c6add1868ba24444e351555b82e2330312","title":"Towards Socially Intelligent Agents with Mental State Transition and Human Value"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":null,"title":"2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales"},{"paperId":null,"title":"An emerging area of research aims to combine elements of informal and formal theorem proving"},{"paperId":null,"title":"language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever"},{"paperId":null,"title":"Estimating numbers without regression"},{"paperId":null,"title":"Enhancing selfconsistency and performance of pretrained language models with nli"},{"paperId":"ec15ff1fc5c9780fd91902f286a9e3fcd00b890d","title":"Math Word Problem Solving with Explicit Numerical Values"},{"paperId":"494a102164b790a63dc99385b0e56d1e80e0a93c","title":"An Edge-Enhanced Hierarchical Graph-to-Tree Network for Math Word Problem Solving"},{"paperId":"a2fff5686bdf8e25003097107516d6ac13bf8b8e","title":"Improving Math Word Problems with Pre-trained Knowledge and Hierarchical Reasoning"},{"paperId":null,"title":"Lisa: Language models of isabelle proofs"},{"paperId":"adc61e21eafecfbf6ebecc570f9f913659a2bfb2","title":"Deep Learning Based Text Classification: A Comprehensive Review"},{"paperId":null,"title":"2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond"},{"paperId":null,"title":"2022b). To address this issue, new benchmarks are proposed from various aspects"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Metamath: A Computer Language for Mathematical Proofs"},{"paperId":"998bcc21375a5639e1b65bb6eb42e8cf5c30907a","title":"Synthesis of Solutions for Shaded Area Geometry Problems"},{"paperId":"eb1179c1a8514fee677d9f23ab112367adef6321","title":"The coq proof assistant reference manual"},{"paperId":"162d958ff885f1462aeda91cd72582323fd6a1f4","title":"Gradient-based learning applied to document recognition"},{"paperId":"b1c61f9aef6b56b711829f39ff70c7e222ba400a","title":"Isabelle - A Generic Theorem Prover (with a contribution by T. Nipkow)"},{"paperId":"78fc3b741828124d8a79667466516a544143bf68","title":"Computers and Thought"},{"paperId":"a4eb8d6ee85af4d82ace0a1989f5af7baeecc747","title":"An Introduction to Java Geometry Expert"},{"paperId":null,"title":"Tianlong Ma, and Liang He. 2022a. A survey of human-in-the-loop for machine learning. Future Generation Computer Systems"}],"id":"d3a7a4543d83f568f79d1febe8379465ff0140c9","summary":"This survey paper reviews the key tasks, datasets, and methods at the intersec-tion of mathematical reasoning and deep learning over the past decade, and evaluates existing benchmarks and methods and discusses future research directions."},{"url":"https://www.semanticscholar.org/paper/37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":18,"citationCount":12,"influentialCitationCount":1,"publicationDate":"10/09/2021","authors":"K. Pal,Chitta Baral","citations":[{"paperId":"ecf0cb0725de18659f9ba25a8cf65a1085564006","title":"Understanding Arithmetic Reasoning in Language Models using Causal Mediation Analysis"},{"paperId":"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"da345b189e4faaaa489f7319640868a37a3932a1","title":"Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"87a31b729295a3357949683276a2625288fdd0f0","title":"PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance"},{"paperId":"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","title":"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"cd54a839ba12e7417e321b4407788dc426ebaefc","title":"Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering"}],"references":[{"paperId":"ad46feec0cefb78dafa60821598c052cf0b9b7bd","title":"NT5?! Training T5 to Perform Numerical Reasoning"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"d5fcad8a3b183642fcf609519a4dbbda9c3541ff","title":"Learning Numeral Embeddings"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"}],"id":"37588705a2af7d5b24d901dd33ade1ff293aabdd","summary":"This work investigates the ability of text-to-text transfer learning model (T5), which has outperformed its predecessors in the conventional NLP tasks, to learn numeracy, and considers four numeracy tasks."},{"url":"https://www.semanticscholar.org/paper/c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals","venue":"Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out","year":2021,"referenceCount":23,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/06/2021","authors":"Taku Sakamoto,Akiko Aizawa","citations":[{"paperId":"c97bcf9e605938c2bef54f12b2156c678f1a2373","title":"Introducing Semantic Information for Numerical Attribute Prediction over Knowledge Graphs"}],"references":[{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"f67fcbb1aec92ae293998ddfd904f61a31bef334","title":"Inducing Relational Knowledge from BERT"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"bd2895717effe3cc52e77fbee3da8117cf1c01e1","title":"Measuring Numerical Common Sense: Is A Word Embedding Approach Effective?"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"f98e135986414cccf29aec593d547c0656e4d82c","title":"Commonsense Knowledge Mining from Pretrained Models"},{"paperId":"b285c067f2da04bf5647beb8853bfddf6d9b4e1b","title":"A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"7e1eea0235e1902b6e3219fd9de9105e2f08fd65","title":"Numerically Grounded Language Models for Semantic Error Correction"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"Evaluating commonsense in pretrained language models"}],"id":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","summary":"This paper measures the NCS acquired by existing neural language models using a masked numeral prediction task as an evaluation task and proposes methods to reflect not only the symbolic aspect but also the quantitative aspect of numerals in the training of language models, using a loss function that depends on the magnitudes of the numerals and a regression model for the masked numal prediction task."},{"url":"https://www.semanticscholar.org/paper/ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Sungjin Park,Seung-kook Ryu,E. Choi","citations":[],"references":[{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"ad46feec0cefb78dafa60821598c052cf0b9b7bd","title":"NT5?! Training T5 to Perform Numerical Reasoning"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"02809fc23aecf33e3ed95b83d1d03b54fb5c3d0a","title":"An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"c9f343b492c170c726f607c255ec6c7177dc5800","title":"Verb Physics: Relative Physical Knowledge of Actions and Objects"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"ec15ff1fc5c9780fd91902f286a9e3fcd00b890d","title":"Math Word Problem Solving with Explicit Numerical Values"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Do language embeddings capture scales? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4889–4896, Online"},{"paperId":null,"title":"Test-set results on different sets of prompts. We report the classification accuracy and the performance difference (∆)"},{"paperId":null,"title":"2021), we included synonyms as an answer to make the prompt diverse. We used the website https://www.wordhippo"},{"paperId":null,"title":"How Can We Know What Language Models Know? Transactions of the Association for Computational Linguistics, 8:423–438"}],"id":"ff8f3dfd9e2f4a92310999722abefab202935521","summary":"This study shows that PLMs lack the capability required for reasoning over measurements, and proposes a simple embedding strategy to better distinguish between numbers and units, which leads to a significant improvement in the probing tasks."},{"url":"https://www.semanticscholar.org/paper/62953ca1252c9febe07c7007a10911726f37792d","title":"TIMEDIAL: Temporal Commonsense Reasoning in Dialog","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":47,"citationCount":30,"influentialCitationCount":3,"publicationDate":"08/06/2021","authors":"Lianhui Qin,Aditya Gupta,Shyam Upadhyay,Luheng He,Yejin Choi,Manaal Faruqui","citations":[{"paperId":"8d848ce7ab6ccde4854d3c658a169215ae483029","title":"An Overview Of Temporal Commonsense Reasoning and Acquisition"},{"paperId":"47a082afc342d94f031c5e92a606d09a0631cf28","title":"Mitigating Temporal Misalignment by Discarding Outdated Facts"},{"paperId":"338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c","title":"STOAT: Structured Data to Analytical Text With Controls"},{"paperId":"24ab6356b355b29a3770db56dd1f2200cdd987fa","title":"Salient Span Masking for Temporal Understanding"},{"paperId":"24bcdce51edb8e1174fbabd072a0c07bf7362d57","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"17a8b5e6fef1f69979d57021a8f30a5159e152c7","title":"Commonsense Reasoning for Conversational AI: A Survey of the State of the Art"},{"paperId":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey"},{"paperId":"bf53543b21819c6d7d397183c38015540cb5f8af","title":"Time expression recognition and normalization: a survey"},{"paperId":"ee153a2c91d36b034dc86c945aee9859b79da812","title":"Test of Time: Instilling Video-Language Models with a Sense of Time"},{"paperId":"77fd0d22b6d361680b364c5561e9288ca4559c85","title":"A Probabilistic-Logic based Commonsense Representation Framework for Modelling Inferences with Multiple Antecedents and Varying Likelihoods"},{"paperId":"1a5f48161df983a0e9485425495121201902433b","title":"DiscoSense: Commonsense Reasoning with Discourse Connectives"},{"paperId":"d0ba95d3c7766038ea47fda8a13377cf3ee1c8e3","title":"Multiview Contextual Commonsense Inference: A New Dataset and Task"},{"paperId":"98f19ca97512361b12475b42b67a617de14d33a1","title":"Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic"},{"paperId":"8f926c0c3f1557a9241b7e75609082a1f207a75e","title":"InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning"},{"paperId":"7ba28d214d98f2a9c2e37e6cdf294d0d4e2a1e50","title":"Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling"},{"paperId":"706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding"},{"paperId":"7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"},{"paperId":"16bf88a6d172699cb9a26a6936efb4941e3f3c13","title":"An Application of Pseudo-Log-Likelihoods to Natural Language Scoring"},{"paperId":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"},{"paperId":"281b4a7e7fb057d8266ec0610888905c46fd715d","title":"Advances in Multi-turn Dialogue Comprehension: A Survey"},{"paperId":"331a87762228ecca2eebf309b859189f033414ba","title":"NarrativeTime: Dense Temporal Annotation on a Timeline"},{"paperId":"856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","title":"Interpreting docstrings without using common sense: the private science of very large language models∗"},{"paperId":"a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b","title":"Good Night at 4 pm?! Time Expressions in Different Cultures"},{"paperId":"f4ebe3bfd4b67595ca35fbfb0eaf3bf934c8b1b9","title":"A Unifying View On Task-oriented Dialogue Annotation"},{"paperId":"867915b9587c046ce8e4b71ab4dee2a1d8bf0b48","title":"DocTime: A Document-level Temporal Dependency Graph Parser"},{"paperId":"2a83a92b08e0f3873d07162c73c67e533321112e","title":"Aligning Generative Language Models with Human Values"},{"paperId":"8b0b5b998d65add5c036b45c36afa3f9df96f4b0","title":"Toward Building a Language Model for Understanding Temporal Commonsense"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"}],"references":[{"paperId":"3d2035edd4dd48e1e638279409e11bf689c461e1","title":"Temporal Reasoning in Natural Language Inference"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"36d6c8895bbc755964b8b2136c6fd6087a7af089","title":"TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions"},{"paperId":"1b04936c2599e59b120f743fbb30df2eed3fd782","title":"Shortcut learning in deep neural networks"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"22d834f7983fbd7cf2418978571f23efcd224bd9","title":"Adversarial Filters of Dataset Biases"},{"paperId":"d08463bd665589d04619f04dbde84183ffcf2e63","title":"Towards a Human-like Open-Domain Chatbot"},{"paperId":"f67fcbb1aec92ae293998ddfd904f61a31bef334","title":"Inducing Relational Knowledge from BERT"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"4d16457cded23bce6eaa91cd17aefd22af2279f0","title":"Counterfactual Story Reasoning and Generation"},{"paperId":"81b4920ad488affaee27389ff9540b7fea90a4ce","title":"“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding"},{"paperId":"f98e135986414cccf29aec593d547c0656e4d82c","title":"Commonsense Knowledge Mining from Pretrained Models"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"27033b8f72bf8cb7662c9f92b3ccb3c476db7135","title":"Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading"},{"paperId":"227458886343b86bd15adf58c769be326b4b058a","title":"Wizard of Wikipedia: Knowledge-Powered Conversational agents"},{"paperId":"bbad0b301561c9b44a43f2880b29f143dc7297ba","title":"Temporal Information Extraction by Predicting Relative Time-lines"},{"paperId":"34901b737b11aa51b688fa18c2eab47639d7b8c6","title":"Joint Reasoning for Temporal and Causal Relations"},{"paperId":"18dacf9ead8dfefcd36ecb765509ebfb6f88abbd","title":"Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge"},{"paperId":"a6ced6341e8bf2d819cbc7de73b869752019afdd","title":"Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"3108f96f80d129036f53684344f4058257b37c4b","title":"DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset"},{"paperId":"a39ffa57ef8e538b3c6a6c2bbc0b641f7cdc60dc","title":"Who did What: A Large-Scale Person-Centered Cloze Dataset"},{"paperId":"85b68477a6e031d88b963833e15a4b4fc6855264","title":"A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories"},{"paperId":"7b82383b09bedb765ab9c5c2153978035aacd830","title":"Context-dependent Semantic Parsing for Time Expressions"},{"paperId":"0d9d8be5ee0c1cda47beafea0ef0b14722cbd908","title":"SemEval-2013 Task 1: TempEval-3: Evaluating Time Expressions, Events, and Temporal Relations"},{"paperId":"39c230241d51b1435472115aaa8c62b94ab9927d","title":"Joint Inference for Event Timeline Construction"},{"paperId":"e95353d65cdad3ca73e6e70634f1964bb14e31de","title":"Parsing Time: Learning to Interpret Time Expressions"},{"paperId":"fe80746646f6ee819205ca9a8476292ea6b83e66","title":"SUTime: A library for recognizing and normalizing time expressions"},{"paperId":"407314c1b491511564c089d26e7e92c0c93af754","title":"Learning Temporal Information for States and Events"},{"paperId":"d50806c7de06356fa207f299dacc7feb1f58dd47","title":"Classifying Temporal Relations Between Events"},{"paperId":"b06cc9e40a6e98db86275f1852eba447b9a7ec25","title":"Learning Sentence-internal Temporal Relations"},{"paperId":"b0c06a9d9adf535d5066f7a7d6dcecd0292ebbac","title":"Temporal and Event Information in Natural Language Text"},{"paperId":"ace31a10ee17fb9676efb47b55007d1c996d6122","title":"Annotating Events and Temporal Information in Newswire Texts"},{"paperId":"d36afe59ad1b706e020f55b54740bc9cddf25dcd","title":"Towards a General Theory of Action and Time"},{"paperId":"f4d642d674aa63aafc11562c2557cb4772946147","title":"Maintaining knowledge about temporal intervals"},{"paperId":"cc2b719c7ff4c7105f30015c3b129480e16c6fa4","title":"Mechanizing Temporal Knowledge"},{"paperId":"7e7343a5608fff1c68c5259db0c77b9193f1546d","title":"The measurement of observer agreement for categorical data."},{"paperId":"766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd","title":"“Cloze Procedure”: A New Tool for Measuring Readability"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9264010159e6de0e88dc883bd06797bc7f23d0ed","title":"ISO-TimeML and the Annotation of Temporal Information"},{"paperId":"ad06d0a4b7bff0e86a6b38cfad4a82c023401901","title":"A Model for Temporal References and Its Application in a Question Answering Program"},{"paperId":null,"title":"Backpropagation-based Decoding for Unsupervised Counterfactual and Abductive Reasoning"},{"paperId":null,"title":"Event cognition"}],"id":"62953ca1252c9febe07c7007a10911726f37792d","summary":"This paper presents the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set, TimeDial, and reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context."},{"url":"https://www.semanticscholar.org/paper/f729e16ee6765e2fd76772cbf9d3dd17b4d25438","title":"Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems","venue":"ArXiv","year":2021,"referenceCount":71,"citationCount":10,"influentialCitationCount":2,"publicationDate":"29/10/2021","authors":"Keyur Faldu,A. Sheth,Prashant Kikani,Manas Gaur,Aditi Avasthi","citations":[{"paperId":"75903522d49c383647e938e8d3ced18176e69c6e","title":"Vector Relation Acquisition and Scene Knowledge for Solving Arithmetic Word Problems"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a","title":"Techniques to Improve Neural Math Word Problem Solvers"},{"paperId":"81481de5cd3dd4f170559958a7110f734d3dcdd0","title":"Prompt-Based Missing Entity Recovery for Solving Arithmetic Word Problems"},{"paperId":"71471561137956d6aeec39173db460a28d86c11b","title":"Analogical Math Word Problems Solving with Enhanced Problem-Solution Association"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"b6b42f59ca3e0c5147b159f7c1fe2169219d5263","title":"Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem"},{"paperId":"c7d7f1c220412988695079677ab7700c4b4d67e3","title":"Solving arithmetic word problems by synergizing syntax-semantics extractor for explicit relations and neural network miner for implicit relations"},{"paperId":"3ce8c07349d91bb3f022a211be36e98eef0e1046","title":"MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems"},{"paperId":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers"}],"references":[{"paperId":"19f4fb23d15172306544d3ab35c4a0faa7f648e9","title":"Knowledge-Intensive Language Understanding for Explainable AI"},{"paperId":"d0dae92c4d37520ae20c072ec64fdb718874bfd0","title":"A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis"},{"paperId":"7654dbd372d8b65e730e3bd477ff9fec96c16dc5","title":"Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks"},{"paperId":"cdd961a274f42a0c4435367745b01f3da6491513","title":"Solving Arithmetic Word Problems with Transformers and Preprocessing of Problem Text"},{"paperId":"4ea544c849aee3e4f99e3d835ab3ea9ed685a5b9","title":"KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"593ca71119fb6ee560926d9b304bde095267432d","title":"SMART: A Situation Model for Algebra Story Problems via Attributed Grammar"},{"paperId":"edaafec59b651d503ac7c8a86f8e2335273e1f7a","title":"Learning by Fixing: Solving Math Word Problems with Weak Supervision"},{"paperId":"04124bc9f1f04e52e6b44385b985d9598613f0b4","title":"Semantics of the Black-Box: Can Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Explainable?"},{"paperId":"f9c07ed1d2113c858b38861790af6a26310b8465","title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"},{"paperId":"99287a5d6c822edf50bf3e9eeb17acf152d6a7c4","title":"A framework for predicting, interpreting, and improving Learning Outcomes"},{"paperId":"eb2f1480eca6195fe8dbab5de8bbac8749e9be58","title":"RODA: Reverse Operation Based Data Augmentation for Solving Math Word Problems"},{"paperId":"44507bde6e9caf60b41c60d703e7972b520d48a6","title":"Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"19af1457c7cb744ed5c6ca12d03e295599ee0d1a","title":"Auto generation of diagnostic assessments and their quality evaluation"},{"paperId":"45f952c21130d655090058e864c2772358a1de72","title":"Commonsense Reasoning for Natural Language Processing"},{"paperId":"b2a839e3ee68e81b863b73ee08c6626c94477fef","title":"WT5?! Training Text-to-Text Models to Explain their Predictions"},{"paperId":"4f03e69963b9649950ba29ae864a0de8c14f1f86","title":"K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"052cdf7abebc62789f13daf00a34b2a3e83bbf10","title":"A Deep Reinforcement Learning Approach to First-Order Logic Theorem Proving"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"b1832b749528755dfcbe462717f4f5afc07243b8","title":"Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches"},{"paperId":"b93daf00cff1e3c703316e759caeaef1dbded6e6","title":"On the Capabilities and Limitations of Reasoning for Natural Language Understanding"},{"paperId":"caeb950e503872a903e18a3b259424e3cc3c6006","title":"Neural Math Word Problem Solver with Reinforcement Learning"},{"paperId":"e889e2dd1647fdfc28edb9abe08a863b704f08be","title":"Using Intermediate Representations to Solve Math Word Problems"},{"paperId":"920febb03475b068286a855c10ea09b968fe7ee3","title":"Reinforcement Learning of Theorem Proving"},{"paperId":"a396effc8987cc0938a4057d9d9008ff2a5452b6","title":"Data-Driven Methods for Solving Algebra Word Problems"},{"paperId":"835c6b524b90b1639aba28742f7161137ddf4397","title":"MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"8f1ddf500dc30a845bb9ff98eb9751c5490fed26","title":"A Meaning-Based Statistical English Math Word Problem Solver"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"3500e99072744ecbeac891eec6683ec867427400","title":"Thinking fast and slow: Optimization decomposition across timescales"},{"paperId":"8b7336f5dd13a45d4aab38428b4a88ce507ea310","title":"Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving"},{"paperId":"f6b5335f27b9583dd152d8cd4ea9134e24bd297b","title":"Learning To Use Formulas To Solve Simple Arithmetic Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"0da2fc91eb560f4245d8e7b6dff9edd0a5727dff","title":"DRAW: A Challenging and Diverse Algebra Word Problem Set"},{"paperId":"bb34a79ac2afa68dd9096d5246cf46b8f39bcde8","title":"Designing a Tag-Based Statistical Math Word Problem Solver with Reasoning and Explanation"},{"paperId":"acfe603d8a90ee3d4187e426b26c4d7bf394b4c8","title":"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning"},{"paperId":"414483f5d80802284885003d6b0bfc8a10f61d42","title":"Learn to Solve Algebra Word Problems Using Quadratic Programming"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"cdddbbdea32cad593b4a8f6225deeb494d792b63","title":"Modeling Math Word Problems with Augmented Semantic Networks"},{"paperId":"299af69ba394a46f2125a6571a7af7f4c66d1165","title":"Frame-Based Calculus of Solving Arithmetic Multi-Step Addition and Subtraction Word Problems"},{"paperId":"03c712cb6d4f14f4dd3eb4740d3934daacc04674","title":"Robust Understanding of Word Problems with Extraneous Information"},{"paperId":"c321e6238cbb5933c0c1166f07f5033eee98b7ae","title":"A computer simulation of children’s arithmetic word-problem solving"},{"paperId":"43489fda8a0c6276a3aa8f068892e31ebfd63c13","title":"Understanding and solving arithmetic word problems: A computer simulation"},{"paperId":"045c3fec7574bbdab16c16f0af5be11696aa9b47","title":"An integrated model of skill in solving elementary word problems cognition and instruction"},{"paperId":"9fc77941297522cc420ce9292193dd04ed2ed1af","title":"Natural Language Input for a Computer Problem Solving System"},{"paperId":"8461dde4f8644517259067de63d9c8b5e589fc70","title":"MWP-BERT: A Strong Baseline for Math Word Problems"},{"paperId":"ec15ff1fc5c9780fd91902f286a9e3fcd00b890d","title":"Math Word Problem Solving with Explicit Numerical Values"},{"paperId":"d0f3423fcea593e693ff4e20f9addc2e454ac563","title":"Mathematical Reasoning Analogies Metaphors And Images"},{"paperId":"a0d7cd519aeffd6b131137b490337ed4d6393dc3","title":"Machine-guided Solution to Mathematical Word Problems"},{"paperId":"c9d0ffbce02bc8ac9432d7df859d59e215421c0f","title":"Extended-HowNet: A Representational Framework for Concepts"},{"paperId":"c06736fddbabbf4f3005e809696a849e32336702","title":"Observational learning."},{"paperId":null,"title":"System and method for recommending personalized content using contextualized knowledge base"},{"paperId":null,"title":"Adaptive learning machine for score improvement and parts thereof"},{"paperId":null,"title":"Calculus word problems"},{"paperId":null,"title":"Approximation with artificial neural networks"}],"id":"f729e16ee6765e2fd76772cbf9d3dd17b4d25438","summary":"This work inspects non-neural and neural methods to solve math word problems narrated in a natural language, and highlights the ability of these methods to be generalizable, mathematically reasonable, interpretable, and explainable."},{"url":"https://www.semanticscholar.org/paper/598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":93,"influentialCitationCount":5,"publicationDate":"22/11/2022","authors":"Wenhu Chen,Xueguang Ma,Xinyi Wang,William W. Cohen","citations":[{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"4993258852711c4e3d0011325ac3db680eae84f4","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"},{"paperId":"4a5af57b2056c4cc0a768d830d5427f0d1bdae33","title":"Large Language Models Perform Diagnostic Reasoning"},{"paperId":"d85d18eefa8d70985a48f77b31ef74d5990d148f","title":"Explaining Competitive-Level Programming Solutions using LLMs"},{"paperId":"50f9f33b284b7363fbd9b9d2da4939b989a1c7cd","title":"Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models"},{"paperId":"0b94b999fdd9488e1a0914d37f8fb3ea7e9ea0fd","title":"Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise Given to Students in Synthetic Dialogues"},{"paperId":"407ebe0b9a1795b44cffcb434788ccba5e11a81b","title":"Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks"},{"paperId":"867073aba36ffefc47b94c29a62bbf3f601037ed","title":"Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models"},{"paperId":"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","title":"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements"},{"paperId":"fae57797d357bfa3b39b220336d1a2e8deba5318","title":"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving"},{"paperId":"cc254f77fce3b8aad674f899f9e70879c1f96af0","title":"Pushing the Limits of ChatGPT on NLP Tasks"},{"paperId":"9efa81ec4954b0859c47dad8f42edfaf8bced69b","title":"Boosting Language Models Reasoning with Chain-of-Knowledge Prompting"},{"paperId":"6618a34210da9e5f537154f6482b061dff56b56f","title":"ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases"},{"paperId":"5bbc3b014f7c2dd151dc6b3cfb183889c44e772d","title":"Natural Language Commanding via Program Synthesis"},{"paperId":"50f44ef10335d59cec145b15effae20ff22c1fdb","title":"ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory"},{"paperId":"86b37cc52a41ce0cae618202bb746dd7802edc77","title":"Deductive Verification of Chain-of-Thought Reasoning"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","title":"An Empirical Study on Challenging Math Problem Solving with GPT-4"},{"paperId":"015f7ef29bebe85b705efead4c737e252baa3da7","title":"Large Language Models Are Not Abstract Reasoners"},{"paperId":"498d1406fc4cddb05cd46477793f2e726a6fe238","title":"The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code"},{"paperId":"14c255e1b399ab9f98a5a9c36c0b454e513369b0","title":"Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"eb971944bccf9793ac463c3e2f4d4251d4e8e071","title":"Do Large Language Models Know What They Don't Know?"},{"paperId":"04270591b6006a83b0a8970ef80bcbfc26a835d9","title":"Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models"},{"paperId":"32dcd0887537cece54e214f531d2c384470b023f","title":"Large Language Models as Tool Makers"},{"paperId":"8e37dc1215681aa153a51c07078ba8befd6a6e01","title":"AdaPlanner: Adaptive Planning from Feedback with Language Models"},{"paperId":"700f9c8a76d7af0fc550d119aa1d1164a496055e","title":"Mixture of Prompt Experts for Generalizable and Interpretable Question Answering"},{"paperId":"21e0a1324522b39e5cec94885501e906942c43d0","title":"InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction"},{"paperId":"ba704774f194938b04b1e2be40b1d111a4ca08e1","title":"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation"},{"paperId":"70431f177f290fa2007073a80b453f6c33fd0503","title":"Evaluating Factual Consistency of Summaries with Large Language Models"},{"paperId":"6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3","title":"Automatic Model Selection with Large Language Models for Reasoning"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"de0593223973eccfee04d598a68bc55784c7fc17","title":"LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models"},{"paperId":"51b81fdaf1cdedb544e3c32fb1d0cc28bc5e74b0","title":"Question Answering as Programming for Solving Time-Sensitive Questions"},{"paperId":"4ee96f0757e517928590a2300af5d40ba768a5a7","title":"PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents"},{"paperId":"073e6b91adc25c656d85002e3cb059e4530db20b","title":"Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation"},{"paperId":"cb46498b4d8427e9e7aa1653424642fb22f2c028","title":"SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables"},{"paperId":"8793066d170b6a742c4fcdb478d4f100c1e4bf17","title":"Fact-Checking Complex Claims with Program-Guided Reasoning"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"4313b009cd30a469400d36b9fb20267620293d04","title":"A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers"},{"paperId":"327e0290fd71609bfc1a30478a95f690668fe622","title":"Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies"},{"paperId":"d7f17d17d04b4ebb4bc132508d53b5cbd7c289c3","title":"Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning"},{"paperId":"8a68b5e6756edb2ad286b9b76a35c1003081a994","title":"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"},{"paperId":"338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c","title":"STOAT: Structured Data to Analytical Text With Controls"},{"paperId":"cd82398475147ba4eead9825d8792f3432388a13","title":"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings"},{"paperId":"1f07d5bf633fdaaf7dcb73397688cf256c6d709f","title":"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"},{"paperId":"972501b057e2b84d6ce6506f70bcac697bab7872","title":"LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation"},{"paperId":"90d6f29ad576d6587fa1b3464a6fab86c5b7fbac","title":"Satisfiability-Aided Language Models Using Declarative Prompting"},{"paperId":"8efbd687804a13762f135db30b6077a1b171ae01","title":"Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media"},{"paperId":"62176de125738e3b95850d1227bac81fd646b78e","title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models"},{"paperId":"46cff1c03c25e262b267942f29f2da301fcb336b","title":"T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering"},{"paperId":"ab9485a6564cdbe5b5ce2233d5b00c64ceaa518c","title":"From Words to Code: Harnessing Data for Program Synthesis from Natural Language"},{"paperId":"0139e689add40a61c9454674edac4e93702aa5fc","title":"Few-shot In-context Learning on Knowledge Base Question Answering"},{"paperId":"0061d6c1aa6c6032120974e8939ee11f5eed8813","title":"Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding"},{"paperId":"6898f302acab95f30822c948c484af5c6e99827b","title":"Exploring the Curious Case of Code Prompts"},{"paperId":"d5f326b3fd6bbfe26fa2b3b2c0a55f21256266ab","title":"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"1d9b7628e5775cdf979de01c2d0bca4f8ed37970","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"c4e4b72da211dbf2ab2fd5263d453cf22ee0cf44","title":"CodeKGC: Code Language Model for Generative Knowledge Graph Construction"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","title":"Tool Learning with Foundation Models"},{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"bebede105aa69a81045bf79682272ee4cfb61475","title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks"},{"paperId":"033275ccc2c7c5c38592ae893da0b5923cf90717","title":"Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases"},{"paperId":"27f32dc1aab7919eb7039deca067c3fbdc719c2a","title":"Koala: An Index for Quantifying Overlaps with Pre-training Corpora"},{"paperId":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"},{"paperId":"0d42221038c05cee8443c5b5af838505ee137dc3","title":"ART: Automatic multi-step reasoning and tool-use for large language models"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"8dfa76db449b8b57a1241701aef7ae4cd5cc1c73","title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"7b52a90a4130d7ca72c43b484d4c8e6dde74dd7f","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"2029349c55c1dba3493c5b3bd25152f18ba21ae2","title":"Augmented Language Models: a Survey"},{"paperId":"873a581320d928249609d3c07229d5af182a379c","title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"69619a2a47faee7a29ec596db13172e2a42ff921","title":"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models"},{"paperId":"ea0688f9e7dfb0d3c2249486af65209c25809544","title":"Faithful Chain-of-Thought Reasoning"},{"paperId":"5988806996e8d12f5d4aa911960d842cf7be0c24","title":"Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning"},{"paperId":"3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context"},{"paperId":"a23e0cf74f10b6a3061ab71497fe2c8c476fecd1","title":"Conversational Automated Program Repair"},{"paperId":"c43a4a7b7ea4f4889de051321cb0073fd577f843","title":"Causal Reasoning of Entities and Events in Procedural Texts"},{"paperId":"e659fa1e79a2a151be331125c14339988542aac3","title":"Batch Prompting: Efficient Inference with Large Language Model APIs"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8","title":"DePlot: One-shot visual language reasoning by plot-to-table translation"},{"paperId":"35922cd0d6b17e45320917338e9f98cb5c1a4f6f","title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"887138f4c365b9d1325de41a522d27bec34e0d7e","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"95e184e2668ad49b5ea7c1c185727ef2d2c30de6","title":"MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering"},{"paperId":"5791c2b41dd23310c53d6738a4c0d587107c2dc8","title":"MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation"},{"paperId":"c2329c685f11efa25c562f97be71ff03103423fd","title":"Prompting Is Programming: A Query Language for Large Language Models"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"285d13bf3cbe6a8a0f164f584d84f8b74067271f","title":"Towards Faithful Model Explanation in NLP: A Survey"},{"paperId":"75c08892179fc478f87d7020b5daff9fca4f3389","title":"Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"a630c70aed27b52f6d04d1e772b153c5a7b6f6fe","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"e66f0f822d4c4853b39b27daaafa2993005fd55e","title":"Large Language Models are few(1)-shot Table Reasoners"},{"paperId":"d96997265f8146e93b4c9350f19d55e46d1317f0","title":"ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"d84643c9f0289df5f9373480025da8f76f6df79c","title":"Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder"},{"paperId":"dd43b2dacded3fbe089085f920f6ff9a0f33d5f6","title":"A Numerical Reasoning Question Answering System with Fine-grained Retriever and the Ensemble of Multiple Generators for FinQA"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"10bd4160b44803ada6a3d2e366c44b7e2a4ffe90","title":"An Explanation of In-context Learning as Implicit Bayesian Inference"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"}],"id":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","summary":"Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoT performance on all math problem datasets and near-SoTA performance on ﬁnancial datasets."},{"url":"https://www.semanticscholar.org/paper/a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":22,"citationCount":8,"influentialCitationCount":0,"publicationDate":2022,"authors":"Matteo Muffo,A. Cocco,Enrico Bertino","citations":[{"paperId":"7ef0cc95ff71c1098414cd61e88ac373ea2db4c4","title":"Performance of Generative Large Language Models on Ophthalmology Board Style Questions."},{"paperId":"24a285b1e7ee9acfec9a82f7123563b62f532203","title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"ade6d2808283ab8372db701ede6ee145a5445a95","title":"SLaDe: A Portable Small Language Model Decompiler for Optimized Assembler"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models"},{"paperId":"763f47b73d0273a93a52a8ee8a63cf19009df6e1","title":"Injecting the BM25 Score as Text Improves BERT-Based Re-rankers"},{"paperId":"651dac86d8bf847ec6780a878cb1e04d3d41f356","title":"GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities"}],"references":[{"paperId":"ac34c70ee85b048ad97328713c790f389656e4eb","title":"Ecco: An Open Source Library for the Explainability of Transformer Language Models"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1a2118bed729579528deb51e745d58dd3629baf6","title":"Learning Important Features Through Propagating Activation Differences"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"}],"id":"a5808ccc50f77083bd3be926fb2af05cf34563ff","summary":"The ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on is evaluated."},{"url":"https://www.semanticscholar.org/paper/1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Jialiang Xu,Mengyu Zhou,Xinyi He,Shi Han,Dongmei Zhang","citations":[{"paperId":"a39f960b28a627d554712a457efa5d3e3b7f8406","title":"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning"}],"references":[{"paperId":"e262e1aa4d1efb909705a2dfcf9df53d76cb4e12","title":"Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation"},{"paperId":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"03488f1a193066b5ea8b9b800e119f07df5c1d9e","title":"Reasoning Like Program Executors"},{"paperId":"49f4b4ca86e574c7ec688cfd45d2e17ff079c313","title":"Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks"},{"paperId":"5d9ac727b4a4e1044b3ba464df3be66eb8568127","title":"Numerical reasoning in machine reading comprehension tasks: are we there yet?"},{"paperId":"68900bf4b9cc820f4b47608871eafcac65a33917","title":"Adversarial Examples for Evaluating Math Word Problem Solvers"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"2406cf39805c70264c4226b7325a09b506c70921","title":"TAPEX: Table Pre-training via Learning a Neural SQL Executor"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"386bfd0e411dee4f512a8737c55dd84846981182","title":"TABBIE: Pretrained Representations of Tabular Data"},{"paperId":"0f08f4458dcf7618263405cbf31e6a48684bc1fa","title":"Discrete Reasoning Templates for Natural Language Understanding"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"53953a19e2fdf9c5ad5ff445c07ce36cc70f551b","title":"Solving Math Word Problems with Multi-Encoders and Multi-Decoders"},{"paperId":"1b055049c568be70d6a762679cdb93f630d5d6e6","title":"Tabular Transformers for Modeling Multivariate Time Series"},{"paperId":"f9c07ed1d2113c858b38861790af6a26310b8465","title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"a5b1d1cab073cb746a990b37d42dc7b67763f881","title":"TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data"},{"paperId":"52cb05d721688cb766c6e282e9d55c3b8e3dc0cf","title":"TaPas: Weakly Supervised Table Parsing via Pre-training"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96","title":"TabFact: A Large-scale Dataset for Table-based Fact Verification"},{"paperId":"b285c067f2da04bf5647beb8853bfddf6d9b4e1b","title":"A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"f67ede490780f939103d4cc4e9c6866b83ee59b3","title":"Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"cbd569036fc72ae7ff747350b91816440282596b","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"ba84b8f16efc032945cd8ed960c121a9bcd064d5","title":"Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":"8461dde4f8644517259067de63d9c8b5e589fc70","title":"MWP-BERT: A Strong Baseline for Math Word Problems"},{"paperId":null,"title":"Methods for numeracypreserving word embeddings"},{"paperId":null,"title":"International Committee on Computational Linguistics"}],"id":"1a174b63d294f96568517b91f2c8d6c9362118b5","summary":"This paper proposes to conduct numerical capability diagnosis on a series of Numerical Question Answering systems and datasets and investigates the effectiveness of applying perturbations as data augmentation to relieve systems’ lack of robust numerical capabilities."},{"url":"https://www.semanticscholar.org/paper/833a2f1817cb9aeb292620454889cae78e26dda4","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":82,"influentialCitationCount":4,"publicationDate":"15/02/2022","authors":"Yasaman Razeghi,Robert L Logan IV,Matt Gardner,Sameer Singh","citations":[{"paperId":"ca0dcc09e69732b7fc582d8a610488f86ad07e36","title":"In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning"},{"paperId":"8f1b0c247171510fc68da27a16a377456376a5a7","title":"Assessing GPT-3.5 and GPT-4 in Generating International Classification of Diseases Billing Codes"},{"paperId":"5bac7d00035bc1e246a34f9ee3152b290f97bb92","title":"Supervised Pretraining Can Learn In-Context Reinforcement Learning"},{"paperId":"36f7bc27c9a37eb337c35df4ae86f148e13d4e9a","title":"Understanding In-Context Learning via Supportive Pretraining Data"},{"paperId":"a4c0144062d8e36485bad438968894cbf49ab998","title":"Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding"},{"paperId":"f4d543ff431359947bf41152ac01233b8062221f","title":"In-Context Learning through the Bayesian Prism"},{"paperId":"d8244cf970b9019cfb7f813275eba10d17c47166","title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks"},{"paperId":"11ae58636a5daf0ea1297f1c4ee94042fcebefa8","title":"Birth of a Transformer: A Memory Viewpoint"},{"paperId":"62729cff7dda7614f648a84e8967076d8878a5ff","title":"ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing"},{"paperId":"29ecb69a22bf34bd3d03990b0df2d3b7e66290de","title":"Mitigating Label Biases for In-context Learning"},{"paperId":"5412e54947f6574095174d7b85da67a5bfba4e46","title":"A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification"},{"paperId":"b6190902997b4ada616786eedf4f4a96ac9dded6","title":"Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations"},{"paperId":"3fb0731538c59f8520a309996a0567b58965f0fe","title":"Pre-Training to Learn in Context"},{"paperId":"9e1ba67d5f443a8bd42a8b856534f50c429baf11","title":"Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency"},{"paperId":"a33daa434b0adcc2cefe5293765f39e4c5b673ed","title":"Towards Mode Balancing of Generative Models via Diversity Weights"},{"paperId":"eb5d7ff323001c5b7acd3458d46b8e1911ef88b5","title":"A Latent Space Theory for Emergent Abilities in Large Language Models"},{"paperId":"bebede105aa69a81045bf79682272ee4cfb61475","title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"27f32dc1aab7919eb7039deca067c3fbdc719c2a","title":"Koala: An Index for Quantifying Overlaps with Pre-training Corpora"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"b6207fe49e29c77402f8dbab052e949990949609","title":"In-context Example Selection with Influences"},{"paperId":"a008cc894024329d832d2c9c489d57440e3fa234","title":"Data Selection for Language Models via Importance Resampling"},{"paperId":"06edda0310b4ec7c5012d012349252a3a77521b6","title":"Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners"},{"paperId":"c6ee979c2da4b55a8486abae4cd720422ab09b26","title":"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories"},{"paperId":null,"title":"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","title":"Large Language Models Are Reasoning Teachers"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"35922cd0d6b17e45320917338e9f98cb5c1a4f6f","title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters"},{"paperId":"0942bd8fad71282994ff4e9a779c09745da68edc","title":"Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations"},{"paperId":"ac9bd7da3331d699008444925ef1121cc7178dc3","title":"Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale"},{"paperId":"ad5573cb25fd403f7620332f363ae87327c69a49","title":"The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning"},{"paperId":"eecb45aa040064cbc0b37fd100706c02e7dc880e","title":"Structured Prompting: Scaling In-Context Learning to 1, 000 Examples"},{"paperId":"7d5175db1b99552491063d2d9581b0b51e1d2932","title":"Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety"},{"paperId":"d03a9b2a0e090cc9fd2ba0a457ecea35372f1018","title":"Demystifying Prompts in Language Models via Perplexity Estimation"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","title":"Galactica: A Large Language Model for Science"},{"paperId":"0a67a5e3f4125445ed84f2db3c92429010aad68a","title":"Prompting Language Models for Linguistic Structure"},{"paperId":"75f7e9e2b59fb640ef9d1dff94097175daf46c4d","title":"Large Language Models Struggle to Learn Long-Tail Knowledge"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"2fa6dbd2f147b6fa34d22c80aa225cc2cad0d96e","title":"Complex Reading Comprehension Through Question Decomposition"},{"paperId":"0cf5c61f367e3937dc07be634a43952a50b589a4","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"e3cd9f01f87a601b274b4ef6513a84c8cde03214","title":"Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning"},{"paperId":"990026128083f5a47b061f6237b8135b2d3a41a9","title":"Don’t Prompt, Search! Mining-based Zero-Shot Learning with Language Models"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"0c56a799e47ffa3fc96a6e85f53e992f1df41381","title":"Rationale Aware Contrastive Learning Based Approach to Classify and Summarize Crisis-Related Microblogs"},{"paperId":"4bdcfb8bc450987c4f91c1cb6598fab04a96cf5c","title":"Towards relation extraction from speech"},{"paperId":"c84bb36ab145cf903c1ad404008e0250f688c162","title":"Behavior Cloned Transformers are Neurosymbolic Reasoners"},{"paperId":"49aec6fb44ab52181960512a6067eded0ce4182b","title":"Benchmarking Long-tail Generalization with Likelihood Splits"},{"paperId":"2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","title":"Mind's Eye: Grounded Language Model Reasoning through Simulation"},{"paperId":"712f21411526e8450036d7199637808590be3579","title":"Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"2303ee0de927266c296287202519f17bdea9e4e8","title":"Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization"},{"paperId":"48c487ccae7fc6085b7edde4d135040f58d79a5d","title":"ChemAlgebra: Algebraic Reasoning on Chemical Reactions"},{"paperId":"e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a","title":"Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought"},{"paperId":"a5378175d31d3dd8fa004037df663aa00f236a0b","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks"},{"paperId":"7ce0c89a452e3c2917b63847495533865697c79c","title":"Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts"},{"paperId":"f643aae4d8a8b17b7704fa310d3c15894806e9b3","title":"What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"},{"paperId":"60240322dd39ad4c22fed2dc884e65a20e9e61f6","title":"Symbols and mental programs: a hypothesis about human singularity"},{"paperId":"4721dd8c4f2681e231040cc5deebdbf938f58392","title":"Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions"},{"paperId":"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","title":"Language models show human-like content effects on reasoning"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7","title":"The Curious Case of Control"},{"paperId":"6a483cd1cbecd66150c9bbcd01606723950281bc","title":"Prototypical Calibration for Few-shot Learning of Language Models"},{"paperId":"146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd","title":"Data Distributional Properties Drive Emergent In-Context Learning in Transformers"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"c37d0b258386293097fa3f71f971dc5dfceb4684","title":"Data Contamination: From Memorization to Exploitation"},{"paperId":"cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e","title":"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models"},{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"8a920123a7c435e60b0912b6a49de40bfefd967f","title":"ClueReader: Heterogeneous Graph Attention Network for Multi-Hop Machine Reading Comprehension"},{"paperId":"a2087860d3a8948ba4b8290f8c9394eb59ee899e","title":"Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"d41a31717e45b8f4712ccf4faad825c15580250c","title":"L ANGUAGE M ODELS A RE G REEDY R EASONERS : A S YSTEMATIC F ORMAL A NALYSIS OF C HAIN - OF - T HOUGHT"},{"paperId":"79dd9e62f0e3ba35ce54d98b32750933e91a972a","title":"Representations and Computations in Transformers that Support Generalization on Structured Tasks"},{"paperId":null,"title":"MIND’S EYE: GROUNDED LANGUAGE MODEL REA-"},{"paperId":"414a476f83634e3b452b243ed7460c9ef3d1aaa4","title":"Benchmarking Progress to Infant-Level Physical Reasoning in AI"},{"paperId":"a8394722079c251d360fbbc53137753849a64c63","title":"Careful Data Curation Stabilizes In-context Learning"},{"paperId":"5c8cb5d39aa8531fcb5962821ac404e4a0e109d1","title":"Is It Smaller Than a Tennis Ball? Language Models Play the Game of Twenty Questions"},{"paperId":"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","title":"20Q: Overlap-Free World Knowledge Benchmark for Language Models"},{"paperId":"7b0f98f51040700aae3cd9f0e3432dedcd69fb30","title":"When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories"}],"references":[{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"4656cd01ab3c47117ecc87f63ca80498f2fc9aae","title":"Counterfactual Memorization in Neural Language Models"},{"paperId":"3962f108081b22c7e54b413f47ba6f2c16f2cc05","title":"Frequency Effects on Syntactic Rule Learning in Transformers"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"4291fe672cf6dc73e237ca0942fa49beb8c98711","title":"Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP"},{"paperId":"7571ed4cf1bbdcf891b576a0da12c910b1f0c72f","title":"Concealed Data Poisoning Attacks on NLP Models"},{"paperId":"5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec","title":"What’s in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus"},{"paperId":"1adadbfa95e43a70fcd17e6ce947a0652b86bfc3","title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus"},{"paperId":"023fc86c932fbc36702a6ad11c94ba419e1d8d88","title":"Competency Problems: On Finding and Removing Artifacts in Language Data"},{"paperId":"4e00843bc5f60d2b9116abc4320af6d184422291","title":"Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"9eea59c34f139f3d2153226c8cf026e975622074","title":"Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"b68b2e81ae2de647394ec05ee62ecf108bf2b50a","title":"Eliciting Knowledge from Language Models Using Automatically Generated Prompts"},{"paperId":"cc3725e66aa600eb12b244e82880f8e0bd225065","title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"9f7e317c6ef0bb15aacc9b19f0f0d00fee6c9a36","title":"What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation"},{"paperId":"cb58542c94ce83b09f5d3809e69518ba52709c92","title":"Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets"},{"paperId":"6f2b90ee5a0feea87264148c25a874f84bae20a0","title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"d0fcdf47561ff742c9a72495102f16646eca43b7","title":"We Need To Talk About Random Splits"},{"paperId":"774319233a107a29622003a115aa6c79f4a7b37f","title":"Probing Neural Language Models for Human Tacit Assumptions"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"68c7a76c8bbab5a3e5901ed03ac084c3e0b5d4b3","title":"Know thy Corpus! Robust Methods for Digital Curation of Web corpora"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"72a1d0256b38dea6c3e7d10a63eacc51abdc96da","title":"End-to-End Bias Mitigation by Modelling Biases in Corpora"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"f98e135986414cccf29aec593d547c0656e4d82c","title":"Commonsense Knowledge Mining from Pretrained Models"},{"paperId":"94befec2a6d96e3a60fb8b77f2e161666743c1a5","title":"We Need to Talk about Standard Splits"},{"paperId":"0ada4f23141e774dcac63cb8569730ddacda75c4","title":"Does learning require memorization? a short tale about a long tail"},{"paperId":"2e282bca209a3bad3ee0e35ea03b24056af7975c","title":"What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"ffca61ff581efb26d52be9189ba028ccb10eccd1","title":"Auditing Data Provenance in Text-Generation Models"},{"paperId":"1d97b7aaf42c798225283987a1cd7df09c5d9b31","title":"What Makes Reading Comprehension Questions Easier?"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6","title":"Hypothesis Only Baselines in Natural Language Inference"},{"paperId":"9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7","title":"Gender Bias in Coreference Resolution"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for datasets"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"520ec00dc35475e0554dbb72f27bd2eeb6f4191d","title":"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks"},{"paperId":"ffb949d3493c3b2f3c9acf9c75cb03938933ddf0","title":"Adversarial Examples for Evaluating Reading Comprehension Systems"},{"paperId":"b1e20420982a4f923c08652941666b189b11b7fe","title":"A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task"},{"paperId":"779a29915f444540191ada00af3ce37cba2db1f7","title":"INAUGURAL ARTICLE by a Recently Elected Academy Member:Mental models and human reasoning"},{"paperId":"a84f4fe31fcfb4ad92c995dba0fc09ed8fe6a4f4","title":"Exploiting Machine Learning to Subvert Your Spam Filter"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX"}],"id":"833a2f1817cb9aeb292620454889cae78e26dda4","summary":"Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, the results raise the question of how much models actually generalize beyond pretraining data, and researchers are encouraged to take thepretraining data into account when interpreting evaluation results."},{"url":"https://www.semanticscholar.org/paper/320c1c6647a5b975c901347f71638c881888686b","title":"NQuAD: 70,000+ Questions for Machine Comprehension of the Numerals in Text","venue":"International Conference on Information and Knowledge Management","year":2021,"referenceCount":28,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Chung-Chi Chen,Hen-Hsen Huang,Hsin-Hsi Chen","citations":[{"paperId":"d1cbec486a54e846107db42eb6a3a89188c8b06a","title":"Improving Numeracy by Input Reframing and Quantitative Pre-Finetuning Task"}],"references":[{"paperId":"01f68682c88c23376dcbf194c846232cba5f28a6","title":"Distilling Numeral Information for Volatility Forecasting"},{"paperId":"6563251e69e4378c189d0a0c94d8d19508d552c8","title":"MathBERT: A Pre-Trained Model for Mathematical Formula Understanding"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"0772212420238b48233256a59912f8e6a31cde3d","title":"NumClaim: Investor's Fine-grained Claim Detection"},{"paperId":"69282df83b94bb1aed32f869fdeb8faf81610064","title":"FinBERT: A Pre-trained Financial Language Representation Model for Financial Text Mining"},{"paperId":"ac67ec5b985a30239926c3362fa45a9a03f017af","title":"Learning to Generate Correct Numeric Values in News Headlines"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"2a0fc534ebf1b22b5851f5b1fb781dd707f7c530","title":"Numeral Attachment with Auxiliary Tasks"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"bab85059937299a18919b6580feee2c4398e5665","title":"Bridging Quantities in Tables and Text"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting"},{"paperId":"2be63d6b7dc137aa202907a471243e91deab258d","title":"Textual Analogy Parsing: What’s Shared and What’s Compared among Analogous Facts"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"d3406f220c16a20b877f10f5609fa589b629cc91","title":"Leveraging Context Information for Natural Question Generation"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"eedc76b6754e3cc17373dfac604e82ab385460a9","title":"MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge"},{"paperId":"eac21b22b2bba3a2311820c3f98702fa1d380ad5","title":"Question Generation for Question Answering"},{"paperId":"636a79420d838eabe4af7fb25d6437de45ab64e8","title":"RACE: Large-scale ReAding Comprehension Dataset From Examinations"},{"paperId":"564257469fa44cdb57e4272f85253efb9acfd69d","title":"MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"2db6b8e36bb5a3594b2200b2d4f931268d6279db","title":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"paperId":"8ce1512e77fa6a646513a60d78e0081afe870c07","title":"Dataset for the First Evaluation on Chinese Machine Reading Comprehension"}],"id":"320c1c6647a5b975c901347f71638c881888686b","summary":"A Numeral-related Question Answering Dataset, NQuAD, for fine-grained numeracy, is presented, and several baselines for future works are proposed and it is shown that N QuAD is more challenging than the numeral- related questions in other datasets."},{"url":"https://www.semanticscholar.org/paper/40a1f266bb5ca853837355bfff272a55f0049c81","title":"Mining Numbers in Text: A Survey","venue":"Information Systems","year":2021,"referenceCount":47,"citationCount":2,"influentialCitationCount":0,"publicationDate":"06/07/2021","authors":"Minoru Yoshida,K. Kita","citations":[{"paperId":"e605f32b628b3a36b72cab0e9189aacac0b4f780","title":"Text2Struct: A Machine Learning Pipeline for Mining Structured Data from Text"},{"paperId":null,"title":"Microsoft Word-Zhou-Text2Struct-Final.docx"}],"references":[{"paperId":"55596cdc41004c50505800aa427ee9d7d2a8c1e0","title":"Learning Numeral Embedding"},{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"4c9a8caf940627126aaa9bd3ac813d07065c86a0","title":"Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"2a0fc534ebf1b22b5851f5b1fb781dd707f7c530","title":"Numeral Attachment with Auxiliary Tasks"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1","title":"Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"dfe50d03bcd8377a03e0ea642146476828ea65c9","title":"Table Topic Models for Hidden Unit Estimation"},{"paperId":"c825b92effcf400ab1679071cef8233a5a6a18ba","title":"How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"1c3884e43bba39c50e6172c403c057659ef3ce83","title":"Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"bc51700cfae5ad693530331875b04ab4b62fc03e","title":"Open-domain quantity queries on web tables: annotation, response, and consensus models"},{"paperId":"ea68d467d08252c8f9002512dce6dfae360e5157","title":"Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web"},{"paperId":"ff6a34e67bb78105ed599d375932e0f580bad384","title":"SCAD: collective discovery of attribute values"},{"paperId":"6c93b11ce414776553ed5cb9eb5d3cffa2a5fc6b","title":"Extraction and Approximation of Numerical Attributes from the Web"},{"paperId":"d4ad37d742179fb6ad7fe4852f4d05faf1690aac","title":"Mining Numbers in Text Using Suffix Arrays and Clustering Based on Dirichlet Process Mixture Models"},{"paperId":"c25e24384cfa66659db8803428c6ba02a157137e","title":"UAIC: Participation in CLEF-IP Track"},{"paperId":"c0873cfd6c19e903b139bc6f465d06da5972aece","title":"Learning to rank for quantity consensus queries"},{"paperId":"217499e1559ddac51ca882a0d2f5a7e5eb5d2b2c","title":"UTH: SVM-based Semantic Relation Classification using Physical Sizes"},{"paperId":"4d1e1b15ba023d3a033c02fa34ca0e893b709696","title":"Numerical Data Integration for Cooperative Question-Answering"},{"paperId":"94271aa76b145af14be87f2c8ee8b9075f17a349","title":"Do Language Embeddings capture Scales?"},{"paperId":"d70aedeafc16123085b1ac08b4b94288222942d6","title":"Estimating Numerical Attributes by Bringing Together Fragmentary Clues"},{"paperId":"d89d4a1fc9ce7003cbc18ca835860c114e78d5ae","title":"Syntactic Difference Based Approach for NTCIR-9 RITE Task"},{"paperId":"97cd342bee2377756f003eec0508f0a710225012","title":"UAIC Participation at RTE-7"},{"paperId":"47790162c24c6bfbd856d0398365a338848a9a6d","title":"UAIC Participation at RTE4"},{"paperId":null,"title":"Information Systems - Intelligent Information Processing Systems, Natural Language Processing."},{"paperId":null,"title":"Hovy : EQUATE : A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":null,"title":"Mihai Alex Moruz: UAIC Participation at RTE-6"},{"paperId":null,"title":"Table Topic 13 Mining Numbers in Text: A Survey DOI: http://dx.doi.org/10.5772/intechopen.98540 Models for Hidden Unit Estimation"},{"paperId":null,"title":"Xiang Ren: Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models"}],"id":"40a1f266bb5ca853837355bfff272a55f0049c81","summary":"A quick overview of the history and recent advances of the research of mining such relations between numerals and words found in text data is provided."},{"url":"https://www.semanticscholar.org/paper/31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":20,"citationCount":12,"influentialCitationCount":1,"publicationDate":2021,"authors":"Avijit Thawani,J. Pujara,F. Ilievski","citations":[{"paperId":"d96e1c2a0b66effa2b00ef117c0df3fcb2cd9928","title":"Comparing scalable strategies for generating numerical perspectives"},{"paperId":"fc097b3dfa6b8dfbcf7275e0014a2a9e39704b2d","title":"Image-to-Text Translation for Interactive Image Recognition: A Comparative User Study with Non-Expert Users"},{"paperId":"be8f769f135b0ffcf829594b9421788781f38008","title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts"},{"paperId":"1f84b6075ffa2d4ed685f31668ea1324bfba4fe5","title":"A Survey on Medical Document Summarization"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"efcd9c1438559d908efd702333232078fd251a0f","title":"Phenotyping in clinical text with unsupervised numerical reasoning for patient stratification"},{"paperId":"cbccb1201eee6432020276762a44ebfbf8f981a0","title":"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"4ef578869c957f5fb969fa9c164dca0433f48042","title":"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining"},{"paperId":"c97bcf9e605938c2bef54f12b2156c678f1a2373","title":"Introducing Semantic Information for Numerical Attribute Prediction over Knowledge Graphs"},{"paperId":"ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","title":"Numerical Correlation in Text"},{"paperId":"b16d916ce411be0b1c7b6139317c652927984f46","title":"Tokenization on the Number Line is All You Need"}],"references":[{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"55596cdc41004c50505800aa427ee9d7d2a8c1e0","title":"Learning Numeral Embedding"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"70c0bbece96ab013050e10ece778ef857858bde6","title":"Wikicorpus: A Word-Sense Disambiguated Multilingual Wikipedia Corpus"},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"09437cba3abec8d7befd5358c0049281eb2afd69","title":"Dataset for Evaluation of Mathematical Reasoning Abilities in Russian"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Methods for numeracypreserving word embeddings"},{"paperId":null,"title":"Pytorch lightning"},{"paperId":null,"title":"English gigaword"}],"id":"31699d03a49e38295298f1b1a53185644abba12e","summary":"A significant improvement in MWP for sentences containing numbers is found, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers."},{"url":"https://www.semanticscholar.org/paper/24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context","venue":"NAACL-HLT","year":2021,"referenceCount":33,"citationCount":4,"influentialCitationCount":1,"publicationDate":"16/12/2021","authors":"Daniel M. Spokoyny,Ivan Lee,Zhao Jin,Taylor Berg-Kirkpatrick","citations":[{"paperId":"ecf0cb0725de18659f9ba25a8cf65a1085564006","title":"Understanding Arithmetic Reasoning in Language Models using Causal Mediation Analysis"},{"paperId":"be8f769f135b0ffcf829594b9421788781f38008","title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","title":"Numerical Correlation in Text"}],"references":[{"paperId":"7aacad59c63520b234923fc25eb14146c788f3f9","title":"Solving Price Per Unit Problem Around the World: Formulating Fact Extraction as Question Answering"},{"paperId":"3ceca3cc39cd90515ccd73afe6539cc953762140","title":"How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"58df3ea12952d35980ce2f7145c8d4669be343a5","title":"Learning to Reason for Text Generation from Scientific Tables"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"99f1898b9466fcf3f13061402a6ac70bb9f42c10","title":"Qsearch: Answering Quantity Queries from Text"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96","title":"TabFact: A Large-scale Dataset for Table-based Fact Verification"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1","title":"Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"c825b92effcf400ab1679071cef8233a5a6a18ba","title":"How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"bc51700cfae5ad693530331875b04ab4b62fc03e","title":"Open-domain quantity queries on web tables: annotation, response, and consensus models"},{"paperId":"2e83b8712b3c1a6c13d99148612752a9e14e9054","title":"“Ask Not What Textual Entailment Can Do for You...”"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":null,"title":"Reasoning about quantities in nat - 575 ural language “ ask not what textual entailment can 579 do for you . . . ”"},{"paperId":null,"title":"2021a), the units in WiCo are heavily biased"},{"paperId":null,"title":"2021a) constructed WiCo with the intent that it be used to further numeracy NLP research"}],"id":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","summary":"This work introduces a novel task, Masked Measurement Prediction (MMP), where a model learns to reconstruct a number 010 together with its associated unit given masked 011 text and introduces a new GeMM model that jointly learns to predict numbers along with 017 their units."},{"url":"https://www.semanticscholar.org/paper/b16d916ce411be0b1c7b6139317c652927984f46","title":"Tokenization on the Number Line is All You Need","venue":"","year":2021,"referenceCount":10,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"","citations":[],"references":[{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":null,"title":"Japanese 346 and korean voice search"},{"paperId":null,"title":"320 Injecting numerical reasoning skills into language 321 models How much coffee was consumed during 328 EMNLP 2019 ? fermi problems : A new reason - 329 ing challenge for AI"},{"paperId":null,"title":"BERT : Pre - training of 305 deep bidirectional transformers for language under - 306 standing"},{"paperId":null,"title":"Szekely , and Filip 363 Ilievski . 2021 b . Representing numbers in NLP : a 364 survey and a vision"},{"paperId":null,"title":"Have 336 you seen that number ? investigating extrapolation 337 in question answering models"},{"paperId":null,"title":"2019. BERT: Pre-training"},{"paperId":null,"title":"Neural machine translation of rare words 351 with subword units"},{"paperId":null,"title":"The number sense: How the"},{"paperId":null,"title":"2020 . 296 An empirical investigation of contextualized number 297 prediction"}],"id":"b16d916ce411be0b1c7b6139317c652927984f46","summary":"A carefully designed tokenization scheme is found to be both the simplest to implement and sufﬁcient to implement, and changes at the 032 tokenization level achieve near state-of-the-art results while requiring minimal resources compared to other number representation schemes."},{"url":"https://www.semanticscholar.org/paper/c2d50f17ea6769f6f5663ccac37a9627a0543184","title":"I NVESTIGATING THE L IMITATIONS OF T RANSFORM ERS WITH S IMPLE A RITHMETIC T ASKS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","citations":[],"references":[],"id":"c2d50f17ea6769f6f5663ccac37a9627a0543184","summary":"It is concluded that modern pretrained language models can easily learn arithmetic from very few examples, as long as they use the proper surface representation, which bolsters evidence that subword tokenizers and positional encodings are compo-nents in current transformer designs that might need improvement."},{"url":"https://www.semanticscholar.org/paper/108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models","venue":"MATHNLP","year":2022,"referenceCount":65,"citationCount":5,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"M. Bueno,Carlos Gemmel,Jeffrey Stephen Dalton,R. Lotufo,Rodrigo Nogueira","citations":[{"paperId":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers"},{"paperId":"6f6e2e0311589a9af045f6acd00b7dee6d19fce4","title":"The Impact of Positional Encoding on Length Generalization in Transformers"},{"paperId":"4d7c4c371d9742d3f2d9c338bfb898fc856e7349","title":"Faith and Fate: Limits of Transformers on Compositionality"},{"paperId":"80b99ed94e991a941ffeabf74826a9ee8c995d6d","title":"Generate, Transform, Answer: Question Specific Tool Synthesis for Tabular Data"},{"paperId":"988ca520327c2ead08d29c4907130b8a8833d769","title":"ExaRanker: Explanation-Augmented Neural Ranker"}],"references":[{"paperId":"07955e96cbd778d0ae2a68f09d073b866dd84c2a","title":"Decomposed Prompting: A Modular Approach for Solving Complex Tasks"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"37e2c7c89325a1a4685a46ff830fe7ecca8f1f80","title":"Learning to Scaffold: Optimizing Model Explanations for Teaching"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"28e89e57092634fefd25bd764c432c5645bbfe3e","title":"STaR: Bootstrapping Reasoning With Reasoning"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"10c86505de83647c7b4157595ab10f64e97c94ef","title":"On the Ability and Limitations of Transformers to Recognize Formal Languages"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"e8984c6e6c24aab26c332728a5fff616dfb3adbb","title":"Learning to Encode Position for Transformer with Continuous Dynamical Model"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"136396e988dbbdaca433096e931b22732c3d95dd","title":"Text2Math: End-to-end Parsing Text into Math Expressions"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"5435f997d98754f68492334eeb87d027047e60cb","title":"Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"5e2bb96c47ccaa16a4e7192e8fadb3b3e1c3acdc","title":"Deep Learning: A Critical Appraisal"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7","title":"Modular Multitask Reinforcement Learning with Policy Sketches"},{"paperId":"3058d4a4fa6ebaac30e0279c6b8c30e5f969d315","title":"Extensions and Limitations of the Neural GPU"},{"paperId":"75ddc7ee15be14013a3462c01b38b0548486fbcb","title":"Learning to Compose Neural Networks for Question Answering"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"5b791cd374c7109693aaddee2c12d659ae4e3ec0","title":"Grid Long Short-Term Memory"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"08dc7b19e679539f0f93db0192a8e8d11538b3dd","title":"Rethinking Eliminative Connectionism"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":"2f21201ac9fcb88a72c56471402388ec2fc365a8","title":"Inferring Implicit Relations in Complex Questions with Language Models"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":"5414be4432fcfe1e83932127ee2463d107e5b61c","title":"A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"},{"paperId":"087d5ea60b0b15fb9d3396ad321a7f941f88e720","title":"Transfer of learning by composing solutions of elemental sequential tasks"},{"paperId":"30110856f45fde473f1903f686aa365cf70ed4c7","title":"Learning Context-free Grammars: Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory (cid:3)"},{"paperId":"550e406b84e46cf8c77baa70a61704e93fd963bc","title":"Towards Compositional Learning in Dynamic NetworksTechnical Report"},{"paperId":null,"title":"and Sutskever"},{"paperId":null,"title":"and Mikolov"},{"paperId":null,"title":"and Berant"}],"id":"108c25905be36b2a7a0fc7256ac314985ecd9699","summary":"This work demonstrates that large language models can succeed in extrapolation without modifying their architecture or training procedure, and shows how generating step-by-step rationales and introducing marker tokens are both required for effective extrapolation."},{"url":"https://www.semanticscholar.org/paper/c97bcf9e605938c2bef54f12b2156c678f1a2373","title":"Introducing Semantic Information for Numerical Attribute Prediction over Knowledge Graphs","venue":"International Workshop on the Semantic Web","year":2022,"referenceCount":53,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Bingcong Xue,Yanzeng Li,Lei Zou","citations":[{"paperId":"3a7e223cf580b61fdc5a92b3c34e91ca5696ab0c","title":"Attribute Enhancement using Aligned Entities between Knowledge Graphs"},{"paperId":"68684537f964262d207ba8454d7e35e1a9b5c52e","title":"AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction"}],"references":[{"paperId":"ae5b587fb5ff55c074a770acf81c27d9d3046748","title":"Knowledge Graph Quality Management: A Comprehensive Survey"},{"paperId":"2a561c9650d27218054aa2c87474f0121ba3f33b","title":"SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models"},{"paperId":"e468efa59a621f49709ded07ba18ca7751ab91e5","title":"PGE: Robust Product Graph Embedding Learning for Error Detection"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"75f1c7dadb4ed733fb4d3a4cc47b9cbde9ad98cc","title":"Combining pre-trained language models and structured knowledge"},{"paperId":"b1e6aa78db5478be5eaa47697382241c2b7aab1f","title":"Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models"},{"paperId":"a9027974b23e5208baabd3f9d78fbc10e9e3c395","title":"Node Attribute Completion in Knowledge Graphs with Multi-Relational Propagation"},{"paperId":"5b2e955de772ae4d128b8a78ebff57982a6ca06d","title":"Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83","title":"LibKGE - A knowledge graph embedding library for reproducible research"},{"paperId":"7b12198a77972508689a904c544010b2a406ed20","title":"YAGO 4: A Reason-able Knowledge Base"},{"paperId":"5bb28ae9adf604c5acc56fd51d0be3ea392048ce","title":"The Value of Paraphrase for Knowledge Base Predicates"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"80376bdec5f534be78ba82821f540590ebce5559","title":"How Much Knowledge Can You Pack into the Parameters of a Language Model?"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"cda5af1f5516a6920f818714ec570952101afe07","title":"A Survey on Knowledge Graph Embeddings with Literals: Which model links better Literal-ly?"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"bfeb827d06c1a3583b5cc6d25241203a81f6af09","title":"Knowledge Enhanced Contextual Word Representations"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"e1b41796b752d6fb6032bbad2f8998302209d79b","title":"A survey on ensemble learning"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"73f303a28d4abcd48593274a90aeecb05085fa52","title":"Learning Numerical Attributes in Knowledge Bases"},{"paperId":"d593a5830a7e7d84443473c3912b59165056d45a","title":"MMKG: Multi-Modal Knowledge Graphs"},{"paperId":"8f096071a09701012c9c279aee2a88143a295935","title":"RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space"},{"paperId":"bca4a782116e663dfd0119b6176a3c228c651bda","title":"Embedding Multimodal Relational Data for Knowledge Base Completion"},{"paperId":"71245f9d9ba0317f78151698dc1ddba7583a3afd","title":"Knowledge Graph Embedding with Numeric Attributes of Entities"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"3448e6a5039417dc1ae890efeca3bef5390ace7c","title":"xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems"},{"paperId":"995f33b2359966dd364d2cf4bcf5e8d339cfecce","title":"Incorporating Literals into Knowledge Graph Embeddings"},{"paperId":"c14347fa745a1f113fdbe8bf1c5ccfb71b5da296","title":"KBlrn: End-to-End Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features"},{"paperId":"729fbc6664890c4e78cbcb2686a9b5e895255485","title":"Multi-Task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs"},{"paperId":"2218e2e1df2c3adfb70e0def2e326a39928aacfc","title":"Complex Embeddings for Simple Link Prediction"},{"paperId":"96acb1c882ad655c6b8459c2cd331803801446ca","title":"Representation Learning of Knowledge Graphs with Entity Descriptions"},{"paperId":"adcfebbe2a1960e5a23243d1a5f3837832109ff1","title":"Distributional vectors encode referential attributes"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","title":"Wikidata: a free collaborative knowledgebase"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"f6764d853a14b0c34df1d2283e76277aead40fde","title":"A Three-Way Model for Collective Learning on Multi-Relational Data"},{"paperId":"168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74","title":"Scikit-learn: Machine Learning in Python"},{"paperId":"6c93b11ce414776553ed5cb9eb5d3cffa2a5fc6b","title":"Extraction and Approximation of Numerical Attributes from the Web"},{"paperId":"e3034935bf12190465f7dbee1c05d74abbbd7767","title":"End-to-End Learning on Multimodal Knowledge Graphs"},{"paperId":"8c93f3cecf79bd9f8d021f589d095305e281dd2f","title":"Knowledge Graph Embedding for Link Prediction: A Comparative Analysis"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":"9aca07d7688e87e996c4a117fc28e200e469b820","title":"Leveraging Literals for Knowledge Graph Embeddings"},{"paperId":"d0d9a32181e3e4b68e7c32d9ffbef86c12d9609b","title":"Leveraging Multilingual Descriptions for Link Prediction: Initial Experiments"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"d2946a868682e4141beabc288d79253ae254c6e1","title":"DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia"},{"paperId":null,"title":"Do language embeddings capture scales? arXiv preprint arXiv:2010.05345"}],"id":"c97bcf9e605938c2bef54f12b2156c678f1a2373","summary":"This paper re-examines the numerical attribute prediction task over KGs, and introduces several novel methods to explore and utilize the rich semantic knowledge of language models (LMs) for this task."},{"url":"https://www.semanticscholar.org/paper/ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","title":"Numerical Correlation in Text","venue":"MATHNLP","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Daniel M. Spokoyny,Chien-Sheng Wu,Caiming Xiong","citations":[],"references":[{"paperId":"34503c0b6a615124eaf82cb0e4a1dab2866e8980","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"72c447f305265f8c030a2736a35fd1a3e8dae505","title":"Improving Downstream Task Performance by Treating Numbers as Entities"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"5f277d2605a272c321615373254097939aae4fac","title":"“When Numbers Matter!”: Detecting Sarcasm in Numerical Portions of Text"},{"paperId":"f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1","title":"Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"}],"id":"ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","summary":"A new task, Numerical Correlation in Text, which requires models to identify the correlation between two numbers in a sentence is proposed and a new dataset is introduced, which contains over 2,000 Wikipedia sentences with two numbers and their correlation labels."},{"url":"https://www.semanticscholar.org/paper/0b7e9b6b588baa3f24fdde06feec26a067aa74bd","title":"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":63,"citationCount":26,"influentialCitationCount":4,"publicationDate":"03/06/2022","authors":"Yilun Zhao,Yunxiang Li,Chenying Li,Rui Zhang","citations":[{"paperId":"520628a4d5b0d609586292c78871ab6b9504a501","title":"Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset"},{"paperId":"beeaa3b353f0afeb2387f8336c052042a9b78cc0","title":"QTSumm: A New Benchmark for Query-Focused Table Summarization"},{"paperId":"338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c","title":"STOAT: Structured Data to Analytical Text With Controls"},{"paperId":"0ea6b7371017721d87f9c5b32b084bd1ca762532","title":"S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering"},{"paperId":"a39f960b28a627d554712a457efa5d3e3b7f8406","title":"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning"},{"paperId":"5bcb7579011ec43b5a60a587f4434c756eddd85d","title":"Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question"},{"paperId":"0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","title":"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs"},{"paperId":"2504035356a92cf2c8ad2beafd361719ac8aa6da","title":"SoarGraph: Numerical Reasoning over Financial Table-Text Data via Semantic-Oriented Hierarchical Graphs"},{"paperId":"6fd4e1c761b32e7f22e6377d7b97d272e4b05831","title":"cTBLS: Augmenting Large Language Models with Conversational Tables"},{"paperId":"05c3a61c5106793dbc0ca4c6d81be888234a13d8","title":"LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control"},{"paperId":"62f7f806a0e914c360f0640d247f4df1974c1f3b","title":"Semi-Structured Object Sequence Encoders"},{"paperId":"1278dc2e5077b9a0fe5266ae6850a59d8231b41f","title":"A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"9cc5c25517c3a78183a052e8e93a44e85bb17432","title":"Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations"},{"paperId":"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","title":"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data"},{"paperId":"4533f5afa772b0b25703a7258993a7c28a85f9a3","title":"NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering"},{"paperId":"e1801f68d42f4a1a96d580b73706733883bf6af8","title":"ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"d84643c9f0289df5f9373480025da8f76f6df79c","title":"Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder"},{"paperId":"52b3087525b262f6f467453e22fdfa843353d40c","title":"TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data"},{"paperId":"117e1323677cb5d78ece0fd07b5cfa81618f4866","title":"Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning"},{"paperId":"1aa2b8d2a2dd685b8d5318eb20cffee093412603","title":"IM-TQA: A Chinese Table Question Answering Dataset with Implicit and Multi-type Table Structures"},{"paperId":"a0ee62ff9a6a00d29634a5146f9ffb91ec730e32","title":"OpenRT: An Open-source Framework for Reasoning Over Tabular Data"},{"paperId":"19856469e29d13b63adde4d198d4785a65204997","title":"cTBL: Augmenting Large Language Models for Conversational Tables"},{"paperId":"834fd676114c0b6e077c735ddffd9fa14ee856d2","title":"Improving compositional generalization for multi-step quantitative reasoning in question answering"},{"paperId":"cc8417aa578016203cb52efc63592bba64b08bb3","title":"FinMath: Injecting a Tree-structured Solver for Question Answering over Financial Reports"}],"references":[{"paperId":"2e43501a14b831744999355c177321709659aff1","title":"TableFormer: Robust Transformer Modeling for Table-Text Encoding"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"927efd299cffcfca3716efefcc904331b70c153e","title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset"},{"paperId":"4ef578869c957f5fb969fa9c164dca0433f48042","title":"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"a1364257028332760208827cd0c7af08d91e058b","title":"HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"d365978adf0a5c9c6028820857e015617856256b","title":"MultiModalQA: Complex Question Answering over Text, Tables and Images"},{"paperId":"388513e8e09ad60f619054361f4d2cdf5a146bc8","title":"FeTaQA: Free-form Table Question Answering"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"084c5afc5b16b0c50c53390f550a13f4ed4c7d3c","title":"TSQA: Tabular Scenario Based Question Answering"},{"paperId":"edaafec59b651d503ac7c8a86f8e2335273e1f7a","title":"Learning by Fixing: Solving Math Word Problems with Weak Supervision"},{"paperId":"24a12899ce97bd4a56f7c6b49d3979b9465f0190","title":"TUTA: Tree-based Transformers for Generally Structured Table Pre-training"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"65be695739d0fa35212e49ccccd129535e6d9e15","title":"Understanding tables with intermediate pre-training"},{"paperId":"3578a7792904e6af3db8ffefdff86ab6a387c7c3","title":"FinBERT: A Pretrained Language Model for Financial Communications"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"73a906a988e54defee536a120125f957059d595e","title":"Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context"},{"paperId":"27db72a2f643f9dfebc0cc2e8b98a9db307f0f07","title":"HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"52cb05d721688cb766c6e282e9d55c3b8e3dc0cf","title":"TaPas: Weakly Supervised Table Parsing via Pre-training"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"55a3b36fd21dbbe9384ab3ba1bcf901235d95f47","title":"Unsupervised Question Decomposition for Question Answering"},{"paperId":"f352298f760c710a6021a66c549f7c65f0866eb7","title":"A Time Series Analysis of Emotional Loading in Central Bank Statements"},{"paperId":"26f8c13adc064a3833437d22ebfe3f89412b9760","title":"Forecasting Firm Material Events from 8-K Reports"},{"paperId":"6bb20fe9099fbee39df252fbff1289d0a8ec3cf2","title":"Complaint Analysis and Classification for Economic and Food Safety"},{"paperId":"b393ece4c0e7f8ff696b10b952a9fa420b419a05","title":"Financial Event Extraction Using Wikipedia-Based Weak Supervision"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"ba783d92d0eaf6a7bff6ced7660150ce38016bbc","title":"Don’t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases"},{"paperId":"7102bb3fe73bd057ff161d9db5214a267c1ef312","title":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"},{"paperId":"5e11a9817db945e4bd965b26fc7060b4afe5574d","title":"A framework for anomaly detection using language modeling, and its applications to finance"},{"paperId":"a77df5291ee644022067f34eec790ea31380792b","title":"Are You for Real? Detecting Identity Fraud via Dialogue Interactions"},{"paperId":"b285c067f2da04bf5647beb8853bfddf6d9b4e1b","title":"A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"0707f1e3791a6805bf4542605245cf4cdee3b9e0","title":"Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"0c4fc6680682910f5c0608c5475b323ac94cbb8c","title":"Deep learning models for bankruptcy prediction using textual disclosures"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"e0c66240239263f16159eef166a391d3939ae2d5","title":"How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks"},{"paperId":"1e75b03bce78e73b7d0e707912f394585262cff0","title":"Leveraging News Sentiment to Improve Microblog Sentiment Classification in the Financial Domain"},{"paperId":"f2d742704a32c7ebef768bd74d1438522269a4c2","title":"NextGen AML: Distributed Deep Learning based Language Technologies to Augment Anti Money Laundering Investigation"},{"paperId":"7191680b572ee7145f1a9d95ff11ab1ff44259f3","title":"WWW'18 Open Challenge: Financial Opinion Mining and Question Answering"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"cbd569036fc72ae7ff747350b91816440282596b","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"2d2b1ce365b879dc8f337c293b69e9c0d62bfc91","title":"Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base"},{"paperId":"d1505c6123c102e53eb19dff312cb25cea840b72","title":"Teaching Machines to Read and Comprehend"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"799b424118419cf7fc78ed1bc407e3af03aba01f","title":"Free-Marginal Multirater Kappa (multirater K[free]): An Alternative to Fleiss' Fixed-Marginal Multirater Kappa."},{"paperId":"0ce43def1c95802a464449f055e78c08d6d99dc2","title":"Towards Table-to-Text Generation with Numerical Reasoning"},{"paperId":"1893f9875fe6a5b40b82838aa3a4259f5763d7f0","title":"SciGen: a Dataset for Reasoning-Aware Text Generation from Scientific Tables"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8f90c243cd7d69c4c92cee1ac924e74592b8df99","title":"Utilizing Pre-Trained Word Embeddings to Learn Classification Lexicons with Little Supervision"},{"paperId":"238ecc4956656d4aaecfa79d8239dd8f10b3d83a","title":"Causality Analysis of Twitter Sentiments and Stock Market Returns"},{"paperId":"1778d275965921e75e93fda7bf40d922071682ff","title":"Word Embeddings-Based Uncertainty Detection in Financial Disclosures"},{"paperId":"dc64a09b1d58f65a6fc4784684a54bed6fe7857d","title":"A Simple End-to-End Question Answering Model for Product Information"}],"id":"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","summary":"A new large-scale benchmark, MultiHiertt, with QA pairs over Multi Hierarchical Tabular and Textual data is constructed and a novel QA model termed MT2Net is introduced, which first applies facts retrieving to extract relevant supporting facts from both tables and text and then uses a reasoning module to perform symbolic reasoning over retrieved facts."},{"url":"https://www.semanticscholar.org/paper/b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":19,"citationCount":10,"influentialCitationCount":1,"publicationDate":2021,"authors":"Jeonghwan Kim,Giwon Hong,Kyung-min Kim,Junmo Kang,Sung-Hyon Myaeng","citations":[{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"3cb4a3ebaf027852e5d185ccedc70990781bff72","title":"Empirical Investigation of Neural Symbolic Reasoning Strategies"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"4497f3ed54ac520b50ffa05df04b37a59d4c1265","title":"Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"cd54a839ba12e7417e321b4407788dc426ebaefc","title":"Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering"}],"references":[{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"},{"paperId":"a48b1b5390d7c8621c0dbb55d6e675da83a2027a","title":"Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning"},{"paperId":"2c0a266f9cb88bb914c138ece0deaab8cf528f78","title":"Neural Sequence-to-grid Module for Learning Symbolic Rules"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"42e863f93203d37a2518da381beaf06e4c70fb3d","title":"Stanza: A Python Natural Language Processing Toolkit for Many Human Languages"},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"2d08cdc45bbed8cea283d92132b4b88a507a7303","title":"Neural Module Networks for Reasoning over Text"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"30eff53e981695c7296d258b8dc44b4c7b482a0c","title":"A Discrete Hard EM Approach for Weakly Supervised Question Answering"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"b285c067f2da04bf5647beb8853bfddf6d9b4e1b","title":"A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Methods for numeracypreserving word embeddings"}],"id":"b59947541d2ac4211c4b17554b2e16c260299bed","summary":"This work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, and proposes the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text."},{"url":"https://www.semanticscholar.org/paper/58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models","venue":"ArXiv","year":2021,"referenceCount":30,"citationCount":11,"influentialCitationCount":1,"publicationDate":"07/09/2021","authors":"Zhihua Jin,Xin Jiang,Xingbo Wang,Qun Liu,Yong Wang,Xiaozhe Ren,Huamin Qu","citations":[{"paperId":"bc0ee1c62b7864afd211e085fb3d58263c84ccfe","title":"Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"efa9fc7d5b6b244d8ae3a9c3db98418ec39aa7a3","title":"Precision information extraction for rare disease epidemiology at scale"},{"paperId":"eb9b60db6832b00b9932584663bec104d6d415dd","title":"Tree-Based Representation and Generation of Natural and Mathematical Language"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"efcd9c1438559d908efd702333232078fd251a0f","title":"Phenotyping in clinical text with unsupervised numerical reasoning for patient stratification"},{"paperId":"cbccb1201eee6432020276762a44ebfbf8f981a0","title":"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"4ef578869c957f5fb969fa9c164dca0433f48042","title":"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining"},{"paperId":"c97bcf9e605938c2bef54f12b2156c678f1a2373","title":"Introducing Semantic Information for Numerical Attribute Prediction over Knowledge Graphs"}],"references":[{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"d5fcad8a3b183642fcf609519a4dbbda9c3541ff","title":"Learning Numeral Embeddings"},{"paperId":"6456d905a63d2741c6d896f59eb6372ceedcecce","title":"Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"f67ede490780f939103d4cc4e9c6866b83ee59b3","title":"Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"caeb950e503872a903e18a3b259424e3cc3c6006","title":"Neural Math Word Problem Solver with Reinforcement Learning"},{"paperId":"835c6b524b90b1639aba28742f7161137ddf4397","title":"MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"}],"id":"58c7a31bf47948d936de937b1cf7b49463608557","summary":"The experiment results show that NumGPT outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as measurement estimation, number comparison, math word problems, and magnitude classification."},{"url":"https://www.semanticscholar.org/paper/cd54a839ba12e7417e321b4407788dc426ebaefc","title":"Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering","venue":"NAACL-HLT","year":2022,"referenceCount":24,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jeonghwan Kim,Junmo Kang,Kyung-min Kim,Giwon Hong,Sung-Hyon Myaeng","citations":[{"paperId":"60be11b0c34038d9ee156cbec6c4df5ae5db68b8","title":"Pretrained Language Model based Web Search Ranking: From Relevance to Satisfaction"},{"paperId":"704332d287bc78ac95fea3d90ec3945e9cec9ab3","title":"A survey on complex factual question answering"},{"paperId":"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","title":"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data"},{"paperId":"4533f5afa772b0b25703a7258993a7c28a85f9a3","title":"NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering"}],"references":[{"paperId":"5d9ac727b4a4e1044b3ba464df3be66eb8568127","title":"Numerical reasoning in machine reading comprehension tasks: are we there yet?"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"238deab37e201c57505a4a47bb854e462af79bd7","title":"Entity-Based Knowledge Conflicts in Question Answering"},{"paperId":"3122a2d7799ba585b993e432b3deb47659b3f3c1","title":"Hurdles to Progress in Long-form Question Answering"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"a48b1b5390d7c8621c0dbb55d6e675da83a2027a","title":"Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"5b6d03ed66473599ee31872b3cd5ad2ce282371f","title":"Roles and Utilization of Attention Heads in Transformer-based Neural Language Models"},{"paperId":"b9a5aa5db8836744ff2073e8368520b7a614049f","title":"Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"42e863f93203d37a2518da381beaf06e4c70fb3d","title":"Stanza: A Python Natural Language Processing Toolkit for Many Human Languages"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"2d08cdc45bbed8cea283d92132b4b88a507a7303","title":"Neural Module Networks for Reasoning over Text"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Methods for numeracypreserving word embeddings"}],"id":"cd54a839ba12e7417e321b4407788dc426ebaefc","summary":"This work proposes a novel attention masked reasoning model, the NC-BERT, that learns to leverage the number-related contextual knowledge to alleviate the over-reliance on parametric knowledge and enhance the numerical reasoning capabilities of the QA model."}]}