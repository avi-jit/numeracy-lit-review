"id","url","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"c8efcc854d97dfc2a42b83316a2109f9d166e43f","https://www.semanticscholar.org/paper/c8efcc854d97dfc2a42b83316a2109f9d166e43f","Self-Attention with Relative Position Representations","This work presents an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements, on the WMT 2014 English-to-German and English- to-French translation tasks.","North American Chapter of the Association for Computational Linguistics",2018,"Peter Shaw,Jakob Uszkoreit,Ashish Vaswani",1269,17,176
"b626754a0fd7de12c87e88165b2484ac5d98212a","https://www.semanticscholar.org/paper/b626754a0fd7de12c87e88165b2484ac5d98212a","Decoupling Strategy and Generation in Negotiation Dialogues","A modular approach based on coarse dialogue acts (e.g., propose(price=50)) that decouples strategy and generation that can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy is proposed.","Conference on Empirical Methods in Natural Language Processing",2018,"He He,Derek Chen,Anusha Balakrishnan,Percy Liang",81,33,26
"cabb0a468af8184e0e930841435b65679b580521","https://www.semanticscholar.org/paper/cabb0a468af8184e0e930841435b65679b580521","Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting","This work is the first attempt to understand numerals in financial social media data, and it provides the first comparison of fine-grained opinion of individual investors and analysts based on their forecast price.","International Conference on Wirtschaftsinformatik",2018,"Chung-Chi Chen,Hen-Hsen Huang,Yow-Ting Shiue,Hsin-Hsi Chen",34,22,0
"81b4920ad488affaee27389ff9540b7fea90a4ce","https://www.semanticscholar.org/paper/81b4920ad488affaee27389ff9540b7fea90a4ce","“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding","It is found that the best current methods used on MCTACO are still far behind human performance, by about 20%, and several directions for improvement are discussed.","Conference on Empirical Methods in Natural Language Processing",2019,"Ben Zhou,Daniel Khashabi,Qiang Ning,D. Roth",109,37,18
"d88f31a0091eee02c5a2aa2013914818cdef114e","https://www.semanticscholar.org/paper/d88f31a0091eee02c5a2aa2013914818cdef114e","Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving","The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems, and incorporates Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure.","ArXiv",2019,"Imanol Schlag,P. Smolensky,Roland Fernandez,N. Jojic,J. Schmidhuber,Jianfeng Gao",40,28,1
"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","https://www.semanticscholar.org/paper/5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","oLMpics-On What Language Model Pre-training Captures","This work proposes eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition, and findings can help future work on designing new datasets, models, and objective functions for pre-training.","Transactions of the Association for Computational Linguistics",2019,"Alon Talmor,Yanai Elazar,Yoav Goldberg,Jonathan Berant",209,61,23
"e081dfa64e5ed12fe15fc885ec82ef1f35ab9830","https://www.semanticscholar.org/paper/e081dfa64e5ed12fe15fc885ec82ef1f35ab9830","iNALU: Improved Neural Arithmetic Logic Unit","An in-depth analysis reveals practical shortcomings by design, such as the inability to multiply or divide negative input values or training stability issues for deeper networks, and proposes an improved model architecture that solves stability issues and outperforms the original NALU model in means of arithmetic precision and convergence.","Frontiers in Artificial Intelligence",2020,"Daniel Schlör,Markus Ring,A. Hotho",9,24,0
"716efab73e1916fdc2a23727b581d200271ed499","https://www.semanticscholar.org/paper/716efab73e1916fdc2a23727b581d200271ed499","BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes","This model is able to predict a negotiation's outcome within 10% for more than 70% of the cases, and suggests that rather than just being a way to realize a negotiation, natural language should be incorporated in the negotiation planning as well.","ArXiv",2020,"Kushal Chawla,Gale M. Lucas,J. Gratch,Jonathan May",2,16,0
"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","https://www.semanticscholar.org/paper/4599f96dd1a2584e00d342953fc7e1361ffd6e1f","Neural Status Registers","The Neural Status Register is introduced, inspired by physical Status Registers, and at the heart of the NSR are arithmetic comparisons between inputs that allow end-to-end differentiation and learns such comparisons reliably.","ArXiv",2020,"Lukas Faber,Roger Wattenhofer",5,51,1
"45e5d7637a585a87d967a4a357d17c5d89aecea2","https://www.semanticscholar.org/paper/45e5d7637a585a87d967a4a357d17c5d89aecea2","A Formal Hierarchy of RNN Architectures","It is hypothesized that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy, and empirical results to support this conjecture are provided.","Annual Meeting of the Association for Computational Linguistics",2020,"William Cooper Merrill,Gail Weiss,Yoav Goldberg,Roy Schwartz,Noah A. Smith,Eran Yahav",32,28,1
"8a5590978930070f50c5f9fbf61f67e5d95794f0","https://www.semanticscholar.org/paper/8a5590978930070f50c5f9fbf61f67e5d95794f0","Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning","A dataset, Machine Number Sense (MNS), consisting of visual arithmetic problems automatically generated using a grammar model—And-Or Graph (AOG), called for attention in fusing the classic search-based algorithms with modern neural networks to discover the essential number concepts in future research.","AAAI Conference on Artificial Intelligence",2020,"Wenhe Zhang,Chi Zhang,Yixin Zhu,Song-Chun Zhu",17,74,2
"36d6c8895bbc755964b8b2136c6fd6087a7af089","https://www.semanticscholar.org/paper/36d6c8895bbc755964b8b2136c6fd6087a7af089","TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions","TORQUE is introduced, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships, and results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.","Conference on Empirical Methods in Natural Language Processing",2020,"Qiang Ning,Hao Wu,Rujun Han,Nanyun Peng,Matt Gardner,D. Roth",58,49,5
"016760dc4a05489ddf5dbb48aecbb49e214e1b71","https://www.semanticscholar.org/paper/016760dc4a05489ddf5dbb48aecbb49e214e1b71","Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models","Investigating whether and to what extent one can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process finds that this may not work for numerical Commonsense knowledge.","Conference on Empirical Methods in Natural Language Processing",2020,"Bill Yuchen Lin,Seyeon Lee,Rahul Khanna,Xiang Ren",85,31,11
"72e6b14c081ad3e6a1c295b60e5c834837e6b304","https://www.semanticscholar.org/paper/72e6b14c081ad3e6a1c295b60e5c834837e6b304","Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks","This work introduces NUMBERGAME, a multifaceted benchmark to evaluate model performance across numerical reasoning tasks of eight diverse formats, and takes forward the recent progress in generic system development, demonstrating the scope of under-explored tasks.","ArXiv",2020,"Swaroop Mishra,Arindam Mitra,Neeraj Varshney,Bhavdeep Singh Sachdeva,Chitta Baral",10,45,1
"c3d487ca75f26adadd94ba7af4b57aed8d661ff3","https://www.semanticscholar.org/paper/c3d487ca75f26adadd94ba7af4b57aed8d661ff3","Automatic Knowledge Acquisition for Object-Oriented Expert Systems","An Object Oriented Model for building Expert Systems is described and original algorithms which deal with total and partial structural similitude of objects to facilitate knowledge acquisition are proposed.","ArXiv",2020,"J. Colloc,D. Boulanger",1,9,0
"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","https://www.semanticscholar.org/paper/0bfd4ed399054eae26c3cdaabc0aed80ca95e125","Neural Power Units","The Neural Power Unit (NPU) is introduced that operates on the full domain of real numbers and is capable of learning arbitrary power functions in a single layer and fixes the shortcomings of existing arithmetic units and extends their expressivity.","Neural Information Processing Systems",2020,"Niklas Heim,T. Pevný,V. Šmídl",6,33,3
"63857190aaf5aab1d94b54bb257b7b03b8cb5a50","https://www.semanticscholar.org/paper/63857190aaf5aab1d94b54bb257b7b03b8cb5a50","GMAT: Global Memory Augmentation for Transformers","This work proposes to augment sparse Transformer blocks with a dense attention-based $\textit{global memory}$ of length $M$ ($\ll L$) which provides an aggregate global view of the entire input sequence to each position, and empirically shows that this method leads to substantial improvement on a range of tasks.","ArXiv",2020,"Ankit Gupta,Jonathan Berant",25,30,3
"79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88","https://www.semanticscholar.org/paper/79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88","Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge","This work provides a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements, and demonstrates that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting.","Neural Information Processing Systems",2020,"Alon Talmor,Oyvind Tafjord,Peter Clark,Yoav Goldberg,Jonathan Berant",76,44,10
"8256f48f759cf85044db251cc512f965834945b3","https://www.semanticscholar.org/paper/8256f48f759cf85044db251cc512f965834945b3","Rethinking Positional Encoding in Language Pre-training","This work investigates the problems in the previous formulations and proposes a new positional encoding method for BERT called Transformer with Untied Positional Encoding (TUPE), which can achieve a higher score than baselines while only using 30% pre-training computational costs.","International Conference on Learning Representations",2020,"Guolin Ke,Di He,Tie-Yan Liu",142,34,22
"51c62d63c6204deecb24a1d3f9ea8e0a42d23817","https://www.semanticscholar.org/paper/51c62d63c6204deecb24a1d3f9ea8e0a42d23817","Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces","This paper formally defines the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku, and proposes an RL based approach to jointly train the selection module with the prediction network.","International Conference on Learning Representations",2020,"Yatin Nandwani,Deepanshu Jindal,Mausam,Parag Singla",8,31,0
"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","https://www.semanticscholar.org/paper/0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","Probing for Multilingual Numerical Understanding in Transformer-Based Language Models","Novel multilingual probing tasks tested on DistilBERT, XLM, and BERT find evidence that the information encoded in these pretrained models’ embeddings is sufficient for grammaticality judgments but generally not for value comparisons.","BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",2020,"Devin J. Johnson,Denise Mak,Drew Barker,Lexi Loessberg-Zahl",10,9,0
"a20712b1b9779ee43ce143a19b3f67f0cacbbf57","https://www.semanticscholar.org/paper/a20712b1b9779ee43ce143a19b3f67f0cacbbf57","Neural Databases","This paper describes NeuralDB, a database system with no pre-defined schema, in which updates and queries are given in natural language, and describes an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators.","ArXiv",2020,"James Thorne,Majid Yazdani,Marzieh Saeidi,F. Silvestri,Sebastian Riedel,A. Halevy",4,58,0
"2cc3ab9fa41ba2804e301f7eae9598636e62422a","https://www.semanticscholar.org/paper/2cc3ab9fa41ba2804e301f7eae9598636e62422a","Investigating the Limitations of the Transformers with Simple Arithmetic Tasks","It is found that how a number is represented in its surface form has a strong influence on the model’s accuracy, and this result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement.","ArXiv",2021,"Rodrigo Nogueira,Zhiying Jiang,Jimmy J. Li",30,46,8
"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5","Measuring Mathematical Problem Solving With the MATH Dataset","This work introduces MATH, a new dataset of 12, 500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations, and shows that accuracy remains relatively low, even with enormous Transformer models.","NeurIPS Datasets and Benchmarks",2021,"Dan Hendrycks,Collin Burns,Saurav Kadavath,Akul Arora,Steven Basart,Eric Tang,D. Song,J. Steinhardt",91,65,20
"13c4e5a6122f3fa2663f63e49537091da6532f35","https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35","Are NLP Models really able to Solve Simple Math Word Problems?","It is shown that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs, and models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy.","North American Chapter of the Association for Computational Linguistics",2021,"Arkil Patel,S. Bhattamishra,Navin Goyal",78,25,31
"d898662fb0519f00f5ccc87f06294fa7322715b4","https://www.semanticscholar.org/paper/d898662fb0519f00f5ccc87f06294fa7322715b4","LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training","The LUNA framework is proposed which improves the numerical reasoning and calculation capabilities of transformer-based language models and bridges the gap between number and vocabulary embeddings with number pre-training and model distillation.","ArXiv",2022,"Hongwei Han,Jialiang Xu,Mengyuan Zhou,Yijia Shao,Shi Han,Dongmei Zhang",1,17,0