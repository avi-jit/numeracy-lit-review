{"papers":[{"url":"https://www.semanticscholar.org/paper/9160673f89ecb326db8c64f8d09ab30f35a74e9d","title":"Arithmetic with Language Models: from Memorization to Computation","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/08/2023","authors":"D. Maltoni,M. Ferrara","id":"9160673f89ecb326db8c64f8d09ab30f35a74e9d","summary":"The findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.","score":3},{"url":"https://www.semanticscholar.org/paper/dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":2,"influentialCitationCount":0,"publicationDate":"22/11/2023","authors":"Changnan Xiao,Bing Liu","id":"dd69049674f41d4ef5314b8f95bacfe59de31376","summary":"This work identifies and proves conditions that decide whether the length generalization problem can be solved or not for a reasoning task in a particular representation.","score":2},{"url":"https://www.semanticscholar.org/paper/49c45d2a2773c537804c38d69cde67e00fbad6fe","title":"Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs","venue":"","year":2024,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/02/2024","authors":"Aaditya K. Singh,DJ Strouse","id":"49c45d2a2773c537804c38d69cde67e00fbad6fe","summary":"This work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns.","score":2},{"url":"https://www.semanticscholar.org/paper/817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":52,"influentialCitationCount":3,"publicationDate":"16/03/2023","authors":"Zheng Yuan,Hongyi Yuan,Chuanqi Tan,Wei Wang,Songfang Huang","id":"817e52b815560f95171d8fa60f78dd965e885a65","summary":"This work proposes an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatG PT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provides a detailed analysis of the ability of largelanguage models.","score":2},{"url":"https://www.semanticscholar.org/paper/8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":34,"influentialCitationCount":3,"publicationDate":"23/05/2023","authors":"Tiedong Liu,K. H. Low","id":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","summary":"Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks, is introduced and an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks into a series of learnable tasks by leveraging basic arithmetic principles is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":16,"influentialCitationCount":0,"publicationDate":"06/09/2023","authors":"Z. Yang,Ming Ding,Qingsong Lv,Zhihuan Jiang,Zehai He,Yuyi Guo,Jinfeng Bai,Jie Tang","id":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","summary":"The MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.","score":2},{"url":"https://www.semanticscholar.org/paper/b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":5,"influentialCitationCount":0,"publicationDate":"13/09/2023","authors":"Eran Malach","id":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","summary":"This work presents a theoretical framework for studying auto-regressive next-token predictors, and introduces a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and examines the interplay between length complexity and other notions of complexity.","score":2},{"url":"https://www.semanticscholar.org/paper/5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey","venue":"arXiv.org","year":2023,"referenceCount":220,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2023","authors":"Wentao Liu,Hanglei Hu,Jie Zhou,Yuyang Ding,Junsong Li,Jiayi Zeng,Mengliang He,Qin Chen,Bo Jiang,Aimin Zhou,Liang He","id":"5ee871537ae51e7e2e93d2a70fff5d100649a655","summary":"A comprehensive survey of mathematical LMs is conducted, systematically categorizing pivotal research endeavors from two distinct perspectives: tasks and methodologies, revealing a large number of proposed mathematical LLMs.","score":2},{"url":"https://www.semanticscholar.org/paper/4372100e5f676b0a2db93a97b6b1e45ae593b75e","title":"Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought","venue":"arXiv.org","year":2024,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/02/2024","authors":"Alex Havrilla,Maia Iyer","id":"4372100e5f676b0a2db93a97b6b1e45ae593b75e","summary":"This work develops the Traced Integer framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers and evaluates the test performance of pretrained models both prompted and fine-tuned on noised datasets with varying levels of dataset contamination and intensity.","score":2},{"url":"https://www.semanticscholar.org/paper/3dc9eb80b806f2db14a45216a7fd840153f45a07","title":"Don’t be Blind to Questions: Question-Oriented Math Word Problem Solving","venue":"International Joint Conference on Natural Language Processing","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Zhenwen Liang,Jipeng Zhang,Xiangliang Zhang","id":"3dc9eb80b806f2db14a45216a7fd840153f45a07","summary":"This approach features an entity-aware encoder that enhances the connection between MWP context and question via entities in established dependency graphs, aiming at obtaining better problem representations and a question-guided decoder is trained using a contrastive learning strategy to enhance the question representations.","score":1},{"url":"https://www.semanticscholar.org/paper/0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","title":"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements","venue":"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)","year":2023,"referenceCount":111,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/06/2023","authors":"Syed Rifat Raiyan,Md. Nafis Faiyaz,S. Kabir,Mohsinul Kabir,H. Mahmud,Md. Kamrul Hasan","id":"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","summary":"This paper proposes a framework for MWP solvers based on the generation of linguistic variants of the problem text and shows that training on linguistic variant of problem statements and voting on candidate predictions improve the mathematical reasoning and robustness of the model.","score":1},{"url":"https://www.semanticscholar.org/paper/c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36","title":"MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":4,"influentialCitationCount":0,"publicationDate":"16/07/2023","authors":"Zhenwen Liang,Dian Yu,Xiaoman Pan,Wenlin Yao,Qingkai Zeng,Xiangliang Zhang,Dong Yu","id":"c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36","summary":"This work introduces a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles, and uniquely considers the various annotation formats as different\"views\" and leverages them in training the model.","score":1},{"url":"https://www.semanticscholar.org/paper/e1414fc1e1a6752524a1807a29ee406e8d808849","title":"Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":38,"citationCount":5,"influentialCitationCount":0,"publicationDate":"19/10/2023","authors":"Zhihan Zhang,Shuo Wang,W. Yu,Yichong Xu,Dan Iter,Qingkai Zeng,Yang Liu,Chenguang Zhu,Meng Jiang","id":"e1414fc1e1a6752524a1807a29ee406e8d808849","summary":"This paper introduces Auto-Instruct, a novel method to automatically improve the quality of instructions provided to LLMs, and leverages the inherent generative ability of LLMs to produce diverse candidate instructions for a given task, and then ranks them using a scoring model trained on a variety of 575 existing NLP tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/98b607e7cb84e1a5c87c8a49562ae35435e6722d","title":"Learning From Mistakes Makes LLM Better Reasoner","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":9,"influentialCitationCount":3,"publicationDate":"31/10/2023","authors":"Shengnan An,Zexiong Ma,Zeqi Lin,Nanning Zheng,Jian-Guang Lou,Weizhu Chen","id":"98b607e7cb84e1a5c87c8a49562ae35435e6722d","summary":"Experiments show that \\textsc{LeMa} consistently improves CoT-alone fine-tuning and sheds light on the non-homogeneous effectiveness between CoT data and correction data, and the contribution from different correction information.","score":1},{"url":"https://www.semanticscholar.org/paper/631b5baa2c34f7095ccdd8761086b49148071d78","title":"Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments","venue":"","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/12/2023","authors":"Ehsan Latif,Luyang Fang,Ping Ma,Xiaoming Zhai","id":"631b5baa2c34f7095ccdd8761086b49148071d78","summary":"A method for knowledge distillation of fine-tuned Large Language Models into smaller, more efficient, and accurate neural networks that has potential to make advanced AI technologies accessible in typical educational settings, particularly for automatic scoring.","score":1},{"url":"https://www.semanticscholar.org/paper/1a6bd4f70bceb26ddb722ce98c0eae2a64147048","title":"Knowledge-Based and Generative-AI-Driven Pedagogical Conversational Agents: A Comparative Study of Grice's Cooperative Principles and Trust","venue":"Big Data and Cognitive Computing","year":2023,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/12/2023","authors":"Matthias Wölfel,Mehrnoush Barani Shirzad,Andreas Reich,Katharina Anderer","id":"1a6bd4f70bceb26ddb722ce98c0eae2a64147048","summary":"This paper investigates how a very limited amount of domain-specific data can be used to build knowledge-based and generative educational chatbots and finds that knowledge-based chatbots allow full control over the system’s response but lack the verbosity and flexibility of GLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/04cc5744fe8b436fb0f8b2763ed8ac7e4a60f7d1","title":"Combining GPT and Colab as learning tools for students to explore the numerical solutions of difference equations","venue":"Eurasia Journal of Mathematics, Science and Technology Education","year":2024,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2024","authors":"Supot Seebut,Patcharee Wongsason,Dojin Kim","id":"04cc5744fe8b436fb0f8b2763ed8ac7e4a60f7d1","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/8f070e301979732e0dd73f6aa6170309cf73aa7d","title":"Large Language Model based Multi-Agents: A Survey of Progress and Challenges","venue":"arXiv.org","year":2024,"referenceCount":77,"citationCount":5,"influentialCitationCount":0,"publicationDate":"21/01/2024","authors":"Taicheng Guo,Xiuying Chen,Yaqi Wang,Ruidi Chang,Shichao Pei,N. Chawla,Olaf Wiest,Xiangliang Zhang","id":"8f070e301979732e0dd73f6aa6170309cf73aa7d","summary":"This survey is presented to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges.","score":1},{"url":"https://www.semanticscholar.org/paper/d43587743abd006be30756b67efd87f61671412b","title":"Tartare: Automatic Generation of C Pointer Statements and Feedback","venue":"IFAC Symposium on Advances in Control Education","year":2024,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/01/2024","authors":"Géraldine Brieven,Valentin Baum,Benoit Donnet","id":"d43587743abd006be30756b67efd87f61671412b","summary":"The techniques implemented in Tartare are described, relying on a pattern template-based approach, and it is believed the approach for Tartare can be transposed for automatic exercises generation in various other fields.","score":1},{"url":"https://www.semanticscholar.org/paper/42445823fb0156afddc8c72eaa5ee81dded5b965","title":"Large Language Models for Mathematical Reasoning: Progresses and Challenges","venue":"arXiv.org","year":2024,"referenceCount":92,"citationCount":4,"influentialCitationCount":0,"publicationDate":"31/01/2024","authors":"Janice Ahn,Rishu Verma,Renze Lou,Di Liu,Rui Zhang,Wenpeng Yin","id":"42445823fb0156afddc8c72eaa5ee81dded5b965","summary":"This survey stands as one of the first extensive examinations of the landscape of LLM-oriented techniques in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.","score":1},{"url":"https://www.semanticscholar.org/paper/7f4bdef8c9d660af6b18a55de0699e5e65ce3b54","title":"Plan-Grounded Large Language Models for Dual Goal Conversational Settings","venue":"arXiv.org","year":2024,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/02/2024","authors":"Diogo Glória-Silva,Rafael Ferreira,Diogo Tavares,David Semedo,João Magalhães","id":"7f4bdef8c9d660af6b18a55de0699e5e65ce3b54","summary":"A novel LLM is proposed that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system's behavior, while also improving the LLM's responses to unexpected user behavior.","score":1},{"url":"https://www.semanticscholar.org/paper/edc5b4b2b89718b42dc90192960084166b7987d8","title":"Multi-Intent Attribute-Aware Text Matching in Searching","venue":"arXiv.org","year":2024,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/02/2024","authors":"Mingzhe Li,Xiuying Chen,Jing Xiang,Qishen Zhang,Changsheng Ma,Chenchen Dai,Jinxiong Chang,Zhongyi Liu,Guannan Zhang","id":"edc5b4b2b89718b42dc90192960084166b7987d8","summary":"This work proposes a multi-intent attribute-aware matching model (MIM), which consists of three main components: attribute-aware encoder, multi-intent modeling, and intent-aware matching, which consists of three main components: attribute-aware encoder, multi-intent modeling, and intent-aware matching.","score":1},{"url":"https://www.semanticscholar.org/paper/f5df0667365764a970fc6abfa0a68b7d1d0ae413","title":"Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models","venue":"arXiv.org","year":2024,"referenceCount":64,"citationCount":6,"influentialCitationCount":1,"publicationDate":"11/01/2024","authors":"Asma Ghandeharioun,Avi Caciularu,Adam Pearce,Lucas Dixon,Mor Geva","id":"f5df0667365764a970fc6abfa0a68b7d1d0ae413","summary":"It is shown that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework, and several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes.","score":1},{"url":"https://www.semanticscholar.org/paper/a14e999e8826f98cfd1cac9dd7baa4d0b61f7266","title":"Increasing Trust in Language Models through the Reuse of Verified Circuits","venue":"arXiv.org","year":2024,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/02/2024","authors":"Philip Quirke,Clement Neo,Fazl Barez","id":"a14e999e8826f98cfd1cac9dd7baa4d0b61f7266","summary":"This paper fully verify a model for n-digit integer addition and discusses how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of language models built using them.","score":1},{"url":"https://www.semanticscholar.org/paper/a05148b077418259989511ed8031ce05689a16aa","title":"AtP*: An efficient and scalable method for localizing LLM behaviour to components","venue":"","year":2024,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2024","authors":"J'anos Kram'ar,Tom Lieberum,Rohin Shah,Neel Nanda","id":"a05148b077418259989511ed8031ce05689a16aa","summary":"This work investigates Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and finds two classes of failure modes of AtP which lead to significant false negatives, and proposes a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability.","score":1},{"url":"https://www.semanticscholar.org/paper/b16d916ce411be0b1c7b6139317c652927984f46","title":"Tokenization on the Number Line is All You Need","venue":"","year":2021,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"","id":"b16d916ce411be0b1c7b6139317c652927984f46","summary":"This work proposes vocabulary 018 level changes in the decoding stage and studies its behavior and finds that changes at the 032 tokenization level achieve near state-of-the-art results while requiring minimal resources com-034 pared to other number representation schemes.","score":1},{"url":"https://www.semanticscholar.org/paper/b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":20,"citationCount":15,"influentialCitationCount":1,"publicationDate":2021,"authors":"Jeonghwan Kim,Giwon Hong,Kyung-min Kim,Junmo Kang,Sung-Hyon Myaeng","id":"b59947541d2ac4211c4b17554b2e16c260299bed","summary":"This work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, and proposes the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text.","score":1},{"url":"https://www.semanticscholar.org/paper/e39dfab0477ae28dbb18d49beb9cd4c7ea30486c","title":"On the Abilities of Mathematical Extrapolation with Implicit Models","venue":"","year":2022,"referenceCount":21,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"H. Larochelle","id":"e39dfab0477ae28dbb18d49beb9cd4c7ea30486c","summary":"This paper compares the robustness of implicitly-defined and classical deep learning models on a series of mathematical extrapolation tasks, where the models are tested with out-of-distribution samples during inference time.","score":1},{"url":"https://www.semanticscholar.org/paper/5df55d94ab5026ff84ec01871592108fbadbddbe","title":"Measurement Extraction with Natural Language Processing: A Review","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":175,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jan Göpfert,Patrick Kuckertz,J. Weinand,Leander Kotzur,D. Stolten","id":"5df55d94ab5026ff84ec01871592108fbadbddbe","summary":"In this review, an overview of prior work on measurement extraction is presented and different approaches to measurement extraction are described and the challenges posed by this task are outlined.","score":1},{"url":"https://www.semanticscholar.org/paper/f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","title":"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning","venue":"","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","summary":"Investigation of the underlying hypothesis in the most simple conceivable scenario – the addition of real numbers finds that two layer neural networks fail to learn the structure of this task and that growing the network’s width leads to a complex division of input space.","score":1},{"url":"https://www.semanticscholar.org/paper/346accd49d1359aa3985c7b298bc2057ae642271","title":"One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development","venue":"","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Pulkit Singh,Julian S. Haimovich,C. Reeder,S. Khurshid,S. Emily,Lau,Jonathan W Cunningham,A. Philippakis,C. D. Anderson,J. Ho,S. Lubitz,P. Batra","id":"346accd49d1359aa3985c7b298bc2057ae642271","summary":"This study demonstrated that a domain-agnostic pretrained transformer model is able to effectively extract quantitative clinical measurements from diagnostic reports with a relatively small number of gold-standard annotations.","score":1},{"url":"https://www.semanticscholar.org/paper/c27f3904490b8e5f9f39fe2b36722090c189e916","title":"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF","venue":"Conference and Labs of the Evaluation Forum","year":2022,"referenceCount":68,"citationCount":5,"influentialCitationCount":0,"publicationDate":2022,"authors":"Wei Zhong,Yuqing Xie,Jimmy J. Lin","id":"c27f3904490b8e5f9f39fe2b36722090c189e916","summary":"This work describes the participation of the team in the ARQMath 2022 Lab, where two highly complementary methods for effective math answer and formula retrieval are applied, using a lexical sparse retriever and a fine-tuned bi-encoder dense retriever to capture contextual similarity and semantic matching.","score":1},{"url":"https://www.semanticscholar.org/paper/58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","title":"JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection","venue":"","year":2022,"referenceCount":16,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Shunsuke Onuma,Kazuma Kadowaki","id":"58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","summary":"The jointlearning withnumerical categories improved the performance of some pre-trained models and numeral format settings.","score":1},{"url":"https://www.semanticscholar.org/paper/6c84fc8c5ec823342f34a020f8b92b7064e96ca2","title":"Towards Efficient and Robust Out-of-Distribution Deep Learning with Implicit Models","venue":"","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Ashwin Ganesh","id":"6c84fc8c5ec823342f34a020f8b92b7064e96ca2","summary":"This paper shows how to decrease implicit model training time by harnessing the state-driven implicit modeling framework to safely eliminate features while maintaining model accuracy.","score":1},{"url":"https://www.semanticscholar.org/paper/1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning","venue":"Trans. Mach. Learn. Res.","year":2023,"referenceCount":75,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"David A. Klindt","id":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","summary":"A tight linkage between the scaling of a network weights’ standard deviation and its effective length scale on a sinusoidal regression problem is demonstrated, suggesting simple modifications to control the length scale of the function learned by a neural network and, thus, its smoothness.","score":1},{"url":"https://www.semanticscholar.org/paper/3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks","venue":"arXiv.org","year":2023,"referenceCount":66,"citationCount":4,"influentialCitationCount":0,"publicationDate":2023,"authors":"Yixuan Weng,Minjun Zhu,Fei Xia,Bin Li,Shizhu He,Kang Liu,Jun Zhao","id":"3693683c4e0405819fae7115ad680f769eb83534","summary":"The method, which call \"Neural Comprehension\", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning.","score":1},{"url":"https://www.semanticscholar.org/paper/28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":70,"citationCount":80,"influentialCitationCount":4,"publicationDate":"24/03/2021","authors":"Avijit Thawani,J. Pujara,Pedro A. Szekely,Filip Ilievski","id":"28a5a53dafacebad8a7c47773079caeffb9a5baa","summary":"This work synthesizes best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.","score":1},{"url":"https://www.semanticscholar.org/paper/a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning","venue":"Natural Language Processing and Chinese Computing","year":2021,"referenceCount":36,"citationCount":14,"influentialCitationCount":0,"publicationDate":"15/08/2021","authors":"Cunxiang Wang,Boyuan Zheng,Y. Niu,Yue Zhang","id":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","summary":"This research finds that the PLMs can easily generalize when the distribution is the same, however, it is still difficult for them to generalize out of the distribution.","score":1},{"url":"https://www.semanticscholar.org/paper/9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation","venue":"International Conference on Learning Representations","year":2021,"referenceCount":48,"citationCount":317,"influentialCitationCount":51,"publicationDate":"27/08/2021","authors":"Ofir Press,Noah A. Smith,M. Lewis","id":"9ca329408813d209b1dcb36936f7f9cba82506bd","summary":"This work shows that extrapolation can be enabled by simply changing the position representation method, though it finds that current methods do not allow for efficient extrapolation, and introduces a simpler and more efficient position method, Attention with Linear Biases (ALiBi).","score":1},{"url":"https://www.semanticscholar.org/paper/0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration","venue":"arXiv.org","year":2021,"referenceCount":47,"citationCount":18,"influentialCitationCount":1,"publicationDate":"05/09/2021","authors":"Gabriel Recchia","id":"0f2199296f01694ee46b6059879260fb80a84fa6","summary":"The results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":18,"citationCount":15,"influentialCitationCount":1,"publicationDate":"10/09/2021","authors":"Kuntal Pal,Chitta Baral","id":"37588705a2af7d5b24d901dd33ade1ff293aabdd","summary":"This work investigates the ability of text-to-text transfer learning model (T5), which has outperformed its predecessors in the conventional NLP tasks, to learn numeracy, to struggle considerably in the extrapolation setting across all four tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics","venue":"AAAI Conference on Artificial Intelligence","year":2021,"referenceCount":35,"citationCount":18,"influentialCitationCount":1,"publicationDate":"28/09/2021","authors":"S. Welleck,Peter West,Jize Cao,Yejin Choi","id":"aead4418733b998792deb9cbf198a834449e00d2","summary":"A methodology for evaluating generalization that takes advantage of the problem domain's structure and access to a verifier is developed, and the problem of symbolic mathematical integration is considered, as it requires generalizing systematically beyond the training set.","score":1},{"url":"https://www.semanticscholar.org/paper/b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval","venue":"arXiv.org","year":2021,"referenceCount":52,"citationCount":11,"influentialCitationCount":1,"publicationDate":"18/10/2021","authors":"Sarthak Mittal,S. Raparthy,I. Rish,Yoshua Bengio,Guillaume Lajoie","id":"b8b813111c411ae61881ab9cd25707d9de6444ec","summary":"This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.","score":1},{"url":"https://www.semanticscholar.org/paper/45ece6f3b0a319dba60c20b3013b5161dd49c58b","title":"Linear algebra with transformers","venue":"Trans. Mach. Learn. Res.","year":2021,"referenceCount":68,"citationCount":33,"influentialCitationCount":2,"publicationDate":"03/12/2021","authors":"Franccois Charton","id":"45ece6f3b0a319dba60c20b3013b5161dd49c58b","summary":"Nine problems of linear algebra are studied, from basic matrix operations to eigenvalue decomposition and inversion, and four encoding schemes to represent real numbers are introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/edd80013e8b9fcba9231cf99884f32e5236ff329","title":"AA-TransUNet: Attention Augmented TransUNet For Nowcasting Tasks","venue":"IEEE International Joint Conference on Neural Network","year":2022,"referenceCount":36,"citationCount":22,"influentialCitationCount":3,"publicationDate":"10/02/2022","authors":"Yimin Yang,S. Mehrkanoon","id":"edd80013e8b9fcba9231cf99884f32e5236ff329","summary":"A novel data-driven predictive model based on TransUNet for precipitation nowcasting task and shows that the proposed model outperforms other examined models on both tested datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/7a25155364476839b6d1fc0653cd8611327ab9ba","title":"mGPT: Few-Shot Learners Go Multilingual","venue":"Transactions of the Association for Computational Linguistics","year":2022,"referenceCount":116,"citationCount":71,"influentialCitationCount":7,"publicationDate":"15/04/2022","authors":"Oleh Shliazhko,Alena Fenogenova,M. Tikhonova,V. Mikhailov,A. Kozlova,Tatiana Shavrina","id":"7a25155364476839b6d1fc0653cd8611327ab9ba","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence","venue":"NAACL-HLT","year":2022,"referenceCount":53,"citationCount":6,"influentialCitationCount":1,"publicationDate":"08/05/2022","authors":"Myeongjun Jang,Frank Mtumbuka,Thomas Lukasiewicz","id":"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","summary":"A novel intermediate training task, names meaning-matching, designed to directly learn a meaning-text correspondence, is proposed that enables PLMs to learn lexical semantic information and is found to be a safe intermediate task that guarantees a similar or better performance of downstream tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/d2018d1b0f69ca6805aa18a22be95cab9b5c44c7","title":"On Neural Architecture Inductive Biases for Relational Tasks","venue":"arXiv.org","year":2022,"referenceCount":50,"citationCount":16,"influentialCitationCount":10,"publicationDate":"09/06/2022","authors":"Giancarlo Kerg,Sarthak Mittal,D. Rolnick,Y. Bengio,B. Richards,Guillaume Lajoie","id":"d2018d1b0f69ca6805aa18a22be95cab9b5c44c7","summary":"It is found that simple architectural choices can outperform existing models in out-of-distribution generalization and show that partitioning relational representations from other information streams may be a simple way to augment existing network architectures' robustness when performing out- of-dist distribution relational computations.","score":1},{"url":"https://www.semanticscholar.org/paper/b92628d13e8d090d042232fe6ae0b8998634b893","title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks","venue":"Neural Information Processing Systems","year":2022,"referenceCount":181,"citationCount":47,"influentialCitationCount":7,"publicationDate":"14/06/2022","authors":"Tuan Dinh,Yuchen Zeng,Ruisu Zhang,Ziqian Lin,Shashank Rajput,Michael Gira,Jy-yong Sohn,Dimitris Papailiopoulos,Kangwook Lee","id":"b92628d13e8d090d042232fe6ae0b8998634b893","summary":"Language-Interfaced Fine-Tuning is proposed and found that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/40f73969cd3415e9c1b03796cbb5a50c79ebd448","title":"SALSA: Attacking Lattice Cryptography with Transformers","venue":"IACR Cryptology ePrint Archive","year":2022,"referenceCount":72,"citationCount":20,"influentialCitationCount":2,"publicationDate":"11/07/2022","authors":"Emily Wenger,Mingjie Chen,Franccois Charton,K. Lauter","id":"40f73969cd3415e9c1b03796cbb5a50c79ebd448","summary":"SALSA can fully recover secrets for small-to-mid size LWE instances with sparse binary secrets, and may scale to attack real-world LWE-based cryptosystems.","score":1},{"url":"https://www.semanticscholar.org/paper/4f451ba06c4c9effd6c4ac0bae222495501a6200","title":"Innovations in Neural Data-to-text Generation","venue":"arXiv.org","year":2022,"referenceCount":370,"citationCount":7,"influentialCitationCount":0,"publicationDate":"25/07/2022","authors":"Mandar Sharma,Ajay K. Gogineni,Naren Ramakrishnan","id":"4f451ba06c4c9effd6c4ac0bae222495501a6200","summary":"This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella.","score":1},{"url":"https://www.semanticscholar.org/paper/2a7ae3e98357569c41424dacd60c62d3df78a0db","title":"Limitations of Language Models in Arithmetic and Symbolic Induction","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":23,"citationCount":36,"influentialCitationCount":2,"publicationDate":"09/08/2022","authors":"Jingu Qian,Hong Wang,Zekun Li,SHIYANG LI,Xifeng Yan","id":"2a7ae3e98357569c41424dacd60c62d3df78a0db","summary":"Surprisingly, large pretrained Language Models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition, when the total number of symbols or repeating symbols increases, the model performance drops quickly.","score":1},{"url":"https://www.semanticscholar.org/paper/108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models","venue":"MATHNLP","year":2022,"referenceCount":66,"citationCount":10,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"M. Bueno,Carlos Gemmel,Jeffrey Stephen Dalton,R. Lotufo,Rodrigo Nogueira","id":"108c25905be36b2a7a0fc7256ac314985ecd9699","summary":"This work demonstrates that large language models can succeed in extrapolation without modifying their architecture or training procedure, and shows how generating step-by-step rationales and introducing marker tokens are both required for effective extrapolation.","score":1},{"url":"https://www.semanticscholar.org/paper/e82e3f4347674b75c432cb80604d38ee630d4bf6","title":"Transformers Learn Shortcuts to Automata","venue":"International Conference on Learning Representations","year":2022,"referenceCount":107,"citationCount":65,"influentialCitationCount":5,"publicationDate":"19/10/2022","authors":"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang","id":"e82e3f4347674b75c432cb80604d38ee630d4bf6","summary":"It is found that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics.","score":1},{"url":"https://www.semanticscholar.org/paper/49e46e615747f258517248de5a736814fada17ee","title":"What is my math transformer doing? - Three results on interpretability and generalization","venue":"arXiv.org","year":2022,"referenceCount":14,"citationCount":9,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Franccois Charton","id":"49e46e615747f258517248de5a736814fada17ee","summary":"This paper investigates the failure cases and out-of-distribution behavior of transformers trained on matrix inversion and eigenvalue decomposition and demonstrates that, when in doubt, math transformers do not hallucinate absurd solutions but remain “roughly right”.","score":1},{"url":"https://www.semanticscholar.org/paper/1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","title":"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)","venue":"arXiv.org","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/11/2022","authors":"Victor Robila,Kexin Pei,Junfeng Yang","id":"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","summary":"A comparison between human-developed machine learning model and models sampled through Neural Architecture Search (NAS) determine an efficient approach to solve the problem of addition using embedded hexadecimal digits.","score":1},{"url":"https://www.semanticscholar.org/paper/1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":49,"citationCount":3,"influentialCitationCount":1,"publicationDate":"14/11/2022","authors":"Jialiang Xu,Mengyu Zhou,Xinyi He,Shi Han,Dongmei Zhang","id":"1a174b63d294f96568517b91f2c8d6c9362118b5","summary":"This paper proposes to conduct numerical capability diagnosis on a series of Numerical Question Answering systems and datasets and investigates the effectiveness of applying perturbations as data augmentation to relieve systems’ lack of robust numerical capabilities.","score":1},{"url":"https://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension","venue":"arXiv.org","year":2022,"referenceCount":62,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Ippei Fujisawa,R. Kanai","id":"965e409a3e7b5670d609837fac9823b160d6639c","summary":"This work defines and characterize logical tasks and discusses system requirements for their solution, and discusses the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias.","score":1},{"url":"https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning","venue":"arXiv.org","year":2022,"referenceCount":68,"citationCount":69,"influentialCitationCount":10,"publicationDate":"15/11/2022","authors":"Hattie Zhou,Azade Nova,H. Larochelle,Aaron C. Courville,Behnam Neyshabur,Hanie Sedghi","id":"4d17732d90440682b0500f4e209c6cc4fac20e0e","summary":"This work shows that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which it is referred to as algorithmic prompting, and evaluates the approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrates significant boosts in performance.","score":1},{"url":"https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models","venue":"International Conference on Machine Learning","year":2022,"referenceCount":67,"citationCount":233,"influentialCitationCount":45,"publicationDate":"18/11/2022","authors":"Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,Yiming Yang,Jamie Callan,Graham Neubig","id":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","summary":"This paper presents Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.","score":1},{"url":"https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":219,"citationCount":126,"influentialCitationCount":2,"publicationDate":"19/12/2022","authors":"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen","id":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","summary":"This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","score":1},{"url":"https://www.semanticscholar.org/paper/cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models","venue":"Applied Sciences","year":2023,"referenceCount":120,"citationCount":8,"influentialCitationCount":0,"publicationDate":"14/03/2023","authors":"Alberto Testolin","id":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","summary":"This survey critically examines the recent literature, concluding that even state-of-the-art architectures and large language models often fall short when probed with relatively simple tasks designed to test basic numerical and arithmetic knowledge.","score":1},{"url":"https://www.semanticscholar.org/paper/aec826ff336ca442697d5f908ab1668f1ea18987","title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model","venue":"Neural Information Processing Systems","year":2023,"referenceCount":53,"citationCount":30,"influentialCitationCount":3,"publicationDate":"30/04/2023","authors":"Michael Hanna,Ollie Liu,Alexandre Variengien","id":"aec826ff336ca442697d5f908ab1668f1ea18987","summary":"The basic mathematical abilities often acquired by pre-trained language models are investigated, and mechanistic interpretability techniques are used to explain the (limited) mathematical abilities of GPT-2 small.","score":1},{"url":"https://www.semanticscholar.org/paper/0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Vaishali Pal,Andrew Yates,E. Kanoulas,M. de Rijke","id":"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","summary":"This model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers, which outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery.","score":1},{"url":"https://www.semanticscholar.org/paper/d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","title":"Exposing Attention Glitches with Flip-Flop Language Modeling","venue":"Neural Information Processing Systems","year":2023,"referenceCount":107,"citationCount":12,"influentialCitationCount":1,"publicationDate":"01/06/2023","authors":"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang","id":"d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","summary":"This work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning, and hypothesizes that attention glitches account for some of the closed-domain hallucinations in natural LLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/3e7a0dc5795dc108c78993bcf3624fc626a9f9cf","title":"Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks","venue":"xAI","year":2023,"referenceCount":28,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/06/2023","authors":"Mohamad Ballout,U. Krumnack,G. Heidemann,Kai-Uwe Kühnberger","id":"3e7a0dc5795dc108c78993bcf3624fc626a9f9cf","summary":"A pre-trained language model is applied to constrained arithmetic problems with hierarchical structure, to analyze their attention weight scores and hidden states and reveals promising results, with the model addressing hierarchical problems in a moderately structured manner, similar to human problem-solving strategies.","score":1},{"url":"https://www.semanticscholar.org/paper/b43383f10634f7e610f22badd4f42c93e5dcb947","title":"Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI","venue":"INNS DLIA@IJCNN","year":2023,"referenceCount":28,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/06/2023","authors":"Mohamad Ballout,U. Krumnack,G. Heidemann,Kai-Uwe Kühnberger","id":"b43383f10634f7e610f22badd4f42c93e5dcb947","summary":"The ability of pre-trained language models to generalize to different non-language tasks such as computer vision, reasoning on hierarchical data, and protein fold prediction is investigated and it is found that using pre- trained embeddings for the input layer is necessary to achieve the desired results.","score":1},{"url":"https://www.semanticscholar.org/paper/fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":15,"influentialCitationCount":3,"publicationDate":"27/06/2023","authors":"Samy Jelassi,Stéphane d'Ascoli,Carles Domingo-Enrich,Yuhuai Wu,Yuan-Fang Li,Franccois Charton","id":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","summary":"It is shown that priming allows models trained on $5-digit $\\times$ $3-digit multiplications to generalize to $35\\times 3$ examples, and that the priming sample size scales as the logarithm of the training set size.","score":1},{"url":"https://www.semanticscholar.org/paper/49d340b7d6108ce5ef2277a165ea83f254671763","title":"The use of weather nowcasting convolutional neural network extrapolators in cardiac PET imaging","venue":"Journal of Medical Radiation Sciences","year":2023,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/06/2023","authors":"S. Sloka","id":"49d340b7d6108ce5ef2277a165ea83f254671763","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/be3a74f9f6010889d049060eab2a4b09eb48bbfb","title":"The Value of Numbers in Clinical Text Classification","venue":"Machine Learning and Knowledge Extraction","year":2023,"referenceCount":54,"citationCount":1,"influentialCitationCount":0,"publicationDate":"07/07/2023","authors":"Kristian Miok,P. Corcoran,Irena Spasic","id":"be3a74f9f6010889d049060eab2a4b09eb48bbfb","summary":"The results showed that even a handful of numerical features can significantly improve text classification performance, and it was concluded that commonly used document representations do not represent numbers in a way that machine learning algorithms can effectively utilize them as features.","score":1},{"url":"https://www.semanticscholar.org/paper/0db0af0cd3ceb0531a050a03e6ceb849580ff53b","title":"Teaching Arithmetic to Small Transformers","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":24,"influentialCitationCount":5,"publicationDate":"07/07/2023","authors":"Nayoung Lee,Kartik K. Sreenivasan,Jason D. Lee,Kangwook Lee,Dimitris Papailiopoulos","id":"0db0af0cd3ceb0531a050a03e6ceb849580ff53b","summary":"This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective.","score":1},{"url":"https://www.semanticscholar.org/paper/8f1b0c247171510fc68da27a16a377456376a5a7","title":"Assessing GPT-3.5 and GPT-4 in Generating International Classification of Diseases Billing Codes","venue":"medRxiv","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/07/2023","authors":"A. Soroush,B. Glicksberg,E. Zimlichman,Y. Barash,R. Freeman,A. Charney,G. Nadkarni,E. Klang","id":"8f1b0c247171510fc68da27a16a377456376a5a7","summary":"While the models appear to exhibit a general conceptual understanding of the codes and their descriptions, they have a propensity for hallucinating key details, suggesting underlying technological limitations of the base LLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/72a9187b489992cad3d54420611d5039eb6b9d86","title":"One Blade for One Purpose: Advancing Math Information Retrieval using Hybrid Search","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2023,"referenceCount":86,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/07/2023","authors":"Wei Zhong,Sheng-Chieh Lin,Jheng-Hong Yang,Jimmy J. Lin","id":"72a9187b489992cad3d54420611d5039eb6b9d86","summary":"This work proposes MABOWDOR, a Math-Aware Bestof-Worlds Domain Optimized Retriever, which has an unsupervised structure search component, a dense retriever, and optionally a sparse retriever on top of a domain-adapted backbone learned by context-enhanced pretraining, each addressing a different need in retrieving heterogeneous data from math documents.","score":1},{"url":"https://www.semanticscholar.org/paper/edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a","title":"It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/08/2023","authors":"Xingcheng Xu,Zihao Pan,Haipeng Zhang,Yanqing Yang","id":"edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a","summary":"It is discovered that the strong ID generalization stems from structured representations, while behind the unsatisfying OOD performance, the models still exhibit clear learned algebraic structures.","score":1},{"url":"https://www.semanticscholar.org/paper/cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6","title":"Can transformers learn the greatest common divisor?","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":4,"influentialCitationCount":0,"publicationDate":"29/08/2023","authors":"Franccois Charton","id":"cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/0b778079946764292de3771a489d5ce9e1868a8b","title":"The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":4,"influentialCitationCount":0,"publicationDate":"12/09/2023","authors":"Dimitris Spathis,F. Kawsar","id":"0b778079946764292de3771a489d5ce9e1868a8b","summary":"Recent works that employ LLMs for human-centric tasks are discussed and a case study showing that popular LLMs tokenize temporal data incorrectly is presented, highlighting potential solutions that can help bridge this \"modality gap\".","score":1},{"url":"https://www.semanticscholar.org/paper/174a9b78350e9561555052bc6901cc44782f4c62","title":"Estimating Numbers without Regression","venue":"arXiv.org","year":2023,"referenceCount":17,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/10/2023","authors":"Avijit Thawani,Jay Pujara,Ashwin Kalyan","id":"174a9b78350e9561555052bc6901cc44782f4c62","summary":"A carefully designed tokenization scheme is both the simplest to implement and sufficient, with similar performance to the state-of-the-art approach that requires making significant architectural changes, in the context of masked number prediction.","score":1},{"url":"https://www.semanticscholar.org/paper/200fcd964f41efe0c35a3f888a520ede08a3269c","title":"From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/10/2023","authors":"Shaoxiong Duan,Yining Shi","id":"200fcd964f41efe0c35a3f888a520ede08a3269c","summary":"It is shown that transformer models are able to generalize to long lengths with the help of targeted attention biasing, and it is demonstrated that using ABC, the transformer model can achieve unprecedented near-perfect length generalization on certain arithmetic tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b","title":"What Algorithms can Transformers Learn? A Study in Length Generalization","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":16,"influentialCitationCount":2,"publicationDate":"24/10/2023","authors":"Hattie Zhou,Arwen Bradley,Etai Littwin,Noam Razin,O. Saremi,Josh Susskind,Samy Bengio,Preetum Nakkiran","id":"1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b","summary":"This work proposes a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task and provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.","score":1},{"url":"https://www.semanticscholar.org/paper/4726d1dc54851db99c29180127d840bd19f20afc","title":"Positional Description Matters for Transformers Arithmetic","venue":"arXiv.org","year":2023,"referenceCount":25,"citationCount":4,"influentialCitationCount":1,"publicationDate":"22/11/2023","authors":"Ruoqi Shen,Sébastien Bubeck,Ronen Eldan,Yin Tat Lee,Yuanzhi Li,Yi Zhang","id":"4726d1dc54851db99c29180127d840bd19f20afc","summary":"This work delve deeper into the role of positional encoding, and proposes several ways to fix the issue, either by modifying the positional encoding directly, or by modified the representation of the arithmetic task to leverage standard positional encoding differently.","score":1},{"url":"https://www.semanticscholar.org/paper/2aeaad5548229dec7fdf716f7e83a5a359665852","title":"Carrying over algorithm in transformers","venue":"arXiv.org","year":2024,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/01/2024","authors":"J. Kruthoff","id":"2aeaad5548229dec7fdf716f7e83a5a359665852","summary":"A simple way of precisely identifying which neurons are responsible for that task in the carrying over algorithm is provided, across a range of hyperparameters for two as well as three-layer models.","score":1},{"url":"https://www.semanticscholar.org/paper/a640215755e23e2649f4b3d3246a47b14fea93f7","title":"Getting the most out of your tokenizer for pre-training and domain adaptation","venue":"arXiv.org","year":2024,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2024","authors":"Gautier Dagan,Gabriele Synnaeve,Baptiste Roziere","id":"a640215755e23e2649f4b3d3246a47b14fea93f7","summary":"It is shown that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance.","score":1},{"url":"https://www.semanticscholar.org/paper/d9e76ae6480114d81da2e9eb98f848df120be057","title":"Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors","venue":"IACR Cryptology ePrint Archive","year":2024,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2024","authors":"Samuel Stevens,Emily Wenger,C. Li,Niklas Nolte,Eshika Saxena,François Charton,Kristin E. Lauter","id":"d9e76ae6480114d81da2e9eb98f848df120be057","summary":"This work is the first instance of ML attacks recovering sparse binary secrets in dimension $n=1024$, the smallest dimension used in practice for homomorphic encryption applications of LWE where sparse binary secrets are proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/bc4e1552d3c12180fd13d59e155861a6eb2fbdab","title":"Lissard: Long and Simple Sequential Reasoning Datasets","venue":"arXiv.org","year":2024,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/02/2024","authors":"M. Bueno,R. Lotufo,Rodrigo Nogueira","id":"bc4e1552d3c12180fd13d59e155861a6eb2fbdab","summary":"This paper introduces Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution.","score":1},{"url":"https://www.semanticscholar.org/paper/61547f92cbba68629f470ece8019c4140c506706","title":"Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models","venue":"arXiv.org","year":2024,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/02/2024","authors":"Mohamad Ballout,U. Krumnack,Gunther Heidemann,Kai-Uwe Kuehnberger","id":"61547f92cbba68629f470ece8019c4140c506706","summary":"It is argued that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model.","score":1},{"url":"https://www.semanticscholar.org/paper/6f63ad08cf27183ceb2976b9fb6599ed9b31a522","title":"OmniPred: Language Models as Universal Regressors","venue":"","year":2024,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/02/2024","authors":"Xingyou Song,Oscar Li,Chansoo Lee,Bangding Yang,Daiyi Peng,Sagi Perel,Yutian Chen","id":"6f63ad08cf27183ceb2976b9fb6599ed9b31a522","summary":"OmniPred is proposed, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments that demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.","score":1},{"url":"https://www.semanticscholar.org/paper/f740a2474b52675287166a003bd1313f8aabcd68","title":"BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning","venue":"","year":2024,"referenceCount":85,"citationCount":2,"influentialCitationCount":0,"publicationDate":"27/02/2024","authors":"Qizhi Pei,Lijun Wu,Kaiyuan Gao,Xiaozhuan Liang,Yin Fang,Jinhua Zhu,Shufang Xie,Tao Qin,Rui Yan","id":"f740a2474b52675287166a003bd1313f8aabcd68","summary":"This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery, and incorporates several novel features that allow BioT5+ to bridge the gap between molecular representations and their textual descriptions, providing a more holistic understanding of biological entities.","score":1},{"url":"https://www.semanticscholar.org/paper/17ba73a2a332a44bb1a00622beab96f33d4b1ba7","title":"RORA: Robust Free-Text Rationale Evaluation","venue":"","year":2024,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/02/2024","authors":"Zhengping Jiang,Yining Lu,Hanjie Chen,Daniel Khashabi,B. V. Durme,Anqi Liu","id":"17ba73a2a332a44bb1a00622beab96f33d4b1ba7","summary":"RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage and aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales.","score":1},{"url":"https://www.semanticscholar.org/paper/36f46391b00a7eb4ffc991f964a36b264811057d","title":"Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks","venue":"","year":null,"referenceCount":28,"citationCount":1,"influentialCitationCount":0,"publicationDate":null,"authors":"Avinash Anand,Mohit Gupta,Kritarth Prasad,Navya Singla,Sanjana Sanjeev,Jatin Kumar,A. Shivam,Rajiv Ratn","id":"36f46391b00a7eb4ffc991f964a36b264811057d","summary":"An extensive mathematics dataset called \" MathQuest \" sourced from the 11th and 12th standard Mathematics NCERT textbooks is introduced, and MAmmoTH-13B establishes itself as a robust and dependable benchmark for addressing NCERT mathematics problems.","score":1},{"url":"https://www.semanticscholar.org/paper/651dac86d8bf847ec6780a878cb1e04d3d41f356","title":"GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities","venue":"Social Science Research Network","year":2023,"referenceCount":36,"citationCount":37,"influentialCitationCount":2,"publicationDate":"11/01/2023","authors":"Jillian Bommarito,M. Bommarito,D. Katz,Jessica Katz","id":"651dac86d8bf847ec6780a878cb1e04d3d41f356","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/763f47b73d0273a93a52a8ee8a63cf19009df6e1","title":"Injecting the BM25 Score as Text Improves BERT-Based Re-rankers","venue":"European Conference on Information Retrieval","year":2023,"referenceCount":73,"citationCount":13,"influentialCitationCount":1,"publicationDate":"23/01/2023","authors":"Arian Askari,Amin Abolghasemi,G. Pasi,Wessel Kraaij,S. Verberne","id":"763f47b73d0273a93a52a8ee8a63cf19009df6e1","summary":"The findings indicate that cross-encoder re-rankers can efficiently be improved without additional computational burden and extra steps in the pipeline by explicitly adding the output of the first-stage ranker to the model input, and this effect is robust for different models and query types.","score":1},{"url":"https://www.semanticscholar.org/paper/f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":8,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"David A. Noever,Forrest McKee","id":"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","summary":"The work examines whether next-token prediction succeeds from sentence completion into the realm of actual numerical understanding and showcases the model's capabilities to group by or pivot categorical sums, infer feature importance, derive correlations, and predict unseen test cases using linear regression.","score":1},{"url":"https://www.semanticscholar.org/paper/ade6d2808283ab8372db701ede6ee145a5445a95","title":"SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly","venue":"IEEE/ACM International Symposium on Code Generation and Optimization","year":2023,"referenceCount":68,"citationCount":5,"influentialCitationCount":0,"publicationDate":"21/05/2023","authors":"Jordi Armengol-Estap'e,Jackson Woodruff,Chris Cummins,M. O’Boyle","id":"ade6d2808283ab8372db701ede6ee145a5445a95","summary":"SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine is presented, utilizing a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/24a285b1e7ee9acfec9a82f7123563b62f532203","title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/05/2023","authors":"Jasivan Sivakumar,N. Moosavi","id":"24a285b1e7ee9acfec9a82f7123563b62f532203","summary":"This work introduces a multi-view evaluation set for numerical reasoning in English, called FERMAT, which evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency.","score":1},{"url":"https://www.semanticscholar.org/paper/7ef0cc95ff71c1098414cd61e88ac373ea2db4c4","title":"Performance of Generative Large Language Models on Ophthalmology Board Style Questions.","venue":"American journal of ophthalmology-glaucoma","year":2023,"referenceCount":8,"citationCount":18,"influentialCitationCount":1,"publicationDate":"01/06/2023","authors":"Louis Z Cai,Abdulla R. Shaheen,Andrew C. Jin,Riya Fukui,Jonathan S. Yi,Nicolas A. Yannuzzi,C. Alabiad","id":"7ef0cc95ff71c1098414cd61e88ac373ea2db4c4","summary":"The frequency of hallucinations and non-logical reasoning suggest room for improvement in the performance of conversational agents in the medical domain.","score":1},{"url":"https://www.semanticscholar.org/paper/a37d5620210276e47cf0c9dd2898c2a82c9d0422","title":"Simple synthetic data reduces sycophancy in large language models","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":18,"influentialCitationCount":1,"publicationDate":"07/08/2023","authors":"Jerry W. Wei,Da Huang,Yifeng Lu,Denny Zhou,Quoc V. Le","id":"a37d5620210276e47cf0c9dd2898c2a82c9d0422","summary":"A straightforward synthetic-data intervention is presented that takes public NLP tasks and encourages models to be robust to user opinions on these tasks and can significantly reduce sycophantic behavior on held-out prompts.","score":1},{"url":"https://www.semanticscholar.org/paper/5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/11/2023","authors":"Mubashara Akhtar,Abhilash Shankarampeta,Vivek Gupta,Arpit Patil,O. Cocarascu,Elena Simperl","id":"5be5619fc22300ef356ec4ef729d567ce7116c57","summary":"A hierarchical taxonomy for numerical reasoning skills with more than ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning is proposed.","score":1}]}