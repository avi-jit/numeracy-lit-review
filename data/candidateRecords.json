{"papers":[{"url":"https://www.semanticscholar.org/paper/cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/03/2023","authors":"Alberto Testolin","id":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","summary":"This survey critically examines the recent literature, concluding that even state-of-the-art architectures often fall short when probed with relatively simple tasks designed to test basic numerical and arithmetic knowledge.","score":10},{"url":"https://www.semanticscholar.org/paper/8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning","venue":"Conference on Automated Knowledge Base Construction","year":2021,"referenceCount":95,"citationCount":18,"influentialCitationCount":3,"publicationDate":2021,"authors":"Chadi Helwe,C. Clavel,Fabian M. Suchanek","id":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","summary":"This survey paper discusses the performance of transformers on diﬀerent reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning.","score":7},{"url":"https://www.semanticscholar.org/paper/262d6a0f6eb3f4c81e47fecd7e14be004295a7cf","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models","venue":"arXiv.org","year":2022,"referenceCount":55,"citationCount":10,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Alessandro Stolfo,Zhijing Jin,K. Shridhar,B. Schölkopf,Mrinmaya Sachan","id":"262d6a0f6eb3f4c81e47fecd7e14be004295a7cf","summary":"This work proposes a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands and math operators on the output solution, and shows that robustness does not appear to continuously improve as a function of scale, but that the recent GPT-3-Instruct achieves a dramatic improvement in both robustness and sensitivity, compared to all other GPT variants.","score":6},{"url":"https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning","venue":"arXiv.org","year":2022,"referenceCount":70,"citationCount":21,"influentialCitationCount":7,"publicationDate":"15/11/2022","authors":"Hattie Zhou,Azade Nova,H. Larochelle,Aaron C. Courville,Behnam Neyshabur,Hanie Sedghi","id":"4d17732d90440682b0500f4e209c6cc4fac20e0e","summary":"This work shows that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which it is referred to as algorithmic prompting, and evaluates the approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrates significant boosts in performance.","score":6},{"url":"https://www.semanticscholar.org/paper/f500523b8362ad8f838da14ff0a2498d0143cd1e","title":"Reasoning with Language Model Prompting: A Survey","venue":"arXiv.org","year":2022,"referenceCount":198,"citationCount":30,"influentialCitationCount":1,"publicationDate":"19/12/2022","authors":"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen","id":"f500523b8362ad8f838da14ff0a2498d0143cd1e","summary":"This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","score":5},{"url":"https://www.semanticscholar.org/paper/817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?","venue":"arXiv.org","year":2023,"referenceCount":32,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/03/2023","authors":"Zheng Yuan,Hongyi Yuan,Chuanqi Tan,Wei Wang,Songfang Huang","id":"817e52b815560f95171d8fa60f78dd965e885a65","summary":"This work proposes an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatG PT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provides a detailed analysis of the ability of largelanguage models.","score":5},{"url":"https://www.semanticscholar.org/paper/8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks","venue":"","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Tiedong Liu,Bryan Kian Hsiang Low","id":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","summary":"Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks, is introduced and an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks into a series of learnable tasks by leveraging basic arithmetic principles is proposed.","score":5},{"url":"https://www.semanticscholar.org/paper/0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension","venue":"ACM Computing Surveys","year":2021,"referenceCount":343,"citationCount":78,"influentialCitationCount":7,"publicationDate":"27/07/2021","authors":"Anna Rogers,Matt Gardner,Isabelle Augenstein","id":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","summary":"This study is the largest survey of the deep learning models in NLP field to date, providing an overview of the various formats and domains of the current resources, and highlighting the current lacunae for future work.","score":5},{"url":"https://www.semanticscholar.org/paper/2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey","venue":"arXiv.org","year":2023,"referenceCount":175,"citationCount":7,"influentialCitationCount":1,"publicationDate":"09/02/2023","authors":"E. Davis","id":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","summary":"A survey of the development and uses of AI commonsense benchmarks, and it is argued that it is worthwhile to invest the work needed ensure that benchmark examples are consistently high quality.","score":5},{"url":"https://www.semanticscholar.org/paper/681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey","venue":"arXiv.org","year":2023,"referenceCount":362,"citationCount":3,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Xavier Daull,P. Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco","id":"681cee58cf7e54199191cf9e0baf6851d8356704","summary":"This paper reviews the state-of-the-art of language models architectures and strategies for \"complex\"question-answering (QA, C QA, CPS) with a focus on hybridization, and discusses some challenges associated with complex QA.","score":5},{"url":"https://www.semanticscholar.org/paper/ba704774f194938b04b1e2be40b1d111a4ca08e1","title":"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation","venue":"","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Cheng Qian,Chi Han,Yi R. Fung,Yujia Qin,Zhiyuan Liu,Heng Ji","id":"ba704774f194938b04b1e2be40b1d111a4ca08e1","summary":null,"score":4},{"url":"https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models","venue":"arXiv.org","year":2022,"referenceCount":57,"citationCount":78,"influentialCitationCount":20,"publicationDate":"18/11/2022","authors":"Luyu Gao,Aman Madaan,Shuyan Zhou,Uri Alon,Pengfei Liu,Yiming Yang,Jamie Callan,Graham Neubig","id":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","summary":"This paper presents Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.","score":4},{"url":"https://www.semanticscholar.org/paper/3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks","venue":"arXiv.org","year":2023,"referenceCount":66,"citationCount":3,"influentialCitationCount":0,"publicationDate":2023,"authors":"Yixuan Weng,Minjun Zhu,Fei Xia,Bin Li,Shizhu He,Kang Liu,Jun Zhao","id":"3693683c4e0405819fae7115ad680f769eb83534","summary":"The method, which call \"Neural Comprehension\", helps language models achieve absolute accuracy in symbolic operations, thereby enhancing their ability for rule reasoning, symbolic reasoning, and arithmetic reasoning.","score":4},{"url":"https://www.semanticscholar.org/paper/3ee9c65366efbb17adf370c39f20dbef60d53670","title":"Towards Reasoning in Large Language Models: A Survey","venue":"arXiv.org","year":2022,"referenceCount":103,"citationCount":38,"influentialCitationCount":2,"publicationDate":"20/12/2022","authors":"Jie Huang,K. Chang","id":"3ee9c65366efbb17adf370c39f20dbef60d53670","summary":"A comprehensive overview of the current state of knowledge on reasoning in large language models, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions are provided.","score":4},{"url":"https://www.semanticscholar.org/paper/be8f769f135b0ffcf829594b9421788781f38008","title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts","venue":"Expert systems with applications","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/04/2023","authors":"Donghee Choi,Mogan Gim,Samy Badreddine,Hajung Kim,Donghyeon Park,Jaewoo Kang","id":"be8f769f135b0ffcf829594b9421788781f38008","summary":"This work introduces KitchenScale, a fine-tuned Pre- trained Pre-trained Language Model (PLM) that predicts a target ingredient's quantity and measurement unit given its recipe context and adopted the Discrete Latent Exponent method to cope with high variance of numerical scales in recipe corpora.","score":4},{"url":"https://www.semanticscholar.org/paper/d331de3b6bebb0f9af1fddf1b730ec057a7026d4","title":"Relational World Knowledge Representation in Contextual Language Models: A Review","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":131,"citationCount":20,"influentialCitationCount":1,"publicationDate":"12/04/2021","authors":"Tara Safavi,Danai Koutra","id":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","summary":"This work proposes to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision, and provides a high-level, extensible taxonomy for knowledge representation in L Ms.","score":4},{"url":"https://www.semanticscholar.org/paper/a630c70aed27b52f6d04d1e772b153c5a7b6f6fe","title":"LILA: A Unified Benchmark for Mathematical Reasoning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":18,"citationCount":24,"influentialCitationCount":2,"publicationDate":"31/10/2022","authors":"Swaroop Mishra,Matthew Finlayson,Pan Lu,Leonard Tang,S. Welleck,Chitta Baral,Tanmay Rajpurohit,Oyvind Tafjord,Ashish Sabharwal,Peter Clark,A. Kalyan","id":"a630c70aed27b52f6d04d1e772b153c5a7b6f6fe","summary":"It is found that multi-tasking leads to significant improvements (average relative improvement of 21.83% F1 score vs. single-task models), indicating the room for improvement in general mathematical reasoning and understanding.","score":4},{"url":"https://www.semanticscholar.org/paper/338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c","title":"STOAT: Structured Data to Analytical Text With Controls","venue":"arXiv.org","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/05/2023","authors":"Deepanway Ghosal,Preksha Nema,A. Raghuveer","id":"338d8a6dba2bcd6e9c8bc2a049f4d3129d5a3c0c","summary":"STOAT model is proposed, which is table and reasoning aware, with vector-quantization to infuse the given reasoning categories in the output and generates 15.3% more faithful and analytical descriptions as compared to the baseline models in human evaluation.","score":4},{"url":"https://www.semanticscholar.org/paper/24bcdce51edb8e1174fbabd072a0c07bf7362d57","title":"Language Model Behavior: A Comprehensive Survey","venue":"arXiv.org","year":2023,"referenceCount":373,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/03/2023","authors":"Tyler A. Chang,B. Bergen","id":"24bcdce51edb8e1174fbabd072a0c07bf7362d57","summary":"This survey discusses over 250 recent studies of English language model behavior before task-specific fine-tuning and synthesizes recent results to highlight what is currently known about what large language models can and cannot do.","score":4},{"url":"https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning","venue":"arXiv.org","year":2022,"referenceCount":57,"citationCount":25,"influentialCitationCount":5,"publicationDate":"29/09/2022","authors":"Pan Lu,Liang Qiu,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Tanmay Rajpurohit,Peter Clark,A. Kalyan","id":"3e565c544a8639cc9c7568833e484d7610f5e5d4","summary":"A novel approach is proposed, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example, which verifies its effectiveness in selecting in- context examples.","score":3},{"url":"https://www.semanticscholar.org/paper/bebede105aa69a81045bf79682272ee4cfb61475","title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks","venue":"","year":2023,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/04/2023","authors":"Yixuan Weng,Minjun Zhu,Fei Xia,Bin Li,Shizhu He,Kang Liu,Jun Zhao","id":"bebede105aa69a81045bf79682272ee4cfb61475","summary":"By incorporating CoNN modules into the LM, the framework effectively tackles rule-intensive challenges and achieves flawless execution on symbolic operations tasks, highlighting the potential of the method in enabling LMs to possess true symbolic comprehension capabilities.","score":3},{"url":"https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":19,"influentialCitationCount":2,"publicationDate":"19/04/2023","authors":"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao","id":"170c97c7215f42edfb20c2248f954879e91ef86e","summary":"This paper presents Chameleon, an AI system that mitigates LLM limitations by augmenting LLMs with plug-and-play modules for compositional reasoning, and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","score":3},{"url":"https://www.semanticscholar.org/paper/fce42753155280051ac64817404b4e1d3be6ebaa","title":"MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/04/2023","authors":"Lei Zhang,Yuge Zhang,Kan Ren,Dongsheng Li,Yuqing Yang","id":"fce42753155280051ac64817404b4e1d3be6ebaa","summary":"This paper aims to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks, and showcases the possibility of extending the capability of LLM to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/a39f960b28a627d554712a457efa5d3e3b7f8406","title":"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/05/2023","authors":"Qianying Liu,Dongsheng Yang,Wenjie Zhong,Fei Cheng,S. Kurohashi","id":"a39f960b28a627d554712a457efa5d3e3b7f8406","summary":"Three pretraining tasks that operate at both the whole program and sub-program level are proposed: Variable Integrity Ranking, which guides the model to focus on useful variables; Variable Operator Prediction, which decomposes the supervision into fine-grained single operator prediction; and Variable Keyphrase Masking, which encourages the modelto identify key evidence that sub- programs are derived from.","score":3},{"url":"https://www.semanticscholar.org/paper/1d9b7628e5775cdf979de01c2d0bca4f8ed37970","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":6,"influentialCitationCount":1,"publicationDate":"19/04/2023","authors":"Chuanyang Zheng,Zhengying Liu,Enze Xie,Zhenguo Li,Yu Li","id":"1d9b7628e5775cdf979de01c2d0bca4f8ed37970","summary":"This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers.","score":3},{"url":"https://www.semanticscholar.org/paper/82875a44fcda48823aeceadd9a7983e997eccc5a","title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":4,"influentialCitationCount":1,"publicationDate":"06/05/2023","authors":"Lei Wang,Wanyu Xu,Yihuai Lan,Zhiqiang Hu,Yunshi Lan,R. Lee,Ee-Peng Lim","id":"82875a44fcda48823aeceadd9a7983e997eccc5a","summary":"The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem.","score":3},{"url":"https://www.semanticscholar.org/paper/a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/05/2023","authors":"Wenhu Chen,Ming Yin,Max Ku,Yixin Wan,Xueguang Ma,Jianyu Xu,Tony Xia,Xinyi Wang,Pan Lu","id":"a52dd1e900200e0733eea927edc7d6c27aeba187","summary":"This paper introduces TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems and finds that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting.","score":3},{"url":"https://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension","venue":"arXiv.org","year":2022,"referenceCount":61,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Ippei Fujisawa,R. Kanai","id":"965e409a3e7b5670d609837fac9823b160d6639c","summary":"This work defines and characterize logical tasks and discusses system requirements for their solution, and discusses the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias.","score":3},{"url":"https://www.semanticscholar.org/paper/763f47b73d0273a93a52a8ee8a63cf19009df6e1","title":"Injecting the BM25 Score as Text Improves BERT-Based Re-rankers","venue":"European Conference on Information Retrieval","year":2023,"referenceCount":68,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/01/2023","authors":"Arian Askari,Amin Abolghasemi,G. Pasi,Wessel Kraaij,S. Verberne","id":"763f47b73d0273a93a52a8ee8a63cf19009df6e1","summary":null,"score":3},{"url":"https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","venue":"Neural Information Processing Systems","year":2022,"referenceCount":103,"citationCount":872,"influentialCitationCount":239,"publicationDate":"28/01/2022","authors":"Jason Wei,Xuezhi Wang,Dale Schuurmans,Maarten Bosma,E. Chi,F. Xia,Quoc Le,Denny Zhou","id":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","summary":"Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration","venue":"arXiv.org","year":2021,"referenceCount":47,"citationCount":15,"influentialCitationCount":1,"publicationDate":"05/09/2021","authors":"Gabriel Recchia","id":"0f2199296f01694ee46b6059879260fb80a84fa6","summary":"The results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/884c0b6db564208d99cadf2548f0aa96dee5f859","title":"Commonsense Reasoning with Implicit Knowledge in Natural Language","venue":"Conference on Automated Knowledge Base Construction","year":2021,"referenceCount":71,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Pratyay Banerjee,Swaroop Mishra","id":"884c0b6db564208d99cadf2548f0aa96dee5f859","summary":"This work takes a middle ground where it uses smaller language models together with a relatively smaller but targeted natural language text corpora to develop unstructured commonsense knowledge sources and explores three strategies for knowledge incorporation.","score":3},{"url":"https://www.semanticscholar.org/paper/0cf5c61f367e3937dc07be634a43952a50b589a4","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation","venue":"arXiv.org","year":2022,"referenceCount":48,"citationCount":2,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski","id":"0cf5c61f367e3937dc07be634a43952a50b589a4","summary":"This work introduces an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test, and demonstrates how the framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.","score":3},{"url":"https://www.semanticscholar.org/paper/6681f0a0cc6ddaa70cdea109b941c47538caaa27","title":"Judicial knowledge-enhanced magnitude-aware reasoning for numerical legal judgment prediction","venue":"Artificial Intelligence and Law","year":2022,"referenceCount":27,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/11/2022","authors":"Sheng Bi,Zhiyao Zhou,Lu Pan,G. Qi","id":"6681f0a0cc6ddaa70cdea109b941c47538caaa27","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","title":"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data","venue":"arXiv.org","year":2022,"referenceCount":40,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Xiao Li,Yin Zhu,Sichen Liu,Jiangzhou Ju,Yuzhong Qu,Gong Cheng","id":"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","summary":"DyRRen is proposed, an extended retriever-reranker-generator framework where each generation step is enhanced by a dynamic reranking of retrieved sentences that outperforms existing baselines on the FinQA dataset.","score":3},{"url":"https://www.semanticscholar.org/paper/b885650f729c62272e632acee7d7c776c0302d83","title":"Numeric Magnitude Comparison Effects in Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Raj Sanjay Shah,Vijay Marupudi,Reba Koenen,Khushi Bhardwaj,S. Varma","id":"b885650f729c62272e632acee7d7c776c0302d83","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/def7c67c3688bc459fbef3ce50a466a0cbf411a9","title":"A Primer for Neural Arithmetic Logic Modules","venue":"arXiv.org","year":2021,"referenceCount":54,"citationCount":4,"influentialCitationCount":0,"publicationDate":"23/01/2021","authors":"Bhumika Mistry,K. Farrahi,Jonathon S. Hare","id":"def7c67c3688bc459fbef3ce50a466a0cbf411a9","summary":"Focusing on the shortcomings of NALU, an in-depth analysis is provided to reason about design choices of recent units to highlight inconsistencies in a fundamental experiment causing the inability to directly compare across papers.","score":3},{"url":"https://www.semanticscholar.org/paper/c36277b67814e0a522e786a38e1768612b0e63f2","title":"Exploring the Learning Mechanisms of Neural Division Modules","venue":"Trans. Mach. Learn. Res.","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Bhumika Mistry,K. Farrahi,Jonathon S. Hare","id":"c36277b67814e0a522e786a38e1768612b0e63f2","summary":"It is shown that robustly learning division in a systematic manner remains a challenge even at the simplest level of dividing two numbers, and a novel approach to division is proposed which is called the Neural Reciprocal Unit (NRU) and the Neural Multiplicative Reciproc Unit (NMRU).","score":3},{"url":"https://www.semanticscholar.org/paper/706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding","venue":"International Conference on Learning Representations","year":2022,"referenceCount":90,"citationCount":8,"influentialCitationCount":2,"publicationDate":"06/04/2022","authors":"Ruibo Liu,Guoqing Zheng,Shashank Gupta,Radhika Gaonkar,Chongyang Gao,Soroush Vosoughi,Milad Shokouhi,A. Awadallah","id":"706c6b3781374b0b11f98f204a4ddd05b26ed009","summary":"Knowledge Infused Decoding (KID) -- a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding, which maintains a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning.","score":3},{"url":"https://www.semanticscholar.org/paper/9a9e68d400069f023f7dc9b982226c95159a509d","title":"Dissociating language and thought in large language models: a cognitive perspective","venue":"arXiv.org","year":2023,"referenceCount":412,"citationCount":32,"influentialCitationCount":2,"publicationDate":"16/01/2023","authors":"Kyle Mahowald,Anna A. Ivanova,I. Blank,N. Kanwisher,J. Tenenbaum,Evelina Fedorenko","id":"9a9e68d400069f023f7dc9b982226c95159a509d","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb","title":"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":39,"citationCount":10,"influentialCitationCount":1,"publicationDate":"20/04/2021","authors":"Lisa Bauer","id":"ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb","summary":"This work presents an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which is called KG-to-task match and demonstrates that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHow-based KGs are the bestmatch for PIQA across all 3 analysis phases.","score":3},{"url":"https://www.semanticscholar.org/paper/6597d61bdb531051678c773526758a6dc113b9ce","title":"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences","venue":"Findings","year":2021,"referenceCount":50,"citationCount":14,"influentialCitationCount":6,"publicationDate":"02/06/2021","authors":"Shikhar Singh,Nuan Wen,Yu Hou,Pegah Alipoormolabashi,Te-Lin Wu,Xuezhe Ma,Nanyun Peng","id":"6597d61bdb531051678c773526758a6dc113b9ce","summary":"This work introduces a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs, and proposes a pairwise accuracy metric to reliably measure an agent's ability to perform Commonsense reasoning over a given situation.","score":3},{"url":"https://www.semanticscholar.org/paper/8b0b5b998d65add5c036b45c36afa3f9df96f4b0","title":"Toward Building a Language Model for Understanding Temporal Commonsense","venue":"AACL","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Mayuko Kimura,L. Pereira,Ichiro Kobayashi","id":"8b0b5b998d65add5c036b45c36afa3f9df96f4b0","summary":"This paper focuses on the development of language models for temporal commonsense inference over several pre-trained language models, and relies on multi-step fine-tuning using multiple corpora, and masked language modeling to predict masked temporal indicators that are crucial for temporal Commonsense reasoning.","score":3},{"url":"https://www.semanticscholar.org/paper/9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","title":"20Q: Overlap-Free World Knowledge Benchmark for Language Models","venue":"IEEE Games Entertainment Media Conference","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans","id":"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","summary":"20Q is introduced, a novel benchmark using the Twenty Questions game to evaluate world knowledge and common sense of language models and shows that in-context learning is inefficient for evaluating language models’ world knowledge — fine-tuning is necessary to show their true capabilities.","score":3},{"url":"https://www.semanticscholar.org/paper/a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b","title":"Good Night at 4 pm?! Time Expressions in Different Cultures","venue":"Findings","year":2022,"referenceCount":46,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"V. Shwartz","id":"a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b","summary":"3 language-agnostic methods are proposed for mapping from expressions such as “morning” in English or “Manhã’ in Portuguese to specific hours in the day and one achieves promising results on gold standard annotations that is collected for a small number of languages.","score":3},{"url":"https://www.semanticscholar.org/paper/7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey","venue":"AAAI Conference on Artificial Intelligence","year":2022,"referenceCount":82,"citationCount":15,"influentialCitationCount":0,"publicationDate":"28/01/2022","authors":"Prajjwal Bhargava,Vincent Ng","id":"7e5ca499cd9b932921bda84db98f75087d0b0683","summary":"A survey of commonsense knowledge acquisition and reasoning tasks, the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions are presented.","score":3},{"url":"https://www.semanticscholar.org/paper/98f19ca97512361b12475b42b67a617de14d33a1","title":"Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic","venue":"AAAI Conference on Artificial Intelligence","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/06/2022","authors":"Bibo Cai,Xiao Ding,Bowen Chen,L. Du,Ting Liu","id":"98f19ca97512361b12475b42b67a617de14d33a1","summary":"A novel neural-logic based Soft Logic Enhanced Event Temporal Reasoning (SLEER) model is proposed for acquiring unbiased TCS knowledge, in which the complementary relationship among dimensions are explicitly represented as logic rules and modeled by t-norm fuzzy logics.","score":3},{"url":"https://www.semanticscholar.org/paper/9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","title":"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":44,"citationCount":3,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Himanshu Gupta,Neeraj Varshney,Swaroop Mishra,Kuntal Kumar Pal,Saurabh Arjun Sawant,Kevin Scaria,Siddharth Goyal,Chitta Baral","id":"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","summary":"This work introduces FeasibilityQA, a question-answering dataset involving binary classification (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility, and shows that even state-of-the-art models such as GPT-3, G PT-2, and T5 struggle to answer the feasibility questions correctly.","score":3},{"url":"https://www.semanticscholar.org/paper/ee153a2c91d36b034dc86c945aee9859b79da812","title":"Test of Time: Instilling Video-Language Models with a Sense of Time","venue":"arXiv.org","year":2023,"referenceCount":131,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/01/2023","authors":"Piyush Bagad,Makarand Tapaswi,Cees G. M. Snoek","id":"ee153a2c91d36b034dc86c945aee9859b79da812","summary":"This work proposes a temporal adaptation recipe on top of one video-language model, VideoCLIP, based on post-pretraining on a small amount of video-text data and conducts a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness.","score":3},{"url":"https://www.semanticscholar.org/paper/b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":52,"citationCount":16,"influentialCitationCount":2,"publicationDate":"24/05/2021","authors":"Shunyu Yao,Binghui Peng,C. Papadimitriou,Karthik Narasimhan","id":"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","summary":"It is proved that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language.","score":3},{"url":"https://www.semanticscholar.org/paper/4f451ba06c4c9effd6c4ac0bae222495501a6200","title":"Innovations in Neural Data-to-text Generation","venue":"arXiv.org","year":2022,"referenceCount":340,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Mandar Sharma,Ajay K. Gogineni,Naren Ramakrishnan","id":"4f451ba06c4c9effd6c4ac0bae222495501a6200","summary":"This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella.","score":3},{"url":"https://www.semanticscholar.org/paper/4533f5afa772b0b25703a7258993a7c28a85f9a3","title":"NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering","venue":"arXiv.org","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Tengxun Zhang,Hongfei Xu,Josef van Genabith,Deyi Xiong,Hongying Zan","id":"4533f5afa772b0b25703a7258993a7c28a85f9a3","summary":"This paper proposes a non-autoregressive program generation framework, which independently generates complete program tuples containing both operators and operands, which can significantly boost the speed of program generation while addressing the error accumulation issue.","score":2},{"url":"https://www.semanticscholar.org/paper/4ef578869c957f5fb969fa9c164dca0433f48042","title":"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":58,"citationCount":9,"influentialCitationCount":2,"publicationDate":"15/09/2021","authors":"Zhoujun Cheng,Haoyu Dong,Fan Cheng,Ran Jia,Pengfei Wu,Shi Han,Dongmei Zhang","id":"4ef578869c957f5fb969fa9c164dca0433f48042","summary":"This paper proposes FORTAP, the first exploration to leverage spreadsheet formulas for table pretraining with tree attention, and proposes two novel self-supervised pretraining objectives derived from formulas, numerical reference prediction (NRP) and numerical calculation prediction (NCP).","score":2},{"url":"https://www.semanticscholar.org/paper/cbccb1201eee6432020276762a44ebfbf8f981a0","title":"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge","venue":"arXiv.org","year":2022,"referenceCount":40,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/04/2022","authors":"Ashwani Tanwar,Jingqing Zhang,Julia Ive,Vibhor Gupta,Yike Guo","id":"cbccb1201eee6432020276762a44ebfbf8f981a0","summary":"A novel unsupervised methodology leveraging external knowledge and contextualized word embeddings from ClinicalBERT for numerical reasoning in a variety of phenotypic contexts is presented.","score":2},{"url":"https://www.semanticscholar.org/paper/efcd9c1438559d908efd702333232078fd251a0f","title":"Phenotyping in clinical text with unsupervised numerical reasoning for patient stratification","venue":"Experimental biology and medicine","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Ashwani Tanwar,Jingqing Zhang,Julia Ive,Vibhor Gupta,Yike Guo","id":"efcd9c1438559d908efd702333232078fd251a0f","summary":"This article proposes a novel unsupervised method that leverages external clinical knowledge and contextualized word embeddings by ClinicalBERT for numerical reasoning in different phenotypic contexts and finds that these phenotypes from clinical text can be used to impute the missing values in structured data, which enrich and improve data quality.","score":2},{"url":"https://www.semanticscholar.org/paper/e605f32b628b3a36b72cab0e9189aacac0b4f780","title":"Text2Struct: A Machine Learning Pipeline for Mining Structured Data from Text","venue":"arXiv.org","year":2022,"referenceCount":11,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Chaochao Zhou,Bo Yang","id":"e605f32b628b3a36b72cab0e9189aacac0b4f780","summary":"Results show that Text2Struct is viable for the mining of structured data from text without special templates or patterns, and it is anticipated to further improve the pipeline by expanding the dataset and investigating other machine learning models.","score":2},{"url":"https://www.semanticscholar.org/paper/a7b060413027cbd25b6144f4a6214c3bd4fb12e3","title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters","venue":"arXiv.org","year":2022,"referenceCount":20,"citationCount":27,"influentialCitationCount":6,"publicationDate":"20/12/2022","authors":"Boshi Wang,Sewon Min,Xiang Deng,Jiaming Shen,You Wu,Luke Zettlemoyer,Huan Sun","id":"a7b060413027cbd25b6144f4a6214c3bd4fb12e3","summary":"It is shown that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference.","score":2},{"url":"https://www.semanticscholar.org/paper/27f32dc1aab7919eb7039deca067c3fbdc719c2a","title":"Koala: An Index for Quantifying Overlaps with Pre-training Corpora","venue":"arXiv.org","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/03/2023","authors":"Thuy-Trang Vu,Xuanli He,Gholamreza Haffari,Ehsan Shareghi","id":"27f32dc1aab7919eb7039deca067c3fbdc719c2a","summary":"Koala, a searchable index over large pre-training corpora using compressed suffix arrays with highly efficient compression rate and search support, is launched, providing a framework to do forensic analysis on the current and future benchmarks as well as to assess the degree of memorization in the output from the LLMs.","score":2},{"url":"https://www.semanticscholar.org/paper/ecf0cb0725de18659f9ba25a8cf65a1085564006","title":"Understanding Arithmetic Reasoning in Language Models using Causal Mediation Analysis","venue":"","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/05/2023","authors":"Alessandro Stolfo,Yonatan Belinkov,Mrinmaya Sachan","id":"ecf0cb0725de18659f9ba25a8cf65a1085564006","summary":"A mechanistic interpretation of LLMs for arithmetic-based questions using a causal mediation analysis framework, which identifies the subset of parameters responsible for specific predictions and investigates the role of the attention mechanism.","score":2},{"url":"https://www.semanticscholar.org/paper/0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","title":"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/05/2023","authors":"Fengbin Zhu,Chao Wang,Fuli Feng,Zifeng Ren,Moxin Li,Tat-Seng Chua","id":"0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","summary":"This work proposes a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements of the given question and document with Semantic-oriented hierarchical Graph structures.","score":2},{"url":"https://www.semanticscholar.org/paper/87a31b729295a3357949683276a2625288fdd0f0","title":"PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":61,"citationCount":6,"influentialCitationCount":2,"publicationDate":"17/10/2022","authors":"Yang Deng,Wenqiang Lei,Wenxuan Zhang,W. Lam,Tat-Seng Chua","id":"87a31b729295a3357949683276a2625288fdd0f0","summary":"A novel method is proposed, namely UniPCQA, to adapt a hybrid format of input and output content in PCQA into the Seq2Seq problem, including the reformulation of the numerical reasoning process as code generation.","score":2},{"url":"https://www.semanticscholar.org/paper/1f84b6075ffa2d4ed685f31668ea1324bfba4fe5","title":"A Survey on Medical Document Summarization","venue":"arXiv.org","year":2022,"referenceCount":132,"citationCount":4,"influentialCitationCount":1,"publicationDate":"03/12/2022","authors":"Raghav Jain,Anubhav Jangra,S. Saha,A. Jatowt","id":"1f84b6075ffa2d4ed685f31668ea1324bfba4fe5","summary":"This paper gives a comprehensive survey of the current techniques and trends in medical summarization.","score":2},{"url":"https://www.semanticscholar.org/paper/9e1ba67d5f443a8bd42a8b856534f50c429baf11","title":"Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/05/2023","authors":"Mandar Sharma,N. Muralidhar,Naren Ramakrishnan","id":"9e1ba67d5f443a8bd42a8b856534f50c429baf11","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers","venue":"arXiv.org","year":2022,"referenceCount":81,"citationCount":4,"influentialCitationCount":0,"publicationDate":"31/05/2022","authors":"S. S. Sundaram,Sairam Gurajada,M. Fisichella,Deepak P,Savitha Sam Abraham","id":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","summary":"This paper critically examine the various models that have been developed for solving word problems, their pros and cons and the challenges ahead, and endeavours to provide a road-map for future math word problem research.","score":2},{"url":"https://www.semanticscholar.org/paper/3ce8c07349d91bb3f022a211be36e98eef0e1046","title":"MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems","venue":"arXiv.org","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/06/2022","authors":"Keyur Faldu,Amit P. Sheth,Prashant Kikani,Darshan Patel","id":"3ce8c07349d91bb3f022a211be36e98eef0e1046","summary":"A novel model MMTM that leverages multi-tasking and multi-decoder during pre-training that achieves better mathematical reasoning ability and generalisability, which is demonstrated by outperforming the best state of the art baseline models from Seq2Seq, GTS, and Graph2Tree with a relative improvement on an adversarial challenge dataset SVAMP.","score":2},{"url":"https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","title":"Language models show human-like content effects on reasoning","venue":"arXiv.org","year":2022,"referenceCount":104,"citationCount":37,"influentialCitationCount":1,"publicationDate":"14/07/2022","authors":"I. Dasgupta,Andrew Kyle Lampinen,Stephanie C. Y. Chan,Antonia Creswell,D. Kumaran,James L. McClelland,Felix Hill","id":"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","summary":"This work hypothesized that language models would show human-like content effects on abstract reasoning problems, and finds that state of the art large language models reflect many of the same patterns observed in humans across these tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","title":"Mind's Eye: Grounded Language Model Reasoning through Simulation","venue":"arXiv.org","year":2022,"referenceCount":81,"citationCount":14,"influentialCitationCount":4,"publicationDate":"11/10/2022","authors":"Ruibo Liu,Jason Wei,S. Gu,Te-Yen Wu,Soroush Vosoughi,Claire Cui,Denny Zhou,Andrew M. Dai","id":"2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","summary":"Mind's Eye is presented, a paradigm to ground language model reasoning in the physical world by using a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then using the simulation results as part of the input, which enables language models to perform reasoning.","score":2},{"url":"https://www.semanticscholar.org/paper/b6b42f59ca3e0c5147b159f7c1fe2169219d5263","title":"Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Wenqi Zhang,Yongliang Shen,Yanna Ma,Xiaoxia Cheng,Zeqi Tan,Qingpeng Nong,Weiming Lu","id":"b6b42f59ca3e0c5147b159f7c1fe2169219d5263","summary":"Experiments show the proposed multi-view consistent contrastive learning approach significantly outperforms the existing baselines, especially on complex problems, and can absorb the merits of both views and generate more diverse results consistent with the mathematical laws.","score":2},{"url":"https://www.semanticscholar.org/paper/f8baff7610d0483caec5dc6faf5bf486298eeb90","title":"Large Language Models Are Reasoning Teachers","venue":"arXiv.org","year":2022,"referenceCount":53,"citationCount":21,"influentialCitationCount":3,"publicationDate":"20/12/2022","authors":"Namgyu Ho,Laura Schmid,Se-Young Yun","id":"f8baff7610d0483caec5dc6faf5bf486298eeb90","summary":"Fine-tune-CoT is proposed, a method that leverages the capabilities of very large LMs to generate reasoning samples and teach smaller models via fine-tuning, and enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance.","score":2},{"url":"https://www.semanticscholar.org/paper/e659fa1e79a2a151be331125c14339988542aac3","title":"Batch Prompting: Efficient Inference with Large Language Model APIs","venue":"arXiv.org","year":2023,"referenceCount":66,"citationCount":5,"influentialCitationCount":0,"publicationDate":"19/01/2023","authors":"Zhoujun Cheng,Jungo Kasai,Tao Yu","id":"e659fa1e79a2a151be331125c14339988542aac3","summary":"Batch prompting, a simple alternative prompting approach that enables the LLM to run inference in batches, instead of one sample at a time, is proposed, which reduces both token and time costs while retaining downstream performance.","score":2},{"url":"https://www.semanticscholar.org/paper/ea0688f9e7dfb0d3c2249486af65209c25809544","title":"Faithful Chain-of-Thought Reasoning","venue":"arXiv.org","year":2023,"referenceCount":31,"citationCount":14,"influentialCitationCount":1,"publicationDate":"31/01/2023","authors":"QING LYU,Shreya Havaldar,Adam Stein,Li Zhang,D. Rao,Eric Wong,Marianna Apidianaki,Chris Callison-Burch","id":"ea0688f9e7dfb0d3c2249486af65209c25809544","summary":"Faithful CoT is proposed, a faithful-by-construction framework that decomposes a reasoning task into two stages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\\ rightarrow$ answer), using an LM and a deterministic solver respectively.","score":2},{"url":"https://www.semanticscholar.org/paper/8bfc22de7fe66286ad9ae705d677246757fbf8a8","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":23,"influentialCitationCount":2,"publicationDate":"31/01/2023","authors":"Freda Shi,Xinyun Chen,Kanishka Misra,Nathan Scales,David Dohan,E. Chi,Nathanael Scharli,Denny Zhou","id":"8bfc22de7fe66286ad9ae705d677246757fbf8a8","summary":"This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description.","score":2},{"url":"https://www.semanticscholar.org/paper/69619a2a47faee7a29ec596db13172e2a42ff921","title":"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":8,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Zhihong Shao,Yeyun Gong,Yelong Shen,Minlie Huang,Nan Duan,Weizhu Chen","id":"69619a2a47faee7a29ec596db13172e2a42ff921","summary":"Synthetic prompting is introduced, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning.","score":2},{"url":"https://www.semanticscholar.org/paper/40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a","title":"Techniques to Improve Neural Math Word Problem Solvers","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/02/2023","authors":"Youyuan Zhang","id":"40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a","summary":"A new encoder-decoder architecture that fully leverages the question text and preserves step-wise commutative law is proposed and outperforms state-of-the-art neural MWP solvers, showing the effectiveness of the techniques.","score":2},{"url":"https://www.semanticscholar.org/paper/873a581320d928249609d3c07229d5af182a379c","title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?","venue":"arXiv.org","year":2023,"referenceCount":86,"citationCount":106,"influentialCitationCount":12,"publicationDate":"08/02/2023","authors":"Chengwei Qin,Aston Zhang,Zhuosheng Zhang,Jiaao Chen,Michihiro Yasunaga,Diyi Yang","id":"873a581320d928249609d3c07229d5af182a379c","summary":"It is found that ChatGPT performs well on many tasks favoring reasoning capabilities while it still faces challenges when solving specific tasks such as sequence tagging, and with extensive empirical studies, both the effectiveness and limitations of the current version of ChatG PT are demonstrated.","score":2},{"url":"https://www.semanticscholar.org/paper/0d42221038c05cee8443c5b5af838505ee137dc3","title":"ART: Automatic multi-step reasoning and tool-use for large language models","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":12,"influentialCitationCount":2,"publicationDate":"16/03/2023","authors":"Bhargavi Paranjape,Scott M. Lundberg,Sameer Singh,Hanna Hajishirzi,Luke Zettlemoyer,Marco Tulio Ribeiro","id":"0d42221038c05cee8443c5b5af838505ee137dc3","summary":"Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program, achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey","venue":"arXiv.org","year":2023,"referenceCount":150,"citationCount":5,"influentialCitationCount":0,"publicationDate":"20/03/2023","authors":"Ruochen Zhao,Hailin Chen,Weishi Wang,Fangkai Jiao,Xuan Long Do,Chengwei Qin,Bosheng Ding,Xiaobao Guo,Minzhi Li,Xingxuan Li,Shafiq R. Joty","id":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","summary":"This survey provides an in-depth review of retrieval-augmented generation in different modalities and discusses potential future directions of this emerging field.","score":2},{"url":"https://www.semanticscholar.org/paper/1d29334cfbe9a1a943082058876f0c22d44c62fd","title":"A Survey of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":415,"citationCount":61,"influentialCitationCount":7,"publicationDate":"31/03/2023","authors":"Wayne Xin Zhao,Kun Zhou,Junyi Li,Tianyi Tang,Xiaolei Wang,Yupeng Hou,Yingqian Min,Beichen Zhang,Junjie Zhang,Zican Dong,Yifan Du,Chen Yang,Yushuo Chen,Z. Chen,Jinhao Jiang,Ruiyang Ren,Yifan Li,Xinyu Tang,Zikang Liu,Peiyu Liu,J. Nie,Ji-rong Wen","id":"1d29334cfbe9a1a943082058876f0c22d44c62fd","summary":"A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.","score":2},{"url":"https://www.semanticscholar.org/paper/0061d6c1aa6c6032120974e8939ee11f5eed8813","title":"Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":7,"influentialCitationCount":0,"publicationDate":"01/05/2023","authors":"Yuxi Xie,Kenji Kawaguchi,Yiran Zhao,Xu Zhao,MingSung Kan,Junxian He,Qizhe Xie","id":"0061d6c1aa6c6032120974e8939ee11f5eed8813","summary":"An effective prompting approach that integrates self-evaluation guidance through stochastic beam search is proposed that explores the reasoning search space using a well-calibrated automatic criterion and enables an efficient search to produce higher-quality final predictions.","score":2},{"url":"https://www.semanticscholar.org/paper/1f07d5bf633fdaaf7dcb73397688cf256c6d709f","title":"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/05/2023","authors":"Tianci Xue,Ziqi Wang,Zhenhailong Wang,Chi Han,Pengfei Yu,Heng Ji","id":"1f07d5bf633fdaaf7dcb73397688cf256c6d709f","summary":"RCoT (Reversing Chain-of-Thought) is proposed, a novel method to improve large language Models' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs' generated solutions and manually writing fine-grained feedback to guide LLMs in revising solutions.","score":2},{"url":"https://www.semanticscholar.org/paper/4313b009cd30a469400d36b9fb20267620293d04","title":"A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/05/2023","authors":"Jordan Meadows,Marco Valentino,Damien Teney,André Freitas","id":"4313b009cd30a469400d36b9fb20267620293d04","summary":"A data generation method for producing intricate mathematical derivations, and systematically perturb them with respect to syntax, structure, and semantics, employing symbolic algebra for scalable data production and augmentation.","score":2},{"url":"https://www.semanticscholar.org/paper/073e6b91adc25c656d85002e3cb059e4530db20b","title":"Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation","venue":"","year":2023,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Zhenwen Liang,Wenhao Yu,Tanmay Rajpurohit,Peter Clark,Xiangliang Zhang,Ashwin Kaylan","id":"073e6b91adc25c656d85002e3cb059e4530db20b","summary":"A novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles.","score":2},{"url":"https://www.semanticscholar.org/paper/6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3","title":"Automatic Model Selection with Large Language Models for Reasoning","venue":"","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Xu Zhao,Yuxi Xie,Kenji Kawaguchi,Junxian He,Qizhe Xie","id":"6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3","summary":"This work demonstrates that it is possible to combine the best of both worlds by using different models for different problems, employing a large language model (LLM) to perform model selection.","score":2},{"url":"https://www.semanticscholar.org/paper/700f9c8a76d7af0fc550d119aa1d1164a496055e","title":"Mixture of Prompt Experts for Generalizable and Interpretable Question Answering","venue":"","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/05/2023","authors":"Chenglei Si,Weijia Shi,Chen Zhao,Luke Zettlemoyer,Jordan L. Boyd-Graber","id":"700f9c8a76d7af0fc550d119aa1d1164a496055e","summary":"A Mixture-of-Prompt-Experts (MOPE) system that ensembles multiple specialized LLMs that significantly outperforms any single specialized model on a collection of 12 QA datasets from four reasoning types.","score":2},{"url":"https://www.semanticscholar.org/paper/4befd752d21a6231a9d930b1946177bd4cba30cb","title":"Mathematical Word Problem Solution Evaluation via Data Preprocessing Approach","venue":"International Conference \"Information Technology and Interactions\"","year":2021,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Andrii D. Nikolaiev,A. Anisimov","id":"4befd752d21a6231a9d930b1946177bd4cba30cb","summary":"An overview of methods for processing mathematical text problems and correspondent domain datasets is described and a new approach to estimate MWP solutions is proposed that could use more context around the problem.","score":2},{"url":"https://www.semanticscholar.org/paper/cc8417aa578016203cb52efc63592bba64b08bb3","title":"FinMath: Injecting a Tree-structured Solver for Question Answering over Financial Reports","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":35,"citationCount":5,"influentialCitationCount":2,"publicationDate":2022,"authors":"Chenying Li,Wenbo Ye,Yilun Zhao","id":"cc8417aa578016203cb52efc63592bba64b08bb3","summary":"A new framework named FinMath is proposed, which improves the model’s numerical reasoning capacity by injecting a tree-structured neural model to perform multi-step numerical reasoning.","score":2},{"url":"https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","venue":"BIGSCIENCE","year":2022,"referenceCount":141,"citationCount":161,"influentialCitationCount":20,"publicationDate":"14/04/2022","authors":"Sid Black,Stella Rose Biderman,Eric Hallahan,Quentin G. Anthony,Leo Gao,Laurence Golding,Horace He,Connor Leahy,Kyle McDonell,Jason Phang,M. Pieler,USVSN Sai Prashanth,Shivanshu Purohit,Laria Reynolds,J. Tow,Benqi Wang,Samuel Weinbach","id":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","summary":"GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.","score":2},{"url":"https://www.semanticscholar.org/paper/c9f48406851954cb098911eccb4124ea5f966675","title":"A Survey on Multi-hop Question Answering and Generation","venue":"arXiv.org","year":2022,"referenceCount":178,"citationCount":8,"influentialCitationCount":0,"publicationDate":"19/04/2022","authors":"Vaibhav Mavi,Anubhav Jangra,A. Jatowt","id":"c9f48406851954cb098911eccb4124ea5f966675","summary":"A general and formal definition of MHQA task is provided, the existing attempts to this highly interesting, yet quite challenging task are organized and summarized, and the best methods to createMHQA datasets are outlined.","score":2},{"url":"https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models","venue":"Neural Information Processing Systems","year":2022,"referenceCount":32,"citationCount":33,"influentialCitationCount":13,"publicationDate":"11/07/2022","authors":"Cem Anil,Yuhuai Wu,Anders Andreassen,Aitor Lewkowycz,Vedant Misra,V. Ramasesh,Ambrose Slone,Guy Gur-Ari,Ethan Dyer,Behnam Neyshabur","id":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","summary":"This paper establishes that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale, and shows that combining pretrained large language models' in-context learning abilities with scratchpad prompting results in a dramatic improvement in lengthgeneralization.","score":2},{"url":"https://www.semanticscholar.org/paper/2fe6060ced80c1c245a718e6188b6516207bf0a8","title":"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":40,"citationCount":8,"influentialCitationCount":2,"publicationDate":"30/09/2022","authors":"Rindranirina Ramamonjison,Haley Li,Timothy T. Yu,Shiqi He,Vishnu Rengan,Amin Banitalebi-Dehkordi,Zirui Zhou,Yong Zhang","id":"2fe6060ced80c1c245a718e6188b6516207bf0a8","summary":"An augmented intelligence system for simplifying and enhancing the modeling experience for operations research is described that receives a suggested formulation of an optimization problem based on its description and enables the users to validate and edit the suggestions.","score":2},{"url":"https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1","title":"Galactica: A Large Language Model for Science","venue":"arXiv.org","year":2022,"referenceCount":107,"citationCount":85,"influentialCitationCount":25,"publicationDate":"16/11/2022","authors":"Ross Taylor,Marcin Kardas,Guillem Cucurull,Thomas Scialom,A. Hartshorn,Elvis Saravia,Andrew Poulton,Viktor Kerkez,Robert Stojnic","id":"7d645a3fd276918374fd9483fd675c28e46506d1","summary":"Galactica is introduced: a large language model that can store, combine and reason about scientific knowledge, and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%.","score":2},{"url":"https://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932","title":"Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety","venue":"arXiv.org","year":2022,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Joshua Albrecht,Ellie Kitanidis,Abraham J. Fetterman","id":"7d5175db1b99552491063d2d9581b0b51e1d2932","summary":"This work provides a simple new prompting strategy that leads to yet another supposedly \"super-human\" result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset).","score":2},{"url":"https://www.semanticscholar.org/paper/819f91874cfa43335401cddedd02b8cfc1c2b84f","title":"Mathematical Capabilities of ChatGPT","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":46,"influentialCitationCount":1,"publicationDate":"31/01/2023","authors":"Simon Frieder,Luca Pinchetti,Ryan-Rhys Griffiths,Tommaso Salvatori,Thomas Lukasiewicz,Philipp Christian Petersen,Alexis Chevalier,J. Berner","id":"819f91874cfa43335401cddedd02b8cfc1c2b84f","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/cd0988714ea326642d2b1bb18753e187fec71e42","title":"A Categorical Archive of ChatGPT Failures","venue":"arXiv.org","year":2023,"referenceCount":66,"citationCount":57,"influentialCitationCount":5,"publicationDate":"06/02/2023","authors":"A. Borji","id":"cd0988714ea326642d2b1bb18753e187fec71e42","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/033275ccc2c7c5c38592ae893da0b5923cf90717","title":"Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases","venue":"arXiv.org","year":2023,"referenceCount":172,"citationCount":3,"influentialCitationCount":0,"publicationDate":"26/03/2023","authors":"Hongyu Ren,Mikhail Galkin,M. Cochez,Zhaocheng Zhu,J. Leskovec","id":"033275ccc2c7c5c38592ae893da0b5923cf90717","summary":"A holistic survey of CLQA is provided with a detailed taxonomy studying the field from multiple angles, including graph types (modality, reasoning domain, background semantics), modeling aspects, modeling aspects (encoder, processor, decoder), supported queries, datasets, evaluation metrics, and applications.","score":2},{"url":"https://www.semanticscholar.org/paper/dca6c3927ade6481a1ae080f5c24decbfeced1be","title":"Boosted Prompt Ensembles for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":2,"influentialCitationCount":0,"publicationDate":"12/04/2023","authors":"Silviu Pitis,Michael Ruogu Zhang,Andrew Wang,Jimmy Ba","id":"dca6c3927ade6481a1ae080f5c24decbfeced1be","summary":"A prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble'' that outperforms single-prompt output-space ensembles and bagged prompt-spaceEnsemble on the GSM8k and AQuA datasets, among others.","score":2},{"url":"https://www.semanticscholar.org/paper/95ca67ba607d7859ee8eec457f4b59b115d69bf5","title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models","venue":"","year":2023,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Z. Chen,Kun Zhou,Beichen Zhang,Zheng Gong,Wayne Xin Zhao,Ji-rong Wen","id":"95ca67ba607d7859ee8eec457f4b59b115d69bf5","summary":"This work proposes ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs that can effectively leverage the multi-turn conversation ability of chat- based LLMs, and integrate the thought chain following and tools manipulation in a unified way.","score":2},{"url":"https://www.semanticscholar.org/paper/58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","title":"JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection","venue":"","year":2022,"referenceCount":17,"citationCount":2,"influentialCitationCount":1,"publicationDate":2022,"authors":"Shunsuke Onuma,Kazuma Kadowaki","id":"58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/c27f3904490b8e5f9f39fe2b36722090c189e916","title":"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF","venue":"Conference and Labs of the Evaluation Forum","year":2022,"referenceCount":58,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Wei Zhong,Yuqing Xie,Jimmy J. Lin","id":"c27f3904490b8e5f9f39fe2b36722090c189e916","summary":"This work describes the participation of the team in the ARQMath 2022 Lab, where two highly complementary methods for effective math answer and formula retrieval are applied, using a lexical sparse retriever and a fine-tuned bi-encoder dense retriever to capture contextual similarity and semantic matching.","score":2},{"url":"https://www.semanticscholar.org/paper/8e5ca53f7633450e2756950c234c5f9d04b5c9f2","title":"One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development","venue":"JMIR Medical Informatics","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/09/2022","authors":"Pulkit Singh,Julian S Haimovich,C. Reeder,S. Khurshid,Emily S. Lau,Jonathan W Cunningham,A. Philippakis,C. D. Anderson,J. Ho,S. Lubitz,P. Batra","id":"8e5ca53f7633450e2756950c234c5f9d04b5c9f2","summary":"A domain-agnostic pretrained transformer model is able to effectively extract quantitative clinical measurements from diagnostic reports with a relatively small number of gold-standard annotations and may serve as a roadmap for other quantitative entity extraction.","score":2},{"url":"https://www.semanticscholar.org/paper/1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","title":"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)","venue":"arXiv.org","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/11/2022","authors":"Victor Robila,Kexin Pei,Junfeng Yang","id":"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","summary":"A comparison between human-developed machine learning model and models sampled through Neural Architecture Search (NAS) determine an efficient approach to solve the problem of addition using embedded hexadecimal digits.","score":2},{"url":"https://www.semanticscholar.org/paper/718aadda34bcc2f26e705cf20809922747231f78","title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/04/2023","authors":"Michael Hanna,Ollie Liu,Alexandre Variengien","id":"718aadda34bcc2f26e705cf20809922747231f78","summary":"The basic mathematical abilities often acquired by pre-trained language models are investigated, and mechanistic interpretability techniques are used to explain the (limited) mathematical abilities of GPT-2 small.","score":2},{"url":"https://www.semanticscholar.org/paper/0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Vaishali Pal,Andrew Yates,E. Kanoulas,M. de Rijke","id":"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","summary":"This model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers, which outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery.","score":2},{"url":"https://www.semanticscholar.org/paper/41f44979cf1cd3f4cbd615dc130bc33721f5281b","title":"MemPrompt: Memory-assisted Prompt Editing with User Feedback","venue":"","year":2022,"referenceCount":41,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Aman Madaan,Niket Tandon,Peter Clark,Yiming Yang","id":"41f44979cf1cd3f4cbd615dc130bc33721f5281b","summary":"It is shown how a (simulated) user can interac-tively teach a deployed GPT -3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT 3, a step towards the low-cost utility enhancement for very large pre-trained LMs.","score":2},{"url":"https://www.semanticscholar.org/paper/5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e","title":"On the Paradox of Learning to Reason from Data","venue":"arXiv.org","year":2022,"referenceCount":41,"citationCount":23,"influentialCitationCount":5,"publicationDate":"23/05/2022","authors":"Honghua Zhang,Liunian Harold Li,Tao Meng,Kai-Wei Chang,Guy Van den Broeck","id":"5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e","summary":"This study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has in fact learned statistical features that inherently exist in logical reasoning problems.","score":2},{"url":"https://www.semanticscholar.org/paper/1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment","venue":"Neural Information Processing Systems","year":2022,"referenceCount":78,"citationCount":8,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Zhijing Jin,Sydney Levine,Fernando Gonzalez,Ojasv Kamal,Maarten Sap,Mrinmaya Sachan,Rada Mihalcea,J. Tenenbaum,B. Schölkopf","id":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","summary":"A novel moral chain of thought (MORALCOT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind.","score":2},{"url":"https://www.semanticscholar.org/paper/c715914c388fa64dd8686cd8755e5adfebbf2388","title":"REFINER: Reasoning Feedback on Intermediate Representations","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":11,"influentialCitationCount":1,"publicationDate":"04/04/2023","authors":"Debjit Paul,Mete Ismayilzada,Maxime Peyrard,Beatriz Borges,Antoine Bosselut,Robert West,B. Faltings","id":"c715914c388fa64dd8686cd8755e5adfebbf2388","summary":"REFINER is a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning that provides structured feedback that the reasoning uses to iteratively improve its intermediate arguments.","score":2},{"url":"https://www.semanticscholar.org/paper/b09420b30fc093d63fb2ee1aac26c71da81da437","title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks","venue":"Neural Information Processing Systems","year":2022,"referenceCount":180,"citationCount":16,"influentialCitationCount":3,"publicationDate":"14/06/2022","authors":"Tuan Dinh,Yuchen Zeng,Ruisu Zhang,Ziqian Lin,Shashank Rajput,Michael Gira,Jy-yong Sohn,Dimitris Papailiopoulos,Kangwook Lee","id":"b09420b30fc093d63fb2ee1aac26c71da81da437","summary":"Language-Interfaced Fine-Tuning is proposed and found that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/e82e3f4347674b75c432cb80604d38ee630d4bf6","title":"Transformers Learn Shortcuts to Automata","venue":"arXiv.org","year":2022,"referenceCount":107,"citationCount":13,"influentialCitationCount":1,"publicationDate":"19/10/2022","authors":"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang","id":"e82e3f4347674b75c432cb80604d38ee630d4bf6","summary":"It is found that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics.","score":2},{"url":"https://www.semanticscholar.org/paper/2a83a92b08e0f3873d07162c73c67e533321112e","title":"Aligning Generative Language Models with Human Values","venue":"NAACL-HLT","year":2022,"referenceCount":55,"citationCount":4,"influentialCitationCount":2,"publicationDate":2022,"authors":"Ruibo Liu,Ge Zhang,Xinyu Feng,Soroush Vosoughi","id":"2a83a92b08e0f3873d07162c73c67e533321112e","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/4497f3ed54ac520b50ffa05df04b37a59d4c1265","title":"Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET","venue":"AACL","year":2022,"referenceCount":51,"citationCount":17,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Chantal Amrhein,Rico Sennrich","id":"4497f3ed54ac520b50ffa05df04b37a59d4c1265","summary":"It is shown that sample-based Minimum Bayes Risk decoding can be used to explore and quantify weaknesses in COMET models, and that these biases are hard to fully remove by simply training on additional synthetic data.","score":2},{"url":"https://www.semanticscholar.org/paper/af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning","venue":"arXiv.org","year":2022,"referenceCount":54,"citationCount":66,"influentialCitationCount":6,"publicationDate":"15/02/2022","authors":"Yasaman Razeghi,Robert L Logan IV,Matt Gardner,Sameer Singh","id":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","summary":"Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, the results raise the question of how much models actually generalize beyond pretraining data, and researchers are encouraged to take thepretraining data into account when interpreting evaluation results.","score":2},{"url":"https://www.semanticscholar.org/paper/4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence","venue":"NAACL-HLT","year":2022,"referenceCount":53,"citationCount":2,"influentialCitationCount":0,"publicationDate":"08/05/2022","authors":"Myeongjun Jang,Frank Mtumbuka,Thomas Lukasiewicz","id":"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","summary":"A novel intermediate training task, names meaning-matching, designed to directly learn a meaning-text correspondence, is proposed that enables PLMs to learn lexical semantic information and is found to be a safe intermediate task that guarantees a similar or better performance of downstream tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/71471561137956d6aeec39173db460a28d86c11b","title":"Analogical Math Word Problems Solving with Enhanced Problem-Solution Association","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":49,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Zhenwen Liang,Jipeng Zhang,Xiangliang Zhang","id":"71471561137956d6aeec39173db460a28d86c11b","summary":"The proposed analogical learning strategy promotes the performance of MWP-BERT on Math23k over the state-of-the-art model Generate2Rank, with 5 times fewer parameters in the encoder.","score":2},{"url":"https://www.semanticscholar.org/paper/e02dce6ee032a13b1653f69b034a35676e7d4dc2","title":"Can language representation models think in bets?","venue":"Royal Society Open Science","year":2022,"referenceCount":106,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Zhi–Bin Tang,M. Kejriwal","id":"e02dce6ee032a13b1653f69b034a35676e7d4dc2","summary":"It is shown that a model is able to ‘think in bets’ if it is first fine-tuned on bet questions with an identical structure, and that LRMs could potentially be applied to tasks that rely on cognitive decision-making skills, but that more research is necessary before these models can robustly make rational decisions.","score":2},{"url":"https://www.semanticscholar.org/paper/17a8b5e6fef1f69979d57021a8f30a5159e152c7","title":"Commonsense Reasoning for Conversational AI: A Survey of the State of the Art","venue":"arXiv.org","year":2023,"referenceCount":90,"citationCount":2,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Christopher Richardson,Larry Heck","id":"17a8b5e6fef1f69979d57021a8f30a5159e152c7","summary":"A survey of recent conversational AI research focused on commonsense reasoning, including preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions.","score":2},{"url":"https://www.semanticscholar.org/paper/867915b9587c046ce8e4b71ab4dee2a1d8bf0b48","title":"DocTime: A Document-level Temporal Dependency Graph Parser","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":67,"citationCount":4,"influentialCitationCount":1,"publicationDate":2022,"authors":"Puneet Mathur,Vlad I. Morariu,Verena Kaynig-Fittkau,Jiuxiang Gu,Franck Dernoncourt,Quan Hung Tran,A. Nenkova,Dinesh Manocha,R. Jain","id":"867915b9587c046ce8e4b71ab4dee2a1d8bf0b48","summary":"DocTime - a novel temporal dependency graph (TDG) parser that takes as input a text document and produces a temporal dependencygraph outperforms previous BERT-based solutions by a relative 4-8% on three datasets from modeling the problem as a graph network with path-prediction loss to incorporate longer range dependencies.","score":2},{"url":"https://www.semanticscholar.org/paper/30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0","title":"MC-LSTM: Mass-Conserving LSTM","venue":"International Conference on Machine Learning","year":2021,"referenceCount":108,"citationCount":17,"influentialCitationCount":5,"publicationDate":"13/01/2021","authors":"Pieter-Jan Hoedt,Frederik Kratzert,D. Klotz,Christina Halmich,Markus Holzleitner,G. Nearing,S. Hochreiter,G. Klambauer","id":"30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0","summary":"This work proposes a novel Mass-Conserving LSTM (MC-LSTM), which sets a new state-of-the-art for neural arithmetic units at learning arithmetic operations, such as addition tasks, which have a strong conservation law, as the sum is constant over time.","score":2},{"url":"https://www.semanticscholar.org/paper/4578747d52aa1b3537612287352a803a7b17e999","title":"Asynchronous Neural Networks for Learning in Graphs","venue":"arXiv.org","year":2022,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/05/2022","authors":"Lukas Faber,Roger Wattenhofer","id":"4578747d52aa1b3537612287352a803a7b17e999","summary":"It is proved that AMP can simulate synchronous GNNs and that it can theoretically distinguish any pair of graphs and experimentally validate AMP’s expressiveness.","score":2},{"url":"https://www.semanticscholar.org/paper/0ce6a798f8222ed8f221326ca566311c648cb4dc","title":"Learning Division with Neural Arithmetic Logic Modules","venue":"arXiv.org","year":2021,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2021","authors":"Bhumika Mistry,K. Farrahi,Jonathon S. Hare","id":"0ce6a798f8222ed8f221326ca566311c648cb4dc","summary":"It is shown that robustly learning division in a systematic manner remains a challenge even at the simplest level of dividing two numbers, and two novel approaches for division are proposed which are called the Neural Reciprocal Unit (NRU) and the Neural Multiplicative Reciproc Unit (NMRU).","score":2},{"url":"https://www.semanticscholar.org/paper/33149f835f391119d287cc2c6b009464e7d14fe4","title":"On the Abilities of Mathematical Extrapolation with Implicit Models","venue":"","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Juliette Decugis,Max Emerling,Ashwin Ganesh,Alicia Y. Tsai,L. Ghaoui","id":"33149f835f391119d287cc2c6b009464e7d14fe4","summary":"This paper compares the robustness of implicitly-defined and classical deep learning models on a series of mathematical extrapolation tasks, where the models are tested with out-of-distribution samples during inference time to showcase implicit models’ unique advantages for mathematics extrapolation thanks to their flexible and selective framework.","score":2},{"url":"https://www.semanticscholar.org/paper/26e07b6ac9b0f04396e2e7f7ca518b6fbae67ce7","title":"Improving the Robustness of Neural Multiplication Units with Reversible Stochasticity","venue":"arXiv.org","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Bhumika Mistry,K. Farrahi,Jonathon S. Hare","id":"26e07b6ac9b0f04396e2e7f7ca518b6fbae67ce7","summary":"It is shown that Neural Multiplication Units (NMUs) are unable to reliably learn tasks as simple as multiplying two inputs when given different training ranges, and stochasticity provides improved robustness with the potential to improve learned representations of upstream networks for numerical and image tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/eb2fc03b8865b8e1b4cb933d917ea269ebe14584","title":"Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training","venue":"","year":2020,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2020","authors":"Chen Xing,Wenhao Liu,Caiming Xiong","id":"eb2fc03b8865b8e1b4cb933d917ea269ebe14584","summary":"It is argued that mis-predictions can help locate such dominating patterns that harm language understanding and propose a new language pre-training method, MisPredictions as Harm Alerts (MPA), which expedites the pre- training of BERT and ELECTRA and improves their performances on downstream tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/011869f932f89d047ce2bd36d73a95cc04888193","title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":55,"citationCount":26,"influentialCitationCount":1,"publicationDate":"02/05/2020","authors":"Pei Zhou,Rahul Khanna,Bill Yuchen Lin,Daniel Ho,J. Pujara,Xiang Ren","id":"011869f932f89d047ce2bd36d73a95cc04888193","summary":"A new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations and shows that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks.","score":2},{"url":"https://www.semanticscholar.org/paper/73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19","title":"Measuring and Improving Consistency in Pretrained Language Models","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":83,"citationCount":99,"influentialCitationCount":24,"publicationDate":"02/02/2021","authors":"Yanai Elazar,Nora Kassner,Shauli Ravfogel,Abhilasha Ravichander,E. Hovy,Hinrich Schütze,Yoav Goldberg","id":"73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19","summary":"The creation of PARAREL, a high-quality resource of cloze-style query English paraphrases, and analysis of the representational spaces of PLMs suggest that they have a poor structure and are currently not suitable for representing knowledge in a robust way.","score":2},{"url":"https://www.semanticscholar.org/paper/008b9fc834f5839a25febe150f3076d550ee442f","title":"Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2","venue":"arXiv.org","year":2021,"referenceCount":50,"citationCount":17,"influentialCitationCount":2,"publicationDate":"24/03/2021","authors":"Gregor Betz,Kyle Richardson,C. Voigt","id":"008b9fc834f5839a25febe150f3076d550ee442f","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/5aab57cc0530560d82c74c055f664280619d7e81","title":"PROST: Physical Reasoning about Objects through Space and Time","venue":"Findings","year":2021,"referenceCount":54,"citationCount":16,"influentialCitationCount":4,"publicationDate":"07/06/2021","authors":"Stephane T Aroca-Ouellette,Cory Paik,A. Roncone,Katharina Kann","id":"5aab57cc0530560d82c74c055f664280619d7e81","summary":"An extensive analysis demonstrates that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer options are presented to them, they struggle when the superlative in a question is inverted, and increasing the amount of pretraining data and parameters only yields minimal improvements.","score":2},{"url":"https://www.semanticscholar.org/paper/d65a064eb837f838faf6ff67781b62450b92b159","title":"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":61,"citationCount":55,"influentialCitationCount":10,"publicationDate":"07/06/2021","authors":"Alon Talmor,Ori Yoran,Ronan Le Bras,Chandrasekhar Bhagavatula,Yoav Goldberg,Yejin Choi,Jonathan Berant","id":"d65a064eb837f838faf6ff67781b62450b92b159","summary":"This work proposes gamification as a framework for data construction, and creates CommonsenseQA 2.0, which includes 14,343 yes/no questions, and demonstrates its difficulty for models that are orders-of-magnitude larger than the AI used in the game itself.","score":2},{"url":"https://www.semanticscholar.org/paper/e55391a9406245584b3e5b3225dad2e171b9a06b","title":"RuleBERT: Teaching Soft Rules to Pre-Trained Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":54,"citationCount":12,"influentialCitationCount":2,"publicationDate":"24/09/2021","authors":"Mohammed Saeed,N. Ahmadi,Preslav Nakov,Paolo Papotti","id":"e55391a9406245584b3e5b3225dad2e171b9a06b","summary":"This work introduces a classification task where, given facts and soft rules, thePLM should return a prediction with a probability for a given hypothesis, and proposes a revised loss function that enables the PLM to learn how to predict precise probabilities for the task.","score":2},{"url":"https://www.semanticscholar.org/paper/2af476f7c2a7c040fc9ab7750bf41a84f66aa947","title":"Knowledge Based Multilingual Language Model","venue":"arXiv.org","year":2021,"referenceCount":29,"citationCount":10,"influentialCitationCount":5,"publicationDate":2021,"authors":"Linlin Liu,Xin Li,Ruidan He,Lidong Bing,Shafiq R. Joty,Luo Si","id":"2af476f7c2a7c040fc9ab7750bf41a84f66aa947","summary":"This work presents a novel framework to pretrain knowledge based multilingual language models (KMLMs) by generating a large amount of code-switched synthetic sentences and reasoning-based multilingual training data using the Wikidata knowledge graphs and designing pretraining tasks to facilitate knowledge learning.","score":2},{"url":"https://www.semanticscholar.org/paper/f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884","title":"LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI","venue":"arXiv.org","year":2021,"referenceCount":58,"citationCount":4,"influentialCitationCount":1,"publicationDate":"04/12/2021","authors":"Ishan Tarunesh,Somak Aditya,M. Choudhury","id":"f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884","summary":"This work proposes an extensible framework to collectively yet categorically test diverse Logical reasoning capabilities required for NLI (and by extension, NLU) and creates a semi-synthetic large test-bench and an associated framework that offers following utilities.","score":2},{"url":"https://www.semanticscholar.org/paper/f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","title":"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning","venue":"","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","summary":"Investigation of the underlying hypothesis in the most simple conceivable scenario – the addition of real numbers finds that two layer neural networks fail to learn the structure of this task and that growing the network’s width leads to a complex division of input space.","score":2},{"url":"https://www.semanticscholar.org/paper/5c8cb5d39aa8531fcb5962821ac404e4a0e109d1","title":"Is It Smaller Than a Tennis Ball? Language Models Play the Game of Twenty Questions","venue":"BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","year":2022,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans","id":"5c8cb5d39aa8531fcb5962821ac404e4a0e109d1","summary":"It is found that only the largest model has enough world knowledge to play the game of Twenty Questions well, although it still has difficulties with the shape and size of objects.","score":2},{"url":"https://www.semanticscholar.org/paper/fed460648303afa32247e493847e4dc73dc1a5b3","title":"Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jesús Lovón-Melgarejo,José G. Moreno,Romaric Besançon,Olivier Ferret,L. Tamine","id":"fed460648303afa32247e493847e4dc73dc1a5b3","summary":"To endow PLMs with incremental reasoning skills, this work proposes a set of inference strategies on relevant facts and distractors allowing us to build automatically generated training datasets, and empirically shows the effectiveness of this proposal on multiple-choice question answering and reading comprehension.","score":2},{"url":"https://www.semanticscholar.org/paper/37bf0bf34603145246c3311df19e2afdf6e0270a","title":"JAKET: Joint Pre-training of Knowledge Graph and Language Understanding","venue":"AAAI Conference on Artificial Intelligence","year":2020,"referenceCount":50,"citationCount":58,"influentialCitationCount":5,"publicationDate":"02/10/2020","authors":"Donghan Yu,Chenguang Zhu,Yiming Yang,Michael Zeng","id":"37bf0bf34603145246c3311df19e2afdf6e0270a","summary":"A novel joint pre-training framework, JAKET, to model both the knowledge graph and language, which enables the pre-trained model to easily adapt to unseen knowledge graphs in new domains.","score":2},{"url":"https://www.semanticscholar.org/paper/2cf3cd3a7a08fc91eecab45e73299940c9c439dc","title":"Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":63,"citationCount":30,"influentialCitationCount":5,"publicationDate":"15/07/2021","authors":"Ori Yoran,Alon Talmor,Jonathan Berant","id":"2cf3cd3a7a08fc91eecab45e73299940c9c439dc","summary":"This work proposes to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph, and shows that the model, PReasM, substantially outperforms T5, a popular pre-trained encoder-decoder model.","score":2},{"url":"https://www.semanticscholar.org/paper/a466d10b80dbdee3b130bef73ec62f3a89eb389b","title":"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":49,"citationCount":5,"influentialCitationCount":1,"publicationDate":"22/11/2021","authors":"Linlin Liu,Xin Li,Ruidan He,Lidong Bing,Shafiq R. Joty,Luo Si","id":"a466d10b80dbdee3b130bef73ec62f3a89eb389b","summary":"This work explores methods to make better use of the multilingual annotation and language agnostic property of KG triples, and presents novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples.","score":2},{"url":"https://www.semanticscholar.org/paper/0b483b550b21ec42d693fc04a372dbb10dd07019","title":"Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":47,"citationCount":3,"influentialCitationCount":0,"publicationDate":"16/12/2021","authors":"Ian Porada,Alessandro Sordoni,J. Cheung","id":"0b483b550b21ec42d693fc04a372dbb10dd07019","summary":"It is found generalization does not improve over the course of pre-training, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.","score":2},{"url":"https://www.semanticscholar.org/paper/7bebb48d34c219b119ca2d4ffc97d7fd4940c35c","title":"On the data requirements of probing","venue":"Findings","year":2022,"referenceCount":63,"citationCount":2,"influentialCitationCount":0,"publicationDate":"25/02/2022","authors":"Zining Zhu,Jixuan Wang,Bai Li,F. Rudzicz","id":"7bebb48d34c219b119ca2d4ffc97d7fd4940c35c","summary":"A novel method to estimate the required number of data samples in such experiments is presented and, across several case studies, it is verified that the estimations have sufficient statistical power.","score":2},{"url":"https://www.semanticscholar.org/paper/50f7d69ddd7f9b34f5121607fbdcc57236d65b8c","title":"Can Pre-trained Language Models Interpret Similes as Smart as Human?","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":54,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/03/2022","authors":"Qi He,Sijie Cheng,Zhixu Li,Rui Xie,Yanghua Xiao","id":"50f7d69ddd7f9b34f5121607fbdcc57236d65b8c","summary":"This paper investigates the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, and shows that PLMs can infer similes’ shared properties while still underperforming humans.","score":2},{"url":"https://www.semanticscholar.org/paper/2aba5bba16dac5cd62683bab9de5d6faaaed0de1","title":"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach","venue":"arXiv.org","year":2022,"referenceCount":44,"citationCount":5,"influentialCitationCount":3,"publicationDate":2022,"authors":"Boshi Wang,Xiang Deng,Huan Sun","id":"2aba5bba16dac5cd62683bab9de5d6faaaed0de1","summary":"An iterative context-aware prompter is proposed which addresses key limitations of existing prompting methods, namely they are either re- stricted to queries with a single identifiable re- 014 lation/predicate, or being agnostic to input con- 015 texts, which makes it difficult to capture vari- 016 abilities across different inference steps.","score":2},{"url":"https://www.semanticscholar.org/paper/3f4d11971f2c64be9125a7fe99c019588bbebf16","title":"Iteratively Prompt Pre-trained Language Models for Chain of Thought","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":48,"citationCount":17,"influentialCitationCount":6,"publicationDate":"16/03/2022","authors":"Boshi Wang,Xiang Deng,Huan Sun","id":"3f4d11971f2c64be9125a7fe99c019588bbebf16","summary":"An iterative prompting framework is explored, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference by learning to dynamically synthesize prompts conditioned on the current step’s contexts.","score":2},{"url":"https://www.semanticscholar.org/paper/cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","title":"A Review on Language Models as Knowledge Bases","venue":"arXiv.org","year":2022,"referenceCount":167,"citationCount":26,"influentialCitationCount":5,"publicationDate":"12/04/2022","authors":"Badr AlKhamissi,Millicent Li,Asli Celikyilmaz,Mona T. Diab,Marjan Ghazvininejad","id":"cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","summary":"This paper presents a set of aspects that it is deemed a pretrained Language Models should have to fully act as a KB, and reviews the recent literature with respect to those aspects.","score":2},{"url":"https://www.semanticscholar.org/paper/e3dae33c5bdf397abdefd971ea34c48fb836dcc0","title":"GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":45,"citationCount":7,"influentialCitationCount":0,"publicationDate":"24/05/2022","authors":"Da Yin,Hritik Bansal,Masoud Monajatipoor,Liunian Harold Li,Kai-Wei Chang","id":"e3dae33c5bdf397abdefd971ea34c48fb836dcc0","summary":"Interestingly, it is found that larger multilingual PLMs variants do not necessarily store geo-diverse concepts better than its smaller variant; and a language may better probe knowledge about a non-native country than its native country.","score":2},{"url":"https://www.semanticscholar.org/paper/a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7","title":"The Curious Case of Control","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/05/2022","authors":"Elias Stengel-Eskin,Benjamin Van Durme","id":"a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review","venue":"arXiv.org","year":2022,"referenceCount":697,"citationCount":15,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"D. Hupkes,Mario Giulianelli,Verna Dankers,Mikel Artetxe,Yanai Elazar,Tiago Pimentel,Christos Christodoulopoulos,Karim Lasri,Naomi Saphra,Arabella J. Sinclair,Dennis Ulmer,Florian Schottmann,Khuyagbaatar Batsuren,Kaiser Sun,Koustuv Sinha,Leila Khalatbari,Maria Ryskina,Rita Frieske,Ryan Cotterell,Zhijing Jin","id":"559bfba3bee31f6061a5d5c7061f22794de47e39","summary":"A taxonomy for characterising and understanding generalisation research in NLP is presented, a taxonomy is used to present a comprehensive map of published generalisation studies, and recommendations for which areas might deserve attention in the future are made.","score":2},{"url":"https://www.semanticscholar.org/paper/ad5573cb25fd403f7620332f363ae87327c69a49","title":"The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning","venue":"arXiv.org","year":2022,"referenceCount":67,"citationCount":5,"influentialCitationCount":1,"publicationDate":"16/12/2022","authors":"Hanlin Zhang,Yi-Fan Zhang,Li Erran Li,Eric P. Xing","id":"ad5573cb25fd403f7620332f363ae87327c69a49","summary":"Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning settings, showing that L MLP enjoys more than 25% higher accuracy than COT on length generalization bench-marks even with fewer parameters.","score":2},{"url":"https://www.semanticscholar.org/paper/06396c7cd5d223a1776abf8811359ec7bc05d420","title":"Knowledge-Augmented Methods for Natural Language Processing","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":86,"citationCount":6,"influentialCitationCount":0,"publicationDate":"27/02/2023","authors":"Chenguang Zhu,Yichong Xu,Xiang Ren,Bill Yuchen Lin,Meng Jiang,Wenhao Yu","id":"06396c7cd5d223a1776abf8811359ec7bc05d420","summary":"This tutorial introduces the key steps in integrating knowledge into NLP, including knowledge grounding from text, knowledge representation and fusing, and introduces recent state-of-the-art applications in fusing knowledge into language understanding, language generation and commonsense reasoning.","score":2},{"url":"https://www.semanticscholar.org/paper/da345b189e4faaaa489f7319640868a37a3932a1","title":"Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2023,"referenceCount":22,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Keito Kudo,Y. Aoki,Tatsuki Kuribayashi,Ana Brassard,Masashi Yoshikawa,Keisuke Sakaguchi,Kentaro Inui","id":"da345b189e4faaaa489f7319640868a37a3932a1","summary":"A skill tree on compositionality in arithmetic symbolic reasoning that defines the hierarchical levels of complexity along with three compositionality dimensions: systematicity, productivity, and substitutivity is introduced.","score":2},{"url":"https://www.semanticscholar.org/paper/8018a68956d6751d7ea76110537d5a2e86ec05c4","title":"The Life Cycle of Knowledge in Big Language Models: A Survey","venue":"arXiv.org","year":2023,"referenceCount":171,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/03/2023","authors":"Boxi Cao,Hongyu Lin,Xianpei Han,Le Sun","id":"8018a68956d6751d7ea76110537d5a2e86ec05c4","summary":"This survey revisits PLMs as knowledge-based systems by dividing the life circle of knowledge in PLMs into five critical periods, and investigating how knowledge circulates when it is built, maintained and used.","score":2},{"url":"https://www.semanticscholar.org/paper/44772fe1c3fa422a3da7e25092db2544893d6bfb","title":"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming","venue":"arXiv.org","year":2023,"referenceCount":53,"citationCount":5,"influentialCitationCount":1,"publicationDate":"05/05/2023","authors":"Hanlin Zhang,Jiani Huang,Ziyang Li,M. Naik,Eric P. Xing","id":"44772fe1c3fa422a3da7e25092db2544893d6bfb","summary":"DSR-LM is proposed, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning, and efficiently learns weighted rules and applies semantic loss to further improve LMs.","score":2},{"url":"https://www.semanticscholar.org/paper/2cffec40f1d7cfc81d498b8939493243bbcadebe","title":"Learning Symbolic Rules for Reasoning in Quasi-Natural Language","venue":"arXiv.org","year":2021,"referenceCount":72,"citationCount":8,"influentialCitationCount":0,"publicationDate":"23/11/2021","authors":"Kaiyu Yang,Jia Deng","id":"2cffec40f1d7cfc81d498b8939493243bbcadebe","summary":"This work proposes MetaQNL, a “Quasi-Natural” language that can express both formal logic and natural language sentences, and MetaInduce, a learning algorithm that inducesMetaQNL rules from training data consisting of questions and answers, with or without intermediate reasoning steps.","score":2},{"url":"https://www.semanticscholar.org/paper/30977afbd4501249e1a320bd2e48581197914ab8","title":"A Short Survey of Systematic Generalization","venue":"arXiv.org","year":2022,"referenceCount":241,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Yuanpeng Li","id":"30977afbd4501249e1a320bd2e48581197914ab8","summary":"This survey includes systematic generalization and a history of how machine learning addresses it, and introduces Classicist and Connectionist, a first look at the definition of systematicgeneralization, and different types of Connectionists and how they approach the generalization.","score":2},{"url":"https://www.semanticscholar.org/paper/237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting","venue":"arXiv.org","year":2022,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/03/2022","authors":"Gabriel Orlanski","id":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","summary":"This work collects and standardizes prompts from a diverse range of tasks for use with tasks they were not designed for, and evaluates these prompts across fixed multiple choice datasets for a quantitative analysis of how certain attributes of a prompt affect performance.","score":2},{"url":"https://www.semanticscholar.org/paper/331a87762228ecca2eebf309b859189f033414ba","title":"NarrativeTime: Dense Temporal Annotation on a Timeline","venue":"","year":2019,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/08/2019","authors":"Anna Rogers,Marzena Karpinska,Ankita Gupta,Vladislav Lialin,Gregory Smelkov,Anna Rumshisky","id":"331a87762228ecca2eebf309b859189f033414ba","summary":"This work presents N ARRATIVE T IME, the first timeline-based annotation framework that achieves full coverage of all possible TLINKS, and performs full re-annotation of TimeBankDense corpus, which shows comparable agreement with a signigicant increase in density.","score":2},{"url":"https://www.semanticscholar.org/paper/3d2035edd4dd48e1e638279409e11bf689c461e1","title":"Temporal Reasoning in Natural Language Inference","venue":"Findings","year":2020,"referenceCount":60,"citationCount":22,"influentialCitationCount":3,"publicationDate":"01/11/2020","authors":"Siddharth Vashishtha,Adam Poliak,Yash Kumar Lal,Benjamin Van Durme,Aaron Steven White","id":"3d2035edd4dd48e1e638279409e11bf689c461e1","summary":"Five new natural language inference (NLI) datasets focused on temporal reasoning are introduced and four existing datasets annotated for event duration and event ordering are recast into more than one million NLI examples.","score":2},{"url":"https://www.semanticscholar.org/paper/d865c87f6ce4e0be29ae1e3780fae66b8034d04b","title":"TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation","venue":"International Workshop on Semantic Evaluation","year":2020,"referenceCount":54,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/12/2020","authors":"Don Teo","id":"d865c87f6ce4e0be29ae1e3780fae66b8034d04b","summary":"This paper examines the ability of large-scale pre-trained language models to distinguish commonsense from non-commonsense statements, and the utility of external resources that aim to supplement the world knowledge inherent in such language models, including commonsense knowledge graph embedding models, word concreteness ratings, and text-to-image generation models.","score":2},{"url":"https://www.semanticscholar.org/paper/9793b07ba09d9f2ac9cabd8117daa93bf3db4346","title":"DEER: A Data Efficient Language Model for Event Temporal Reasoning","venue":"arXiv.org","year":2020,"referenceCount":16,"citationCount":11,"influentialCitationCount":1,"publicationDate":"30/12/2020","authors":"Rujun Han,Xiang Ren,Nanyun Peng","id":"9793b07ba09d9f2ac9cabd8117daa93bf3db4346","summary":"This work proposes DEER, a language model that is trained to focus on event temporal relations and performs better under low-resource settings than original LMs and uses a generator-discriminator structure to reinforce the LMs' capability of event temporal reasoning.","score":2},{"url":"https://www.semanticscholar.org/paper/e6d84cf9ae6efa10919bff765613e883a761db62","title":"Open Temporal Relation Extraction for Question Answering","venue":"Conference on Automated Knowledge Base Construction","year":2021,"referenceCount":25,"citationCount":3,"influentialCitationCount":0,"publicationDate":2021,"authors":"Chao Shang,Peng Qi,Guangtao Wang,Jing Huang,Youzheng Wu,Bowen Zhou","id":"e6d84cf9ae6efa10919bff765613e883a761db62","summary":"This paper proposes to reformulate the task of TQA as open temporal relation extraction, which allows to learn context-agnostic, free-text-based relation representations that generalize across diﬀerent contexts and events, which leads to higher data eﬃciency.","score":2},{"url":"https://www.semanticscholar.org/paper/6bb369f874f49cd51415f216f1a3f635f2ca1eed","title":"ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":33,"citationCount":20,"influentialCitationCount":5,"publicationDate":2021,"authors":"Rujun Han,I-Hung Hsu,Jiao Sun,J. Baylón,Qiang Ning,D. Roth,Nanyun Peng","id":"6bb369f874f49cd51415f216f1a3f635f2ca1eed","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/30602e3382df3abedb5f225b55b7efce8580f74d","title":"ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":59,"citationCount":16,"influentialCitationCount":1,"publicationDate":"02/05/2020","authors":"Woojeong Jin,Suji Kim,Rahul Khanna,Dong-Ho Lee,Fred Morstatter,A. Galstyan,Xiang Ren","id":"30602e3382df3abedb5f225b55b7efce8580f74d","summary":"This work aims to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data, and introduces ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts.","score":2},{"url":"https://www.semanticscholar.org/paper/9d436d25981ea61db728bd490f0d54376d08953e","title":"Temporal Reasoning on Implicit Events from Distant Supervision","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":50,"citationCount":42,"influentialCitationCount":7,"publicationDate":"24/10/2020","authors":"Ben Zhou,Kyle Richardson,Qiang Ning,Tushar Khot,Ashish Sabharwal,D. Roth","id":"9d436d25981ea61db728bd490f0d54376d08953e","summary":"A neuro-symbolic temporal reasoning model, SymTime, is proposed, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times and generalizes to other temporal reasoning tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/6eee69031d2e11aa03a5a8fcb219cff4562863be","title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":38,"citationCount":13,"influentialCitationCount":3,"publicationDate":"30/12/2020","authors":"Rujun Han,Xiang Ren,Nanyun Peng","id":"6eee69031d2e11aa03a5a8fcb219cff4562863be","summary":"A continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations and design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts.","score":2},{"url":"https://www.semanticscholar.org/paper/33b06c74eea3f400b6f5ef14ef163aef1db42d16","title":"Conditional Generation of Temporally-ordered Event Sequences","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":63,"citationCount":11,"influentialCitationCount":2,"publicationDate":"31/12/2020","authors":"Shih-Ting Lin,Nathanael Chambers,G. Durrett","id":"33b06c74eea3f400b6f5ef14ef163aef1db42d16","summary":"A single model is proposed that addresses both temporal ordering, sorting given events into the order they occurred, and event infilling, predicting new events which fit into an existing temporally-ordered sequence.","score":2},{"url":"https://www.semanticscholar.org/paper/d6c919ee0d51432496513ef4b6b2dbd128819779","title":"Microtask Detection","venue":"ACM Trans. Inf. Syst.","year":2021,"referenceCount":93,"citationCount":5,"influentialCitationCount":0,"publicationDate":"08/01/2021","authors":"Ryen W. White,E. Nouri,James Woffinden-Luey,Mark J. Encarnación,S. Jauhar","id":"d6c919ee0d51432496513ef4b6b2dbd128819779","summary":"This article introduces the novel challenge of microtask detection, and it presents machine-learned models for automatically determining which tasks are actionable and which of these actionable tasks are microtasks, which have implications for the design of systems to help people make the most of their time.","score":2},{"url":"https://www.semanticscholar.org/paper/c5943bc4b22a63f595cb1c2823a449e03aad4787","title":"AR-LSAT: Investigating Analytical Reasoning of Text","venue":"arXiv.org","year":2021,"referenceCount":43,"citationCount":10,"influentialCitationCount":0,"publicationDate":"14/04/2021","authors":"Wanjun Zhong,Siyuan Wang,Duyu Tang,Zenan Xu,Daya Guo,Jiahai Wang,Jian Yin,Ming Zhou,Nan Duan","id":"c5943bc4b22a63f595cb1c2823a449e03aad4787","summary":"It is found that the Transformer-based models struggle to solve this task as their performance is close to random guess and ARM achieves better performance by leveraging symbolic knowledge and interpretable reasoning steps.","score":2},{"url":"https://www.semanticscholar.org/paper/2ef4be35f8424ea768aa2e1b44392b3eddbc780b","title":"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":74,"citationCount":31,"influentialCitationCount":3,"publicationDate":"16/04/2021","authors":"Yanai Elazar,Hongming Zhang,Yoav Goldberg,D. Roth","id":"2ef4be35f8424ea768aa2e1b44392b3eddbc780b","summary":"It is suggested that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning, and the observed progress is mostly due to the use of supervision in training WS models, which is not likely to successfully support all the required Commonsense reasoning skills and knowledge.","score":2},{"url":"https://www.semanticscholar.org/paper/c7f977f556d2060238fdc1286d057d46958afaf9","title":"ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning","venue":"arXiv.org","year":2021,"referenceCount":38,"citationCount":7,"influentialCitationCount":3,"publicationDate":"16/04/2021","authors":"Rujun Han,I-Hung Hsu,Jiao Sun,J. Baylón,Qiang Ning,D. Roth,Nanyun Pen","id":"c7f977f556d2060238fdc1286d057d46958afaf9","summary":"ESTER is introduced, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning that leverages natural language queries to reason about the five most common event semantic relations, provides more than 6K questions and captures 10.1K event relation pairs.","score":2},{"url":"https://www.semanticscholar.org/paper/7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":171,"citationCount":92,"influentialCitationCount":20,"publicationDate":"18/04/2021","authors":"Qinyuan Ye,Bill Yuchen Lin,Xiang Ren","id":"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","summary":"This paper presents the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format, and reveals that the few- shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/174a0e6da0dfb7f96d4a0a4076eed154c439e41a","title":"Probing Language Models for Understanding of Temporal Expressions","venue":"BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","year":2021,"referenceCount":27,"citationCount":5,"influentialCitationCount":1,"publicationDate":"03/10/2021","authors":"Shivin Thukral,Kunal Kukreja,Christian Kavouras","id":"174a0e6da0dfb7f96d4a0a4076eed154c439e41a","summary":"It is found that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.","score":2},{"url":"https://www.semanticscholar.org/paper/d83c7aa12420d5d035f43d7737bfa6893e9e2c61","title":"Exploring Universal Intrinsic Task Subspace via Prompt Tuning","venue":"","year":2021,"referenceCount":124,"citationCount":6,"influentialCitationCount":1,"publicationDate":"15/10/2021","authors":"Yujia Qin,Xiaozhi Wang,Yusheng Su,Yankai Lin,Ning Ding,Jing Yi,Weize Chen,Zhiyuan Liu,Juanzi Li,Lei Hou,Peng Li,Maosong Sun,Jie Zhou","id":"d83c7aa12420d5d035f43d7737bfa6893e9e2c61","summary":"Evidence is empirically found indicating that the adaptations of PLMs to various few-shot tasks can be reparameterized as optimizing only a few free parameters in a unified low-dimensional intrinsic task subspace, which may help to understand why PLMs could easily adapt to various NLP tasks with small-scale data.","score":2},{"url":"https://www.semanticscholar.org/paper/b672d2ec81c9395c312d57a27931864d07664592","title":"A Meta-framework for Spatiotemporal Quantity Extraction from Text","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Qiang Ning,Ben Zhou,Hao Wu,Haoruo Peng,Chuchu Fan,Matt Gardner","id":"b672d2ec81c9395c312d57a27931864d07664592","summary":"This paper formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it, which contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models.","score":2},{"url":"https://www.semanticscholar.org/paper/7db7fb25a8753c9efc6b3722d178f94fcc1f82d3","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP","venue":"arXiv.org","year":2022,"referenceCount":118,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yufei Wang,Jiayi Zheng,Can Xu,Xiubo Geng,Tao Shen,Chongyang Tao,Daxin Jiang","id":"7db7fb25a8753c9efc6b3722d178f94fcc1f82d3","summary":"It is demonstrated that the proposed Knowledge Mixture enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences).","score":2},{"url":"https://www.semanticscholar.org/paper/82c6f5ffd4d2d43ae7684601f607eae26a759a5a","title":"Analytical Reasoning of Text","venue":"NAACL-HLT","year":2022,"referenceCount":55,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Wanjun Zhong,Siyuan Wang,Duyu Tang,Zenan Xu,Daya Guo,Yining Chen,Jiahai Wang,Jian Yin,Ming Zhou,Nan Duan","id":"82c6f5ffd4d2d43ae7684601f607eae26a759a5a","summary":"This paper studies the challenge of analytical reasoning of text and collects a new dataset consisting of questions from the Law School Admission Test from 1991 to 2016, and presents an approach dubbed ARM, which outperforms pre-trained models significantly.","score":2},{"url":"https://www.semanticscholar.org/paper/b8768fbba4d356a1c49dcd8e4cd33aea0c6472bc","title":"How about Time? Probing a Multilingual Language Model for Temporal Relations","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Tommaso Caselli,Irene Dini,F. Dell’Orletta","id":"b8768fbba4d356a1c49dcd8e4cd33aea0c6472bc","summary":"This paper presents a comprehensive set of probing experiments using a multilingual language model, XLM-R, for temporal relation classification between events in four languages, and results show an advantage of contextualized embeddings over static ones and a detrimen- tal role of sentence level embeddeddings.","score":2},{"url":"https://www.semanticscholar.org/paper/cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":54,"citationCount":172,"influentialCitationCount":27,"publicationDate":"18/04/2021","authors":"Swaroop Mishra,Daniel Khashabi,Chitta Baral,Hannaneh Hajishirzi","id":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","summary":"This work introduces NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances, and adopts generative pre-trained language models to encode task-specific instructions along with input and generate task output.","score":2},{"url":"https://www.semanticscholar.org/paper/ac8d33e4c0a45e227a47353f3f26fbb231482dc1","title":"Time-Aware Language Models as Temporal Knowledge Bases","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":54,"citationCount":87,"influentialCitationCount":18,"publicationDate":"29/06/2021","authors":"Bhuwan Dhingra,Jeremy R. Cole,Julian Martin Eisenschlos,D. Gillick,Jacob Eisenstein,William W. Cohen","id":"ac8d33e4c0a45e227a47353f3f26fbb231482dc1","summary":"This work proposes a simple technique for jointly modeling text with its timestamp that improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods and shows that models trained with temporal context can be efficiently \"refreshed\" as new data arrives.","score":2},{"url":"https://www.semanticscholar.org/paper/47df3fd32d00220c85c2c51a571254fd99b2ecc7","title":"MetaICL: Learning to Learn In Context","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":135,"citationCount":126,"influentialCitationCount":30,"publicationDate":"29/10/2021","authors":"Sewon Min,M. Lewis,Luke Zettlemoyer,Hannaneh Hajishirzi","id":"47df3fd32d00220c85c2c51a571254fd99b2ecc7","summary":"This work introduces MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/c3a454e50ec0610f1380d55b1988a5eb5d45207b","title":"Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization","venue":"AAAI Conference on Artificial Intelligence","year":2022,"referenceCount":43,"citationCount":10,"influentialCitationCount":2,"publicationDate":"01/01/2022","authors":"Zi-Yi Dou,Nanyun Peng","id":"c3a454e50ec0610f1380d55b1988a5eb5d45207b","summary":"Four translation methods that can translate natural questions into cloze-style sentences to better solicit commonsense knowledge from language models are investigated, including a syntactic-based model, an unsupervised neural model, and two supervised neural models.","score":2},{"url":"https://www.semanticscholar.org/paper/078f4efd448822b0e25d3ee0aec842ced606a595","title":"Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":50,"citationCount":8,"influentialCitationCount":1,"publicationDate":"04/05/2022","authors":"Rujun Han,Hong Chen,Yufei Tian,Nanyun Peng","id":"078f4efd448822b0e25d3ee0aec842ced606a595","summary":"A Plan-and-Write framework enhanced by reinforcement learning to generate storylines and stories end-to-end using structured storylines to encode events and their pair-wise temporal relations as **temporal prompts** that guide how stories should unfold temporally.","score":2},{"url":"https://www.semanticscholar.org/paper/e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP","venue":"","year":2022,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/06/2022","authors":"Yufei Wang,Jiayi Zheng,Can Xu,Xiubo Geng,Tao Shen,Chongyang Tao,Daxin Jiang","id":"e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e","summary":"The goal of KoMT is to condense diverse NLP task-speciﬁc knowledge into the single KnowDA model (i.e., all-in-one) such that KnowDA could utilize these knowledge to quickly grasp the inherent synthesis law of the target task through limited training instances.","score":2},{"url":"https://www.semanticscholar.org/paper/15bacb240e2598457af4ded3039b6988aa9706f0","title":"Few-shot Adaptation Works with UnpredicTable Data","venue":"arXiv.org","year":2022,"referenceCount":143,"citationCount":3,"influentialCitationCount":1,"publicationDate":"01/08/2022","authors":"Jun Shern Chan,M. Pieler,Jonathan Jao,J. Scheurer,Ethan Perez","id":"15bacb240e2598457af4ded3039b6988aa9706f0","summary":"This work automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets - and finds that narrow subsets of the authors' dataset sometimes outperform more diverse datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/285d13bf3cbe6a8a0f164f584d84f8b74067271f","title":"Towards Faithful Model Explanation in NLP: A Survey","venue":"arXiv.org","year":2022,"referenceCount":272,"citationCount":7,"influentialCitationCount":0,"publicationDate":"22/09/2022","authors":"QING LYU,Marianna Apidianaki,Chris Callison-Burch","id":"285d13bf3cbe6a8a0f164f584d84f8b74067271f","summary":"This survey reviews over 110 model explanation methods in NLP through the lens of faithfulness, grouping existing approaches into five categories: similarity methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models.","score":2},{"url":"https://www.semanticscholar.org/paper/1a5f48161df983a0e9485425495121201902433b","title":"DiscoSense: Commonsense Reasoning with Discourse Connectives","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Prajjwal Bhargava,Vincent Ng","id":"1a5f48161df983a0e9485425495121201902433b","summary":"It is shown that state-of-the-art pre-trained language models struggle to perform well on DiscoSense, which makes this dataset ideal for evaluating next-generation commonsense reasoning systems.","score":2},{"url":"https://www.semanticscholar.org/paper/1d417bdd331912a458de920459f23fcc7f6e8699","title":"Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":4,"influentialCitationCount":0,"publicationDate":"30/10/2022","authors":"Ben Zhou,Kyle Richardson,Xiaodong Yu,D. Roth","id":"1d417bdd331912a458de920459f23fcc7f6e8699","summary":"It is shown that with such intermediate pre-training, developing robust decomposition-based models for a diverse range of tasks becomes more feasible, and the model, DecompT5, improves 20% to 30% on two datasets, Overnight and TORQUE, over the baseline language model.","score":2},{"url":"https://www.semanticscholar.org/paper/64ad3e1fbbc9a4bec2a414fb31b05ee9a62d50cb","title":"tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation","venue":"arXiv.org","year":2023,"referenceCount":145,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Damien Sileo","id":"64ad3e1fbbc9a4bec2a414fb31b05ee9a62d50cb","summary":"A dataset annotation framework and dataset annotations for more than 400 English tasks are released that provide metadata, like the name of the columns that should be used as input or labels for all datasets, and can save time for future dataset preprocessings, even if they do not use the framework.","score":2},{"url":"https://www.semanticscholar.org/paper/fd9c6baf8e485f13285777d8ec24da2359194461","title":"tieval: An Evaluation Framework for Temporal Information Extraction Systems","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/01/2023","authors":"Hugo Sousa,A. Jorge,Ricardo Campos","id":"fd9c6baf8e485f13285777d8ec24da2359194461","summary":"T tieval, a Python library that provides a concise interface for importing different corpora and facilitates system evaluation, is developed and the first public release of tieval is presented and its most relevant features are highlighted.","score":2},{"url":"https://www.semanticscholar.org/paper/9af3b6b3f8dcedbb02b88936c428e1cd02503a8a","title":"Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?","venue":"arXiv.org","year":2023,"referenceCount":167,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Chengwei Qin,Shafiq R. Joty,Q. Li,Ruochen Zhao","id":"9af3b6b3f8dcedbb02b88936c428e1cd02503a8a","summary":"Meta prompt tuning is studied to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks and an in-depth analysis from the perspective of task similarity.","score":2},{"url":"https://www.semanticscholar.org/paper/24ab6356b355b29a3770db56dd1f2200cdd987fa","title":"Salient Span Masking for Temporal Understanding","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2023,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/03/2023","authors":"Jeremy R. Cole,Aditi Chaudhary,Bhuwan Dhingra,P. Talukdar","id":"24ab6356b355b29a3770db56dd1f2200cdd987fa","summary":"This analysis suggests that the effectiveness of SSM stems from the sentences chosen in the training data rather than the mask choice: sentences with entities frequently also contain temporal expressions, and the additional targeted spans of TSM can still improve performance, especially in a zero-shot context.","score":2},{"url":"https://www.semanticscholar.org/paper/4d7571441f507f39133209e8afa7ad088da2199c","title":"ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":5,"influentialCitationCount":0,"publicationDate":"29/03/2023","authors":"Ning Bian,Xianpei Han,Le Sun,Hongyu Lin,Yaojie Lu,Ben He","id":"4d7571441f507f39133209e8afa7ad088da2199c","summary":"The experimental results show that GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge, which raises the need to investigate better mechanisms for utilizing commonsense knowledge in LLMs.","score":2},{"url":"https://www.semanticscholar.org/paper/367e0d715ef66c5aea8024e0a97cfa48e29d52c3","title":"Event Knowledge Incorporation with Posterior Regularization for Event-Centric Question Answering","venue":"arXiv.org","year":2023,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/05/2023","authors":"Junru Lu,Gabriele Pergola,Lin Gui,Yulan He","id":"367e0d715ef66c5aea8024e0a97cfa48e29d52c3","summary":"The proposed approach can effectively inject event knowledge into existing pre-trained language models and achieves strong performance compared to existing QA models in answer evaluation.","score":2},{"url":"https://www.semanticscholar.org/paper/f576288b7f5ca4aff3a6492a9db1db5148b7616f","title":"Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/05/2023","authors":"Neeraj Varshney,Mihir Parmar,Nisarg Patel,Divij Handa,Sayantan Sarkar,Man Luo,Chitta Baral","id":"f576288b7f5ca4aff3a6492a9db1db5148b7616f","summary":"It is shown that while NLP models are doing fairly well on contexts that follow the common assumptions, the models struggle to correctly reason over contexts that break those assumptions and the performance gap is as high as 20% absolute points.","score":2},{"url":"https://www.semanticscholar.org/paper/c059d251d8f0c2491892271eb40ea3cec4aea830","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue in Multiple Domains","venue":"arXiv.org","year":2023,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Shuzheng Si,Wentao Ma,Yuchuan Wu,Yinpei Dai,Haoyu Gao,Ting-En Lin,Hangyu Li,Rui Yan,Fei Huang,Yongbin Li","id":"c059d251d8f0c2491892271eb40ea3cec4aea830","summary":"SpokenWOZ is introduced, a large-scale speech-text dataset for spoken TOD, which consists of 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from human-to-human spoken conversations and presents cross-turn slot and reasoning slot detection as new challenges based on the spoken linguistic phenomena.","score":2},{"url":"https://www.semanticscholar.org/paper/47a082afc342d94f031c5e92a606d09a0631cf28","title":"Mitigating Temporal Misalignment by Discarding Outdated Facts","venue":"","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/05/2023","authors":"Michael J.Q. Zhang,Eunsol Choi","id":"47a082afc342d94f031c5e92a606d09a0631cf28","summary":"This work proposes fact duration prediction: the task of predicting how long a given fact will remain true, and shows how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment by discarding volatile facts.","score":2},{"url":"https://www.semanticscholar.org/paper/251c3afbaafcc9b5178534be9109f644bfc5e912","title":"Enhancing Knowledge Bases with Quantity Facts","venue":"The Web Conference","year":2022,"referenceCount":47,"citationCount":3,"influentialCitationCount":1,"publicationDate":"25/04/2022","authors":"Vinh Thinh Ho,D. Stepanova,Dragan Milchevski,Jannik Strotgen,G. Weikum","id":"251c3afbaafcc9b5178534be9109f644bfc5e912","summary":"A recall-oriented approach to close the gap in knowledge-base coverage, based on iterative learning for extracting quantity facts, with two novel contributions to boost recall for KB augmentation without sacrificing the quality standards of the knowledge base.","score":2},{"url":"https://www.semanticscholar.org/paper/ac4ae352e2434d4a71c6a79bf5f93df5f600b058","title":"Increasing Learning Efficiency of Self-Attention Networks through Direct Position Interactions, Learnable Temperature, and Convoluted Attention","venue":"International Conference on Computational Linguistics","year":2020,"referenceCount":27,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/12/2020","authors":"Philipp Dufter,Martin Schmitt,Hinrich Schütze","id":"ac4ae352e2434d4a71c6a79bf5f93df5f600b058","summary":"This work investigates three modifications to SANs: direct position interactions, learnable temperature, and convoluted attention that enable faster learning, i.e., higher accuracies after fewer update steps.","score":2},{"url":"https://www.semanticscholar.org/paper/f5a2ce064ea0efc3d7f26acd91041824c3254cd8","title":"NRTSI: Non-Recurrent Time Series Imputation for Irregularly-sampled Data","venue":"arXiv.org","year":2021,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Siyuan Shan,Junier B. Oliva","id":"f5a2ce064ea0efc3d7f26acd91041824c3254cd8","summary":"This work views the imputation task from the perspective of permutation equivariant modeling of sets and proposes a novel imputation model called NRTSI without any recurrent modules, which achieves state-of-the-art performance across a wide range of commonly used time series imputation benchmarks.","score":2},{"url":"https://www.semanticscholar.org/paper/89bc3b21c15c0656643918488ea25af939d0881a","title":"MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with Just One Transformer VAE","venue":"arXiv.org","year":2021,"referenceCount":79,"citationCount":24,"influentialCitationCount":2,"publicationDate":2021,"authors":"Shih-Lun Wu,Yi-Hsuan Yang","id":"89bc3b21c15c0656643918488ea25af939d0881a","summary":"Experiments show that MuseMorphose outperforms recurrent neural network (RNN) based baselines on numerous widely-used metrics for style transfer tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/976a609cf540d1ded373b872d34779f7164d840a","title":"Rethinking the Design Principles of Robust Vision Transformer","venue":"","year":2021,"referenceCount":39,"citationCount":17,"influentialCitationCount":0,"publicationDate":2021,"authors":"Xiaofeng Mao,Gege Qi,Yuefeng Chen,Xiaodan Li,Shaokai Ye,Yuan He,Hui Xue","id":"976a609cf540d1ded373b872d34779f7164d840a","summary":"RVT is a new vision transformer, which has superior performance and strong robustness, and two new plug-and-play techniques called position-aware attention rescaling and patch-wise augmentation to train the authors' RVT are proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/a28861bdef17182df3b64fc99e9359e2e23e7672","title":"Transformer with Syntactic Position Encoding for Machine Translation","venue":"Recent Advances in Natural Language Processing","year":2021,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Yikuan Xie,Wenyong Wang,Mingqian Du,Qing He","id":"a28861bdef17182df3b64fc99e9359e2e23e7672","summary":"This work proposes global positional encoding for dependency tree, a new scheme that facilitates syntactic relation modeling between any two words with keeping exactness and without immediate neighbor constraint, which is more effective than existing approaches.","score":2}]}