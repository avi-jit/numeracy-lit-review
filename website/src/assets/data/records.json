{"papers":[{"url":"https://www.semanticscholar.org/paper/cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting","venue":"International Conference on Wirtschaftsinformatik","year":2018,"referenceCount":22,"citationCount":40,"influentialCitationCount":0,"publicationDate":"14/09/2018","authors":"Chung-Chi Chen,Hen-Hsen Huang,Yow-Ting Shiue,Hsin-Hsi Chen","citations":[{"paperId":"6ad83dbd9a2da9607ece139c797694fd95738f62","title":"When Does Aggregating Multiple Skills with Multi-Task Learning Work? A Case Study in Financial NLP"},{"paperId":"b836382cf055651667738d320e5face5f4279d8e","title":"Evolving Social Media Background Representation with Frequency Weights and Co-Occurrence Graphs"},{"paperId":"251c3afbaafcc9b5178534be9109f644bfc5e912","title":"Enhancing Knowledge Bases with Quantity Facts"},{"paperId":"90e39ec249b42a23ee9847c45aa21b1fc3c6d22f","title":"An Overview of Financial Technology Innovation"},{"paperId":"fa118351f9088723499d89cc754c1caea4be16d2","title":"Twitter-aided decision making: a review of recent developments"},{"paperId":"01f68682c88c23376dcbf194c846232cba5f28a6","title":"Distilling Numeral Information for Volatility Forecasting"},{"paperId":"320c1c6647a5b975c901347f71638c881888686b","title":"NQuAD: 70,000+ Questions for Machine Comprehension of the Numerals in Text"},{"paperId":"40a1f266bb5ca853837355bfff272a55f0049c81","title":"Mining Numbers in Text: A Survey"},{"paperId":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"},{"paperId":"2e4aeff09548d160f91cb95b54cbc02ef53925b6","title":"Evaluating the Rationales of Amateur Investors"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"0772212420238b48233256a59912f8e6a31cde3d","title":"NumClaim: Investor's Fine-grained Claim Detection"},{"paperId":"3d9a08afe6c34db82d83d6927d2cbd8de4be44e0","title":"Fine-grained Financial Opinion Mining: A Survey and Research Agenda"},{"paperId":"a5ae9f992264908e51c7925280f42ee17a500858","title":"NLP in FinTech Applications: Past, Present and Future"},{"paperId":"c4295b53a1f6c4d609d6dfabd7e08d4b416b02e4","title":"Issues and Perspectives from 10,000 Annotated Financial Social Media Data"},{"paperId":"45ed7728a9b863d4c10efb53a8a4423ab7e67378","title":"Predictive Analytics on Emotional Data Mined from Digital Social Networks with a Focus on Financial Markets"},{"paperId":"0b078d4bbed8bcfd57fdab6da40ecd8a9601fc62","title":"Crowd View: Converting Investors' Opinions into Indicators"},{"paperId":"d84fa6b830ebb6188042d66aa850fe5ddcfbb2d4","title":"Next Cashtag Prediction on Social Trading Platforms with Auxiliary Tasks"},{"paperId":"2a0fc534ebf1b22b5851f5b1fb781dd707f7c530","title":"Numeral Attachment with Auxiliary Tasks"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"226144c37145e815d8bebfb8544fca9d69490348","title":"Final Report of the NTCIR-14 FinNum Task: Challenges and Current Status of Fine-Grained Numeral Understanding in Financial Social Media Data"},{"paperId":"ed7f9ecb033ea3f139bc97096caf406a8f0d91a9","title":"CrowdPT: Summarizing Crowd Opinions as Professional Analyst"},{"paperId":"5a6b6fbb1ab905d305223d1920205620f0580433","title":"MultiFin: A Dataset for Multilingual Financial NLP"},{"paperId":"c77feebb06b530079a34f04c64de359ae28c0a32","title":"Predicting Numerals in Text Using Nearest Neighbor Language Models"},{"paperId":"e722bcf6a706259dbae2950e385141a1ab7c4a3d","title":"WUST at NTCIR-16 FinNum-3 Task"},{"paperId":"4b470b68603b86582779fa33e5f034116c73685c","title":"Overview of the NTCIR-16 FinNum-3 Task: Investor’s and Manager’s Fine-grained Claim Detection"},{"paperId":"5df55d94ab5026ff84ec01871592108fbadbddbe","title":"Measurement Extraction with Natural Language Processing: A Review"},{"paperId":"2ecde642d9aa18d0b9b385ca76ab2520a66b5ace","title":"Financial Opinion Mining"},{"paperId":"3b458728091590afaa9a2b0d86444b5fa6c80275","title":"Sources and Corpora"},{"paperId":"0cb1f68586f1d0e1f4a970b902b33f4cd5a6c7cd","title":"Is Domain Adaptation Worth Your Investment? Comparing BERT and FinBERT on Financial Tasks"},{"paperId":"dd97b0b041587fe73d4dbfc8a72e2cb615e85cb8","title":"Numerals in Financial Narratives"},{"paperId":"7a318a00d61b1f063e954061e3bec35faf84b372","title":"Organizing Financial Opinions"},{"paperId":"fae743a3c57699793f59b81ac28b6a54015bcc82","title":"TMUNLP at the NTCIR-15 FinNum-2"},{"paperId":"e35a35e92e8b45c4826173dfb92440685189e025","title":"MIG at the NTCIR-15 FinNum-2 Task: Use the Transfer Learning and Feature Engineering for Numeral Attachment Task"},{"paperId":"a6b5266f340c36773f85a02673f35f381a1fc964","title":"Overview of the NTCIR-14 FinNum Task Fine-Grained Numeral Understanding in Financial Social Media Data"},{"paperId":"bb5f0975be40fc85907a3baadc66f16f73611679","title":"WUST at the NTCIR-14 FinNum Task"},{"paperId":"97a223895354de39bd6ac1ba2cddc14946e9bb8e","title":"Overview of the NTCIR-15 FinNum-2 Task: Numeral Attachment in Financial Tweets"},{"paperId":"21d95e881ab980b599a2086285204933d76149df","title":"FinNum Task : Enriched Sequence Labeling for Numeral Classification"},{"paperId":"14314873db4e784d8dff288483ceccffa9ec0c2e","title":"ASNLU at the NTCIR-14 finnum task : Incorporating Knowledge into DNN for Financial Numeral Classification"},{"paperId":"1d88b554dadf7a70dde8e5bbfe72393784a29ffb","title":"aiai at the NTCIR-14 FinNum Task : Financial Numeral Tweets Fine-Grained Classification Using Deep Word and Character Embedding-Based Attention Model"}],"references":[{"paperId":"dd0a60b3dbd35db5cac6680e4a5bf8efbfabb52e","title":"SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News"},{"paperId":"6642586d72329dbb78af964670876b8308f91c80","title":"Predicting Stock Market Behavior using Data Mining Technique and News Sentiment Analysis"},{"paperId":"055c6b7ca71dee209689264fec2d93cb97e69a79","title":"Learning to Generate Market Comments from Stock Prices"},{"paperId":"2366429e00310005da25612a3941fdaed1854fd1","title":"Temporal information extraction from clinical text"},{"paperId":"24158c9fc293c8a998ac552b1188404a877da292","title":"Neural Architectures for Named Entity Recognition"},{"paperId":"513167c08db5139162710aad9b2c217b344df2c4","title":"Numerical Relation Extraction with Minimal Supervision"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"70c621519bc02d7d2aaa79ca2320fe1af38b6f87","title":"News impact on stock price return via sentiment analysis"},{"paperId":"1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba","title":"Convolutional Neural Networks for Sentence Classification"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"1a8bd823c82bd68ab4fb83d13a730badbfe0726d","title":"The future of financial services"},{"paperId":"e498784edf2c02fe0b228479f88120f08b381cb6","title":"Twitter mood predicts the stock market"},{"paperId":"6c93b11ce414776553ed5cb9eb5d3cffa2a5fc6b","title":"Extraction and Approximation of Numerical Attributes from the Web"},{"paperId":"8d8536e5f330d9aca2e9ad608563b7a2cacd5f91","title":"Temporal Information Extraction"},{"paperId":"fa22ad98844764c014fd8e79cb1ed9e4caf8a9ee","title":"ISO-TimeML: An International Standard for Semantic Annotation"},{"paperId":"04b23f577c20d1a0e2a67aadda555f58e6d23d6e","title":"Support vector machines"},{"paperId":"24424f4050700dfa940851385d2e1ab7ba5d0cdc","title":"Extended Named Entity Ontology with Attribute Information"},{"paperId":"11b1cd032ebbc9d25ca661c5595ce32de614c90b","title":"Analyzing the Analysts: Career Concerns and Biased Earnings Forecasts"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"e23c34414e66118ecd9b08cf0cd4d016f59b0b85","title":"Bidirectional recurrent neural networks"},{"paperId":"7e7343a5608fff1c68c5259db0c77b9193f1546d","title":"The measurement of observer agreement for categorical data."},{"paperId":"e9c7307e1278c63e32af87ef0ae69a2cfbb6083d","title":"An Evaluation of Security Analysts' Forecasts"}],"id":"cabb0a468af8184e0e930841435b65679b580521","summary":"This work is the first attempt to understand numerals in financial social media data, and it provides the first comparison of fine-grained opinion of individual investors and analysts based on their forecast price."},{"url":"https://www.semanticscholar.org/paper/c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations","venue":"North American Chapter of the Association for Computational Linguistics","year":2018,"referenceCount":15,"citationCount":1700,"influentialCitationCount":176,"publicationDate":"06/03/2018","authors":"Peter Shaw,Jakob Uszkoreit,Ashish Vaswani","citations":[{"paperId":"9b73bce58160f1eec6ca6de7329aa6b09bd03bfd","title":"BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model"},{"paperId":"c7e06504d95de61ce0ae8e1fe15e7f52e843409d","title":"Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions"},{"paperId":"8c9e80c793867681ced0dd62ab0eb4c88c45240c","title":"Human Cognition-Based Consistency Inference Networks for Multi-Modal Fake News Detection"},{"paperId":"a8b19c4d7c5c516f1e9fb59eb4e34f0507770144","title":"BEAST: Online Joint Beat and Downbeat Tracking Based on Streaming Transformer"},{"paperId":"3689b7ca7b07924b6135b8a71b9f1b7937b0a3d5","title":"Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding"},{"paperId":"dbaa101f96855451efc81559f4ada91e102696a2","title":"Algebraic Positional Encodings"},{"paperId":"d621335afd543125c50dfb1449fe57ea1948b997","title":"Cross Initialization for Personalized Text-to-Image Generation"},{"paperId":"2148fdf2411d99da3f1e54c2e47b53a2fba815d0","title":"Delving Deeper Into Astromorphic Transformers"},{"paperId":"5b437e0f5353ce5ce5c599bfc778bc099185accd","title":"Agent Attention: On the Integration of Softmax and Linear Attention"},{"paperId":"36697944858ab17ca23b23ae2043aa6c0b2e3d5d","title":"Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention"},{"paperId":"a4f85b72f44ee8608f470f46c0a0e671e43d9b63","title":"Out of Context: How important is Local Context in Neural Program Repair?"},{"paperId":"07f1202447aedad79d35148ad6543b3619234539","title":"Assessing the utility of text-to-SQL approaches for satisfying software developer information needs"},{"paperId":"6ce3a7c0c0035a39420d27c2ce01f48f5fab79c3","title":"DiffiT: Diffusion Vision Transformers for Image Generation"},{"paperId":"b68876ef90b79344ed2febc42ba0c5000167c572","title":"EstraNet: An Efficient Shift-Invariant Transformer Network for Side-Channel Analysis"},{"paperId":"754f76a3434e43ad628bc5f17b2a972c401fb14e","title":"A Local Self-Attention Sentence Model for Answer Selection Task in CQA Systems"},{"paperId":"a44d96a9ae26d1939e12d7cb27067e594e0d8542","title":"Addressing Information Inequality for Text-Based Person Search via Pedestrian-Centric Visual Denoising and Bias-Aware Alignments"},{"paperId":"34ca346431d27fdac3ec4ef4bb05d83fc313baa6","title":"HCS-Net: Multi-level deformation strategy combined with quadruple attention for image registration."},{"paperId":"adf5060b3f822be08b3ca42fd954d6d035ceb607","title":"INarIG: Iterative Non-autoregressive Instruct Generation Model For Word-Level Auto Completion"},{"paperId":"f9d6ca556c2189990963d7972a5db241213fa621","title":"Appaction: Automatic GUI Interaction for Mobile Apps via Holistic Widget Perception"},{"paperId":"487778af89e87b37c09200e60757e75196915274","title":"CLAD-Net: cross-layer aggregation attention network for real-time endoscopic instrument detection"},{"paperId":"543d49234de8c55e37e712d8da470e7873e24331","title":"Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation"},{"paperId":"321160254c3b059ec7aed0bcbd7d7c6d1be17372","title":"Large Language Models in Law: A Survey"},{"paperId":"7757b3346e6578fabcfbc74754df1354e1d197d8","title":"Large Language Models in Education: Vision and Opportunities"},{"paperId":"e5835e192d0b08f78a87669ae54cd12831a57ee1","title":"Named Entity Recognition in Persian Language based on Self-attention Mechanism with Weighted Relational Position Encoding"},{"paperId":"4ea5ca620122e6a9a2b000444d36491cebf49c7c","title":"Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey"},{"paperId":"488c486fef0d58259c46d7be42b285c1de118acb","title":"Long-MIL: Scaling Long Contextual Multiple Instance Learning for Histopathology Whole Slide Image Analysis"},{"paperId":"9a0baebb1aadd029da71b7eb5df3f21e7d7654cf","title":"Empowering Vision Transformer by Network Hyper-Parameter Selection for Whole Pelvis Prostate Planning Target Volume Auto-Segmentation"},{"paperId":"01c31e1c0ec09111ffb53c60f297d34ae5076966","title":"TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree transformation"},{"paperId":"1ebebb7785828a7808d00e9d7b11793562d04a61","title":"Multi-View Stereo Network Based on Attention Mechanism and Neural Volume Rendering"},{"paperId":"41aba4d1b78ff5802689804ea16474519cd583cc","title":"Window Attention is Bugged: How not to Interpolate Position Embeddings"},{"paperId":"8f0e5d434fc69269975de9ed673a704de4552292","title":"Data Factors for Better Compositional Generalization"},{"paperId":"d3211905a65e5a9751c559a1489c41836f92f67c","title":"An Automatic Privacy-Aware Framework for Text Data in Online Social Network Based on a Multi-Deep Learning Model"},{"paperId":"017bc4b451726ea90d132a20452b99e708d9c510","title":"Multi-resolution Time-Series Transformer for Long-term Forecasting"},{"paperId":"a2e92c76a070b72202d41613eb3b9ba7031af907","title":"Comparative analysis of classification techniques for topic-based biomedical literature categorisation"},{"paperId":"9747c168a3c5bf6e11fafa4a5f0425d76af0f06f","title":"Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual Assistants"},{"paperId":"5045d5160ab962861a9f22b150fa16940c331faf","title":"Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model"},{"paperId":"275fbc43f59fc1373edf300d928bb5fa43abcaa1","title":"An interpretable multi-scale lightweight network for patient-ventilator asynchrony detection during mechanical ventilation"},{"paperId":"87e36f66b0c28ca7a327257ccb00282bdf7fe7d5","title":"Controllable Music Production with Diffusion Models and Guidance Gradients"},{"paperId":"f26e3cdc994d174414d7912e3eaf66d00111fdb9","title":"COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning"},{"paperId":"f8f8bee22835b74b00b235b95099718c50d00cc0","title":"DualGGAN: A New Facial Style Transfer Network"},{"paperId":"95192b39c82e8286e77b8eb4824f6e2faeda2194","title":"Learning to Enhance the Position Embedding and Coherence"},{"paperId":"c495014ad01a84d3155698476ce56be356f3600b","title":"Multi-task Piano Transcription with Local Relative Time Attention"},{"paperId":"6dba33d8dd274bcf4d57f6ae420f60082eed3f0a","title":"Learning Multifaceted Self-Similarity for Musical Structure Analysis"},{"paperId":"8f7d971a20767f31bfd33b72d58fe7fb9c2ef0a0","title":"TS-Fastformer: Fast Transformer for Time-Series Forecasting"},{"paperId":"40cf13a1b4cbc3cd8cba0379ca3a30f656f005a8","title":"Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering"},{"paperId":"1c8f9fd695aba1fa06d77619540f8277e91fca1a","title":"HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding"},{"paperId":"40e2efc3d4900f7a1471d96e8fa668722b7cedfa","title":"The Expressibility of Polynomial based Attention Scheme"},{"paperId":"901fedcff9e8c5a564d469d4fd5e8db9aed80f83","title":"ASTormer: An AST Structure-aware Transformer Decoder for Text-to-SQL"},{"paperId":"6efadd36de5cd066ef8d7dae04d6629a0058fe37","title":"TabAttention: Learning Attention Conditionally on Tabular Data"},{"paperId":"96e81dbc07de0fefd6244b8dba12fe3b088cb4aa","title":"Transformers as Graph-to-Graph Models"},{"paperId":"697a4904a7874dd08626d73d6cbf181f8e7dd11e","title":"Non-Local Geometry and Color Gradient Aggregation Graph Model for No-Reference Point Cloud Quality Assessment"},{"paperId":"ff83d375248c300440db3fe6f8df152a120578ce","title":"Improving Image Captioning through Visual and Semantic Mutual Promotion"},{"paperId":"1594467d6b20f14acc270c386835ed4f465ed0e8","title":"HELIOS: Hyper-Relational Schema Modeling from Knowledge Graphs"},{"paperId":"34eb3f28380c01360c30e2686115e7591a524f9e","title":"IceCube -- Neutrinos in Deep Ice The Top 3 Solutions from the Public Kaggle Competition"},{"paperId":"e135fadb3e08e0004c9623cb568677a7e0e87a4d","title":"CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code"},{"paperId":"9bcbd4922f7e0d1962ff35cbba6c14929734bc27","title":"A Survey on Review - Aware Recommendation Systems"},{"paperId":"8ed225549a6f54275a6c72a3ac900a787edb1144","title":"DPP-TTS: Diversifying prosodic features of speech via determinantal point processes"},{"paperId":"3866b90dd5818cd43d11853e7be139fce807ddf2","title":"Toward Flare-Free Images: A Survey"},{"paperId":"4031cf9e1f1cd767e99f50ac5cc7134f68355f1e","title":"GraphFADE: Field-aware Decorrelation Neural Network for Graphs with Tabular Features"},{"paperId":"a03f5b8d293fdb0e56b178cfaa01788a9c5a8458","title":"Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding"},{"paperId":"2fd5de4b4f75234f980157b00e4db8af7a228578","title":"The Locality and Symmetry of Positional Encodings"},{"paperId":"7afc9df8a8b7d2ea79c24185a17197033305a830","title":"Not Just Learning From Others but Relying on Yourself: A New Perspective on Few-Shot Segmentation in Remote Sensing"},{"paperId":"6a255110655114a1ac049243d4f503dbf81c5d1f","title":"T-MGCL: Molecule Graph Contrastive Learning Based on Transformer for Molecular Property Prediction"},{"paperId":"200fcd964f41efe0c35a3f888a520ede08a3269c","title":"From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers"},{"paperId":"260bb42405845b5763d1b949945ca6e191adef8c","title":"GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers"},{"paperId":"dcfc69c3fdc5d59785e3a8284143c7be989fae36","title":"Equivariant Matrix Function Neural Networks"},{"paperId":"445060e3b10e17fed8e5a14ec6f9888c677052f5","title":"TEclass2: Classification of transposable elements using Transformers"},{"paperId":"9949efd0e5f3988bdb4adf933d98b64275dcc548","title":"A comparative atlas of single-cell chromatin accessibility in the human brain"},{"paperId":"8ee2568e1fb4c9c98175a85d389cdcb80040d12c","title":"HeightFormer: A Multilevel Interaction and Image-adaptive Classification-regression Network for Monocular Height Estimation with Aerial Images"},{"paperId":"9ef4754ed32a64e2068a256edfe98eff2ea57229","title":"EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention"},{"paperId":"197bd8cc694716adadffc98eeab6f48bc3c109ce","title":"A malicious programs detection method incorporating transformer and co-occurrence matrix"},{"paperId":"058a802ec756964471f53a4c327b5bfb756b4c71","title":"Fibonet: A Light-weight and Efficient Neural Network for Image Segmentation"},{"paperId":"6d73d0547057134982acdaa190e741b1d037bd40","title":"Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem Formulation and Dataset"},{"paperId":"42b877f7423fc727bff5b6e173ad336da33079a9","title":"Uncovering hidden geometry in Transformers via disentangling position and context"},{"paperId":"dc48bc1a4d81e0f37603013fd2a95644dc233bd0","title":"Functional Interpolation for Relative Positions Improves Long Context Transformers"},{"paperId":"95a01fb1ba026fabdb3b278cfc892c8223d844c9","title":"Neural Language Model Pruning for Automatic Speech Recognition"},{"paperId":"0607e78b1b560847378055283c6d90d5dd73df7e","title":"GET: Group Event Transformer for Event-Based Vision"},{"paperId":"f8a6a032718cce60a0dd6d5c75f42b97e644b34b","title":"Spherical Position Encoding for Transformers"},{"paperId":"ecba3911696134697267e1aa4daf44e992783ef8","title":"RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image Classification"},{"paperId":"fe518b7bfc287b5cefef817095ef4aab29696181","title":"Improving attention network to realize joint extraction for the construction of equipment knowledge graph"},{"paperId":"d7210369040219366ec4a82abe831f1c6d4568a4","title":"SE-VisionTransformer: Hybrid Network for Diagnosing Sugarcane Leaf Diseases Based on Attention Mechanism"},{"paperId":"c00ccde8d1dd184deacf320bfca9e30474edb5d1","title":"LiDAR Missing Measurement Detection for Autonomous Driving in Rain"},{"paperId":"0393ec6aad1b5240bb3c9df5ed11b6cbbb398085","title":"Empirical Study on Transformer-based Techniques for Software Engineering"},{"paperId":"40224e048fe957015b7f96e7fc5aa8df943d3be4","title":"A brain tumor computer-aided diagnosis method with automatic lesion segmentation and ensemble decision strategy"},{"paperId":"217ba497bef71f14fca31bede5233ff1f759a1c4","title":"Federated Learning with Differential Privacy for End-to-End Speech Recognition"},{"paperId":"2280bf14441c360648d9b11c9250a79853b72ea6","title":"YOLOv5s-gnConv: detecting personal protective equipment for workers at height"},{"paperId":"a8f05b5ef3d60fb310f8366aa775118df5b700c3","title":"Tackling VQA with Pretrained Foundation Models without Further Training"},{"paperId":"2bf171af7f46351e3c78a81ea3461b7e2362d65c","title":"Asca: less audio data is more insightful"},{"paperId":"46399941bbdf9efbc33bd4e90b5f01f1f1597e54","title":"DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image"},{"paperId":"182077ea2480bb9b222dfe393c090820efc256f0","title":"Health State Estimation of Lithium Battery Based on Wavelet Packet Transform and Transformer"},{"paperId":"ec700e5cf6999e4ec893251ac850b4e937dcd4ba","title":"Convolutional Transformer-Based Image Compression"},{"paperId":"0cc3738b1870e3323b9cce9557c199710a8d3d22","title":"PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement"},{"paperId":"4eb0be1bfd0eadf7c505a0ddca4fa5591963fbb8","title":"Weight-guided feature fusion and non-local balance model for aluminum surface defect detection"},{"paperId":"a5213bdd9be116369920b7fa0827977e27b4d47e","title":"Self-Supervised Triplet Contrastive Learning for Classifying Endometrial Histopathological Images"},{"paperId":"0e4f49aac34e7a0c290b680b2ed6e5c1d8454d81","title":"CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling"},{"paperId":"74b91baef6641bc80f7daa5df4507717bf6e77cd","title":"Long-Range Transformer Architectures for Document Understanding"},{"paperId":"c55ec6a805949c311983922cb1ec0f06cac0b9f3","title":"Toward a Deeper Understanding: RetNet Viewed through Convolution"},{"paperId":"3da90938a5315a9f61b5a300631f6bb0eb71a69b","title":"How to Train Your Neural Bug Detector: Artificial vs Real Bugs"},{"paperId":"d5a68862523b575fdd0907d3ac9910eb3cb4648e","title":"Towards Smaller, Faster, and Greener Language Models of Code"},{"paperId":"ecd03eaa6ad69d64c1335113fe2a0c41361ff744","title":"Towards Unsupervised Graph Completion Learning on Graphs with Features and Structure Missing"},{"paperId":"48eeb044e50403f9220efb63f8d229df70ca5ba4","title":"Matcha-TTS: A fast TTS architecture with conditional flow matching"},{"paperId":"9e75f4dea6690201488eaec7859a3266ff458e1b","title":"Rubric-Specific Approach to Automated Essay Scoring with Augmentation Training"},{"paperId":"6d0e4bfdf023f18cb68e5155d5675216fb2749b4","title":"Inferring Actual Treatment Pathways from Patient Records"},{"paperId":"9be1f788ab36ccec282ee29cb3ba60499b3fdcf2","title":"Pose-aware Disentangled Multiscale Transformer for Pose Guided Person Image Generation"},{"paperId":"218d05e8478b4c401566c61a714bc75cd404a7c5","title":"A fine-grained causality extraction model incorporating relative location coding"},{"paperId":"f1a1ec62f457392afb065a852392292169e92737","title":"Evaluating Transformer’s Ability to Learn Mildly Context-Sensitive Languages"},{"paperId":"8a8125dce54eed4bef35d059ce66ed01cb885ce0","title":"Predicting RNA-seq coverage from DNA sequence as a unifying model of gene regulation"},{"paperId":"73564a6d6400106ea22ad363699244e09475aaa5","title":"CFRLA-Net: A Context-Aware Feature Representation Learning Anchor-Free Network for Pedestrian Detection"},{"paperId":"5a188619af29065d1a21b1ca64902d06ed48c828","title":"SFPFusion: An Improved Vision Transformer Combining Super Feature Attention and Wavelet-Guided Pooling for Infrared and Visible Images Fusion"},{"paperId":"1382b391fb7b6756fe18650bfef1d48b8900aff0","title":"IoTSim: Internet of Things-Oriented Binary Code Similarity Detection with Multiple Block Relations"},{"paperId":"f0b0080e34efb8643d9d46a05b8f22055f57faec","title":"STTRE: A Spatio-Temporal Transformer with Relative Embeddings for multivariate time series forecasting"},{"paperId":"60f01f48eeb4507f9f46e0b7a563dc927f873809","title":"Separate and Locate: Rethink the Text in Text-based Visual Question Answering"},{"paperId":"a569eae6c4806f580557d81a4ef9c908286c1ee0","title":"Learning to Represent Patches"},{"paperId":"819bbdc2dac9e13d9ca3e2508a6e063186ce5e40","title":"YaRN: Efficient Context Window Extension of Large Language Models"},{"paperId":"b5df4582f2c6c824dc07d96b7688f7ce376bdd71","title":"E-HRNet: Enhanced Semantic Segmentation Using Squeeze and Excitation"},{"paperId":"baad0dd2bb574f52aa18811d036cd3567850e712","title":"SAGDTI: self-attention and graph neural network with multiple information representations for the prediction of drug-target interactions"},{"paperId":"5aa33f774ba55a6ce6dfa7d6daadc7d92dfcc3a8","title":"Tomato Fruit Detection Using Modified Yolov5m Model with Convolutional Neural Networks"},{"paperId":"84b5be248e87e1cdb2c0d2a70cb3cf56f0d44f56","title":"Trustworthy Dynamic Object Tracking Using Deep Reinforcement Learning with the Self-Attention Mechanism"},{"paperId":"90a9a6f2b8d9f94ee5055d4a96654fa5de9eb53d","title":"Refine Neutrino Events Reconstruction with BEiT-3"},{"paperId":"9532796372159c700891af7c4e68f5226371cdc8","title":"A Novel Cross-Sensor Transfer Diagnosis Method with Local Attention Mechanism: Applied in a Reciprocating Pump"},{"paperId":"374ebdc8240a35820cb7ab8bfca37e180e21b605","title":"Sparks of Large Audio Models: A Survey and Outlook"},{"paperId":"81bcc09848ec019a986d7c9940b8d4d5575186c6","title":"MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation"},{"paperId":"2dfb9171e180dcb0af23d305e024d43d311708ab","title":"Giraffe: Adventures in Expanding Context Lengths in LLMs"},{"paperId":"52c936a187bb6b089ed56f65d01052137bb42404","title":"Self-Feedback DETR for Temporal Action Detection"},{"paperId":"59a438952214794f978593e72f56430357e75b7a","title":"Multi-Channel Speech Separation with Cross-Attention and Beamforming"},{"paperId":"6ead5cf59020f89638a0b2fc5d9cd4821d060897","title":"Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition"},{"paperId":"48eb6106d3abd2306e2b7deac5eed15a673a2668","title":"Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model"},{"paperId":"81f592069b5e6ff3c3628017edbda1df3c8d9662","title":"An Visual Question Answering System using Cross-Attention Network"},{"paperId":"6c88d10910bad16b13890813205d6914f4ebd028","title":"Self-Supervised Dynamic Hypergraph Recommendation based on Hyper-Relational Knowledge Graph"},{"paperId":"8d3cbf4d16c61e199a8ff754ad9c605a8e62ccb2","title":"SU2GE-Net: a saliency-based approach for non-specific class foreground segmentation"},{"paperId":"92fc58cf731cde4040ee83baa46b06ad88d1d2ef","title":"PAT: Position-Aware Transformer for Dense Multi-Label Action Detection"},{"paperId":"b3c6159082d976b968d528d3ccbdcc12048cc9d6","title":"An Interpretable and Attention-based Method for Gaze Estimation Using Electroencephalography"},{"paperId":"45003215fbe939e631a8524fe81a34f32552fe47","title":"CUI-Net: a correcting uneven illumination net for low-light image enhancement"},{"paperId":"bca68a52f24941269bed015a37b25df1d0954de0","title":"PERT-GNN: Latency Prediction for Microservice-based Cloud-Native Applications via Graph Neural Networks"},{"paperId":"f7f3a38bcb5ddacc4bf1ba39b923957c5c7f7bf9","title":"A Sequence-to-Sequence Approach with Mixed Pointers to Topic Segmentation and Segment Labeling"},{"paperId":"4dc935e258ad9df3a76958d9824073c52155ff35","title":"DETR Doesn't Need Multi-Scale or Locality Design"},{"paperId":"d23df2e0e345093957c3bfa8d9e89c8a3874d4bd","title":"CDNM: Clustering-Based Data Normalization Method For Automated Vulnerability Detection"},{"paperId":"625d5162adbc5db195fdb4e1d9f625c309a7e6b1","title":"Lightweight Convolutional-iConformer for Sound Event Detection"},{"paperId":"2ab3e082ee8a90d2a452c2a257915ff70d0b8911","title":"A Survey on Data-Driven Runoff Forecasting Models Based on Neural Networks"},{"paperId":"326ab83068d6f91c0513b74ea004e6d5fedccd51","title":"MIX-TPI: a flexible prediction framework for TCR–pMHC interactions based on multimodal representations"},{"paperId":"866f286d635e77aaa27474546ca4d0626d8e656c","title":"Customized Positional Encoding to Combine Static and Time-varying Data in Robust Representation Learning for Crop Yield Prediction"},{"paperId":"a14fe37f45c178597766cfc65ca32d567c3eecf6","title":"Enhancing the sustainable management of fine particulate matter-related health risks at subway stations through sequential forecast and gated probabilistic transformer"},{"paperId":"e7eed4f6efbf075b4152f94c852e1f8c33300d3f","title":"HYDRO-3D: Hybrid Object Detection and Tracking for Cooperative Perception Using 3D LiDAR"},{"paperId":"7da05f84e16a6003f7167ec004c413f3b1e91fa2","title":"Don't be so negative! Score-based Generative Modeling with Oracle-assisted Guidance"},{"paperId":"547e7cf61ada3a3b92de878c62402af84ef02474","title":"Anomaly-Based Intrusion Detection in IIoT Networks Using Transformer Models"},{"paperId":"f70494a1280c1f817f6ecfbb65217736bc357f8f","title":"MLIC++: Linear Complexity Attention-based Multi-Reference Entropy Modeling for Learned Image Compression"},{"paperId":"07bc02bd16f6fe78a7ea3bb8d966fcc6e3893195","title":"Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners"},{"paperId":"16be9876eebcda031c15efaf4cfe3ba49146b329","title":"ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis"},{"paperId":"45806c90fe4cf9574d35814b4f85f6fac8abe167","title":"A Heterogeneous Graph to Abstract Syntax Tree Framework for Text-to-SQL"},{"paperId":"58d1a002a0ff0aa40b6633f0a7073d48f1cdff53","title":"Empower Your Model with Longer and Better Context Comprehension"},{"paperId":"e54c859515b68c5c7a56f66382b9e9605953df0b","title":"3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding"},{"paperId":"3d04f5bb0599ce02d8fa47420f72f500758c4660","title":"How to Scale Your EMA"},{"paperId":"0fa35c54e165f3c8f30a40bf05f4e60cfd93267e","title":"Chinese medical entity recognition based on the dual-branch TENER model"},{"paperId":"9e3e56957e249cdebdd8673fd1174980ed694560","title":"Efficient Beam Tree Recursion"},{"paperId":"4064ebec539718b2d77607df8d86c6d436def83e","title":"Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model"},{"paperId":"8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8","title":"Linearized Relative Positional Encoding"},{"paperId":"27f753d63dbc55034cfb0e67343894433729140d","title":"On Answer Position Bias in Transformers for Question Answering"},{"paperId":"f490898ec8363d9dd0040479ed392745ff40c425","title":"ECSIC: Epipolar Cross Attention for Stereo Image Compression"},{"paperId":"453e3b4e7ee2b0a8ad0a4fc14467a29c347fff69","title":"A Review of Text Sentiment Analysis Methods and Applications"},{"paperId":"061116d951524b0f3cce48f7c2438ed2ac467502","title":"RayMVSNet++: Learning Ray-Based 1D Implicit Fields for Accurate Multi-View Stereo"},{"paperId":"b550e855522f05816dafe39ad521c655b7f84664","title":"Transformers are Universal Predictors"},{"paperId":"3cc89f9e1a06960fd349cae901f4f5a1fb72a291","title":"Multimodal Molecular Pretraining via Modality Blending"},{"paperId":"8d6e9a096cb2f3c8bf40b37aad7db0d9c69066ba","title":"Learning to Identify Graphs from Node Trajectories in Multi-Robot Networks"},{"paperId":"9acd6d8b695187831222d94277f3ffcf1d561d10","title":"CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training"},{"paperId":"b036e281c6d73d352b963e97f3b36bec549837f8","title":"VampNet: Music Generation via Masked Acoustic Token Modeling"},{"paperId":"177c3a4fc5e6603f09b08568e5f93bf78bcafe21","title":"The Ethical Implications of Generative Audio Models: A Systematic Literature Review"},{"paperId":"1906a9a9eb23ceedfab2c328f1ae3064b684c520","title":"Equivariant Single View Pose Prediction Via Induced and Restricted Representations"},{"paperId":"0db0af0cd3ceb0531a050a03e6ceb849580ff53b","title":"Teaching Arithmetic to Small Transformers"},{"paperId":"c4d4f66681b99a51908fa06d7d5fc7f534d11cd8","title":"LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias"},{"paperId":"76e8dd7acb3a9e4ae65189817bfa7e1adedf805a","title":"Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation"},{"paperId":"956697da5f282c1bc89b19b4127f7c040aa2268e","title":"Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation"},{"paperId":"764299bcfdbc16361e57e3045a3921f284adf399","title":"Self-attention mechanism at the token level: Gradient analysis and algorithm optimization"},{"paperId":"216853c0dc33a3bed011e71641b8d323dbe7b53b","title":"A Dual Architecture Fusion and AutoEncoder for Automatic Morphological Classification of Human Sperm"},{"paperId":"eb9fc16bd99443af8bca10f9f01f92854ab7cbbc","title":"Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation"},{"paperId":"5d433a43aa8b5c4687e9edebf2fac6e8e248bb10","title":"Predicting Music Hierarchies with a Graph-Based Neural Decoder"},{"paperId":"fecbf14c6afa8d17a0cae8697db8abbadb7abe7d","title":"A Hybrid System for Systematic Generalization in Simple Arithmetic Problems"},{"paperId":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers"},{"paperId":"d560d653544a23347cb8ab17b44c3f2cc85b3e7b","title":"Exploiting Pseudo Future Contexts for Emotion Recognition in Conversations"},{"paperId":"5aeecb086d34368a5539e449d5310ed114a29b8a","title":"Toward a Spectral Foundation Model: An Attention-Based Approach with Domain-Inspired Fine-Tuning and Wavelength Parameterization"},{"paperId":"79efb734cef71f93ad4c875044f9fb4d2cd6079d","title":"Fine-Grained Position Helps Memorizing More, a Novel Music Compound Transformer Model with Feature Interaction Fusion"},{"paperId":"d8088d6b45e3ae523b9472745971f698a3b1d7f5","title":"SKIER: A Symbolic Knowledge Integrated Model for Conversational Emotion Recognition"},{"paperId":"2f4b864db579dd094bc65955127f0ab74f14172c","title":"AI in drug discovery and its clinical relevance"},{"paperId":"92630c3017c9a878dbed0178e66a9498abe2daaf","title":"Sumformer: recursive positional encoding for transformer in short text classification"},{"paperId":"5c828011a508611df4d58cced9cc48d049dc4eb9","title":"A Data Source for Reasoning Embodied Agents"},{"paperId":"94d1500cb6b280ff201047bf831593c4659581b7","title":"Learning Dynamic Latent Spaces for Lifelong Generative Modelling"},{"paperId":"7617b8476756479789a1a0964427b5a81024e123","title":"SheetPT: Spreadsheet Pre-training Based on Hierarchical Attention Network"},{"paperId":"d6c2ce832157f54e1fe717bd3b1a9c4d4619c26e","title":"Random Walk Conformer: Learning Graph Representation from Long and Short Range"},{"paperId":"a6159daf277e73ca511da98a0d05432f6bab0de7","title":"LightGlue: Local Feature Matching at Light Speed"},{"paperId":"d2d0371158803df93a249c9f7237ffd79b875816","title":"Sparse Modular Activation for Efficient Sequence Modeling"},{"paperId":"072e2deab443a135d922f7cb03b8b1f274712ec2","title":"A Transformer-Based Cross-Window Aggregated Attentional Image Inpainting Model"},{"paperId":"a249bc9a63fcd2f39d0b61c02221ce46f306cb70","title":"A transformers-based approach for fine and coarse-grained classification and generation of MIDI songs and soundtracks"},{"paperId":"5edf25a3339ec38374566b70ac73d10736dd0dd4","title":"Passive Vital Sign Monitoring via Facial Vibrations Leveraging AR/VR Headsets"},{"paperId":"e23ea2db9561cad034a77eb00e661cf04b89d539","title":"Generating Questions via Unexploited OCR Texts: Prompt-Based Data Augmentation for TextVQA"},{"paperId":"ba591af0202f58875ab5b51c10c2e8dde50e8e68","title":"A Semantic and Structural Transformer for Code Summarization Generation"},{"paperId":"400cee04737f6a2012a2677acd8c052d233335eb","title":"Understanding Parameter Sharing in Transformers"},{"paperId":"5654655944c4a07646dfd280bdb819080ad35fa6","title":"Research on Named Entity Recognition in Improved transformer with R-Drop structure"},{"paperId":"9ced9dac3aca804522f474d39595b4c44d277303","title":"Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series"},{"paperId":"906ff01d437489fb505e634b956cdb7990c2547a","title":"Partially Trained Music Generation based on Transformer"},{"paperId":"345ba0a4fad9c762d87ef0268c3c047f774e9b5c","title":"SSIN: Self-Supervised Learning for Rainfall Spatial Interpolation"},{"paperId":"a9a46d098a453d635355517b7d977c260c48d8ff","title":"PauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and Pause-based Prosody Modeling"},{"paperId":"12d1ea06e103b0c0059a9388a0b7157525e93f4a","title":"HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models"},{"paperId":"c5e3e543d55a145fe4c7e5dd7fd97b9f30054f46","title":"E(2)-Equivariant Vision Transformer"},{"paperId":"4bbe913078e73493452b5c35fd6f12b267eb0e4d","title":"Complementary Shifted Transformer for Image Captioning"},{"paperId":"7d22e6d110a2be18ef7236fb2238fd85463de4b2","title":"DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles"},{"paperId":"698f58447fd147a20f1b43fea46e15875e756473","title":"POVNet: Image-Based Virtual Try-On Through Accurate Warping and Residual"},{"paperId":"f03fa16bee6b7296ea86fa4fc3a0416ef135f77d","title":"Improving Long Context Document-Level Machine Translation"},{"paperId":"b8e57e8c81877d18998413b3e3c724fe142bb34e","title":"CoAt-Mixer: Self-attention deep learning framework for left ventricular hypertrophy using electrocardiography"},{"paperId":"6321a92d3e4fbe34ed6688da0a1164528d053092","title":"Learning Probabilistic Coordinate Fields for Robust Correspondences"},{"paperId":"2d89a84314f1b83e58f3cb4088bd4b95663b40c7","title":"Everybody Compose: Deep Beats To Music"},{"paperId":"a559a59ac6bc9c7c90caec294216de11a3e78703","title":"Human-imperceptible, Machine-recognizable Images"},{"paperId":"ccef05840870479632f9055479a1269d53033b7b","title":"Certified Reasoning with Language Models"},{"paperId":"c7889e7180490b7f829777f6cc97bbb4686671a8","title":"A2B: Anchor to Barycentric Coordinate for Robust Correspondence"},{"paperId":"357c9cbdcd164d86d3a559ef9bab750c996da13b","title":"Classification of Edge-dependent Labels of Nodes in Hypergraphs"},{"paperId":"81cac3c2a2eccf20dac0489f8a16e06f98a2d506","title":"Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning"},{"paperId":"fc1774419f418fb79451c0bde5d64ae4ce8d5b31","title":"Systematic Visual Reasoning through Object-Centric Relational Abstraction"},{"paperId":"94563424394a17d3a9245147b5f828b5e4e1f86a","title":"Streaming Stroke Classification of Online Handwriting"},{"paperId":"425f4f6f96d6066467ab640b29dd444a7bbdfa92","title":"Mixed-former: multi-fusion remote sensing change detection"},{"paperId":"963cf5270b0ecf4d1f4b7f0477977604f6ea6f9b","title":"The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles"},{"paperId":"14347ae05bebfd9cb594ca2d2ebf07dd179962b7","title":"Data-Efficient French Language Modeling with CamemBERTa"},{"paperId":"9b417791b2e908d1e2a5486578642da19a0817b7","title":"Effect of Feedback on Drug Consumption Disclosures on Social Media"},{"paperId":"a947d5063b000031b3401bbcefd10d2552624f0d","title":"Learning Accurate 3D Shape Based on Stereo Polarimetric Imaging"},{"paperId":"52b10ae66d025e99fbb602935e155f97f4f0696f","title":"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance"},{"paperId":"fb70eac98eb79ce1639b2cd8db84f4f805cffccd","title":"An End-to-End Blind Image Quality Assessment Method Using a Recurrent Network and Self-Attention"},{"paperId":"df55f9f174bd2343356ea43dd3ef2a6ac6951b59","title":"FaceFormer: Aggregating Global and Local Representation for Face Hallucination"},{"paperId":"4fac904a7f23d69db347441036cb9d5e44a233ab","title":"Generative Pre-trained Transformer (GPT) based model with relative attention for de novo drug design"},{"paperId":"31e50a562de6a27b642339e49d6552dd1ca76cb9","title":"Context-Aware Pretraining for Efficient Blind Image Decomposition"},{"paperId":"aae83ebf6581a7f9be439aa59d826bad39a8898c","title":"Global Motion Understanding in Large-Scale Video Object Segmentation"},{"paperId":"6f6e2e0311589a9af045f6acd00b7dee6d19fce4","title":"The Impact of Positional Encoding on Length Generalization in Transformers"},{"paperId":"08e5d8d1a9d481204b4caff58490755f59b0e88f","title":"Monotonic Location Attention for Length Generalization"},{"paperId":"93be625e495c685301023774647d4446beeabc33","title":"Inverse-design of nonlinear mechanical metamaterials via video denoising diffusion models"},{"paperId":"1355d894cc9a5c9aaf7b1933cb582bbacc148943","title":"RINGER: Rapid Conformer Generation for Macrocycles with Sequence-Conditioned Internal Coordinate Diffusion"},{"paperId":"c134bfc429e955d22b25cf9d3bd15e80d6518781","title":"Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input"},{"paperId":"7a724b23e207ac2618d1e4c53b5266757953583c","title":"Graph Enhanced Transformer for Aspect Category Detection"},{"paperId":"080c5f84d5faf4b80d4891e365afeb1bb9c0cc06","title":"Graph Generation with $K^2$-trees"},{"paperId":"ad4be183eef6aa8d81b3a7cf3c06250e5b983583","title":"SimpSON: Simplifying Photo Cleanup with Single-Click Distracting Object Segmentation Network"},{"paperId":"119a3ed0898499fce0ce6af6958d566d82390ba5","title":"GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning"},{"paperId":"16e2cabe68ad1d651b21d0e7d1cea6e0a293741c","title":"Identity-Guided Spatial Attention for Vehicle Re-Identification"},{"paperId":"905a401efd96d4a5705f64b67cef486b43e83e20","title":"Improving position encoding of transformers for multivariate time series classification"},{"paperId":"b4ee74a1f294cb5bb028a958ce43219cabb0f347","title":"TranSFormer: Slow-Fast Transformer for Machine Translation"},{"paperId":"43972d51d944c6a1499d1bebd4421768a872c379","title":"FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models"},{"paperId":"8556212e1f4ebfd7df71dc7f0cfdc88f0995d5e6","title":"Research on abnormal detection of gas load based on LSTM-WGAN"},{"paperId":"639977efc7e5e6f0e7c67cfd25536501748e6c5b","title":"Duplex Diffusion Models Improve Speech-to-Speech Translation"},{"paperId":"0d4354ed28ff3924a8cc39725b11587ab5aa4aae","title":"RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration"},{"paperId":"0bec2d1e1ff6d2b11ca2dad729cbfe13f3bc07aa","title":"Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness"},{"paperId":"5c15f4070bd46314e823723efcff26e844d490c4","title":"Learning Sequence Descriptor based on Spatiotemporal Attention for Visual Place Recognition"},{"paperId":"279a2ba6f22a543e8235fe95603f034d11adc6ad","title":"Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization"},{"paperId":"b91083c6cb7490f039b22319b5932f7138282593","title":"Deep Multiple Instance Learning with Distance-Aware Self-Attention"},{"paperId":"9fe29c834afbe1848d9df713ae6e0ca3bd053605","title":"Probing the Role of Positional Information in Vision-Language Models"},{"paperId":"45a647629c5f9e58ad4463ba9b9087445720e2fc","title":"Five A+ Network: You Only Need 9K Parameters for Underwater Image Enhancement"},{"paperId":"70d59e24719a3e29f94916476d005dc59167a1a4","title":"MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation"},{"paperId":"02bdb2ba006344464ead68ad672a51327cddf21d","title":"Toward Moiré-Free and Detail-Preserving Demosaicking"},{"paperId":"be20e7d79f494cab1e3cef573c138ea262ecf533","title":"GCNSLIM: Graph Convolutional Network with Sparse Linear Methods for E-government Service Recommendation"},{"paperId":"838a53870a700b60de3dee1d1398f1eaf64ebfd6","title":"Reconstruct Before Summarize: An Efficient Two-Step Framework for Condensing and Summarizing Meeting Transcripts"},{"paperId":"8e65c94ba0664735bfe15c8dea4baf69a3a4d905","title":"Meta-Polyp: A Baseline for Efficient Polyp Segmentation"},{"paperId":"2f16e71bed0a4ec47ceee98dffd80a13fb6af092","title":"Sinogram Domain Angular Upsampling of Sparse-View Micro-CT with Dense Residual Hierarchical Transformer and Noise-Aware Loss"},{"paperId":"4755545bdd5c3eaef08cd0f5a45ccde5f03b93b2","title":"RAANet: Residual Aggregation Attention Network for Classification of Small Intestinal Endoscopic Images"},{"paperId":"acf397c89002167b4580e86ede9a472b7a55be45","title":"Multi-scale feature fusion pedestrian detection algorithm based on Transformer"},{"paperId":"29bce162bd5b83cbc4c00732d03844b33b8a6007","title":"QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing"},{"paperId":"901de03bb54f7025f11340ed24252a48fa8415bc","title":"What should be encoded by position embedding for neural network language models?"},{"paperId":"5e8427b13f8691697564b565f9d03f3e3e294b0b","title":"Understanding Gaussian Attention Bias of Vision Transformers Using Effective Receptive Fields"},{"paperId":"f35f5aedc30e2c5ded210d9c91ba6e84bd029425","title":"Toeplitz Neural Network for Sequence Modeling"},{"paperId":"53f59f389ed55b1576f65058123f5234f65b576e","title":"Laziness Is a Virtue When It Comes to Compositionality in Neural Semantic Parsing"},{"paperId":"2883fc7580e336c05d6e808b76b4b19feb2d6cd2","title":"Overview of Deep Learning Methods for Sentiment Analysis"},{"paperId":"bb624a2a88a3c960d5b59360baf9a8259cbcfa81","title":"Hierarchical Transformer for Scalable Graph Learning"},{"paperId":"82e0cae01a57fd1d18f80dffd6367f85a4c92536","title":"Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks"},{"paperId":"3bbc8841012fdb5971d1c86dff528edd8590f1b8","title":"SLTUNET: A Simple Unified Model for Sign Language Translation"},{"paperId":"b02ea0ddcc8515fdb94ffc5c4355132335ef71b4","title":"An Extensive Study of the Structure Features in Transformer-based Code Semantic Summarization"},{"paperId":"6fa1e9593fdfbc0688e6bb53ee19228bf2232aa7","title":"Self-attention enhanced deep residual network for spatial image steganalysis"},{"paperId":"57c289c912c43ab6f2388766a0329d89bbff1534","title":"VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning"},{"paperId":"3b9f0912b78f7ad1c1ac3e7e18e25135593bf778","title":"Improving Arabic Named Entity Recognition with a Modified Transformer Encoder"},{"paperId":"631e3879b7daa24da4014a797850146bfd9f7926","title":"Cross-Shaped Windows Transformer with Self-supervised Pretraining for Clinically Significant Prostate Cancer Detection in Bi-parametric MRI"},{"paperId":"74fa10978af75a3af8dc87eb5b3b91e6aa9cbb5e","title":"Comparison and analysis of computer vision models based on images of catamount and canid"},{"paperId":"20cd09f73421bae53a37c17d0ecc2c5147abf6c0","title":"Learning Neural PDE Solvers with Parameter-Guided Channel Attention"},{"paperId":"b5416fc248f3dbcece522ef59f571a1f56e5e9a4","title":"Technical Report on Token Position Bias in Transformers"},{"paperId":"b4d2d8ba34a98d71af861bfc5a11f630bd66b8db","title":"Streamlined Global and Local Features Combinator (SGLC) for High Resolution Image Dehazing"},{"paperId":"8dc22bea5ddb1b3e4d5f522d3e6dbf9ebf6d531e","title":"Img2Vec: A Teacher of High Token-Diversity Helps Masked AutoEncoders"},{"paperId":"8a94922c7d45df945a41b950c8d05f23c71001cf","title":"MMST: A Multi-Modal Ground-Based Cloud Image Classification Method"},{"paperId":"753ca5e059089a3eed0872c5caadad6f05f502e7","title":"SACANet: scene-aware class attention network for semantic segmentation of remote sensing images"},{"paperId":"8f7ace63f99df0ed0638a27a9cf861139a63641b","title":"Multi-Modal Deep Learning for Credit Rating Prediction Using Text and Numerical Data Streams"},{"paperId":"458c8a10ec37466a6a554202c472e8faea0834ad","title":"Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers"},{"paperId":"850b8f31a1bb762544bd35163923784a664b315a","title":"Prompt-Learning for Cross-Lingual Relation Extraction"},{"paperId":"a955eeda762dce23d254fb76e1071ebaf7d78fce","title":"STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning"},{"paperId":"ad426f36a41d03e97b09108b5c50260efff7deb5","title":"Region-Enhanced Feature Learning for Scene Semantic Segmentation"},{"paperId":"148501704f8c218347ace30a45a5aa7bfa9c20b2","title":"Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding"},{"paperId":"ba7592b6052ad470eb75bd47659e27b58ed81631","title":"Dynamic Graph Representation Learning with Neural Networks: A Survey"},{"paperId":"b032f324a0d4a24fd917551345bd100dc368e41a","title":"Diffusion Models as Masked Autoencoders"},{"paperId":"79a574788b99f3a065cdc972c94497a921af5239","title":"Inductive biases in deep learning models for weather prediction"},{"paperId":"c40b7278b9440778dd79f44535f822886b42532f","title":"LenM: Improving Low-Resource Neural Machine Translation Using Target Length Modeling"},{"paperId":"dc26a39c9a02d805dfd96742d66718e8d931e5c0","title":"OVERVIEW OF TRANSFORMER-BASED MODELS FOR MEDICAL IMAGE SEGMENTATION"},{"paperId":"6c9d243af44f8e8d0614d4951fe8672af84f6daa","title":"Ti-SSE: a time interval based sequential recommendation model via stochastic shared embeddings"},{"paperId":"2edb3554b2e8bd5ec09bdff391a938b4cf930997","title":"Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe Based Motion Interpolation"},{"paperId":"9f9f5f10e804cbec812a8f8639769e9a03dee44c","title":"TransCODE: Co-Design of Transformers and Accelerators for Efficient Training and Inference"},{"paperId":"bb7d682e5f9c4a8985d93673a07a267f4080739a","title":"CoRe-Sleep: A Multimodal Fusion Framework for Time Series Robust to Imperfect Modalities"},{"paperId":"d11ee2658083e6dceccf6dfcf699aa5270f34dcb","title":"S-ViT: Sparse Vision Transformer for Accurate Face Recognition"},{"paperId":"2d56b6e462d90df56849bd450b79bd9ef1066c5d","title":"SF-YOLOv5: Improved YOLOv5 with swin transformer and fusion-concat method for multi-UAV detection"},{"paperId":"b695d8e4edf66d7aa7a049cfb48038e35b0845b1","title":"EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms"},{"paperId":"74e0fd858b07b929ddcd52d0ae9b9d0f87c82a55","title":"Promptable Game Models: Text-Guided Game Simulation via Masked Diffusion Models"},{"paperId":"3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","title":"EVA-02: A Visual Representation for Neon Genesis"},{"paperId":"12d16f426edc6ab248fb476007bd1646282d4d68","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"e6a39ffc46bf7ba050da9d803ce2da72d67f196c","title":"Digital future beyond pandemic outbreak: systematic review of the impact of COVID-19 outbreak on digital psychology"},{"paperId":"4b572031d82294d4392723b746518f8f53d9fe3f","title":"ViTO: Vision Transformer-Operator"},{"paperId":"513dd6422b5e3f839e34e3a77b58a92b48e433ad","title":"AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+"},{"paperId":"abe4ea0e1f1d04288406818048bc3f973d930ec9","title":"A comprehensive study of automatic video summarization techniques"},{"paperId":"4ce36fb3e0abfe5260d8cb70b5a83fdfbfdfa428","title":"CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale Attention"},{"paperId":"385c363ea8e450f362d389f401beaeb5b42a0022","title":"Stabilizing Transformer Training by Preventing Attention Entropy Collapse"},{"paperId":"edef246794a1eb2df69e9d6d1a3c72ca0516f124","title":"Xformer: Hybrid X-Shaped Transformer for Image Denoising"},{"paperId":"bb3faec0406af580a5c1dfe05bba59295a8272f7","title":"Unifying Layout Generation with a Decoupled Diffusion Model"},{"paperId":"c40ec51ddd4145402bd95eeb3ce6977778d87881","title":"Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling"},{"paperId":"4a6bf17538ded4eae48629728ee17adebb688094","title":"EG-TransUNet: a transformer-based U-Net with enhanced and guided models for biomedical image segmentation"},{"paperId":"86ddf98d18ef3839e97f7d7ae6d017610d4054f7","title":"Less Is More: Brain Functional Connectivity Empowered Generalizable Intention Classification With Task-Relevant Channel Selection"},{"paperId":"3c8b0cfd1bdf8aac541f033181aac8b005417d35","title":"Cross-Temporal Snapshot Alignment for Dynamic Networks"},{"paperId":"a0d0002be16a1b70dcbb4b152813da55a81c2201","title":"Diffusing Graph Attention"},{"paperId":"e48e7b8c8faa8705c5d1155c12d403f1c13653b6","title":"ADE-CycleGAN: A Detail Enhanced Image Dehazing CycleGAN Network"},{"paperId":"b596fb7e6fcb32344f47ebe945a6c493454dbf7b","title":"Slice-Based Code Change Representation Learning"},{"paperId":"cc5e79419a0bfaa067e897d35c23a69774d7c2b5","title":"A Novel Cross Grouping CG MLP based on local mechanism"},{"paperId":"03e2b78724b499e5275ff5580fc77453a81e64e2","title":"Non-symmetrical Sibling-Stream Network with Adaptive Positional Encoding for Automatic Medical Report Generation"},{"paperId":"1c202cb729ea41d7339ba282f8542b1238836ef8","title":"Applying Plain Transformers to Real-World Point Clouds"},{"paperId":"43d8c95ae1c975657771bc6dd58c747c5cf6c874","title":"A Novel Transformer-Based IMU Self-Calibration Approach through On-Board RGB Camera for UAV Flight Stabilization"},{"paperId":"99c679a80a1381151b3273f525c50bdb841b66aa","title":"Korean Sign Language Recognition Using Transformer-Based Deep Neural Network"},{"paperId":"7eaf4a0fe5a4199c109dc8ae9add981dbeba1e69","title":"Research on Chinese traditional opera costume recognition based on improved YOLOv5"},{"paperId":"5c9ccbeaeadffa6ae1a966d52e3373c6b525f971","title":"SATBA: An Invisible Backdoor Attack Based On Spatial Attention"},{"paperId":"b45ea6ddc9d964116addaf1dafd0641d78a6228e","title":"Sequential Query Encoding For Complex Query Answering on Knowledge Graphs"},{"paperId":"44ca4241feef086a19e90902ba1ad27ed2f82edd","title":"Named entity recognition for Chinese based on global pointer and adversarial training"},{"paperId":"b94f2e22dd30bb9e016e88e053c91e2794213b82","title":"Dual channel Chinese sentiment analysis of characters and words based on deep learning"},{"paperId":"77786eda28e5f0b25d682c27846334d53daf43e4","title":"Complex-Valued Relative Positional Encodings for Transformer"},{"paperId":"a6f9da52f9ef80249aaf9e3a54e0afb6d35c7ac0","title":"Embeddings for Tabular Data: A Survey"},{"paperId":"1592dbe6a111ce94fcb562fc7d910978a8a5ca1e","title":"Reference aware attention based medical image diagnosis"},{"paperId":"ade3705d102ffb574bd355e58f7f8066f2aa73fc","title":"The Virtual Sleep Lab—A Novel Method for Accurate Four-Class Sleep Staging Using Heart-Rate Variability from Low-Cost Wearables"},{"paperId":"b8c236dc5963dac36b0d8e419beb5876e3a18f96","title":"Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation"},{"paperId":"4d1be5f81204f968a34a08975fece5f626f79618","title":"Learning Language Representations with Logical Inductive Bias"},{"paperId":"e5469d770062491dd9e2b27d77c4295aa296809a","title":"GH-DDM: the generalized hybrid denoising diffusion model for medical image generation"},{"paperId":"b621b2ecbeef82cee82570297700a8dcd4cfe94e","title":"Semiconductor Fab Scheduling with Self-Supervised and Reinforcement Learning"},{"paperId":"9e449d611bec60ff2e3faaec38dce3b79abac44e","title":"Enhancing Multivariate Time Series Classifiers through Self-Attention and Relative Positioning Infusion"},{"paperId":null,"title":"Multilayer self‐attention residual network for code search"},{"paperId":"f2909fcd0a1c265097490ce43f5065ef6486310d","title":"Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters"},{"paperId":"6d51f4b220cb2c8321dc5f9755b7d66f10f1cad6","title":"RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL"},{"paperId":"03541e93629055228357814c4c59f87b6090fc23","title":"Single Motion Diffusion"},{"paperId":"f0e4a66a73cc2f8a8a1a4d9fb041bf9be19f97f6","title":"Predicting Synthetic Lethality in Human Cancers via Knowledge Graph Summarization"},{"paperId":"c159eff49a643f305d6bfbb39951d873f911b794","title":"Target detection algorithm based on multilayer attention mechanism-adaptive feature fusion network"},{"paperId":"5f811bd7f115eea0454ed7cb6813705ff5d1de96","title":"Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames"},{"paperId":"7aaf4014eb09ce6dfb6b0adf6593f84ed37e08b4","title":"Augmented FCN: rethinking context modeling for semantic segmentation"},{"paperId":"d0b1e6e636236855417a7ca4d2b3cdb72feb9a5c","title":"PhysFormer++: Facial Video-Based Physiological Measurement with SlowFast Temporal Difference Transformer"},{"paperId":"1e8fcf495dbc386591fcbab75df75ac41a503859","title":"Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation"},{"paperId":"d30c21212e0e1165b2e3a05f9bc31ce8b49e5f0a","title":"Knowledge Distillation in Vision Transformers: A Critical Review"},{"paperId":"17b5d59d4b9fdd1d879f9ac3cca224ae2e0eb464","title":"Iterative convolutional enhancing self-attention Hawkes process with time relative position encoding"},{"paperId":"4ce987d4f8ae0f4680808c318980d42a82b9aa89","title":"Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers"},{"paperId":"e7a6c58afcb0ce07baa21c75a8a4fae0e3b75945","title":"Learning Neighbor User Intention on User-Item Interaction Graphs for Better Sequential Recommendation"},{"paperId":"90ebe1cc509cb80f7e6324f81226562f222f912c","title":"More than Syntaxes: Investigating Semantics to Zero-shot Cross-lingual Relation Extraction and Event Argument Role Labelling"},{"paperId":"7c3fd26cf7eb1a9e6b286c567bd0055b47638d07","title":"FPT: Fine-Grained Detection of Driver Distraction Based on the Feature Pyramid Vision Transformer"},{"paperId":"6fb8f8977101ddbb7fd747feb1ae3449f6b9f6e8","title":"HCTNet: A hybrid CNN-transformer network for breast ultrasound image segmentation"},{"paperId":"c341106a1b5490284cc36c44a1a16b815e76d91a","title":"WLiT: Windows and Linear Transformer for Video Action Recognition"},{"paperId":"5dfe7236c85ce5569be48a4f567b85d0ebd7e5be","title":"Spatially Aware Transformer Networks for Contextual Prediction of Diabetic Nephropathy Progression from Whole Slide Images"},{"paperId":"75844016491add33e7452f8ded95bd1b62a04f21","title":"Relation-mining self-attention network for skeleton-based human action recognition"},{"paperId":"e18748951afdd39d451acf8fbd3133c4c6619322","title":"COVAD: Content-Oriented Video Anomaly Detection using a Self-Attention based Deep Learning Model"},{"paperId":"c4fcaf7fc7d9ff791f8e5a5f1b724940835a3caf","title":"Predicting Physiological Response in Heart Failure Management: A Graph Representation Learning Approach using Electronic Health Records"},{"paperId":"9529f886486713c27df1d32c58d615e3ad332d82","title":"A Survey on Low-Latency DNN-Based Speech Enhancement"},{"paperId":"a7f3d8fa8c05b4533e652e965593ffad99803bd8","title":"Variation-Aware Semantic Image Synthesis"},{"paperId":"ee224cb6ecd14515490504717ed15ff098229f48","title":"DeepTP: A Deep Learning Model for Thermophilic Protein Prediction"},{"paperId":"7195ed3c7f11220f29634cecb68b1d39db2e36d9","title":"Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness"},{"paperId":"0e84679cf0945a2868245ba2be68c90453e48f2e","title":"Screen Correspondence: Mapping Interchangeable Elements between UIs"},{"paperId":"2d33f48e9839e4d25fc2d75c5117e175d7e9a347","title":"Multi-task Label-wise Transformer for Chinese Named Entity Recognition"},{"paperId":"2129788d72f30343c3f082d8814bbb44c299b346","title":"LSTM-SN: complex text classifying with LSTM fusion social network"},{"paperId":"69358aff5a7b35384ed82e1857b50f20978bc49a","title":"DPC-MSGATNet: dual-path chain multi-scale gated axial-transformer network for four-chamber view segmentation in fetal echocardiography"},{"paperId":"3985999efd7ff8505e412b6e73ad8a7dd2db1132","title":"PromptShots at the FinNLP-2022 ERAI Task: Pairwise Comparison and Unsupervised Ranking"},{"paperId":"97899e818cc83efba48a5e9892dfa0a6a6e1d274","title":"Enhancing the robustness of vision transformer defense against adversarial attacks based on squeeze-and-excitation module"},{"paperId":"5435ed7c26f0c250493f244acffb69dd929d116b","title":"Structured Case-based Reasoning for Inference-time Adaptation of Text-to-SQL parsers"},{"paperId":"86f3b14e3229be8e06060f08b6e65fd031efbc50","title":"Untied Positional Encodings for Efficient Transformer-Based Speech Recognition"},{"paperId":"10422b7bcf17eae9b04d4a020a591b3a61b45593","title":"Towards Autoformalization of Mathematics and Code Correctness: Experiments with Elementary Proofs"},{"paperId":"8623d66cafbea0b45c0935897fa090fa277ee210","title":"Teaching Old DB Neu(ral) Tricks: Learning Embeddings on Multi-tabular Databases"},{"paperId":"936d3f497c81216cfa33b9cb2deab8fee95a8b72","title":"A Global-Information-Constrained Deep Learning Network for Digital Elevation Model Super-Resolution"},{"paperId":"b6e80e715784aa7cdddf519ef91b130a17aa958f","title":"SADLN: Self-attention based deep learning network of integrating multi-omics data for cancer subtype recognition"},{"paperId":"87430e72a28d048221850e23297f9ed225c2c21c","title":"Modeling the Rhythm from Lyrics for Melody Generation of Pop Song"},{"paperId":"0261af3e2c668849a771b6f667c74a4339da5052","title":"MFCFlow: A Motion Feature Compensated Multi-Frame Recurrent Network for Optical Flow Estimation"},{"paperId":"9c082d9c217e0e1014da3b24ffe7569924724067","title":"Enhanced multihead self-attention block network for remote sensing image scene classification"},{"paperId":"4f48df33374f5b5b7bd73776a668ada02fc6708a","title":"SSR-Net: A Spatial Structural Relation Network for Vehicle Re-identification"},{"paperId":"6b9eadaa1cfc5bebc727be1228b6c9cb9195a75b","title":"Cross‐domain sentiment classification using decoding‐enhanced bidirectional encoder representations from transformers with disentangled attention"},{"paperId":"060cee8411181e8151ab1e3212b81528accd9b8b","title":"On Transforming Reinforcement Learning by Transformer: The Development Trajectory"},{"paperId":"9075eb79c8bcc130a9ba069b1dadb0c500bca36a","title":"Swin MAE: Masked Autoencoders for Small Datasets"},{"paperId":"8cac1a726a7fa1160ce86733e66ebba6c78b71eb","title":"Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal"},{"paperId":"e3d371bdc9ac9cecbfbbf03d20485efa17c81e36","title":"Optimizing Deep Transformers for Chinese-Thai Low-Resource Translation"},{"paperId":"55153dceb76a147f1080af72c7e502835208503e","title":"An Efficient and Intelligent Detection Method for Fabric Defects based on Improved YOLOv5"},{"paperId":"7ead0cae4e67f390b2eb0083117ea8ab90c53b47","title":"Landslide Susceptibility Mapping by Fusing Convolutional Neural Networks and Vision Transformer"},{"paperId":"5bdca463873f4f2dfc6897b6c5fd51e799bd76c7","title":"Generating music with sentiment using Transformer-GANs"},{"paperId":"9575afb5702bc33d7df14c48feeee5901ea00369","title":"A Length-Extrapolatable Transformer"},{"paperId":"cb14ddc525be523cb55374e37a5802e80671448c","title":"Scene designer: compositional sketch-based image retrieval with contrastive learning and an auxiliary synthesis task"},{"paperId":"e1d71a6958460b9a9e75d6c59dc7b10c8dbe198b","title":"Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments"},{"paperId":"8129197043363ddcc40f3ba1768864bc8f9776c9","title":"Multi-level Attention-based Domain Disentanglement for BCDR"},{"paperId":"f4847d928cdb071f0abc5683ca63d6bc38bcecb0","title":"Auto-Regressive Self-Attention Models for Diagnosis Prediction on Electronic Health Records"},{"paperId":"fe5f1760fceee44846998e0714f50df8b5e9f154","title":"LLU-Swin: Low-Light Image Enhancement with U-shaped Swin Transformer"},{"paperId":"df6e42e06d77ebc28b50492b1fd26b39c1bd4277","title":"Convolution-Enhanced Evolving Attention Networks"},{"paperId":"f7884dca33438307fd384d4a246665cff188faec","title":"Spatio-Temporal Graph-TCN Neural Network for Traffic Flow Prediction"},{"paperId":"9c2f756a1401e657aaedecc7832364af09e1e3ba","title":"Solar Filament Segmentation Based on AA-UNet"},{"paperId":"4a899f4f65ea022f949d9506b659d095f64c17fc","title":"Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model"},{"paperId":"661e8d555c4424b5953f17434f2ba910bfcf3afe","title":"Efficient Long Sequence Modeling via State Space Augmented Transformer"},{"paperId":"ccd30015e71d413ab34f3d306f87da892fb3c64c","title":"An End-to-End Heart Rate Estimation Scheme Using Divided Space-Time Attention"},{"paperId":"410a958c4b51650e055c9a2c5bd9e9a7fa2162d4","title":"Bridging Graph Position Encodings for Transformers with Weighted Graph-Walking Automata"},{"paperId":"1a0975aeefaa2fc98059e3c63065c4294028c565","title":"CNN-transformer mixed model for object detection"},{"paperId":"6498252603503c4ceab80ec40c4e86a0943a94c1","title":"Dual-stream Self-attention Network for Image Captioning"},{"paperId":"f6d381941e2964872a283d5290a390126115ad03","title":"P-Transformer: Towards Better Document-to-Document Neural Machine Translation"},{"paperId":"4a43a773f0725c998c65f2d51ddd49d5919183f7","title":"Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning"},{"paperId":"e4555f6325b0401f6ec430414c03ad124e50c002","title":"Position Embedding Needs an Independent Layer Normalization"},{"paperId":"97b9c17847c36a34ae059eb19902dc9f801a4249","title":"Expanding Intra-class Difference and Boosting Frame-level Classification for Continuous Sign Language Recognition"},{"paperId":"bb1486013aedbbb4f9a960c83731fbb91a04b1f3","title":"Mitigation of Spatial Nonstationarity with Vision Transformers"},{"paperId":"1f0739e299d35a2f8089dd5570fd2fca9c066fbf","title":"Multi-source Interaction Network for TextVQA"},{"paperId":"ec0299281353f93875b481984c2f54347c3c6d2d","title":"Rec-AUNet: Attentive UNet for Reconstruction of ECG from BCG"},{"paperId":"690558178fa6829f1e08552b9636f9feebac93c1","title":"Boosting Neural Networks to Decompile Optimized Binaries"},{"paperId":"1c7bdad6bdcee8da7305646b60ab8776eeb43f85","title":"Human Health Activity Intelligence Based on mmWave Sensing and Attention Learning"},{"paperId":"934f03956a5092345ea941df915d79358ddd2581","title":"CLeBPI: Contrastive Learning for Bug Priority Inference"},{"paperId":"aec9ae3c0e5a5784e171dbb7aa58c209de89e404","title":"A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling"},{"paperId":"376345572946f8710e265e10be270373587d4c4e","title":"Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning"},{"paperId":"457f4e838790a04173b0cd4ede8852cbf0d8cefe","title":"An Optimized Method for Large-Scale Pre-Training in Symbolic Music"},{"paperId":"15a591ec89cf2548f405af6aee887074096ae435","title":"Video Prediction by Efficient Transformers"},{"paperId":"341beb3bc23d2aebb5a6c4eb2a44e4ff90936f0a","title":"ResFormer: Scaling ViTs with Multi-Resolution Training"},{"paperId":"704332d287bc78ac95fea3d90ec3945e9cec9ab3","title":"A survey on complex factual question answering"},{"paperId":"81d86d4270e20eb5e3e93637acc5b96c5e7d0bfd","title":"Improving the Quality of Machine Translation Using the Reverse Model"},{"paperId":"0833632548ef61df1aea98cd8f770f1209c8a964","title":"Visualization-Based Software Defect Prediction via Convolutional Neural Network with Global Self-Attention"},{"paperId":"c08fc55896a053d3c0426391739b0dc3d059b0ce","title":"SWAM: Driver Distraction Recognition Based on Attention Mechanism"},{"paperId":"f5ecee770fad9e6ec531df3795bef9f1886b8ac1","title":"LSANet: Lesion-Specific Attention Network for Monkeypox Categorization"},{"paperId":"b064def719fc5d8b5cadf8a3292f690c23806b21","title":"Motion In-Betweening via Two-Stage Transformers"},{"paperId":"7a20e80fdd7eb0acc30ee3a585c6e4580dbbff20","title":"Measurement of Investment activity in China based on Natural language processing technology"},{"paperId":"82774f03c39bf1232aa213e472e607ed49fb7abb","title":"Deepfake Detection with Spatio-Temporal Consistency and Attention"},{"paperId":"4125a258059f05dbbd393dd8e6d740a9272493b7","title":"Differentiable Short-Term Models for Efficient Online Learning and Prediction in Monophonic Music"},{"paperId":"2e935268b66e12dd310af5c715013147d454e560","title":"Mutual Exclusivity Training and Primitive Augmentation to Induce Compositionality"},{"paperId":"6787343183353807a2f896d8c29d082d611dba51","title":"Seformer: a long sequence time-series forecasting model based on binary position encoding and information transfer regularization"},{"paperId":"f1f78bbb3e146874501e3c52b56cff6abf731842","title":"Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges"},{"paperId":"5dfbca6d0d3d25e338482592213fa6e6e83737bb","title":"Special Vehicle Target Detection Based on Improved Yolov5 + Jetson NX"},{"paperId":"a49ec0f1e5f8673f2357ccfb3c0d1a4a64ff0e36","title":"Generating High Quality Titles in StackOverflow via Data Denoising Method"},{"paperId":"740c588d3983e1e46839c7d25560de7563efe4b7","title":"An Improved Method and Application of Recurrent Convolutional Neural Network With Self-attention Mechanism"},{"paperId":"d94e7aeab88d07f0b50b7866ceb5baa3c29be859","title":"NQE: N-ary Query Embedding for Complex Query Answering over Hyper-relational Knowledge Graphs"},{"paperId":"cee8a6e75d891145e6028f7562d84bb3727a69a0","title":"On the Typicality of Musical Sequences"},{"paperId":"38ec6fc80ccbf1dd8a7e66111ff37473f9edbad8","title":"A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding"},{"paperId":"8d6520112cf35d84cf680de38411ba84dfc4a4da","title":"Vision Transformer with Super Token Sampling"},{"paperId":"1c73635ec5636d0ac5b73cad545a54f131eefa00","title":"Application of Machine Learning Combined with Wireless Network in Design of Online Translation System"},{"paperId":"474043deeca83742be2c6ac7d36d279b90080c36","title":"Unifying Tracking and Image-Video Object Detection"},{"paperId":"27cbfd0197f77351c24c77bbff7b0ff45a2382fa","title":"Improved Cross-view Completion Pre-training for Stereo Matching"},{"paperId":"9bb4ac256a8edd931a202af2be9344771d8ce905","title":"Transformer Network with Self-Supervised Learning for Stenosis Detection in CT Angiography"},{"paperId":"0cf7d980b62555bbed101821742de9558bd7b0e0","title":"Parameter-Efficient Transformer with Hybrid Axial-Attention for Medical Image Segmentation"},{"paperId":"313d893228532485752e6744c831fbe4e2c712da","title":"Hypergraph Transformer for Skeleton-based Action Recognition"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"6782d501d8a1220436d7e1551b2c2c9bc73f126a","title":"ParCNetV2: Oversized Kernel with Enhanced Attention"},{"paperId":"8b22adf610de65da2e8832d2f29a9b8510004cb1","title":"Continuous Soft Pseudo-Labeling in ASR"},{"paperId":"43bab0d20e282e6d878c6279f2c1528981ed2a40","title":"Transformer Variational Wave Functions for Frustrated Quantum Spin Systems."},{"paperId":"4e06fa91a71690fddae3d4618e8ee25993d09dd1","title":"Cross-Attention is all you need: Real-Time Streaming Transformers for Personalised Speech Enhancement"},{"paperId":"e5bd79d935e63e6a70b6c5dd5f68ae7b170fbf6e","title":"VulRepair: a T5-based automated software vulnerability repair"},{"paperId":"7763db3e5cda340e63a818923e0e2849a6e80590","title":"Recurrent graph encoder for syntax-aware neural machine translation"},{"paperId":"2a5890d18f83e96634c5dbc927a0039394c8f394","title":"Improving Speech Prosody of Audiobook Text-To-Speech Synthesis with Acoustic and Textual Contexts"},{"paperId":"3e851661e1521c5353281cb6a86cf356aa5753a3","title":"Hybrid Pipeline for Building Arabic Tunisian Dialect-standard Arabic Neural Machine Translation Model from Scratch"},{"paperId":"c02aa9f62cc42630fa8ca60452664f5ee0fb9b00","title":"More Speaking or More Speakers?"},{"paperId":"344f93efa2d507d0920a443ffa33c8b26a61e4b8","title":"Classification of retinopathy of prematurity based on mixed attention"},{"paperId":"171e87462886d79656e6bab15714d766fbb185d4","title":"Deep-Learning Enabled Assessment of Neurocognitive Performance in Object Following in Mixed Reality"},{"paperId":"d589946b8d4969f008cb9acb2edcd107a34d78ee","title":"A Feedback Learning Segmentation Algorithm for Medical Images of Breast Cancer Incorporating Attention Mechanism"},{"paperId":"e9ea44f3d6ff674117e8c0337d95cfd64599ce5d","title":"Structured State Space Decoder for Speech Recognition and Synthesis"},{"paperId":"a8d8e5093c78a6664dcf7aa488febec1b118922a","title":"TransSizer: A Novel Transformer-Based Fast Gate Sizer"},{"paperId":"697452b24c3b884cff8f7dbbc9c8e62bc59e0fdf","title":"Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers"},{"paperId":"b6115e422e59c464db2d8257724c66827202b6c8","title":"Grafting Vision Transformers"},{"paperId":"c15d153a846103fca7c6a2a633d6605a9a701203","title":"Building Indonesian Dependency Parser Using Cross-lingual Transfer Learning"},{"paperId":"c12a38e4413afb3451b67a7a9d1760ce9248be25","title":"Meeting Record Generation Based on Hierarchical Decomposition Encoding"},{"paperId":"b6ed84e0e69fc87de61645c1a50bcc169c95b983","title":"Does Joint Training Really Help Cascaded Speech Translation?"},{"paperId":"b67a259454e7a75c53bff807440ae6c65d8418d3","title":"Transformers over Directed Acyclic Graphs"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"a52a3aac68f4fd67b67ccf6839765403a101f681","title":"Adversarial Pretraining of Self-Supervised Deep Networks: Past, Present and Future"},{"paperId":"c524690e7b0a2ff6b2b7210d5c85e1fd0cf86781","title":"Revisiting Checkpoint Averaging for Neural Machine Translation"},{"paperId":"c10195a628f31315ec56c219e219b25d81bb2b7f","title":"Amos: An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale"},{"paperId":"bccbd500f2f8af6bdfb5ef3cf77c2b99d088ee4c","title":"LittleBird: Efficient Faster & Longer Transformer for Question Answering"},{"paperId":"cb36c3c85075ae1bcd48bfaea57b2212a46e383d","title":"Is Encoder-Decoder Redundant for Neural Machine Translation?"},{"paperId":"a6adef01397f9444272be46b2cfe86516ded4f8e","title":"Detecting Depression from Social Media Data as a Multiple-Instance Learning Task"},{"paperId":"1f8160b7e91b570e60e9cba2631cf0472cbc764c","title":"Continuous Pseudo-Labeling from the Start"},{"paperId":"6b0ea6a6869b5f4207a6a8672ba153b747975126","title":"TrajFormer: Efficient Trajectory Classification with Transformers"},{"paperId":"c85ed937562106a6c9016dbb42e2b48cc298cadc","title":"Efficient Learning with Pseudo Labels for Query Cost Estimation"},{"paperId":"0cfd0ce0c0ab737db8f717250bf0e62939dad444","title":"Disentangling Past-Future Modeling in Sequential Recommendation via Dual Networks"},{"paperId":"9fca53cdd498bd4a6806b4c76da525923f1787da","title":"Le-BEiT: A Local-Enhanced Self-Supervised Transformer for Semantic Segmentation of High Resolution Remote Sensing Images"},{"paperId":"3db61a38efc4cb4c0172065e07348b42a3fbd160","title":"Grid Synchronization using Machine Learning"},{"paperId":"8047495ec6e5cbf90e01364cc3a5a04d7895614c","title":"Local Embedding for Axial Attention"},{"paperId":"d5519f66e15297f9b13bfc49449ade8229f13193","title":"SAVE: Spatial-Attention Visual Exploration"},{"paperId":"0cd9fb453d71212f08937da7fe16b924f22bc536","title":"Learning to Jointly Transcribe and Subtitle for End-To-End Spontaneous Speech Recognition"},{"paperId":"44fc5bc7cd0957e00159d588e433acfc3cbe7dbe","title":"Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries"},{"paperId":"687733531aa65ea28b8a389399d3cbce9c99cee8","title":"LSG Attention: Extrapolation of pretrained Transformers to long sequences"},{"paperId":"1f86bf1e334200ec0481349255559fbfe7a33caa","title":"MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"18c129d202b5153ae5373a24d0deee2ded1ae7e0","title":"Percept U-Net: Percept Attention-based Convolutional Neural Network for Atrial Fibrillation Episode Localization"},{"paperId":"2919cfcfc4e48fe0a17ac18ef41949bdb1d50891","title":"Relational Attention: Generalizing Transformers for Graph-Structured Tasks"},{"paperId":"aca4df69152787da747b09a694ae226edf356676","title":"Designing Robust Transformers using Robust Kernel Density Estimation"},{"paperId":"7aa43fc01045d36c2babb3edd81b0147dfa8cb45","title":"Towards All Weather and Unobstructed Multi-Spectral Image Stitching: Algorithm and Benchmark"},{"paperId":"a0b4e5f73adb829f2b7c51eb373f0559c110e2ac","title":"Better Pre-Training by Reducing Representation Confusion"},{"paperId":"2c379f8106fabf2c3f0f1f2a4d4451cb7d664e6d","title":"Spatio-Temporal Attention-based Graph Convolution Networks for Traffic Prediction"},{"paperId":"e9ce0ca5f485e8ff4632693b76cdab3b64c34a84","title":"Latent Neural ODEs with Sparse Bayesian Multiple Shooting"},{"paperId":"628fb0736c5484f7a0006d0bb2d4a7a2ef9ae6b3","title":"SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training"},{"paperId":"03b7d2e942eafabd14b229f8962fa6e943053f75","title":"Melody Infilling with User-Provided Structural Context"},{"paperId":"43a37611194f9019dde2d1ef9c86b61e7dc555bb","title":"A Reverse Positional Encoding Multi-Head Attention-Based Neural Machine Translation Model for Arabic Dialects"},{"paperId":"61b9b5187e8c5ca0632d16d7c916a42784eca348","title":"Point Cloud Recognition with Position-to-Structure Attention Transformers"},{"paperId":"a8a2a8229f99c291bf71ec92b801a073854c52e2","title":"MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models"},{"paperId":"4e1489d54bf802e588e08975a086108e3b6bf169","title":"Enhancing 3D-2D Representations for Convolution Occupancy Networks"},{"paperId":"fb5ed9d348ca6be8544f4bc10d19da2a7046fb35","title":"Extended context-based semantic communication system for text transmission"},{"paperId":"343e4883a3aa8ab7a1b5950f8f8fb8091a2a76be","title":"The encoding method of position embeddings in vision transformer"},{"paperId":"986d5cc495fcb02a598c6f2aa8750b0a610fc012","title":"nn-TransUNet: An Automatic Deep Learning Pipeline for Heart MRI Segmentation"},{"paperId":"504a2bca67b1b7cbd837f19c6f419ce12c37ffda","title":"Masked face recognition with convolutional visual self-attention network"},{"paperId":"6edada77cb8d008f9d026926a90ee71fe5063f78","title":"FGCNet: Fast Graph Convolution for Matching Features"},{"paperId":"f6e2ad0fe7fe6153a70a3e1cb1a612c27727bbc2","title":"Attention Constraint Mechanism through Auxiliary Attention"},{"paperId":"fd82a861a0bdb8693a5e3596516f1c0848a3d80a","title":"E-Branchformer: Branchformer with Enhanced Merging for Speech Recognition"},{"paperId":"15586fec836ffcb60eba81491e04c225e6914aac","title":"Protein structure generation via folding diffusion"},{"paperId":"562a3b8d6e9c5bd968907e828367397ad2dad28e","title":"SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data"},{"paperId":"9ae9e58cdd5975be4af3fac9a9e948adaf4bc4da","title":"Verifiable and Energy Efficient Medical Image Analysis with Quantised Self-attentive Deep Neural Networks"},{"paperId":"61d8390fffca540517e325dbfc1df27a1ecfbfe1","title":"Thermal Infrared Tracking Method Based on Efficient Global Information Perception"},{"paperId":"f882c90f6ec93f35ecdd2aed9d08673832d03cbb","title":"Improving Code Completion by Sequence Features and Structural Features"},{"paperId":"850235ad91b805c1b1f6269b1c430d6e7681cbf2","title":"Unraveling Key Elements Underlying Molecular Property Prediction: A Systematic Study"},{"paperId":"aee1142e10e9c702f6f96322500f9e8095000388","title":"S^2-Transformer for Mask-Aware Hyperspectral Image Reconstruction"},{"paperId":"607ab597edf96c0ed2b8b7f2b4fc25efcecef2f4","title":"RERG: Reinforced evidence reasoning with graph neural network for table-based fact verification"},{"paperId":"defeec6de4645db49820c6057682581ef16844be","title":"Genetic Algorithm-based Transformer Architecture Design for Neural Machine Translation"},{"paperId":"d513ace9bdd8ace3241491fcd563357ae0af49b7","title":"Automatic Classification of Software Bug Reports Based on LDA and Word2Vec"},{"paperId":"e901a9748feccb2eef7d9ff8959b060b94cb7d3a","title":"Beyond the Transformer: A Novel Polynomial Inherent Attention (PIA) Model and Its Great Impact on Neural Machine Translation"},{"paperId":"f49d9a6c4db99aaf12d4237044ca0770aac075ca","title":"Code samples summarization for knowledge exchange in developer community"},{"paperId":"28e7e17abff47b0236fe58de206038b7fb38cdbf","title":"ESPnet-ONNX: Bridging a Gap Between Research and Production"},{"paperId":"fb0f163627621f1879bacf917d93d749f07953d1","title":"Exploring vision transformer: classifying electron-microscopy pollen images with transformer"},{"paperId":"daf74b8f24ef9a63def244a977d90dd9583933eb","title":"Dynamic Graph Message Passing Networks"},{"paperId":"c3e1bc1b9fb2d92c3b76b22ff666af42a75491f1","title":"WA-Transformer: Window Attention-based Transformer with Two-stage Strategy for Multi-task Audio Source Separation"},{"paperId":"1f5381823d48be4fa280d5cfbfd66512e258f580","title":"BiCAPT: Bidirectional Computer-Assisted Pronunciation Training with Normalizing Flows"},{"paperId":"d4d8db8bf134107e93b2d089a9d022e05066ce0c","title":"Self-Attention Based Time-Rating-Aware Context Recommender System"},{"paperId":"79b7f6965242af82405ad47647450f6684c0d121","title":"Parameter-Efficient Conformers via Sharing Sparsely-Gated Experts for End-to-End Speech Recognition"},{"paperId":"54e511310d40ae2ebb4448f92f27865df1b7bae1","title":"LogGD: Detecting Anomalies from System Logs with Graph Neural Networks"},{"paperId":"fd17b2b6fdd9a6d67c8534bf0a2507c99242f62d","title":"Graph-to-Text Generation with Dynamic Structure Pruning"},{"paperId":"7d51e0d810d7b4567c0f377860428b06ab3a6a14","title":"CAT-CPI: Combining CNN and transformer to learn compound image features for predicting compound-protein interactions"},{"paperId":"c2737d1165399f3826d14ab10e40d23aaee73edc","title":"Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention"},{"paperId":"9acc94aed6b6a6bf4ec5f4579999f175d40e3089","title":"Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation"},{"paperId":"5d3e28f9cc108202cf97a2aa80971dbe3d4f5b4a","title":"SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers"},{"paperId":"e420180e2b6da334ef2accf90b08087862cbff85","title":"TRQ3DNet: A 3D Quasi-Recurrent and Transformer Based Network for Hyperspectral Image Denoising"},{"paperId":"387f9bd2412c2b1154aa6c9a2156e1162cc5f2c9","title":"Semantic Representation Using Sub-Symbolic Knowledge in Commonsense Reasoning"},{"paperId":"34b43419f85f44751b28310d133d351c24dbead2","title":"Local-Aware Global Attention Network for Person Re-Identification"},{"paperId":"e7c835eeb56f28e043dbe5af866cd1e1f9b31b12","title":"An End-to-End Mutually Interactive Emotion–Cause Pair Extractor via Soft Sharing"},{"paperId":"3ea15bb1e2956c54b141b07e0cda933d3c94ded2","title":"DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation"},{"paperId":"5ce9641a5caf7fa5377edb68b449185a281eddce","title":"Structural Bias for Aspect Sentiment Triplet Extraction"},{"paperId":"35466623d3f0a8426e13d349d8260bfc83948d75","title":"A generalized-template-based graph neural network for accurate organic reactivity prediction"},{"paperId":"c2efc25fa1a2b0192b95dc92d9dccf90e602c74c","title":"Efficient Methods for Natural Language Processing: A Survey"},{"paperId":"e05aca5f3d24f250596fb7a7d8cc008f0478dfaf","title":"ELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer"},{"paperId":"cdc027a961da229e870be3ac41a429f4c165788d","title":"Modeling Spatial Trajectories Using Coarse-Grained Smartphone Logs"},{"paperId":"c8483055b1dbef7e23bf31f544fde4a2b1de417e","title":"Time-aware Self-Attention Meets Logic Reasoning in Recommender Systems"},{"paperId":"1aac692ca061feb846cf32cd61a1d422f89593a1","title":"A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions"},{"paperId":"6e7b5fca044ac77e1572591988f4be4b0f621358","title":"Progressive Transformer Machine for Natural Character Reenactment"},{"paperId":"623fe930267bdcbc50637298ea6a72969edd05a9","title":"K-Order Graph-oriented Transformer with GraAttention for 3D Pose and Shape Estimation"},{"paperId":"1ec5f71509c1fe981f751eb4b48b09ee62d64ca2","title":"Flat Multi-modal Interaction Transformer for Named Entity Recognition"},{"paperId":"39b1b104ff08aaea959a9a58dc8b4d158b4e9ab4","title":"Efficient Attention-free Video Shift Transformers"},{"paperId":"1f621d02b124f0e2d8aa2d71c831deb5ea0ee834","title":"PointTransformer: Encoding Human Local Features for Small Target Detection"},{"paperId":"a555364fdd7c8ae0d20d1d506501dcc253e0a16f","title":"ATLAS-MVSNet: Attention Layers for Feature Extraction and Cost Volume Regularization in Multi-View Stereo"},{"paperId":"025be76b0fbc593f74fff00b61b470bf2df40c23","title":"Deep Guided Context-aware Network for Anomaly Detection in Musculoskeletal Radiographs"},{"paperId":"2482e19a2e77d106deb7550144ff9d2cc910d5e2","title":"Accelerating Vision Transformer Training via a Patch Sampling Schedule"},{"paperId":"5c092c7bb0f9deecebb365dc79e52cb1e24614cc","title":"Position-aware Structure Learning for Graph Topology-imbalance by Relieving Under-reaching and Over-squashing"},{"paperId":"c6ea3671b2caef1562c60a8479a49ea60f91006a","title":"Bar transformer: a hierarchical model for learning long-term structure and generating impressive pop music"},{"paperId":"0f40c0957494c8d4bfdd6a9e2231a554b617fe28","title":"Learning to Rotate: Quaternion Transformer for Complicated Periodical Time Series Forecasting"},{"paperId":"2940f734dc3880545a82c5f1c354c20fcd2c7bcd","title":"TaxoTrans: Taxonomy-Guided Entity Translation"},{"paperId":"39ae1805a8d67bc833a104ba83edf7894ef968d8","title":"Non-stationary Time-aware Kernelized Attention for Temporal Event Prediction"},{"paperId":"a64cca16c7cb61d59afc39fef82ec774caffe5ea","title":"TGAN-AD: Transformer-Based GAN for Anomaly Detection of Time Series Data"},{"paperId":"42bc0824d8ca35105d181aaa0183654535325f55","title":"Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages"},{"paperId":"a37dcb3fc71f93136de370195d2227083589d69c","title":"Txt2Img-MHN: Remote Sensing Image Generation From Text Using Modern Hopfield Networks"},{"paperId":"8c69cee64a08e2e74fbf737c001c01d2cf073e7e","title":"Bidirectional Transformer with absolute-position aware relative position encoding for encoding sentences"},{"paperId":"f12076b11b90e653c4a14f20646b13537db49cbb","title":"PointConvFormer: Revenge of the Point-based Convolution"},{"paperId":"076deb54a5d776cd21eabf2c40cdd839f53d6d77","title":"giMLPs: Gate with Inhibition Mechanism in MLPs"},{"paperId":"976bb85ba9e426f8faf0c85b37c96e2a9f95cc0d","title":"Time Series Forecasting of Motor Bearing Vibration Based on Informer"},{"paperId":"a60cb9c5ca4c4b437d4087f549ef12be12d4bdbd","title":"Frequency Stability Prediction of Power Systems Using Vision Transformer and Copula Entropy"},{"paperId":"114636918f6a99853420e6d1c975798774a3c14a","title":"End-to-end Modeling for Chinese Dialect Speech Recognition"},{"paperId":"ba3ea95532803d1a9947c877b751dc0590a49b99","title":"3D Point Cloud Feature Information Assisted Audio Source Separation"},{"paperId":"a6d0b96bdbb2267867826a15830c8a45faf852d6","title":"Global-Local Self-Distillation for Visual Representation Learning"},{"paperId":"6e4b798b39b42a6ecb738bf8e530f002064c52e8","title":"Integration of Multiple Time Embedding and GLU for Sequential Recommendation"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"437815d4f186b0a96f5464a6672c4b07eb8f18bf","title":"Learning a Dual-Mode Speech Recognition Model VIA Self-Pruning"},{"paperId":"23b6ea803e9b707edbc4aa286d2b5d9878cdba2a","title":"Innovations in Neural Data-to-text Generation: A Survey"},{"paperId":"e9d1a851d4f7950db360d12fe7f96647aaf547da","title":"Generalized Attention Mechanism and Relative Position for Transformer"},{"paperId":"833e5fe55853d0cb847bfc27b638123869f86c63","title":"Learning Object Placement via Dual-path Graph Completion"},{"paperId":"3b5a08ab2faa5b1826c66e25b5efe167940c5356","title":"Molormer: a lightweight self-attention-based method focused on spatial structure of molecular graph for drug-drug interactions prediction"},{"paperId":"aa16fe6dda0349acd2281183c7fff5f48140ead0","title":"Multi-head Cascaded Swin Transformers with Attention to k-space Sampling Pattern for Accelerated MRI Reconstruction"},{"paperId":"0a4d0f5a69336aee8e85a974dfdecdc9aa518ee2","title":"CMSBERT-CLR: Context-driven Modality Shifting BERT with Contrastive Learning for linguistic, visual, acoustic Representations"},{"paperId":"9a5e384e032337abbbedae06fb0867eb725ee6b7","title":"DGR: Decomposition Graph Reconstruction for Question Understanding"},{"paperId":"52147112360c0cb80adc0a491ff8d2de30a754fc","title":"Mobi-Trans: A Hybrid Network with Attention Mechanism for Myocardial Infarction Localization"},{"paperId":"4299353235595391e2b4f7298baffd00b5acf9d1","title":"LordBERT: Embedding Long Text by Segment Ordering with BERT"},{"paperId":"a7b9d34bf98a56f11a6d6513a7535f0b42784c22","title":"Boosting the Transformer with the BERT Supervision in Low-Resource Machine Translation"},{"paperId":"2797abe9e6a59a3fc5554f1dc49e3c9c0009e553","title":"Decision Making for Autonomous Driving Via Multimodal Transformer and Deep Reinforcement Learning*"},{"paperId":"b9607608ed65a53326d64f773055f0b7e6ac4f88","title":"CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising"},{"paperId":"4d80dae4039b4de6c1ba30227d5981fd1cbb4a1f","title":"Parameterization of Cross-token Relations with Relative Positional Encoding for Vision MLP"},{"paperId":"e7e5cec591285f5ab50a83a0312867ce63b2a7fb","title":"Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization"},{"paperId":"c6bf37c47eabc93b552130bd052fe8d3d90c8a27","title":"Position Prediction as an Effective Pretraining Strategy"},{"paperId":"c27c68597065dad70cb0b4f882cd861826bf28fa","title":"Automated Construction Contract Summarization Using Natural Language Processing and Deep Learning"},{"paperId":"eedf999f58b2fd888feb210e4ea5c5a6ce681880","title":"Transformer-Based Global Zenith Tropospheric Delay Forecasting Model"},{"paperId":"9692886ba8e2c9d8990b0505e9c67a696d9f28a7","title":"A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities"},{"paperId":"5f3c2a31fc84d13a72008f70106163bd92f2f9d0","title":"kMaX-DeepLab: k-means Mask Transformer"},{"paperId":"8bd623131b4a849a77e92aca6d9c113912b45646","title":"Aspect-Based Sentiment Analysis Using Local Context Focus Mechanism with DeBERTa"},{"paperId":"e28adeb4db46469df9f9bd653501871ddc5f4318","title":"MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization"},{"paperId":"0b7b72e5325416250e5dd224395f7b78aedac634","title":"Translation-Based Implicit Annotation Projection for Zero-Shot Cross-Lingual Event Argument Extraction"},{"paperId":"0b97eef60bccf79e76b7c50d5f11e44638b58bd1","title":"Distance Matters in Human-Object Interaction Detection"},{"paperId":"1ed91a88cfb55aa8e1b2a2145c7fff7c644bb284","title":"Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency"},{"paperId":"5046d276dbaa4c5245e8427791fe7f30b26eef8e","title":"Can we learn from developer mistakes? Learning to localize and repair real bugs from real bug fixes"},{"paperId":"3ee323d224391b610b18985237a633f64f55566a","title":"Relational local electroencephalography representations for sleep scoring"},{"paperId":"f16656c5a8702523c3340938a7040817736b83f2","title":"Unaligned Multimodal Sequences for Depression Assessment From Speech"},{"paperId":"1c3229a0a589467560f808b0234744181f051ea5","title":"Table2Graph: Transforming Tabular Data to Unified Weighted Graph"},{"paperId":"ffd2a9c326b68e3feed96855367647eb32f7b1da","title":"Guiding transformer to generate graph structure for AMR parsing"},{"paperId":"bccfcca7b28c4792bd59659c56e1d7843542dfe9","title":"Attention Biasing and Context Augmentation for Zero-Shot Control of Encoder-Decoder Transformers for Natural Language Generation"},{"paperId":"1c85c2a214b3e6a64c121e1bee8c5ca46005c6ab","title":"Capture Salient Historical Information: A Fast and Accurate Non-autoregressive Model for Multi-turn Spoken Language Understanding"},{"paperId":"765082d2cbc5511b2fa3fc2fa79809d90b2f2a32","title":"LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs"},{"paperId":"2e6028f8b156c8b344e1a68d15d88403a978c71d","title":"Learning Multiscale Transformer Models for Sequence Generation"},{"paperId":"1404b42b66d6f13af3aadf3d253ac531d260f38d","title":"VReBERT: A Simple and Flexible Transformer for Visual Relationship Detection"},{"paperId":"591fc9d7642d4501686f418d6059c66a914e7cb0","title":"Image De-Raining Transformer"},{"paperId":"0fda2f1ada85dda45cbb3aa926620bbbd9c476b9","title":"SP-ViT: Learning 2D Spatial Priors for Vision Transformers"},{"paperId":"07e8fd26db7949b3d0b018f85ac360fec1e73492","title":"A Survey : Neural Networks for AMR-to-Text"},{"paperId":"6fb05e3483d05f79e59e5ce957708d8ab8932fdc","title":"Peripheral Vision Transformer"},{"paperId":"2fac281ccb4f52fe578e548aa62a75bf9e7fd2ad","title":"Unsupervised method for video action segmentation through spatio-temporal and positional-encoded embeddings"},{"paperId":"2a03b152e5758aac2d6a6aa3befbea11e59bd48c","title":"Code comment generation based on graph neural network enhanced transformer model for code understanding in open-source software ecosystems"},{"paperId":"31a84391e1b47fa15f3a521c43d62385a7757637","title":"MetaTPTrans: A Meta Learning Approach for Multilingual Code Representation Learning"},{"paperId":"619d53bcebcf72d77387bfef56be7d628e438c26","title":"The YiTrans Speech Translation System for IWSLT 2022 Offline Shared Task"},{"paperId":"9ea9e48950e311387c177b5195782c8c324ac119","title":"Positional Label for Self-Supervised Vision Transformer"},{"paperId":"014b2480e7f9487d571e9f5f56a6d6b27d1d706e","title":"Cortical activity during naturalistic music listening reflects short-range predictions based on long-term experience"},{"paperId":"eef43c8d4205d93258d9976379cffd768211506b","title":"Revisiting End-to-End Speech-to-Text Translation From Scratch"},{"paperId":"728fff6344f9ce65fafcbf3b9aea4f0eb908d44d","title":"How to Dissect a Muppet: The Structure of Transformer Embedding Spaces"},{"paperId":"dcec176b9a76cbc22dd2c285b25d306d9b8e04a9","title":"Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech"},{"paperId":"35941b8adb6cc3fec66292d355c504463a429e74","title":"EAANet: Efficient Attention Augmented Convolutional Networks"},{"paperId":"a9c72a0aedd209c2f565d09fd46dd27c78585a91","title":"Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives"},{"paperId":"ca285c906eac9b1b31483d2bc3cddc52c90ac564","title":"KPGT: Knowledge-Guided Pre-training of Graph Transformer for Molecular Property Prediction"},{"paperId":"4c5aa3e450b7c626beb7a27f65f1a238b13e9606","title":"Voice activity detection using a local-global attention model"},{"paperId":"45bf4b6468a3fa643bc1e2f9b7cf049d3d45c2fc","title":"Graph Neural Networks with Precomputed Node Features"},{"paperId":"0dd163f1c10480128eeb25d77afec493c1c695eb","title":"Transformer with Fourier Integral Attentions"},{"paperId":"f5d292b97c0af02506c60c6615e1b58b0f4f421a","title":"Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection"},{"paperId":"31a9744bd5421b3fbbad2ab38ce33bb2f352c77a","title":"CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation"},{"paperId":"a1700c57759fe6482e972c30076d04529a08bf2d","title":"Preparing an endangered language for the digital age: The Case of Judeo-Spanish"},{"paperId":"66b7836001407120ad0000369af8b30c44788a33","title":"Transformer with Tree-order Encoding for Neural Program Generation"},{"paperId":"8e6ada0242dc9b3680ccf6ddeec9e7c3cd41766f","title":"Dynamically Relative Position Encoding-Based Transformer for Automatic Code Edit"},{"paperId":"e1461a3aa9f4bef86435fa2cc87b16d9f34d5ab0","title":"Do We Really Need Temporal Convolutions in Action Segmentation?"},{"paperId":"746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a","title":"Your Transformer May Not be as Powerful as You Expect"},{"paperId":"ed4e8f54074ac075148d29cf7650d0bff2ca95a8","title":"Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers"},{"paperId":"1bcd42583a7b4475d3b456678e7f3752acd9edd1","title":"FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?"},{"paperId":"805748eec6be59ae0cd92a48400a902b3b7ed8e6","title":"Flexible Diffusion Modeling of Long Videos"},{"paperId":"d6c5aab433d9871cabc01ffb1e5e1ea89141155b","title":"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"},{"paperId":"3978fd4769d528b47d0233c2d86dcc8b54b9de3f","title":"BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video"},{"paperId":"68309384f7a8e96d98ede4271e7c04425c23f3f2","title":"AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential Recommendation"},{"paperId":"6e98a35c439c5c7387de6bffd96dea9b2943a548","title":"Trading Positional Complexity vs. Deepness in Coordinate Networks"},{"paperId":"004b97aea43f9f62cc49dec20f449abfbae28811","title":"Masked Autoencoders As Spatiotemporal Learners"},{"paperId":"77cb1ca1485f88f4061570dc999524da339863af","title":"Hero-Gang Neural Model For Named Entity Recognition"},{"paperId":"14793aa93920cb8f748776cc45c3895de6df5fbf","title":"RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL"},{"paperId":"9d03a164759bb5cc2fa6b575254b58f790ab6785","title":"Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction"},{"paperId":"137ec10dc0ca5f14deaa9c69ec6f6db3f63ac20d","title":"Sentence model based subword embeddings for a dialog system"},{"paperId":"c2335cd6ff6eab494148dd605cb725973dc9caf9","title":"CSI-Fingerprinting Indoor Localization via Attention-Augmented Residual Convolutional Neural Network"},{"paperId":"b4da9f3505e22d3e766ba21890285b822dc71599","title":"EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers"},{"paperId":"9b66c808f38f50d7f4e88e910caad015c23d118d","title":"Capturing Time Dynamics From Speech Using Neural Networks for Surgical Mask Detection"},{"paperId":"703f79763d534dbf9674132cec890f432dcc19ec","title":"A Survey of Deep Learning Models for Structural Code Understanding"},{"paperId":"4990be2f9092f02df3560fca63d8aef26954bca6","title":"AST-Trans: Code Summarization with Efficient Tree-Structured Attention"},{"paperId":"a6dd1b1088a4ce34e0779cb0c6ae4547d0068b7e","title":"Detecting Loaded Trajectories for Hazardous Chemicals Transportation"},{"paperId":"4d90e08472949c936d6f1c962f4d8b038327f76b","title":"Recognition of Chinese Legal Elements Based on Transfer Learning and Semantic Relevance"},{"paperId":"9ae4666bf38e820292f8a889cb4a9fd796d2dba1","title":"DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation"},{"paperId":"3f200791d33165673740a3224a0afc5daf36f387","title":"Attention mechanism in neural networks: where it comes and where it goes"},{"paperId":"b9a701c90f3d3df27366f5b29a97f798eb940ac7","title":"ChapterBreak: A Challenge Dataset for Long-Range Language Models"},{"paperId":"c6eef27b7ecefe7b8efe15ba228e70137761da10","title":"SinTra: Learning an inspiration model from a single multi-track music segment"},{"paperId":"0aac6adb54813cdf16b321dbee76dea82d7c1d09","title":"Cross-stitched Multi-modal Encoders"},{"paperId":"2193d898e556ad2ac14f0ababf02f90e6fdfe663","title":"DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks"},{"paperId":"633851a323990270461b49e8068a2c9ca1cea530","title":"Dynamic Position Encoding for Transformers"},{"paperId":"a99cbe3d7b2e201491a2fa1f4ba57b1e80cca3a3","title":"LaMemo: Language Modeling with Look-Ahead Memory"},{"paperId":"2e03ee15c00561a891eaf197515da720d1566411","title":"Causal Transformer for Estimating Counterfactual Outcomes"},{"paperId":"215fd67a51240194b816c695ac46e0d6d1fdbdc8","title":"GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation"},{"paperId":"f37288e2e61ead8d8941213bd28ec00f059b8b5e","title":"A collection of deep learning-based feature-free approaches for characterizing single-objective continuous fitness landscapes"},{"paperId":"fd472735d69fe44870fe6aad0133eb3388eed527","title":"Accurate Portraits of Scientific Resources and Knowledge Service Components"},{"paperId":"01c93b745dac8d6d463fce3de4ac634c3048f065","title":"TANet: Thread-Aware Pretraining for Abstractive Conversational Summarization"},{"paperId":"3b2a675bb617ae1a920e8e29d535cdf27826e999","title":"Video Diffusion Models"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"c04e3a0d4c3d5d734eadac8ca70dace565e34921","title":"LSTM-RASA Based Agri Farm Assistant for Farmers"},{"paperId":"e07890cf7e92ef8bac0b124ba2ecbe28bf488b65","title":"Heterogeneous Target Speech Separation"},{"paperId":"fb3761d27765536b204191d2a8bca2898055cb95","title":"Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection"},{"paperId":"1984b0f7605e2884616466c92126269ad3a1e426","title":"A Complementary Joint Training Approach Using Unpaired Speech and Text for Low-Resource Automatic Speech Recognition"},{"paperId":"07b5755c833c240b1ee48629cf545442a16bd7d5","title":"Micro-Behavior Encoding for Session-based Recommendation"},{"paperId":"346adcf3ab9cbd06d816586ad30bd3112a5abd0f","title":"Dynamic Focus-aware Positional Queries for Semantic Segmentation"},{"paperId":"2ad12a7be5eaf339a98c4defd8669e11fe726acc","title":"MaxViT: Multi-Axis Vision Transformer"},{"paperId":"652a892f36f576726a9fe6ea512e8cc540f5a144","title":"ELECTRIcity: An Efficient Transformer for Non-Intrusive Load Monitoring"},{"paperId":"c99a0aac9a7166ff7659c7b0c6c4a65b3c5642b7","title":"Pair-wise aspect and opinion terms extraction as graph parsing via a novel mutually-aware interaction mechanism"},{"paperId":"df1b3e55e8092a2aa42bb3eab8db778f2b94d66c","title":"The auto segmentation for cardiac structures using a dual‐input deep learning network based on vision saliency and transformer"},{"paperId":"e56b425d5264a30366986f875f2a754f7c27c9fc","title":"Early Detection of Alzheimer's Disease Using Bottleneck Transformers"},{"paperId":"49995802ad0d6ef327647868868458d7619d430d","title":"Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data"},{"paperId":"a2fc77f075f666b462d9350e7576f0ba9845c61b","title":"Transformer Language Models without Positional Encodings Still Learn Positional Information"},{"paperId":"35947a79360829331010011209927f8cb619f97a","title":"VPTR: Efficient Transformers for Video Prediction"},{"paperId":"de4fa71618e56128286a7b6f8302fa78eb155fff","title":"Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture"},{"paperId":"336672adf959e5eb4a2a189c976a2847a68f85c5","title":"ANNA”:\" Enhanced Language Representation for Question Answering"},{"paperId":"c52db31d0535ee38b7ca311350c6f490f0529924","title":"Federated Learning with Position-Aware Neurons"},{"paperId":"b90f8533cb6717121a741d60ac5f466afa71ee99","title":"Coordinate Transformer Network for Prediction of Pseudomonas Aeruginosa’s Drug Resistance"},{"paperId":"2a04c7d083eec666b2a1edcade876d9be9dcdfe8","title":"DPE-BoTNeT: Dual Position Encoding Bottleneck Transformer Network for Skin Lesion Classification"},{"paperId":"cc1a8af59affa863aede0dcd601ccab920d99548","title":"HELoC: Hierarchical Contrastive Learning of Source Code Representation"},{"paperId":"838a2297b94f7bad96c4f8370a5f58487f194f44","title":"Visual Abductive Reasoning"},{"paperId":"444e76c94a77b0ba6a401d5196b927b17251c610","title":"Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions"},{"paperId":"0b568a599c7aac6dae52e5a7412ad5a7429ac344","title":"Lane detection with position embedding"},{"paperId":"fa118487bf333d20573a9dc10b5809d74b696e02","title":"Speech Enhancement by Multiple Propagation through the Same Neural Network"},{"paperId":"b05f94bb89ff5ae8ab3e33ad40aae252554ea024","title":"Scalable Video Object Segmentation with Identification Mechanism"},{"paperId":"dc998860b4b2562fb3a87e658ffcf9ef2a11438f","title":"Autotts: End-to-End Text-to-Speech Synthesis Through Differentiable Duration Modeling"},{"paperId":"2b7b7eb96fc30089d53e92ad97ac2db0035dc3cb","title":"Generalized Classification of Satellite Image Time Series with Thermal Positional Encoding"},{"paperId":"04a061d959ff3a777c266c8975ca1e36f05528ac","title":"ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation"},{"paperId":"36d649496b91ce22f380502553a488ccaf3027dd","title":"AAPFE: Aligned Assembly Pre-Training Function Embedding for Malware Analysis"},{"paperId":"e28ad66a8ee09278e00b8fa064b695eefca52d02","title":"FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction"},{"paperId":"d71915cf609dd0e785da481fcdab826c38611243","title":"HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing"},{"paperId":"93c1dffe2bae737da8f342fd749aa783df572a14","title":"XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding"},{"paperId":"a3c18ea5548acbc885836e8d0f202852db192268","title":"S^2SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers"},{"paperId":"acbe813244e07f32eb034d6c27547d772a995d1d","title":"Uncertainty Estimation for Language Reward Models"},{"paperId":"9f1b0e4c42a5a85d4c023030557ade4419f82ecf","title":"Scaling Up Your Kernels to 31×31: Revisiting Large Kernel Design in CNNs"},{"paperId":"0239063c7a7782ead73fe72e4b2d21c3988be23d","title":"Challenges of Neural Machine Translation for Short Texts"},{"paperId":"9aacdbc8b04fa63e6fe93f62a737a11c613f08fb","title":"Recent Advances in Neural Text Generation: A Task-Agnostic Survey"},{"paperId":"63de5aacac3c29f7a3bb17ec65e50229a6a179ba","title":"HDNet: High-resolution Dual-domain Learning for Spectral Compressive Imaging"},{"paperId":"4e9cd6be4a8fcad2ca562fcf41a1f882387a3167","title":"LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network"},{"paperId":"f8f3ba03d0350754fbb502192dc0a201a053024f","title":"A Unified Query-based Paradigm for Point Cloud Understanding"},{"paperId":"83b455fd69b7cd1b9409c68fa262ae92335b8e61","title":"KG-SQL: Hybrid Knowledge-Guided Semantic Understanding for Text-to-SQL"},{"paperId":"189e36ba5b50a854eb45f612bcad51a621d7a7f1","title":"MS-Transformer: Introduce multiple structural priors into a unified transformer for encoding sentences"},{"paperId":"2e43501a14b831744999355c177321709659aff1","title":"TableFormer: Robust Transformer Modeling for Table-Text Encoding"},{"paperId":"3921e7cd03c440c6d065a51b499c9f581d1d2f1f","title":"A Survey of Automatic Source Code Summarization"},{"paperId":"50af83ea20201b51014358534650213e6133650c","title":"FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks"},{"paperId":"7a9df88b2e10fd071d045cad1d4239116095ca99","title":"Machine Reading Comprehension-Enabled Public Service Information System: A Large-Scale Dataset and Neural Network Models"},{"paperId":"48df6b4c106bbb4735fcc9f1e2ac4e23bba10565","title":"SFINet: Shuffle–and–Fusion Interaction Networks for Wind Power Forecasting"},{"paperId":"5f104a804ed245c79847ad0593e8f86196d697b1","title":"Transformers in Time Series: A Survey"},{"paperId":"3c4fe540aea86efa07c921472b0d7dc224c14a90","title":"AI‐based abstractive text summarization towards AIoT and edge computing"},{"paperId":"a996989466bb584602eb3027f308d78b2867893d","title":"Source Code Summarization with Structural Relative Position Guided Transformer"},{"paperId":"a7de7968bd9d4d97daaac74c3c1d466f3342ff45","title":"Entroformer: A Transformer-based Entropy Model for Learned Image Compression"},{"paperId":"10af057b73c206d5b999c200b4de4a7a67435dee","title":"Personalized Long-distance Fuel-efficient Route Recommendation Through Historical Trajectories Mining"},{"paperId":"2491a706049da76ab7fe17f74c6ea7f195502fdf","title":"Multi-Resolution Attention for Personalized Item Search"},{"paperId":"106ec729604861c1494f17fe7f277c130acef8dc","title":"Improving the Sample-Complexity of Deep Classification Networks with Invariant Integration"},{"paperId":"9d054fa2ce6b0b3160aa52712f44f7967057205b","title":"Corrupted Image Modeling for Self-Supervised Visual Pre-Training"},{"paperId":"ea0e4a9778e33b7f8e7b3246d63071330950995a","title":"Structure-Aware Transformer for Graph Representation Learning"},{"paperId":"d0194169a4787cf2977e5574746f5466ed6b96cc","title":"Curvature-Driven Deformable Convolutional Networks for End-To-End Object Detection"},{"paperId":"385dab6963b7e9deae1f32f68e53c846a2122a9b","title":"Self-attention-based time-variant neural networks for multi-step time series forecasting"},{"paperId":"77365b30336ac46d620d958dc4c108a159c02834","title":"WebFormer: The Web-page Transformer for Structure Information Extraction"},{"paperId":"2b34ee1b7f4cf5e7dac38d8e66849a02d96fade3","title":"Graph-based Neural Acceleration for Nonnegative Matrix Factorization"},{"paperId":"4c75564731f564e78cafc76e18739bbcf4fceeb2","title":"Knowledge Base Question Answering Based on Multi-head Attention Mechanism and Relative Position Coding"},{"paperId":"a81e30bbbf5422fbac75f6a017206ac97a508521","title":"Efficient Self-attention with Relative Position Encoding for Electric Power Load Forecasting"},{"paperId":"b92898a28bfad42a053726c2707cc05686cd332a","title":"GRPE: Relative Positional Encoding for Graph Transformer"},{"paperId":"7cf779d889dbf155e089289bab1495be2b186b11","title":"Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal Point Processes"},{"paperId":"83ebb3a6f8c81492456d024d5dfb9a9bc0221434","title":"Generative Cooperative Networks for Natural Language Generation"},{"paperId":"c817dbd4aa035b3e3d8d244f5e07c319f0e6fb06","title":"Position-Enhanced Multi-Head Self-Attention Based Bidirectional Gated Recurrent Unit for Aspect-Level Sentiment Classification"},{"paperId":"076d06d66886dedf3cb6e5dc31393dac38fef300","title":"Dual-Flattening Transformers through Decomposed Row and Column Queries for Semantic Segmentation"},{"paperId":"b51fa25bf1ce7fd18737a36b72e80f8e5808973e","title":"MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition"},{"paperId":"d10864946d1733444f0472acf1294f15b350e65e","title":"Continual Transformers: Redundancy-Free Attention for Online Inference"},{"paperId":"3e906906d475c73b6d8ce24ac5ebdac9979fd01b","title":"Video Transformers: A Survey"},{"paperId":"e55c6651a83e0565feda69276c25b7976b5a8fda","title":"Reverse-engineering information presentations: recovering hierarchical grouping from layouts of visual elements"},{"paperId":"49c3f85573a3204c5e66317289e4cecfed50f38a","title":"Assemble Foundation Models for Automatic Code Summarization"},{"paperId":"47b97fb6398122ef41344b7e5301a2389b1a94bb","title":"Feature Pyramid Multi-View Stereo Network Based on Self-Attention Mechanism"},{"paperId":"4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c","title":"Classification of Long Sequential Data using Circular Dilated Convolutional Neural Networks"},{"paperId":"063e410c2c52ccbaa22c62130ac2c58969bb3efa","title":"SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations"},{"paperId":"ab00d5e3183cd8d49f62b1424c2503fd1e6edaee","title":"PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture"},{"paperId":"c59a685133c8b0969fb8c5490e3c741b6e89541f","title":"Semi-MsST-GAN: A Semi-Supervised Segmentation Method for Corneal Ulcer Segmentation in Slit-Lamp Images"},{"paperId":"3d261c68c964fc2f9ba759e71fdaa9bf075d1c98","title":"D-former: a U-shaped Dilated Transformer for 3D medical image segmentation"},{"paperId":"d3ff0a2e58e29d652fd15cd86f0a0f53df8e8af8","title":"Transmorph: a transformer based morphological disambiguator for Turkish"},{"paperId":"ca1d71b52b3b81a7e1cca297f19d47f353cf3340","title":"Relation-Aware Graph Transformer for SQL-to-Text Generation"},{"paperId":"6449f3ea73692d3a8cac8c51cefe608aa4b22a02","title":"S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation"},{"paperId":"59a8dbbbf0002f1f497fddc4d01e92c0b9645c10","title":"Diformer: Directional Transformer for Neural Machine Translation"},{"paperId":"63b5ada6cbe9df123d18cd467b56fbcfbaf49acb","title":"PointSCNet: Point Cloud Structure and Correlation Learning Based on Space-Filling Curve-Guided Sampling"},{"paperId":"a1031489ac747006b0ecb1a1a6d74b2e76ada4d4","title":"Joint-training on Symbiosis Networks for Deep Nueral Machine Translation models"},{"paperId":"122ac5e90051fe41919884fe274a1cb66528f326","title":"Semantics-Recovering Decompilation through Neural Machine Translation"},{"paperId":"d81ec6393db9d67741b90915911180a52e42413d","title":"ViT-based VQ-VAE Generative Network for Accompaniment Generation"},{"paperId":"c44da47301bae65f687650997293ba1933ec9015","title":"A Learnable Radial Basis Positional Embedding for Coordinate-MLPs"},{"paperId":"9eb3726604cd436010c8f19089598b97fb1bf8a3","title":"Improving Face-Based Age Estimation With Attention-Based Dynamic Patch Fusion"},{"paperId":"0366d4355cbce731054714a96dbd3bbdb7399a0d","title":"Protein-Ligand Binding Affinity Prediction Using Deep Learning"},{"paperId":"008a428e049003fe768068a0f1fa1416af5c4982","title":"Masked Feature Prediction for Self-Supervised Visual Pre-Training"},{"paperId":"94c5ab1786266cf01d2a3f9dafd498b49d2b46a2","title":"Seizure prediction in scalp EEG based channel attention dual-input convolutional neural network"},{"paperId":"ae31edefdfd8b964625568f9c56d3f9c692b4ea4","title":"Developing A Deep Learning Natural Language Processing Algorithm For Automated Reporting Of Adverse Drug Reactions"},{"paperId":"9816ddd36859eb80e4d22e83f080f3e69ddc708d","title":"Towards More Efficient Insertion Transformer with Fractional Positional Encoding"},{"paperId":"79d9422ff0761c12f5b13b7199c08ee4e4fdfb29","title":"A Convolutional Neural Network Based on Self-Attention Mechanism for Molecular Property Prediction Using Molecular Hidden Fingerprints: An efficient molecular property prediction method"},{"paperId":"fdfa5490dd47e7dc8556bcfafe7a4f6ea07b4dbb","title":"UNITER-Based Situated Coreference Resolution with Rich Multimodal Input"},{"paperId":"9137efc758f80dd22bb56f82cca5c94f78a5db3e","title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"},{"paperId":"91a4cbae6553e975ddc3b2f6850ed725ff475307","title":"SwinTrack: A Simple and Strong Baseline for Transformer Tracking"},{"paperId":"5a829129e8357ba14143a30c93d971941ae9d255","title":"Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-Sentence Dependency Graph"},{"paperId":"2c11b720624dc669f78fd34557fe91b5b2097969","title":"Deep attentive style transfer for images with wavelet decomposition"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"2099da1395984292fe59f42e787027409da7b08c","title":"Leveraging contextual embeddings and self-attention neural networks with bi-attention for sentiment analysis"},{"paperId":"4c93661c3ad6fe4e3e092975afd9a451c49319a6","title":"KARL-Trans-NER: Knowledge Aware Representation Learning for Named Entity Recognition using Transformers"},{"paperId":"e73d87e7c19457b2e79878638ea44ee66196743c","title":"SANTM: Efficient Self-attention-driven Network for Text Matching"},{"paperId":"366766cbf762ba8f9d50fd77bfcf1e6956b275a4","title":"Dual-axial self-attention network for text classification"},{"paperId":"becb855b7cf635316ec634bdbb851bda13399fc0","title":"Deps-SAN: Neural Machine Translation with Dependency-Scaled Self-Attention Network"},{"paperId":"972706306f85b1bfb40c7d35c796ad5174eb0c9c","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"},{"paperId":"a782b1d91d90e3d3946cf019703162f7f9661cdf","title":"GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization"},{"paperId":"45f686be3b96302ede327645227134e1c304dbab","title":"Attention mechanisms in computer vision: A survey"},{"paperId":"c1793d88db202bd416cab2776224f17e4da7457a","title":"PESTO: Switching Point based Dynamic and Relative Positional Encoding for Code-Mixed Languages"},{"paperId":"9da405a29af85b16fff126db1e52158677ed8e05","title":"Remote Sensing Image Scene Classification Based on Global Self-Attention Module"},{"paperId":"333212e246fb65f7c9d43862021e78f007c48449","title":"A Survey of Visual Transformers"},{"paperId":"9418741ce0f9d1d0a00d7631adb17fdbfd06e4e5","title":"A Chinese Multi-type Complex Questions Answering Dataset over Wikidata"},{"paperId":"36d1aeff3f1e57f2f2bf3cd4f596d7797862bc86","title":"A Contextual Latent Space Model: Subsequence Modulation in Melodic Sequence"},{"paperId":"0a22b5df6c1c25b776d1015e9490974ed704c044","title":"ER-SQL: Learning enhanced representation for Text-to-SQL using table contents"},{"paperId":"8e488694a0f9675eab577d45e9bbee7d6982d741","title":"With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition"},{"paperId":"5ef4e013ce7bf0c19873c83ffd6898ce33ffd542","title":"AST-Transformer: Encoding Abstract Syntax Trees Efficiently for Code Summarization"},{"paperId":"e3e230fc3d31238700924c015f9c3db64f02cee1","title":"Pseudo-Labeling for Massively Multilingual Speech Recognition"},{"paperId":"2f05f579f6c48f43e6ced36d96eff9190e57bd40","title":"PatchFormer: An Efficient Point Transformer with Patch Attention"},{"paperId":"ff653651ddc57081f0a2b82ebcc2126905089182","title":"Two Heads are Better than One: Geometric-Latent Attention for Point Cloud Classification and Segmentation"},{"paperId":"c6481af1c5ebba68a8f59ade7d07eab1265d8e15","title":"Dispensed Transformer Network for Unsupervised Domain Adaptation"},{"paperId":"cb416e2d8bd72203bc73c1005251dcb12b153d20","title":"Language Representation Models: An Overview"},{"paperId":"063eee315e864f0842d3074629dccc4bb36d19e7","title":"Discovering Non-monotonic Autoregressive Orderings with Variational Inference"},{"paperId":"a26d810b78fa39b23253ee072aeedac88c4c87c4","title":"Operation Diagnosis on Procedure Graph: The Task and Dataset"},{"paperId":"1dc2edea1b328c846bd48c59541f7b7c49b9d59e","title":"Cache-based GNN System for Dynamic Graphs"},{"paperId":"0eb024e6591f071f832be7a18657e9d6eb85fa1c","title":"Positional Mask Attention for Video Sequence Modeling"},{"paperId":"df473ca1b625f63335cd20524b16ddeb9c45e8b9","title":"Hierarchical Semantic Enhanced Directional Graph Network for Visual Commonsense Reasoning"},{"paperId":"eb36750a31af99baf22ed01d0b8c24d9ff550b40","title":"EAPT: Efficient Attention Pyramid Transformer for Image Processing"},{"paperId":"1df0f782b0d2264c455ba3a4332c26dbc014f2ee","title":"Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction"},{"paperId":"c29e21c58fc12024ef9b273b826050f536368071","title":"Image generation step by step: animation generation-image translation"},{"paperId":"b67ff4b6fca84d57f951a3ce1cc9829f6c902dce","title":"Multimodal Video Summarization via Time-Aware Transformers"},{"paperId":"46c8b4b133a359dec97a03d54b36de054384d5be","title":"Image Search with Text Feedback by Deep Hierarchical Attention Mutual Information Maximization"},{"paperId":"03da17fc6d1868efc0b91872f1823e8ff4612824","title":"Direction Relation Transformer for Image Captioning"},{"paperId":"42c248f36f8fe9b1cd530ca9b40ecc16fb29afbd","title":"Multi-View Stereo Network with attention thin volume"},{"paperId":"b34e7babf9443d792a765b65fd9f5dc3b9b3ddee","title":"RTJTN: Relational Triplet Joint Tagging Network for Joint Entity and Relation Extraction"},{"paperId":"1067c44e473b6998f89e13f0d4c0de730def43f0","title":"SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing"},{"paperId":"e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"},{"paperId":"991adf5065c2f0cb52a7273edf2773453066f857","title":"Study of Positional Encoding Approaches for Audio Spectrogram Transformers"},{"paperId":"701f5eb3337c7ea3731168615886bba810aa1ff6","title":"A Speaker-aware Parallel Hierarchical Attentive Encoder-Decoder Model for Multi-turn Dialogue Generation"},{"paperId":"c501d6f1eecf3b0fdae7d428e1829bb6607a6a37","title":"Relative molecule self-attention transformer"},{"paperId":"813f6e34feb3dc0346b6392d061af12ff186ba7e","title":"Learning Efficient Multi-Agent Cooperative Visual Exploration"},{"paperId":"281fc1783a91481ebbd22e01f85e17ac67990a6f","title":"A Systematic Review of Deep Learning Approaches for Natural Language Processing in Battery Materials Domain"},{"paperId":"013e80651953899f656c63ba7e670dfd9630d26f","title":"Dual contextual module for neural machine translation"},{"paperId":"3356254badd0652bbb75515eae688517dc8d260f","title":"Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"68f14f333dad84ec35fc0eb9fcfd2c41fdc02596","title":"ATISS: Autoregressive Transformers for Indoor Scene Synthesis"},{"paperId":"12640af46eaf4c16125557b517a2d37fca70a82d","title":"Ripple Attention for Visual Perception with Sub-quadratic Complexity"},{"paperId":"87e879c2465b2414a58dd3a1184f8b346d48f3e7","title":"Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer"},{"paperId":"540ed994eb00b5279748d1f26d04371e3a67ec0d","title":"Molformer: Motif-Based Transformer on 3D Heterogeneous Molecular Graphs"},{"paperId":"7d34244c64fec4808503e21d648eaaefe5d85208","title":"SPaR.txt, a Cheap Shallow Parsing Approach for Regulatory Texts"},{"paperId":"7dbeb92e7c234b3887cc484ce406ad86b5743add","title":"Detecting Persuasive Atypicality by Modeling Contextual Compatibility"},{"paperId":"74fc2b3999897b0a09934224100212531335055a","title":"ResSaNet: A Hybrid Backbone of Residual Block and Self-Attention Module for Masked Face Recognition"},{"paperId":"8b2d357347b57217966863a36c95f4b52e6be3a6","title":"Gaze estimation via self-attention augmented convolutions"},{"paperId":"cdeb1f6188666809bcdeec2e3b916671fc012634","title":"GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation"},{"paperId":"98eafdb2a4d6242cc916f1278a0113b102de6391","title":"PortaSpeech: Portable and High-Quality Generative Text-to-Speech"},{"paperId":"232acef2483a26fe95c70b619f88fa0b82c1a105","title":"Multiplicative Position-aware Transformer Models for Language Understanding"},{"paperId":"27c4102b00a04681e5adbde4493006fd41628b35","title":"Modeling Dynamic Attributes for Next Basket Recommendation"},{"paperId":"ef8873006f9d715369063c854db37e65d6c2fffd","title":"The NiuTrans Machine Translation Systems for WMT21"},{"paperId":"25dc7c6e808ffb097682941fe767d09709472480","title":"Underwater image enhancement via LBP-based attention residual network"},{"paperId":"8b6b9d426776c3c7c82f5dadee0e654d5088c14e","title":"Audio-Visual Speech Recognition is Worth $32\\times 32\\times 8$ Voxels"},{"paperId":"df6f3c607ae3956db722f76f3f81e672c3dfa803","title":"CodeQA: A Question Answering Dataset for Source Code Comprehension"},{"paperId":"4a8964ea0de47010fb458021b68fa3ef5c4b77b2","title":"Primer: Searching for Efficient Transformers for Language Modeling"},{"paperId":"8a3882fd35847d78997a8491f06ebbb3019d8775","title":"Generating Music Transition by Using a Transformer-Based Model"},{"paperId":"0a35064caba35a80f21fb9798ada7abb1377ead9","title":"The NiuTrans System for the WMT 2021 Efficiency Task"},{"paperId":"a299947b4d588dee748d0a3b3c6a4dec55c8b212","title":"A Two-Stage Short-Term Load Forecasting Method Using Long Short-Term Memory and Multilayer Perceptron"},{"paperId":"255c526f78bbfec35d4ed73b6dd8eacd9ef2c0b3","title":"RankNAS: Efficient Neural Architecture Search by Pairwise Ranking"},{"paperId":"64a0a4f357be12aaf30cc6e4964d1c3a9d927aac","title":"Sequence Length is a Domain: Length-based Overfitting in Transformer Models"},{"paperId":"9c5a5e932139621da37c93d48f8a8df40a9c61d4","title":"Dialogue State Tracking with a Language Model using Schema-Driven Prompting"},{"paperId":"586cafd248d01c1a1c67a57b3cef9f807173110c","title":"Performance-Efficiency Trade-Offs in Unsupervised Pre-Training for Speech Recognition"},{"paperId":"64522a5b3476e9f201f6a5b3e312ef0005c562f1","title":"SHAPE: Shifted Absolute Position Embedding for Transformers"},{"paperId":"bd7907735793f122c400d3afc4e73187dc57f376","title":"SPARQLing Database Queries from Intermediate Question Decompositions"},{"paperId":"32d7233aab1a5020f688333007f70bce3bf0a97a","title":"Leveraging Multi-Faceted User Preferences for Improving Click-Through Rate Predictions"},{"paperId":"33c831326bb47b2ba2031fd7213b6918d23eb01e","title":"The Impact of Positional Encodings on Multilingual Compression"},{"paperId":"23d11338be48471b3979b13eb172ec67fc22244b","title":"Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model"},{"paperId":"64f3a18921f7f3a384dca073cd6d2476b9af47f2","title":"Zero-Shot Dialogue State Tracking via Cross-Task Transfer"},{"paperId":"bb363c8c5bc1c473f0801c647c88d0c071792858","title":"PermuteFormer: Efficient Relative Position Encoding for Long Sequences"},{"paperId":"aec7e7143bfe082752f428fe01e4090dd7fc411c","title":"Ultra-high Resolution Image Segmentation via Locality-aware Context Fusion and Alternating Local Enhancement"},{"paperId":"e8f39c0bde3a3f624d2198576f5a0b99a87f862f","title":"Improving neural machine translation using gated state network and focal adaptive attention networtk"},{"paperId":"5b95ef4fea818f08897f0dddb56e5d3a8e5cb9d3","title":"ShopTalk: A System for Conversational Faceted Search"},{"paperId":"8e9d44502f5b6de37d5a7181e169a2899a91f7b3","title":"Self-Attention-Based Temporary Curiosity in Reinforcement Learning Exploration"},{"paperId":"51b5db5c679be0ce9a39a2ee21def42bca165efe","title":"Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning"},{"paperId":"678f0cdb6b8e59aa2ef6c409b97fecc529059703","title":"An Enhanced Visual Attention Siamese Network That Updates Template Features Online"},{"paperId":"1b45714103c276ebadb12cfe0a0d08f0667d98e5","title":"LinearSpeech: Parallel Text-to-Speech with Linear Complexity"},{"paperId":"053ac76f7b6e12f406eaf6e953b3bd5313fd69c2","title":"Ultra Fast Speech Separation Model with Teacher Student Learning"},{"paperId":"35e31785d64574f6f5a89be81ae934c616d3753a","title":"Lightweight Causal Transformer with Local Self-Attention for Real-Time Speech Enhancement"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"015b2fb3a12bd0134b91af272b29b62371ef9022","title":"Recurrent multiple shared layers in Depth for Neural Machine Translation"},{"paperId":"5882f98d17780c6c0b2638f57cd1764dd5267c18","title":"Deep Contrast Learning Approach for Address Semantic Matching"},{"paperId":"2eaf36dc40fd9a61f8f0fad14a6d3c9594b1c2be","title":"An Effective Non-Autoregressive Model for Spoken Language Understanding"},{"paperId":"e3d06054af531ee2f42270d43100b309c28546ef","title":"MUSIQ: Multi-scale Image Quality Transformer"},{"paperId":"564c7e51f930b208cf05f68a8c478d1195ccad05","title":"Learning Fair Face Representation With Progressive Cross Transformer"},{"paperId":"79678d2f10bddf14b2aedf3427f8a4c39908931f","title":"Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"2a52e0bb16638df4216cf63ea9708891741e565e","title":"Multi-Branch with Attention Network for Hand-Based Person Recognition"},{"paperId":"5019fc4c08177fb70045aa0ce1edd08099cbe1ec","title":"An end-to-end CNN with attentional mechanism applied to raw EEG in a BCI classification task"},{"paperId":"a5c41f188b0eb0acb444cb4899bf6af378ee9ede","title":"CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"},{"paperId":"17b4a75b432b9f58de143918608de9234a4da988","title":"Towards Continual Entity Learning in Language Models for Conversational Agents"},{"paperId":"a9c214e846188adb645021cd7b1964b8ea1fef6f","title":"Rethinking and Improving Relative Position Encoding for Vision Transformer"},{"paperId":"ae30c7199df1991de6a90508d40c593bfee760e0","title":"CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation"},{"paperId":"4159a98819c10c2710f68d239fcc03bdb7ece472","title":"Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation"},{"paperId":"a514cc9796034bcc01450ac968b2f576fa35c70f","title":"Multi-Scale Local-Temporal Similarity Fusion for Continuous Sign Language Recognition"},{"paperId":"87e823d2cb58e741230c0fa3b83f3459c7e32241","title":"PiSLTRc: Position-Informed Sign Language Transformer With Content-Aware Convolution"},{"paperId":"fcf0cf8510e19fec46256c0c472665766fcdea7f","title":"Residual Tree Aggregation of Layers for Neural Machine Translation"},{"paperId":"2cac91a6022af7ba5fd6cb1b5ff6ebb815880bb4","title":"Text classification using improved bidirectional transformer"},{"paperId":"a4cc58555e2459168a8472b35d159151f6d90f30","title":"A Structural Transformer with Relative Positions in Trees for Code-to-Sequence Tasks"},{"paperId":"a5896974ae46a3232419e93b5f7522b92995003e","title":"Semantically Constrained Document-Level Chinese-Mongolian Neural Machine Translation"},{"paperId":"9058d322a09bfc0c93a070f87cac8fd840e63088","title":"From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers"},{"paperId":"485fb51d8e2e579d42a1f9be45b1806519a6ad8c","title":"DeepMutants: Training neural bug detectors with contextual mutations"},{"paperId":"0b036cd5dfc49d835d0c759c8ca31d89f2410e65","title":"CMT: Convolutional Neural Networks Meet Vision Transformers"},{"paperId":"1306680402070eecd37da6bb0d531017166c5f80","title":"The Piano Inpainting Application"},{"paperId":"00a92e580cb4801fc32f2ed8cdb215bfb9c1a575","title":"Conformer-based End-to-end Speech Recognition With Rotary Position Embedding"},{"paperId":"fc19e94109d4c2f05f3639a67327c708543def98","title":"Locally Enhanced Self-Attention: Combining Self-Attention and Convolution as Local and Context Terms"},{"paperId":"059783a60339601faabf8f9ec739fc2da89dff56","title":"Tool wear prediction based on multidomain feature fusion by attention-based depth-wise separable convolutional neural network in manufacturing"},{"paperId":"60df6300c2810d417152d0b66d8e76bb26a8e792","title":"UniSpeech at scale: An Empirical Study of Pre-training Method on Large-Scale Speech Recognition Dataset"},{"paperId":"d3f51870f4da5dd9c2a08a55cfa8a380b8d49208","title":"Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation"},{"paperId":"3d6315f836c6ab33320df965c5504c95bd7c4a35","title":"ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data"},{"paperId":"8dfa41ada085940f3e4cb2af3eb7ba6fe1082a41","title":"The NiuTrans End-to-End Speech Translation System for IWSLT 2021 Offline Task"},{"paperId":"fbac64d617914ab8dac0682639fbd6012faed771","title":"Rethinking Positional Encoding"},{"paperId":"8c7aa01a5d57082474ed9188376eeea82608a3c0","title":"Investigation of Practical Aspects of Single Channel Speech Separation for ASR"},{"paperId":"fbc34e0bac913acf81bebfc1be2909465543f49b","title":"Relative Position Representation over Interaction Space for Natural Language Inference"},{"paperId":"1bed382373aed687c045bb65bc7541b16fc7a6be","title":"Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN"},{"paperId":"4cfaea33c6cd6e72d843e808cf7c69387e100e7b","title":"Exploiting Positional Information for Session-Based Recommendation"},{"paperId":"6d3558a70490ffa25393996a7c09cd439aaaede2","title":"Polarized Self-Attention: Towards High-quality Pixel-wise Regression"},{"paperId":"800cfb3d23115cdcd4d114234b65bbdf2080f798","title":"CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"},{"paperId":"a255b4674ecac416eb37d9441e1cda2fed147b0c","title":"StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR"},{"paperId":"d1919dc2e2fad6c32caa85da4a3a40372c1d9d33","title":"MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity"},{"paperId":"d645bd08fc19d52164695f9cd5ae863345459a06","title":"AutoFormer: Searching Transformers for Visual Recognition"},{"paperId":"034869f2f55b01f240b30923983ea197ff9fc32c","title":"FastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis"},{"paperId":"cf444d60da03f6a4dbabeb44186b80de1a717db1","title":"Interpreting Depression From Question-Wise Long-Term Video Recording of SDS Evaluation"},{"paperId":"36de6b3b18ea763619bdca2d39035a8adf1582d2","title":"Language Models are Good Translators"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"663251e39e301bb9a8bdaedcc9ab2489e1be7ef3","title":"Semi-Supervised Representation Learning for Segmentation on Medical Volumes and Sequences"},{"paperId":"a76d5f07caad631c196e109cc1052b85d45fb83d","title":"Learning Explainable Representations of Malware Behavior"},{"paperId":"0d508600d77d8a7e6a655cdb6d139779732f649f","title":"Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"},{"paperId":"0d46cbaf914da31a06ef2753e00b7f47e055e70d","title":"Probabilistic Attention for Interactive Segmentation"},{"paperId":"34ff62922161332cef417af9b470b77a7225a477","title":"Exemplars-Guided Empathetic Response Generation Controlled by the Elements of Human Communication"},{"paperId":"7cf872dbad1cd780a2ebf56f8bee76fb9497c018","title":"ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction"},{"paperId":"b99c61f6957c1b04ec1376b74f82dd1e83559695","title":"JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"df59d0098c1b2c1ee8995da802dd6b12d158c2b8","title":"Large-scale chemical language representations capture molecular structure and properties"},{"paperId":"f0993e68c76d940763ef7f8106db86cdc97b3a49","title":"Multi-head or Single-head? An Empirical Comparison for Transformer Training"},{"paperId":"e7473d3a44ad252b6bbd98c2f240498921fa4c87","title":"Learning to Combine Per-Example Solutions for Neural Program Synthesis"},{"paperId":"bcd53292bb2999a0e0a8aa7307f760a827d7f296","title":"Structure-Regularized Attention for Deformable Object Representation"},{"paperId":"92bf1c069747374fbc3efb55e7a916a3e2d736da","title":"Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"},{"paperId":"5d7d34abbc14739e40b53ec3c33a3c698a37e70e","title":"To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs"},{"paperId":"0a12ea86c9193bf2496d65d25d1f234c1fac923b","title":"Zero-Shot Controlled Generation with Encoder-Decoder Transformers"},{"paperId":"1dbb523a6555d6e0c5727620e2b57daaa5b79dc0","title":"Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models"},{"paperId":"a6337d9ebb0b7de84588806110157806f9c0383b","title":"GraphiT: Encoding Graph Structure in Transformers"},{"paperId":"de282eff05d89f9fb53d1585c1377c4f0499ee12","title":"Transformed CNNs: recasting pre-trained convolutional layers with self-attention"},{"paperId":"47ae807cd511b35e78a2cd4e198283dea6dafd41","title":"Do Transformers Really Perform Bad for Graph Representation?"},{"paperId":"9f4b69762ffb1ba42b573fd4ced996f3153e21c0","title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes"},{"paperId":"b50815251c948f00baedccaf5f56c281ffa7650f","title":"Staircase Attention for Recurrent Processing of Sequences"},{"paperId":"d8d2e574965fe733eb1416e03df2b5c2914fc530","title":"A Survey of Transformers"},{"paperId":"5ee4108f56e20398361d43420b5bee6223901d21","title":"Image super-resolution via channel attention and spatial attention"},{"paperId":"95f5bafba97beb9b4f8c1fe607f04ec28efab7f9","title":"Learning to Efficiently Sample from Diffusion Probabilistic Models"},{"paperId":"576c462dbc1f3d732b919ef1daac37a817123e52","title":"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias"},{"paperId":"2d98048c2d2fcd3f6b989d2a54003808906ab4b7","title":"Efficient Training of Visual Transformers with Small Datasets"},{"paperId":"31b14acb6caa45cd0a0f28a3cc9834819139611a","title":"Image super-resolution via channel attention and spatial attention"},{"paperId":"2835951fabf12804e17d5a525b2be2bee70e7910","title":"Uformer: A General U-Shaped Transformer for Image Restoration"},{"paperId":"7509c66a666e2e3f14bc8676b969b945ee6e136f","title":"CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"},{"paperId":"94576783bc73bf55a0091203a3d45a0a4665a1ae","title":"Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding"},{"paperId":"e7e626e379e67b8ae59689fa1c055ab83a5b2313","title":"Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction"},{"paperId":"27c09d07f1a7dba51889fef9ee8f925c87fb6b00","title":"X-volution: On the unification of convolution and self-attention"},{"paperId":"93892c5bbc489ee7bccb1b97eed92465013ea826","title":"Scalable Transformers for Neural Machine Translation"},{"paperId":"ab0ef8e705fa88f079a055842ba20b4f413a69db","title":"Associating Objects with Transformers for Video Object Segmentation"},{"paperId":"d8e7bad2681ce70277c900c77a22181d4b03d705","title":"The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"},{"paperId":"4dc2c552bfb0abcf15db9cfe795da9e97551ee42","title":"An Improved Model for Voicing Silent Speech"},{"paperId":"bc528b7af23199c660481ee8d452b900705c3c8f","title":"Complementary spatiotemporal network for video question answering"},{"paperId":"e4a3ef32779313f30e299eda9b2660943b5eeed1","title":"Complementary spatiotemporal network for video question answering"},{"paperId":"50db74aa7e662b640ccbf37788af62cd8af3e930","title":"LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations"},{"paperId":"806dc5c64d3a65f89e0f26ff9f51bb029c6908b2","title":"A Multi-Level Attention Model for Evidence-Based Fact Checking"},{"paperId":"f4b20bd6a7ec7c57f8afc2228bcfd00a7346197c","title":"Smart-Start Decoding for Neural Machine Translation"},{"paperId":"7af513e8a395b82628dbf627fe9269a44cea14ae","title":"Does Structure Matter? Encoding Documents for Machine Reading Comprehension"},{"paperId":"0e09c6b34b34a2ca3eb16e8ebfbe0c2472666206","title":"Enhancing Transformer with Horizontal and Vertical Guiding Mechanisms for Neural Language Modeling"},{"paperId":"9983dd0b93d268c31cc821b9f4071372d191000e","title":"TuringAdvice: A Generative and Dynamic Evaluation of Language Use"},{"paperId":"77366bef01df1ab277149b330336a0ef9c5041c4","title":"Transformer"},{"paperId":"1079b5459291ec73fc0a2d0692d38eaa97cb41a6","title":"StyTr2: Image Style Transfer with Transformers"},{"paperId":"8c4f89a9ac30cf94186916be1bfaa02dbfb3600d","title":"CoDesc: A Large Code–Description Parallel Dataset"},{"paperId":"685ca8238989d93a4c196abc6627bb960967d786","title":"Learning to Extend Program Graphs to Work-in-Progress Code"},{"paperId":"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages"},{"paperId":"50be8476315d8e520313f21dbb9908b47a91c3d1","title":"Explainable enterprise credit rating using deep feature crossing"},{"paperId":"08ffdec40291a2ccb5f8a6cc048b01247fb34b96","title":"Relative Positional Encoding for Transformers with Linear Complexity"},{"paperId":"9f863fcb229e762a95b8bb0e4a89d7aeeb3d8640","title":"Link Prediction on N-ary Relational Facts: A Graph-based Approach"},{"paperId":"f07c5c540233b22f0ca154c80c713e2aed3c9606","title":"Symbolic Music Generation with Transformer-GANs"},{"paperId":"a3196e65467b80f4755968923b382e40c02ccb51","title":"Two-Stream Convolution Augmented Transformer for Human Activity Recognition"},{"paperId":"52d01d9f71caf0d021fccb75b75b7d3dfc7460f5","title":"On Scalar Embedding of Relative Positions in Attention Models"},{"paperId":"d82a03951796a352dc4442387782c21d2b761480","title":"GTA: Graph Truncated Attention for Retrosynthesis"},{"paperId":"b8cee43a51c44f8f4448e78e41ecf081987707cf","title":"Towards Robust Vision Transformer"},{"paperId":"50a9ff8b1a3f49220baa0950bc4645ad6f88f013","title":"HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding"},{"paperId":"10d9c1f66742ac68ea8cd13f5c0a3968b3771c54","title":"GCN-MIF: Graph Convolutional Network with Multi-Information Fusion for Low-dose CT Denoising (preprint)"},{"paperId":"64a29bee2e1ad29547d590a3cc26274f4c537145","title":"Not All Memories are Created Equal: Learning to Forget by Expiring"},{"paperId":"95080f0ee0a2d4fcb9fd330bf4060870849a8b47","title":"Global Structure-Aware Drum Transcription Based on Self-Attention Mechanisms"},{"paperId":"fda805c6e85a03d10549acdc5489420ca8f3d405","title":"MuseMorphose: Full-Song and Fine-Grained Piano Music Style Transfer With One Transformer VAE"},{"paperId":"aecc84aa9ff29a99f44556689592cd0fff84cb87","title":"Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue StateTracking"},{"paperId":"6a3e13d7926a4aaa0ddcd3acc7c08e8d24c330e5","title":"ReadTwice: Reading Very Large Documents with Memories"},{"paperId":"61cce75554a6d1bb802f26758c3b0ba97de6918d","title":"Graph Attention Networks with Positional Embeddings"},{"paperId":"7f066a5fb06c0abe09ce67e9e420868d49ea47ad","title":"Duplex Sequence-to-Sequence Learning for Reversible Machine Translation"},{"paperId":"a83902f8b3aadfda633968a840ca1738bedef837","title":"Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs"},{"paperId":"5213e56878c0df2f6f8f59c1d412daf98de1d8bf","title":"COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 prediction"},{"paperId":"b18261a2b718cf29a87cb0237592a4725d5e0a6f","title":"AGMB-Transformer: Anatomy-Guided Multi-Branch Transformer Network for Automated Evaluation of Root Canal Therapy"},{"paperId":"01fe50c8023f15bce3628d4581f59bfe33bcdf16","title":"Incorporating Transformer and LSTM to Kalman Filter with EM algorithm for state estimation"},{"paperId":"a3c50f601a6b872751556a4324d29cd10db1704f","title":"GasHis-Transformer: A multi-scale visual transformer approach for gastric histopathological image detection"},{"paperId":"6dd0d7749691ea5f9d826fd7e8b3680d16b07427","title":"Score-Transformer: A Deep Learning Aid for Music Composition"},{"paperId":"32e6c2b2dda7b92d2425998e6e71a57c9bbaabec","title":"MVS2D: Efficient Multiview Stereo via Attention-Driven 2D Convolutions"},{"paperId":"2c9fdba6bf846e0986cbbf30d56b467d9e334333","title":"ConTNet: Why not use convolution and transformer at the same time?"},{"paperId":"52f79cb91dd3149960dd5d436d8746441fbffce4","title":"Optimizing small BERTs trained for German NER"},{"paperId":"1841d235095ba022f1463f0ac2436e29601829e4","title":"Neural Machine Translation for Harmonized System Codes prediction"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"ceb1a7e4bccb14b048e81e023b3219edca08862b","title":"Deep drug-target binding affinity prediction with multiple attention blocks"},{"paperId":"db46b0de44c5113c47f0ec5392eb91d0726497bf","title":"A Simple and Effective Positional Encoding for Transformers"},{"paperId":"eaa9fde5756852745e7e14383794ba75f797cc8b","title":"Question Decomposition with Dependency Graphs"},{"paperId":"1abd71fb70b8c3639af1d087cd179eaef8c718b0","title":"MIMO Self-attentive RNN Beamformer for Multi-speaker Speech Separation"},{"paperId":"286d2e0f3d882a37f486623c716d8a54a4a58fdc","title":"Dynamic Graph Neural Networks for Sequential Recommendation"},{"paperId":"999d78a048ae723be2787e25841863b725665e9c","title":"Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling"},{"paperId":"5b68522f58b61e7235b852677337ef3725075fd9","title":"Co-Scale Conv-Attentional Image Transformers"},{"paperId":"d27eac86c86a953a5b1ad13f7c7bc9d5fb127837","title":"Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN"},{"paperId":"5db0b92418dfe1f44a9440c682fae05b77bc6587","title":"Nonparametric analysis of inter‐individual relations using an attention‐based neural network"},{"paperId":"19be83b770b96905288fa47dd882665a4c64bb45","title":"Estimating articulatory movements in speech production with transformer networks"},{"paperId":"c114db5f1c38cbe6797bc74ef98072cac71f6cc6","title":"ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser"},{"paperId":"dcd0cb3efe3a21c9b5240a5c4e1f1fdd98200b7c","title":"Context-Self Contrastive Pretraining for Crop Type Semantic Segmentation"},{"paperId":"b43b69a6d303fe3c349d5ecf5920ff1dd47ec650","title":"Dual self-attention with co-attention networks for visual question answering"},{"paperId":"e12e837cb2e9baeaefdcab06fe1c75add8f46389","title":"Effective gene expression prediction from sequence by integrating long-range interactions"},{"paperId":"17a2834c6540703fd35edde4e5f7cc50da4ab41f","title":"MuSLCAT: Multi-Scale Multi-Level Convolutional Attention Transformer for Discriminative Music Modeling on Raw Waveforms"},{"paperId":"319d6c88152780f2835687100921393457668cc0","title":"ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation"},{"paperId":"5b81580712f6e16abd05824c162537674e99b095","title":"Learning Neural Representation of Camera Pose with Matrix Representation of Pose Shift via View Synthesis"},{"paperId":"003326a15fc4a8833785a47a741d7712474fa256","title":"LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference"},{"paperId":"44930df2a3186edb58c4d6f6e5ed828c5d6a0089","title":"Attention, please! A survey of neural attention models in deep learning"},{"paperId":"c478ad9eb6ffd16f3c63cfc942fd77081fbb6c3f","title":"TDJEE: A Document-Level Joint Model for Financial Event Extraction"},{"paperId":"92255f5be9f254057f809943398c156808ef47eb","title":"Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention"},{"paperId":"bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1","title":"A Practical Survey on Faster and Lighter Transformers"},{"paperId":"712bf9c7202b8dec3d06491a380bbac9c9600fbc","title":"Mask Attention Networks: Rethinking and Strengthen Transformer"},{"paperId":"6b9564d94aa4415b90561b430ac85b0ee11a5833","title":"NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction"},{"paperId":"91e8117e7ebc966bc76de2cb52ec717d2acdb1a4","title":"Scaling Local Self-Attention for Parameter Efficient Visual Backbones"},{"paperId":"05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14","title":"Language-Agnostic Representation Learning of Source Code from Structure and Context"},{"paperId":"c9963e89add5e02bf6e5b340586ee6406e087089","title":"API2Com: On the Improvement of Automatically Generated Code Comments Using API Documentations"},{"paperId":"5dc1e65fdad52b1847c49fae1dbc6a1ba8ac232f","title":"Neural Machine Translating from XML to RDF"},{"paperId":"cfb5eb84d6956de1207c18bf27137affbafb6141","title":"Question-relationship guided graph attention network for visual question answer"},{"paperId":"395221cd3ff22539f261ef1fc305fa3e928fca35","title":"Unsupervised Transfer Learning in Multilingual Neural Machine Translation with Cross-Lingual Word Embeddings"},{"paperId":"6a66cf7dd6d1f70e99018518e765917fb76491ad","title":"Variable-rate discrete representation learning"},{"paperId":"483b7857df6104d5c8899c61c64ab0397f87f7fa","title":"I2Net: Mining intra-video and inter-video attention for temporal action localization"},{"paperId":"41bc8ad57ae622a88de5611c4933a77a5cc5cdcc","title":"Symbolic Integration by Integrating Learning Models with Different Strengths and Weaknesses"},{"paperId":"e369e6a871eb9ca85cc41ff309631f6da2a79c2b","title":"A Survey on Document-level Neural Machine Translation"}],"references":[{"paperId":"33998aff64ce51df8dee45989cdca4b6b1329ec4","title":"Graph Attention Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"2cd8e8f510c89c7c18268e8ad51c061e459ad321","title":"A Decomposable Attention Model for Natural Language Inference"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e","title":"End-To-End Memory Networks"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"8fd61ae673e79de6723f800e06b38b2bda1dc3db","title":"Convolutional Sequence to Sequence Learning"}],"id":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","summary":"This work presents an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements, on the WMT 2014 English-to-German and English- to-French translation tasks."},{"url":"https://www.semanticscholar.org/paper/81b4920ad488affaee27389ff9540b7fea90a4ce","title":"“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding","venue":"Conference on Empirical Methods in Natural Language Processing","year":2019,"referenceCount":36,"citationCount":145,"influentialCitationCount":18,"publicationDate":"06/09/2019","authors":"Ben Zhou,Daniel Khashabi,Qiang Ning,D. Roth","citations":[{"paperId":"8d3d792c28ad8b72899193449a2603efec24275d","title":"Temporal Validity Change Prediction"},{"paperId":"4318e4ab5e2f5e2a50469aa043fe66c0744370a4","title":"Catwalk: A Unified Language Model Evaluation Framework for Many Datasets"},{"paperId":"6b97aa78bcdb88548c44e7e1671c0ed37ed37976","title":"Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"},{"paperId":"f37d1ef3c4fd85f608439d239306a3b3302e3add","title":"TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models"},{"paperId":"066dc4d3550dce456856344acb1434a5ef46ac5d","title":"Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning"},{"paperId":"5b9bf4a82da690e738821ac0460b96c2770ed5dd","title":"Are Large Language Models Temporally Grounded?"},{"paperId":"ec5f1c1bc361ca4c53932e3a3a14d6f3f814e268","title":"MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document"},{"paperId":"8dd279a7efc611906d5547e29eb19fc25a5505fd","title":"Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation"},{"paperId":"23cc6b2ed88872fcd3767cf054100e8eddcdb0a1","title":"CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks"},{"paperId":"1f57b86940be99190f009aa7ebe450196501b181","title":"Commonsense Temporal Action Knowledge (CoTAK) Dataset"},{"paperId":"e707c2e1d15e82b4eb796080a122a92877ca43ea","title":"How Much Consistency Is Your Accuracy Worth?"},{"paperId":"6d021c63331c4ff4e6673f9920f86eb37d1dd854","title":"Instructive Dialogue Summarization with Query Aggregations"},{"paperId":"811f451f1991ec5508e67d00375ca4f5d05e0eeb","title":"TRAM: Benchmarking Temporal Reasoning for Large Language Models"},{"paperId":"11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"},{"paperId":"4ed50b9ff154f2725503306f3cf0a21e2681838f","title":"Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that Don't have a Definitive Answer?"},{"paperId":"cb1b43d25f18ad37d7ca1110dcdb87287b4186c6","title":"An Overview Of Temporal Commonsense Reasoning and Acquisition"},{"paperId":"60b4b7514ac3a3d9fbebf10f935dd89267c93621","title":"SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning"},{"paperId":"24a285b1e7ee9acfec9a82f7123563b62f532203","title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning"},{"paperId":"de5e0a70af551df17e693223d29a1840900d6b74","title":"Mitigating Temporal Misalignment by Discarding Outdated Facts"},{"paperId":"05bfdd1c7280d2dcd794d1ba4b83dcaa0b8b5376","title":"Few-shot Unified Question Answering: Tuning Models or Prompts?"},{"paperId":"a7307613447a828fc0fa2d233311b6f62fbadfb4","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents"},{"paperId":"f576288b7f5ca4aff3a6492a9db1db5148b7616f","title":"Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?"},{"paperId":"05526b42336c74298ca4ccbcb792107900574062","title":"Prompting with Pseudo-Code Instructions"},{"paperId":"d3d8f37a809a0e3efe1cf6419033ed36424ffc8a","title":"ReTAG: Reasoning Aware Table to Analytic Text Generation"},{"paperId":"6cec825e32b1790a69893a5b2506818241506217","title":"A Glimpse in ChatGPT Capabilities and its impact for AI research"},{"paperId":"f3f608d3296c85edb89db1ac625ee5ab9a7f801d","title":"Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering"},{"paperId":"367e0d715ef66c5aea8024e0a97cfa48e29d52c3","title":"Event Knowledge Incorporation with Posterior Regularization for Event-Centric Question Answering"},{"paperId":"4d7571441f507f39133209e8afa7ad088da2199c","title":"ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models"},{"paperId":"24ab6356b355b29a3770db56dd1f2200cdd987fa","title":"Salient Span Masking for Temporal Understanding"},{"paperId":"746bb45433f6b24d3ae64d6cd51c4e9d00a0ffa7","title":"Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"9af3b6b3f8dcedbb02b88936c428e1cd02503a8a","title":"Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?"},{"paperId":"2ef41c8d42959c13ef4752c63bdee07416872236","title":"What’s New? Identifying the Unfolding of New Events in a Narrative"},{"paperId":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey"},{"paperId":"fd9c6baf8e485f13285777d8ec24da2359194461","title":"tieval: An Evaluation Framework for Temporal Information Extraction Systems"},{"paperId":"ee153a2c91d36b034dc86c945aee9859b79da812","title":"Test of Time: Instilling Video-Language Models with a Sense of Time"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"22d314c347a6c5c0bbf528a102455229ce0b36b5","title":"Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review"},{"paperId":"f3b41224f1153783a32e0d317cc107b23e92f6c4","title":"Effective Masked Language Modeling for Temporal Commonsense Reasoning"},{"paperId":"1d417bdd331912a458de920459f23fcc7f6e8699","title":"Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts"},{"paperId":"bb15f3727f827a3cb88b5d3ca48415c09b40a88f","title":"What Language Model to Train if You Have One Million GPU Hours?"},{"paperId":"1a5f48161df983a0e9485425495121201902433b","title":"DiscoSense: Commonsense Reasoning with Discourse Connectives"},{"paperId":"868f9bb603dfa8f6951787040fc6d62c909a15c2","title":"JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions"},{"paperId":"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","title":"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"285d13bf3cbe6a8a0f164f584d84f8b74067271f","title":"Towards Faithful Model Explanation in NLP: A Survey"},{"paperId":"6923b2dd4f2a691496a6931c59150189cd496d76","title":"A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of Computational Approaches"},{"paperId":"2a89c7cf5f8e8014dbd743d0965ba83b1f67f137","title":"ILLUME: Rationalizing Vision-Language Models through Human Interactions"},{"paperId":"15bacb240e2598457af4ded3039b6988aa9706f0","title":"Few-shot Adaptation Works with UnpredicTable Data"},{"paperId":"4e6f63dc99c09560991cf89207f9fa12c739e711","title":"Can Language Models perform Abductive Commonsense Reasoning?"},{"paperId":"e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP"},{"paperId":"1da214f8f265445b5997f5d677452819b334bdfb","title":"Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts"},{"paperId":"7890ece03cfb88e0620f8e791105569bd7128c76","title":"Housekeep: Tidying Virtual Households using Commonsense Reasoning"},{"paperId":"6e5944b759ef1c2ea2f694221cf54ef718ec95ee","title":"Reasoning about Procedures with Natural Language Processing: A Tutorial"},{"paperId":"078f4efd448822b0e25d3ee0aec842ced606a595","title":"Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts"},{"paperId":"81986b8a3d3fe6c5be06fc4527953fb514ad12e8","title":"Improving In-Context Few-Shot Learning via Self-Supervised Training"},{"paperId":"3a6a97a50695d43d95a015bbb554b2bc0d40394e","title":"Don’t Blame the Annotator: Bias Already Starts in the Annotation Instructions"},{"paperId":"659f3a959e59eb0e5382143417e7d9d2667d2fff","title":"Extracting seizure frequency from epilepsy clinic notes: a machine reading approach to natural language processing"},{"paperId":"97f456643712e9618edd7465676c62af3c8ae690","title":"A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models"},{"paperId":"7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"},{"paperId":"c3a454e50ec0610f1380d55b1988a5eb5d45207b","title":"Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization"},{"paperId":"47df3fd32d00220c85c2c51a571254fd99b2ecc7","title":"MetaICL: Learning to Learn In Context"},{"paperId":"d83c7aa12420d5d035f43d7737bfa6893e9e2c61","title":"Exploring Universal Intrinsic Task Subspace via Prompt Tuning"},{"paperId":"174a0e6da0dfb7f96d4a0a4076eed154c439e41a","title":"Probing Language Models for Understanding of Temporal Expressions"},{"paperId":"3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b","title":"Reframing Instructional Prompts to GPTk’s Language"},{"paperId":"2375e447e86f52a35655c3069bcb047e6655eeb8","title":"Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning"},{"paperId":"5f6a790722f18f0aa7419f8e0c6404c46acd99ba","title":"Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding"},{"paperId":"b1a71d8fb272016d618917a3fc392551fa941c81","title":"Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers"},{"paperId":"f977d79980ed2dfc7b2194fe680895e49b3b60a9","title":"From LSAT: The Progress and Challenges of Complex Reasoning"},{"paperId":"32c1767b956dec644f8bfc1f009fc5a1af47f0cc","title":"Discourse-Level Event Temporal Ordering with Uncertainty-Guided Graph Completion"},{"paperId":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"},{"paperId":"ac8d33e4c0a45e227a47353f3f26fbb231482dc1","title":"Time-Aware Language Models as Temporal Knowledge Bases"},{"paperId":"62953ca1252c9febe07c7007a10911726f37792d","title":"TIMEDIAL: Temporal Commonsense Reasoning in Dialog"},{"paperId":"6597d61bdb531051678c773526758a6dc113b9ce","title":"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences"},{"paperId":"b39f76381897ab744250673a058e5dd06c009813","title":"Event Time Extraction and Propagation via Graph Attention Networks"},{"paperId":"4ca39cf99747b8962fe37e7e025e284872df3425","title":"Comparing Test Sets with Item Response Theory"},{"paperId":"d3a1e7f060bfe7d97c2e430da9eb6967bbbe358a","title":"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"},{"paperId":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions"},{"paperId":"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"},{"paperId":"2ef4be35f8424ea768aa2e1b44392b3eddbc780b","title":"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema"},{"paperId":"c7f977f556d2060238fdc1286d057d46958afaf9","title":"ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations"},{"paperId":"c5943bc4b22a63f595cb1c2823a449e03aad4787","title":"AR-LSAT: Investigating Analytical Reasoning of Text"},{"paperId":"7a9ef45ec4a6fe04c373715f455924b6c1c51ddd","title":"DAGN: Discourse-Aware Graph Network for Logical Reasoning"},{"paperId":"85084fbba9965bdf3446ede255041793f2aa4545","title":"Decomposing and Recomposing Event Structure"},{"paperId":"54bb329be4b557d38e0628b651e7074524d35be2","title":"LOME: Large Ontology Multilingual Extraction"},{"paperId":"d6c919ee0d51432496513ef4b6b2dbd128819779","title":"Microtask Detection"},{"paperId":"33b06c74eea3f400b6f5ef14ef163aef1db42d16","title":"Conditional Generation of Temporally-ordered Event Sequences"},{"paperId":"6eee69031d2e11aa03a5a8fcb219cff4562863be","title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning"},{"paperId":"9793b07ba09d9f2ac9cabd8117daa93bf3db4346","title":"DEER: A Data Efficient Language Model for Event Temporal Reasoning"},{"paperId":"d865c87f6ce4e0be29ae1e3780fae66b8034d04b","title":"TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation"},{"paperId":"f2143dd2a2c9c43d9d28462c1cd2a51f5f3db2c6","title":"Improving Event Duration Prediction via Time-aware Pre-training"},{"paperId":"a0e7d24f91da6a1be5f8856963e3b160d47ca7b7","title":"Towards Zero Shot Conditional Summarization with Adaptive Multi-task Fine-Tuning"},{"paperId":"3d2035edd4dd48e1e638279409e11bf689c461e1","title":"Temporal Reasoning in Natural Language Inference"},{"paperId":"9d436d25981ea61db728bd490f0d54376d08953e","title":"Temporal Reasoning on Implicit Events from Distant Supervision"},{"paperId":"6bd8c038fdcae6cc155f27bcca9acdf04c06c786","title":"Deriving Commonsense Inference Tasks from Interactive Fictions"},{"paperId":"f642ddb69f712e2b517cb6c6d9a062506991d1ed","title":"Commonsense Learning: An Indispensable Path towards Human-centric Multimedia"},{"paperId":"5dfc43bb697acf5eacf8b8a05d78dba8beb0dd42","title":"Paragraph-Level Commonsense Transformers with Recurrent Memory"},{"paperId":"65906e6027246ae9e4ecd18d6e019a24505c842e","title":"Aligning AI With Shared Human Values"},{"paperId":"45f952c21130d655090058e864c2772358a1de72","title":"Commonsense Reasoning for Natural Language Processing"},{"paperId":"5f4a70c0abeafa4ecff90739bd8efbe05ea3bac5","title":"Applying the T5 language model and duration units normalization to address temporal common sense understanding on the MCTACO dataset"},{"paperId":"d075301119b79e5b6b9306f9981d256191882981","title":"Adversarial Training for Commonsense Inference"},{"paperId":"07c1c2429b63fefdae41eb546c31b40de2a880f7","title":"INFOTABS: Inference on Tables as Semi-structured Data"},{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"30602e3382df3abedb5f225b55b7efce8580f74d","title":"ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data"},{"paperId":"5cc4fa603085753610f18c1429b383c62cbed043","title":"ForecastQA: Machine Comprehension of Temporal Text for Answering Forecasting Questions"},{"paperId":"36d6c8895bbc755964b8b2136c6fd6087a7af089","title":"TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions"},{"paperId":"9808d59113029d96f48a0376b1578dbab5427bb4","title":"Unsupervised Commonsense Question Answering with Self-Talk"},{"paperId":"35e6783307f82d1faa39be0653431305abec7271","title":"Evaluating Models’ Local Decision Boundaries via Contrast Sets"},{"paperId":"9fec5868542b4d9070306f1418d1d21666226e90","title":"Evaluating NLP Models via Contrast Sets"},{"paperId":"331a87762228ecca2eebf309b859189f033414ba","title":"NarrativeTime: Dense Temporal Annotation on a Timeline"},{"paperId":"287ed28f1e0f56ff56834580a1bf6720daeb6b2b","title":"Reasoning-Driven Question-Answering for Natural Language Understanding"},{"paperId":"4043a936960de8e149dc208178fe1bcb157c7fa4","title":"Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches"},{"paperId":"64ad3e1fbbc9a4bec2a414fb31b05ee9a62d50cb","title":"tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation"},{"paperId":"c059d251d8f0c2491892271eb40ea3cec4aea830","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue in Multiple Domains"},{"paperId":"a884bc8bc5b04e5ad096649856df5b7931fd3d23","title":"Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer"},{"paperId":"5ecac123c447d1f2e87d7780882df239dd93dca4","title":"Task duration prediction from a textual description"},{"paperId":"a5584d2d9b0de9e1692241d46d0c70942919cd60","title":"Answer-level Calibration for Free-form Multiple Choice Question Answering"},{"paperId":"624c6580cf5abd31db63b38596ffa3622c731b00","title":"Generating Temporally-ordered Event Sequences via Event Optimal Transport"},{"paperId":"a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b","title":"Good Night at 4 pm?! Time Expressions in Different Cultures"},{"paperId":"d29faea4f03cfe337a56dabce84b2d5ce7e473f5","title":"Attention-Focused Adversarial Training for Robust Temporal Reasoning"},{"paperId":"b672d2ec81c9395c312d57a27931864d07664592","title":"A Meta-framework for Spatiotemporal Quantity Extraction from Text"},{"paperId":"b0291bee1d532e8dc082753329d2579549100479","title":"Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts"},{"paperId":"7db7fb25a8753c9efc6b3722d178f94fcc1f82d3","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP"},{"paperId":"190d5823c9405a34ccdfcf6591a54d5a0bcf3f3c","title":"Improving Event Duration Question Answering by Leveraging Existing Temporal Information Extraction Data"},{"paperId":"82c6f5ffd4d2d43ae7684601f607eae26a759a5a","title":"Analytical Reasoning of Text"},{"paperId":"b3f1f0d6bdc9504a17747b9ae49136b3fe41051a","title":"ILLUME: Rationalizing Vision-Language Models by Interacting with their Jabber"},{"paperId":"efaee35df6cb22caa6988a26159b7959f42b14ee","title":"Are Visual-Linguistic Models Commonsense Knowledge Bases?"},{"paperId":"52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"8b0b5b998d65add5c036b45c36afa3f9df96f4b0","title":"Toward Building a Language Model for Understanding Temporal Commonsense"},{"paperId":"b8768fbba4d356a1c49dcd8e4cd33aea0c6472bc","title":"How about Time? Probing a Multilingual Language Model for Temporal Relations"},{"paperId":"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","title":"20Q: Overlap-Free World Knowledge Benchmark for Language Models"},{"paperId":"655db2fd59e4112d0a5596e1695c5765457ca53c","title":"Extraction of Common-Sense Relations from Procedural Task Instructions using BERT"},{"paperId":"7b486a6eac4b46cf39e41c97b25ea22c5d27a883","title":"Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions"},{"paperId":"5d1e996bb64a0d082c85010b9c78036db4da02e0","title":"GCRC: A New Challenging MRC Dataset from Gaokao Chinese for Explainable Evaluation"},{"paperId":"e6d84cf9ae6efa10919bff765613e883a761db62","title":"Open Temporal Relation Extraction for Question Answering"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"8b383bd2eb054787d84636dab7b6ecf4a318c6c7","title":"Towards a Language Model for Temporal Commonsense Reasoning"},{"paperId":"21f018727ce0fa0f932188fa5abcc32426aba7b6","title":"On End-to-end Automatic Fact-checking Systems"},{"paperId":"9485076ed295c6997b8a3148e1f37156eff97813","title":"Research Statement: Climbing the Generality Ladder in NLP"},{"paperId":"65017016b35928cd0e2ab0037200d0f92619b120","title":"Low-resource Learning with Knowledge Graphs: A Comprehensive Survey"},{"paperId":"00dea030e47950e55d6497a5c8acf5053baebcbd","title":"ALICE++: Adversarial Training for Robust and Effective Temporal Reasoning"},{"paperId":"177bc611389fac0e6239355a2d9eaa66f7aac53e","title":"Improving Unsupervised Commonsense Reasoning Using Knowledge-Enabled Natural Language Inference"},{"paperId":"23446cc15226c46eae9e4414598f7375eacb6ea1","title":"Back to Square One: Bias Detection, Training and Commonsense Disentanglement in the Winograd Schema"},{"paperId":"35115a7e16fb658a5f8e09eeb60a5f3204b24598","title":"Tell Me About Your Day: Designing a Conversational Agent for Time and Stress Management"},{"paperId":null,"title":"Later datasets increase in complexity and scale , incorporating reading comprehension"}],"references":[{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e20cf061851898e8477fea6f195cfe44f8ad7a86","title":"CogCompTime: A Tool for Understanding Time in Natural Language"},{"paperId":"711b1f7cc4e92d6f40c7813c6f0e1c2e179d48ad","title":"Commonsense for Generative Multi-Hop Question Answering Tasks"},{"paperId":"190eb1dff4c08fc12a085242b67c0442cfe5fc84","title":"Reasoning about Actions and State Changes by Injecting Commonsense Knowledge"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"bbad0b301561c9b44a43f2880b29f143dc7297ba","title":"Temporal Information Extraction by Predicting Relative Time-lines"},{"paperId":"34901b737b11aa51b688fa18c2eab47639d7b8c6","title":"Joint Reasoning for Temporal and Causal Relations"},{"paperId":"15d742935a5bb28e464048ed33ff18128c9b09e7","title":"MITRE at SemEval-2018 Task 11: Commonsense Reasoning without Commonsense Knowledge"},{"paperId":"99ad0533f84c110da2d0713d5798e6e14080b159","title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"},{"paperId":"82580fe4c429ac76f94c514c1ffc066844b13192","title":"Determining Event Durations: Models and Error Analysis"},{"paperId":"6c5eb60ac554f2210eb18124ca0f052806eec2b0","title":"SemEval-2018 Task 11: Machine Comprehension Using Commonsense Knowledge"},{"paperId":"2754380bf487a455112d9c5594914688f9205ac0","title":"Constructing Narrative Event Evolutionary Graph for Script Event Prediction"},{"paperId":"b7fbce48dfd734adbab95c669c290d9e4aaf3272","title":"Event2Mind: Commonsense Inference on Events, Intents, and Reactions"},{"paperId":"a6ced6341e8bf2d819cbc7de73b869752019afdd","title":"Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource"},{"paperId":"bff8ae9e28323d217b9ad5a7321e58f79607f557","title":"A Multi-Axis Annotation Scheme for Event Temporal Relations"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"a9e0fa94e59261f5905951e93d515af64c3fc7cb","title":"A Structured Learning Approach to Temporal Relation Extraction"},{"paperId":"c9f343b492c170c726f607c255ec6c7177dc5800","title":"Verb Physics: Relative Physical Knowledge of Actions and Objects"},{"paperId":"13f2792d63e933f8a9acda03d95a2d801d7c70f3","title":"Ordinal Common-sense Inference"},{"paperId":"83e7654d545fbbaaf2328df365a781fb67b841b4","title":"Enhanced LSTM for Natural Language Inference"},{"paperId":"dde70b2fe5ead57cea2556c52a975865e3bb4d30","title":"What Happens Next? Event Prediction Using a Compositional Neural Network Model"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cf3955f27c54f425c29c372af3aa19778abe2051","title":"Representations of commonsense knowledge"},{"paperId":"7b82383b09bedb765ab9c5c2153978035aacd830","title":"Context-dependent Semantic Parsing for Time Expressions"},{"paperId":"32a257bb8cbee2e6f833f7c7f6e9bcacde4919d7","title":"Extracting fine-grained durations for verbs from Twitter"},{"paperId":"407314c1b491511564c089d26e7e92c0c93af754","title":"Learning Temporal Information for States and Events"},{"paperId":"fb0b11046474b8f1c810f947f313c7c7229a988f","title":"SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"},{"paperId":"085730e9355193859e4172c729821aaedc71f4c2","title":"Using Query Patterns to Learn the Duration of Events"},{"paperId":"7716a474569d94a9dc0074a2cdfc3b32641b721e","title":"HeidelTime: High Quality Rule-Based Extraction and Normalization of Temporal Expressions"},{"paperId":"0c739b915d633cc3c162e4ef1e57b796c2dc2217","title":"VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations"},{"paperId":"bb0968ab69b8655d442f17cdfe317ce3bfebd789","title":"Can we derive general world knowledge from texts"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"0e9d94639ac964754f812019af91b7c519103b46","title":"Learning Scalar Adjective Intensity from Paraphrases"},{"paperId":"8eb9b2ea146e3a381a29b4a9314c36f61b71e367","title":"Extracting Commonsense Properties from Embeddings with Limited Human Guidance"},{"paperId":"fc090a68e45e0e6337136777d21c87b76a90ae72","title":"From TreeBank to PropBank"}],"id":"81b4920ad488affaee27389ff9540b7fea90a4ce","summary":"It is found that the best current methods used on MCTACO are still far behind human performance, by about 20%, and several directions for improvement are discussed."},{"url":"https://www.semanticscholar.org/paper/d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving","venue":"ArXiv","year":2019,"referenceCount":29,"citationCount":40,"influentialCitationCount":1,"publicationDate":"25/09/2019","authors":"Imanol Schlag,P. Smolensky,Roland Fernandez,N. Jojic,J. Schmidhuber,Jianfeng Gao","citations":[{"paperId":"cc9a2d59033867227f319806783094ac37d2f8fe","title":"LLMs as Potential Brainstorming Partners for Math and Science Problems"},{"paperId":"179237bc5fb46f34ef936b1552600bf3521c3c64","title":"Differentiable Tree Operations Promote Compositional Generalization"},{"paperId":"1d9bfa5cee2d4a01eace2e77e7639599fe143e39","title":"Vector Symbolic Finite State Machines in Attractor Neural Networks"},{"paperId":"30977afbd4501249e1a320bd2e48581197914ab8","title":"A Short Survey of Systematic Generalization"},{"paperId":"ab080117bfdd1cb2ab7d913b45a904f78ee542be","title":"Semantic Representations of Mathematical Expressions in a Continuous Vector Space"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"42bc0824d8ca35105d181aaa0183654535325f55","title":"Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages"},{"paperId":"d9f65f6e3d13001309bf6a93f2c968f927c06149","title":"Artificial neural network modelling of the neural population code underlying mathematical operations"},{"paperId":"40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language"},{"paperId":"f2611a09cf0942170785ee3025cb511de3bdec2e","title":"Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems"},{"paperId":"2e03ee15c00561a891eaf197515da720d1566411","title":"Causal Transformer for Estimating Counterfactual Outcomes"},{"paperId":"1ee909a2a78d287cac549d5deea96f3cbd1c7092","title":"Enhancing Neural Mathematical Reasoning by Abductive Combination with Symbolic Library"},{"paperId":"2db6c10f135d5701ae7aec45986124ce264c1344","title":"Learning Symbolic Rules for Reasoning in Quasi-Natural Language"},{"paperId":"04db9b694280134f09af5fa787a306907edba29d","title":"How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN"},{"paperId":"2ec21ae2f7a72a1246482c915da7a8c077c8b959","title":"Generating Symbolic Reasoning Problems with Transformer GANs"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"d0dae92c4d37520ae20c072ec64fdb718874bfd0","title":"A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis"},{"paperId":"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","title":"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"},{"paperId":"45f7c0caf0f1f88a568279de52d7a8c29aa8e28f","title":"Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization"},{"paperId":"f3879fedf036175aefdb750c5527d184f038b932","title":"Compositional Processing Emerges in Neural Networks Solving Math Problems"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"58c74cec28f3416b9a1d308bb2d6519d21d53ab0","title":"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"},{"paperId":"6027247d7f4256c10bdc71b8584d5927e616fa37","title":"Logic Tensor Networks"},{"paperId":"d642868ce4325ebf3026c0aa0c497a079f112a8d","title":"On the Binding Problem in Artificial Neural Networks"},{"paperId":"c2b4d96db34bd472e84c9234838cc4e808eb1ba9","title":"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language"},{"paperId":"5e11e806d24dd80ecf0f91e7aacedbba8d9fd6fc","title":"Learning Associative Inference Using Fast Weight Memory"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"804a6d7c23335bbca6eec3b7d3c8366dcbe395a5","title":"Hopfield Networks is All You Need"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"9ebb28851b253817f9a0ea5ddc22b0fd9a934a2f","title":"Deep Learning--based Text Classification"},{"paperId":"7e9af8a6081dc00187bd4a6727751d1721bd7816","title":"Evaluating Logical Generalization in Graph Neural Networks"},{"paperId":"b32e631ae7779d93b9979c61c5b920a76342063e","title":"Teaching Temporal Logics to Neural Networks"},{"paperId":"5ade9c2fabc0fc526ea05445f8d0f92666266681","title":"Transformers Generalize to the Semantics of Logics"},{"paperId":"97d69e7e8c04714bf58dcbe5ae7454db69b657a7","title":"The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence"},{"paperId":"dc88d2bbcebd810d7c80ba281739908005b12235","title":"Neurocompositional computing in human and machine intelligence: A tutorial"},{"paperId":"d129841cb2e30e25000dcd9edb83c880fc4babc1","title":"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention"},{"paperId":"adc61e21eafecfbf6ebecc570f9f913659a2bfb2","title":"Deep Learning Based Text Classification: A Comprehensive Review"},{"paperId":"caa202043b553cea0b099d591308e66db8a300ba","title":"Dreaming with ARC"},{"paperId":"f886a0dc2dda508f744bc9e1c3750b468ed2f65b","title":"Transformer Model for Mathematical Reasoning-CS 230 Final Report"},{"paperId":"b4ccbef0feee0225b6d8548cbc449114e2e13a06","title":"Probabilistic Abduction for Visual Abstract Reasoning via Learning Rules in Vector-symbolic Architectures"}],"references":[{"paperId":"59a916cdc943f0282908e6f3fa0360f4c5fb78d0","title":"Stabilizing Transformers for Reinforcement Learning"},{"paperId":"830995ef17cc291c13f42dfd9f462137de1d2179","title":"Augmenting Self-attention with Persistent Memory"},{"paperId":"afd110eace912c2b273e64851c6b4df2658622eb","title":"Visualizing and Measuring the Geometry of BERT"},{"paperId":"165d51a547cd920e6ac55660ad5c404dcb9562ed","title":"Open Sesame: Getting inside BERT’s Linguistic Knowledge"},{"paperId":"67146c46304720c026a3c9bc324ffc2285df2c0b","title":"A Perspective on Objects and Systematic Generalization in Model-Based RL"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"f6fa98b0d1f6bff608a056acd7b82e2a9dd0a68e","title":"Learning to Reason with Third-Order Tensor Products"},{"paperId":"735708fb10c3420a1138b0d1c7a0923ff4de3c41","title":"A Simple Recurrent Unit with Reduced Tensor Product Representations"},{"paperId":"b5ecf634902b329aea683176c56aac35029da621","title":"Learning Distributed Representations of Symbolic Structure Using Binding and Unbinding Operations"},{"paperId":"6398cb8f2af1c988a097ed1e1cefb380195edfb8","title":"(Preprint)"},{"paperId":"fd4ae71916cf400bfd1490f275e91b154eb69160","title":"Relational recurrent neural networks"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"d4a9876f00fd9caf0358fd73e5572e53a47cda12","title":"Question-Answering with Grammatically-Interpretable Representations"},{"paperId":"dc620583dc06f7262707b4e2a9b7df91dbd384e0","title":"Deep Learning of Grammatically-Interpretable Representations Through Question-Answering"},{"paperId":"b92aa7024b87f50737b372e5df31ef091ab54e62","title":"Training Very Deep Networks"},{"paperId":"ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":"9ca9f28676ad788d04ba24a51141a9a0a0df4d67","title":"A new model for learning in graph domains"},{"paperId":"61639af1a89c69094bcc0ed40fad752832b037c3","title":"Reducing the Ratio Between Learning Complexity and Number of Time Varying Variables in Fully Recurrent Nets"},{"paperId":"bc22e87a26d020215afe91c751e5bdaddd8e4922","title":"Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"},{"paperId":"24484e1105bd28acbf0184c94ac9833511328087","title":"Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems"},{"paperId":"3106e66537a0c8f53278e553bcb38f0b0992ec0e","title":"Distributed Representations"},{"paperId":"b7efb6b6f7e9ffa017e970a098665f76d4dfeca2","title":"Polynomial Theory of Complex Systems"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Systematic Generalization: What Is Required and Can It Be Learned?"},{"paperId":null,"title":"Learning task-dependent distributed representations by backpropagation through structure"},{"paperId":"e62f7643f616aaad65ffd47155a53bfa325e455d","title":"The Correlation Theory of Brain Function"}],"id":"d88f31a0091eee02c5a2aa2013914818cdef114e","summary":"The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems, and incorporates Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure."},{"url":"https://www.semanticscholar.org/paper/5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures","venue":"Transactions of the Association for Computational Linguistics","year":2019,"referenceCount":62,"citationCount":260,"influentialCitationCount":23,"publicationDate":"31/12/2019","authors":"Alon Talmor,Yanai Elazar,Yoav Goldberg,Jonathan Berant","citations":[{"paperId":"f9410c2a81ba236166d82151b53d214122e730f1","title":"Probing Pretrained Language Models with Hierarchy Properties"},{"paperId":"9cd0837a6204e62e0819fbd9238a4e41b18482aa","title":"The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation"},{"paperId":"2896edafc003e1be02b897c99ca0a8c864853c5d","title":"The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models"},{"paperId":"99be42f807311159efeb19f89fe0f0a6c3e97861","title":"An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping"},{"paperId":"7de1bd8dc7b271b2fbde7406a7fc2547335510b7","title":"Non-Axiomatic Logic Modeling of English Texts for Knowledge Discovery and Commonsense Reasoning"},{"paperId":"cba8ae02f3e0e677281b1b57a04a7e53317fb073","title":"Chinese legal judgment prediction via knowledgeable prompt learning"},{"paperId":"22a52b4a74334bd17ae75ef9f0618e75c69e8986","title":"Hypert: hypernymy-aware BERT with Hearst pattern exploitation for hypernym discovery"},{"paperId":"3d85460139dd4a49e3d601daebfce86c50577bca","title":"Cognitive Effects in Large Language Models"},{"paperId":"9a4765547cb43ab221fe262df7405f6795557d8c","title":"Efficient Benchmarking (of Language Models)"},{"paperId":"f5ec0b22930faf15c3a7912bda367e8a9f4c8bd4","title":"On the Unexpected Abilities of Large Language Models"},{"paperId":"01c5e9404cb72c32f2c07fbd2639dc84ef7115a8","title":"The 1st International Workshop on Implicit Author Characterization from Texts for Search and Retrieval (IACT'23)"},{"paperId":"4323a09460c03cb31b3823ade9f75fba66d45316","title":"Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features"},{"paperId":"85d9151aa2efd0cbe822e403138cfe49f9536703","title":"SITTA: A Semantic Image-Text Alignment for Image Captioning"},{"paperId":"fa0c364b28f8ff721ac3aa2e82dca312024c19dd","title":"A BERT-based deontic logic learner"},{"paperId":"230137d02910e43a9d161e21af24b80fd94d351e","title":"Feature Normalization and Cartography-Based Demonstrations for Prompt-Based Fine-Tuning on Emotion-Related Tasks"},{"paperId":"d89d8c93f62e66f8f22f0fbbddc688fb4017a15d","title":"ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling"},{"paperId":"2e403ad2cd02409e1fdc15839da0a3f89886a990","title":"MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text Classification"},{"paperId":"c50348d3491567b2cdad5ea981620c31f876dad9","title":"Semantic HELM: A Human-Readable Memory for Reinforcement Learning"},{"paperId":"9a1c481a25c609fe1dbb9b9d43faf892c3c71368","title":"Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks"},{"paperId":"56267168c5f1cea5049c0f1724e39166988d32b7","title":"Probing Physical Reasoning with Counter-Commonsense Context"},{"paperId":"6b09c5e94a62e6c1580e7183b1e3caa700b5512e","title":"Knowledge-guided Aspect-based Summarization"},{"paperId":"604149ebaa727cb0840e0117a59f6fc13ca66690","title":"Not Wacky vs. Definitely Wacky: A Study of Scalar Adverbs in Pretrained Language Models"},{"paperId":"84cde4abc973e3bb508f03506a7fa946222f4e6b","title":"Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization"},{"paperId":"2cfb1f44b34204213d789731871e599c756bdb83","title":"Exploring Large Language Models for Classical Philology"},{"paperId":"c82e95e282e85f649f901f16e3cbf434b582ba74","title":"Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks"},{"paperId":"99a062c83e4ec3574f6a93c7b551f3ef5e3635fa","title":"Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer"},{"paperId":"9c77b4343600a325f5c54a78b468cc183a36fbaa","title":"Harvesting Event Schemas from Large Language Models"},{"paperId":"44e7c477cdb3cd09ea7a6459eef1d9c07fced577","title":"ComputeGPT: A computational chat model for numerical problems"},{"paperId":"44772fe1c3fa422a3da7e25092db2544893d6bfb","title":"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming"},{"paperId":"3ffd2d2e0479e6d3bffe9f444b5b96a70de66871","title":"Comparing Selective Masking Methods for Depression Detection in Social Media"},{"paperId":"10e5d2f2e8a59686567ae0d2d0ce741cea9e9ea3","title":"Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning"},{"paperId":"4fff82435192ea9bc75bbdbbe413effc35084938","title":"Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World"},{"paperId":"29a7644583d7042c4476af126f3fe0a372897abe","title":"LION: Implicit Vision Prompt Tuning"},{"paperId":"8018a68956d6751d7ea76110537d5a2e86ec05c4","title":"The Life Cycle of Knowledge in Big Language Models: A Survey"},{"paperId":"06396c7cd5d223a1776abf8811359ec7bc05d420","title":"Knowledge-Augmented Methods for Natural Language Processing"},{"paperId":"aad8b1ca56aef4a7512789102ce0cc3fc8b064e4","title":"Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views"},{"paperId":"992cff59f13f5944a74eb2e62a8cb11849204780","title":"KILM: Knowledge Injection into Encoder-Decoder Language Models"},{"paperId":"da345b189e4faaaa489f7319640868a37a3932a1","title":"Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?"},{"paperId":"2379d69c98548d8c291a53a9a932b5ea7911fbe5","title":"Probabilistic coherence, logical consistency, and Bayesian learning: Neural language models as epistemic agents"},{"paperId":"bf8491bef353df126e2306ad2fe4b898697b906a","title":"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity"},{"paperId":"ae766548699f27e669932de14e1c0f47b2828536","title":"Prompting for Multimodal Hateful Meme Classification"},{"paperId":"c40aa04f1577e8f0f83208d1815e032d127b60a8","title":"Creating a Large Language Model of a Philosopher"},{"paperId":"a43425af7413e62c816a8fc007ab443465761829","title":"Unifying Structure Reasoning and Language Pre-training for Complex Reasoning Tasks"},{"paperId":"9a9e68d400069f023f7dc9b982226c95159a509d","title":"Dissociating language and thought in large language models: a cognitive perspective"},{"paperId":"9ac29a207d58496c730674af89b572243fed2616","title":"Knowing Knowledge: Epistemological Study of Knowledge in Transformers"},{"paperId":"5bc43db8a438af826ddff02024ab9c036ab6f11c","title":"Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering"},{"paperId":"d651691e42d42423f0b13a0ee7dfc67087d87379","title":"'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers"},{"paperId":"8db5adadc0ab39f95a26a2eb6499d340d6c5ea21","title":"From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation"},{"paperId":"ad5573cb25fd403f7620332f363ae87327c69a49","title":"The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning"},{"paperId":"2f2dd927eebe91ce40ffc4bddc419f7337abbafb","title":"An Analogy based Approach for Solving Target Sense Verification"},{"paperId":"4651f8deca3d571e8e683b243903fbf194b3b085","title":"Event knowledge in large language models: the gap between the impossible and the unlikely"},{"paperId":"17fc99b691d6fe1f4a84c774f01e86a2aae2c2df","title":"Demystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application"},{"paperId":"a26623d52d24e03044a158cddad931ec5ab7304c","title":"A Survey of Knowledge-Enhanced Pre-trained Language Models"},{"paperId":"bcec7d17e68aceb91d020dd796ece075694f77c6","title":"COPEN: Probing Conceptual Knowledge in Pre-trained Language Models"},{"paperId":"6ae3e52ae55578c10722db3c2f898442f20e336c","title":"LMentry: A Language Model Benchmark of Elementary Language Tasks"},{"paperId":"1933a0ef47f8d2ba4a8277d702d522a06319302c","title":"IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"f663c1b574d0a56c2e1c8e7aee3d8caf90b52808","title":"LMPriors: Pre-Trained Language Models as Task-Specific Priors"},{"paperId":"72e3a1b4745d6f25b8cd186bd56fbad6e3a8464b","title":"Judgment aggregation, discursive dilemma and reflective equilibrium: Neural language models as self-improving doxastic agents"},{"paperId":"8f60454584a8d1f1d6f278198af5e0bd2d103067","title":"A Survey of Parameters Associated with the Quality of Benchmarks in NLP"},{"paperId":"f2dadc182d33f3ad74f95505e5b6611c3abafe02","title":"MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks"},{"paperId":"ad62d710f1854daf372680263f50a4e135e309f2","title":"CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models"},{"paperId":"e070ff286709db28312e08b52b05539debe88146","title":"Measuring and Narrowing the Compositionality Gap in Language Models"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"ed99a2572fb5f4240aa6068e3bf274832e831306","title":"Recitation-Augmented Language Models"},{"paperId":"f2878e3a27950c674425ba896bfd1c2be7dd67e5","title":"Negation, Coordination, and Quantifiers in Contextualized Language Models"},{"paperId":"c8ff9fef927b6ccf242c20b3939a7442a0633ab3","title":"VIPHY: Probing \"Visible\" Physical Commonsense Knowledge"},{"paperId":"416e92370e15e25ec23547ef32e6c1702c1ca1fc","title":"CommunityLM: Probing Partisan Worldviews from Language Models"},{"paperId":"a6e39438f766a4df502d6922c7f253398af60c14","title":"Pre-Trained Language Models and Their Applications"},{"paperId":"aa3bae7def250ba247ea2c187ad1c1a99b3bf737","title":"Lost in Context? On the Sense-Wise Variance of Contextualized Word Embeddings"},{"paperId":"94e5ca13e8c884509ccad233c77979923b677a3e","title":"UnCommonSense: Informative Negative Knowledge about Everyday Concepts"},{"paperId":"bb4e1dd3a46117abeaaa98200db4f463d8688e55","title":"Information Theory–based Compositional Distributional Semantics"},{"paperId":"5b5f9554b71321dc6df2518e1347354e4ac67c0d","title":"Pro-tuning: Unified Prompt Tuning for Vision Tasks"},{"paperId":"f3cf71c51b882fe3111d71c4bf104297d38197f8","title":"Inner Monologue: Embodied Reasoning through Planning with Language Models"},{"paperId":"f8d44802ac8190864c61c9aaf4a8b450261873ab","title":"An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics"},{"paperId":"9ecc1b8494328f7b2d1b58595b2c28eb768c2c7c","title":"longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks."},{"paperId":"3f5d3c57ccb3b9059eb8a94fa7af4d9f299c56c6","title":"Entity-Graph Enhanced Cross-Modal Pretraining for Instance-Level Product Retrieval"},{"paperId":"e3dae33c5bdf397abdefd971ea34c48fb836dcc0","title":"GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models"},{"paperId":"3f3c01adbdd433d515c19ac8cf6c61c905f0061a","title":"History Compression via Language Models in Reinforcement Learning"},{"paperId":"a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7","title":"The Curious Case of Control"},{"paperId":"4e5f7cd537a1bbcd090f9887b1b59f39a3715dba","title":"Instruction Induction: From Few Examples to Natural Language Task Descriptions"},{"paperId":"a005a6160efa2fd055581b8222b41f71f966ea50","title":"Life after BERT: What do Other Muppets Understand about Language?"},{"paperId":"7f84d56fb8feb4e50cd6c3da3e3fd4ff6c4772cf","title":"ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models"},{"paperId":"c90a14b54ed56c975fda929418342cf373925e4e","title":"Entity-aware Transformers for Entity Search"},{"paperId":"cd471b5ef162906ef3d9a84398b3f98e9ee4bf56","title":"A Review on Language Models as Knowledge Bases"},{"paperId":"706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding"},{"paperId":"c4e991c0c5c21608a5a21d31fd478ce7b7fb527d","title":"Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View"},{"paperId":"537d98e241975dc5c32d9372ae85134dffe45532","title":"DP-KB: Data Programming with Knowledge Bases Improves Transformer Fine Tuning for Answer Sentence Selection"},{"paperId":"3f4d11971f2c64be9125a7fe99c019588bbebf16","title":"Iteratively Prompt Pre-trained Language Models for Chain of Thought"},{"paperId":"50f7d69ddd7f9b34f5121607fbdcc57236d65b8c","title":"Can Pre-trained Language Models Interpret Similes as Smart as Human?"},{"paperId":"18dc24da603896db2264e9174116407a95c593d5","title":"“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction"},{"paperId":"7bebb48d34c219b119ca2d4ffc97d7fd4940c35c","title":"On the data requirements of probing"},{"paperId":"490002dbbeeda2c0c147538edec4afe7c956848b","title":"Do Transformers know symbolic rules, and would we know if they did?"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"926689bf5f21e82a24830a68a0dc879ac0783784","title":"Kformer: Knowledge Injection in Transformer Feed-Forward Layers"},{"paperId":"c3a454e50ec0610f1380d55b1988a5eb5d45207b","title":"Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization"},{"paperId":"0b483b550b21ec42d693fc04a372dbb10dd07019","title":"Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge"},{"paperId":"63f17017257063ee034c4082d93005dc4b25d42d","title":"Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability"},{"paperId":"e92653643e444d165523bb1f48763bf8d0f56772","title":"LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI"},{"paperId":"2db6c10f135d5701ae7aec45986124ce264c1344","title":"Learning Symbolic Rules for Reasoning in Quasi-Natural Language"},{"paperId":"a466d10b80dbdee3b130bef73ec62f3a89eb389b","title":"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples"},{"paperId":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","title":"Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"},{"paperId":"d5784fd3ac7e06ec030abb8f7787faa9279c1a50","title":"Interpreting Deep Learning Models in Natural Language Processing: A Review"},{"paperId":"290867638c5ca520de5c48aa4336f196d426c226","title":"Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey"},{"paperId":"c5fbf9a62a91e3182f65e3746d3263387effa4a7","title":"The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail"},{"paperId":"c131a9eb834055a3b846c3d3816cd23a3434e528","title":"DeepA2: A Modular Framework for Deep Argument Analysis with Pretrained Neural Text2Text Language Models"},{"paperId":"48a3184b25a90d6864326aec7950af6aee60ef49","title":"A Survey of Knowledge Enhanced Pre-Trained Language Models"},{"paperId":"e55391a9406245584b3e5b3225dad2e171b9a06b","title":"RuleBERT: Teaching Soft Rules to Pre-Trained Language Models"},{"paperId":"e113570afc29562bfde48f89fbb5efa624610573","title":"Distilling Relation Embeddings from Pretrained Language Models"},{"paperId":"1a40d4c7ee1044618c08e9f3cfee19eb2c12071d","title":"SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis"},{"paperId":"6cc9484612ab146c9fed9f7dce283c815af3cbc8","title":"Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids’ Representations"},{"paperId":"ceef266c59698999c9283a0cda852d8bc1ce27ea","title":"How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy"},{"paperId":"a418c0daa98a3639e1b1bd682c68644250259944","title":"Transformers in the loop: Polarity in neural models of language"},{"paperId":"299983121dec88d4cc8e4ea2aa06514787d8d878","title":"Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models"},{"paperId":"884c7aac3358a6f91887dd0d091759963764bedd","title":"A Bayesian Framework for Information-Theoretic Probing"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"26c60f8ffb0d66d8732d22af6f5b539e49f2a1e6","title":"Discover AI Knowledge to Preserve Cultural Heritage"},{"paperId":"e6f94081276a7a5e6aef34a080cb3d3a4b1b9c20","title":"Rethinking Why Intermediate-Task Fine-Tuning Works"},{"paperId":"9fabd0280e910cf4608dc27dc0268d5b8ed2d3ba","title":"SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation"},{"paperId":"ad0e6dfee9be05e2c9021779f2f995a3caeb3642","title":"How Can the [MASK] Know? The Sources and Limitations of Knowledge in BERT"},{"paperId":"2cf3cd3a7a08fc91eecab45e73299940c9c439dc","title":"Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills"},{"paperId":"f07bcf49a92e09437359be788bbe3f9237c5ec40","title":"A Closer Look at How Fine-tuning Changes BERT"},{"paperId":"ada0e2e476523714a5109c8bb19588140e2314e7","title":"Core Challenges in Embodied Vision-Language Planning"},{"paperId":"896c76bf99de9c1ba6fc3dfb695dfa66e73e1eb3","title":"Probing Pre-Trained Language Models for Disease Knowledge"},{"paperId":"7747ecbc26b1688e6cad1a6ce83914efa2a3c04c","title":"Prompting Contrastive Explanations for Commonsense Reasoning Tasks"},{"paperId":"5aab57cc0530560d82c74c055f664280619d7e81","title":"PROST: Physical Reasoning about Objects through Space and Time"},{"paperId":"d65a064eb837f838faf6ff67781b62450b92b159","title":"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification"},{"paperId":"f62acd332fd7a6f35b117ed4ffaf93b19483dcf7","title":"Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?"},{"paperId":"143310c074eb09d5e60adea4c42250dbe03bf9f2","title":"Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution"},{"paperId":"38cfcd4681cf85ab507ec0586c753182a4c8eecb","title":"John praised Mary because _he_? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs"},{"paperId":"84c8d939c765dd30574e6c7e6d0a3eb82db1c8fb","title":"ERNIE-NLI: Analyzing the Impact of Domain-Specific External Knowledge on Enhanced Representations for NLI"},{"paperId":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"},{"paperId":"bc5f9a01a0b9e3015c094ff92f5ff1863612a2b7","title":"What BERTs and GPTs know about your brand? Probing contextual language models for affect associations"},{"paperId":"489ffd70cb2afd550ab809bc90f5a766eb07aa80","title":"Inside ASCENT: Exploring a Deep Commonsense Knowledge Base and its Usage in Question Answering"},{"paperId":"12b336abe8b9889bfd2b82ff790e53603a899cbe","title":"Inspecting the concept knowledge graph encoded by modern language models"},{"paperId":"4f92221c7cedb1bd6212276e1c122dcac9860750","title":"Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates"},{"paperId":"408cc1103ab953a26c7071ffd9ce808469f77a01","title":"Understanding by Understanding Not: Modeling Negation in Language Models"},{"paperId":"befbef9f4e4c6269fa712294430ff916cf2fd51c","title":"Let’s Play Mono-Poly: BERT Can Reveal Words’ Polysemy Level and Partitionability into Senses"},{"paperId":"d3a1e7f060bfe7d97c2e430da9eb6967bbbe358a","title":"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"},{"paperId":"db74b20baa82dbcaf641f82fb776e71e581f679b","title":"Novel Aficionados and Doppelgängers: a referential task for semantic representations of individual entities"},{"paperId":"1c0f8a6b8e6f4cc6fc81c4019fc07a2e5ec17107","title":"Probing for Bridging Inference in Transformer Language Models"},{"paperId":"b2816194217673d0b3be5db0b04666cb2f4cdbf5","title":"Modeling Age of Acquisition Norms Using Transformer Networks"},{"paperId":"0672f88d5dc762002b515ca4a0a9f101017fea35","title":"Probing Across Time: What Does RoBERTa Know and When?"},{"paperId":"2ef4be35f8424ea768aa2e1b44392b3eddbc780b","title":"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema"},{"paperId":"209f9bde2dee7cf1677801586562ffe56d435d38","title":"Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"},{"paperId":"c5943bc4b22a63f595cb1c2823a449e03aad4787","title":"AR-LSAT: Investigating Analytical Reasoning of Text"},{"paperId":"2e2ea29356e006fdbedb7592caf5090260f81f1e","title":"What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models"},{"paperId":"00c209ea764f709a5ce7d7fdc16c2352551bfa83","title":"DirectProbe: Studying Representations without Classifiers"},{"paperId":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","title":"Relational World Knowledge Representation in Contextual Language Models: A Review"},{"paperId":"8b652c4d7a8d5836925ce0fe28a91dc661778524","title":"What's the best place for an AI conference, Vancouver or ______: Why completing comparative questions is difficult"},{"paperId":"b265827019f420b44c79fd87be1cc6000329c762","title":"Exploring the Role of BERT Token Representations to Explain Sentence Probing Results"},{"paperId":"1a0d8dbd0252193abe9d64f72fc56cc1f05ed3eb","title":"CURIE: An Iterative Querying Approach for Reasoning About Situations"},{"paperId":"008b9fc834f5839a25febe150f3076d550ee442f","title":"Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2"},{"paperId":"f5d1dbdaa6c8a44f388f3f7fe538403baefc1252","title":"Large pre-trained language models contain human-like biases of what is right and wrong to do"},{"paperId":"54df9a3f12fa4810a94a7a5929d0cc7a672b13c4","title":"Information to Wisdom: Commonsense Knowledge Extraction and Compilation"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19","title":"Measuring and Improving Consistency in Pretrained Language Models"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"634e8fbeba53d45828846dd541ce0a0078c57b68","title":"Syntax-Enhanced Pre-trained Model"},{"paperId":"1841cf23c65ff2f27f21ba0d2268c3445f20332f","title":"Few-Shot Text Generation with Natural Language Instructions"},{"paperId":"eb2fc03b8865b8e1b4cb933d917ea269ebe14584","title":"Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training"},{"paperId":"1d7f3297924a9dd90cfc0df522ebe9138c28b46f","title":"Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals"},{"paperId":"9ba6ad0de7dbe1a3b10c44106049adb96f87d483","title":"KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning"},{"paperId":"33422275fbb9958f55419620697faf531482699b","title":"How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering"},{"paperId":"9d1f9406ed676171d9975e27606c95633ca898b1","title":"On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT"},{"paperId":"d865c87f6ce4e0be29ae1e3780fae66b8034d04b","title":"TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation"},{"paperId":"6906c562e182ea22c086b2bb44ed9cec8602a220","title":"Classifier Probes May Just Learn from Linear Context Features"},{"paperId":"42595cb533b03ad44738102fb0c3cef3e4b5c27c","title":"Controlling the Imprint of Passivization and Negation in Contextualized Representations"},{"paperId":"c0c6f3ba310099b9a1ebe899f5f58bb04574df97","title":"Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"15bcf3b7aa6511a55d7066419453a6d5906b2db8","title":"It’s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT"},{"paperId":"14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9","title":"Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI"},{"paperId":"01400290c7db96c4d665d1c29519c42ba47401e0","title":"A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks"},{"paperId":"5b71bcf769e7efab90ceba56b9e4f898899538fe","title":"BERT Knows Punta Cana Is Not Just Beautiful, It’s Gorgeous: Ranking Scalar Adjectives with Contextualised Representations"},{"paperId":"1c6f94fb3d888167355afb580f04d55cd517ebc6","title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"},{"paperId":"b4013141cceb937c46ea5f84f8c06f6bf1215106","title":"PRover: Proof Generation for Interpretable Reasoning over Rules"},{"paperId":"4571feb26d903053661efd2f2f144e010902f458","title":"Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading"},{"paperId":"37bf0bf34603145246c3311df19e2afdf6e0270a","title":"JAKET: Joint Pre-training of Knowledge Graph and Language Understanding"},{"paperId":"fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7","title":"What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams"},{"paperId":"ee5fff85d3ec62698eddba162f054b7e73670b2a","title":"Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics"},{"paperId":"260cce438595c708433719a75c72889fefa5f731","title":"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA"},{"paperId":"f30444fbb6ad806168e2564db4815cd27faa7fd9","title":"It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"},{"paperId":"4d766ae60fbbaea4f6c4ca8ad7f14569ec3189d1","title":"Critical Thinking for Language Models"},{"paperId":"15aa86556be1579eadf8fbb0bc486f9427e681f0","title":"Evaluating representations by the complexity of learning low-loss predictors"},{"paperId":"64da659c0687762359226b4cf455520c78acd165","title":"Neural Language Generation: Formulation, Methods, and Evaluation"},{"paperId":"cc4db47a416aba22d1073e59a6867b6997928b7c","title":"What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation"},{"paperId":"ea8c46e193d5121e440daf96edfd15a47151c293","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"a3aa1562e72aecaa06bcec4613cc3d020f4b8e49","title":"Pre-trained Language Models as Symbolic Reasoners over Knowledge?"},{"paperId":"6f2b90ee5a0feea87264148c25a874f84bae20a0","title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?"},{"paperId":"79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88","title":"Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge"},{"paperId":"f56fc41eff96afbc64030ce5e97d444706e4997c","title":"Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge"},{"paperId":"c8b00d4706fc8979a9c5f410addccbcfe1c0d894","title":"When Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions"},{"paperId":"26cfb57a9722599b361858d454ec816420723e36","title":"Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models"},{"paperId":"7a5d33bccf1f7fa6d00c070f6a6ed22467267368","title":"Commonsense Evidence Generation and Injection in Reading Comprehension"},{"paperId":"011869f932f89d047ce2bd36d73a95cc04888193","title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms"},{"paperId":"3c2f6c2cf7c9b121a0e46a660846bb0a5dad0ee8","title":"DQI: Measuring Data Quality in NLP"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"688e324770a8c3db0af25968bd15e2cc33b9a83d","title":"Can BERT Reason? Logically Equivalent Probes for Evaluating the Inference Capabilities of Language Models"},{"paperId":"567469dc08fbb702df9f525637e9a3fc43bb1fbb","title":"Probing Contextual Language Models for Common Ground with Visual Representations"},{"paperId":"467ac47b2e01ce6ae74e8d70561ca0f8f66c7b8c","title":"Probing Text Models for Common Ground with Visual Representations"},{"paperId":"2297c559ac3509b3ab456229f7032b2a0fbf23c1","title":"Pretraining on Non-linguistic Structure as a Tool for Analyzing Learning Bias in Language Models"},{"paperId":"c30b457fdfb0623b87379de79ffaa570a7f3bb48","title":"Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation"},{"paperId":"0ebb1d1fbf488fba8c18a5a6057a6ccd9e87510f","title":"Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models"},{"paperId":"196c558ce126f5f7d66df4ea52e2442848fc65be","title":"Asking without Telling: Exploring Latent Ontologies in Contextual Representations"},{"paperId":"1e391e60081dcb148cc2732e1f8aa26655fef31e","title":"Explaining Question Answering Models through Text Generation"},{"paperId":"885e52a5b1a4fb36fae0e3fe31e9d4da21e03b41","title":"On the Existence of Tacit Assumptions in Contextualized Language Models"},{"paperId":"774319233a107a29622003a115aa6c79f4a7b37f","title":"Probing Neural Language Models for Human Tacit Assumptions"},{"paperId":"f4b585c9a79dfce0807b445a09036ea0f9cbcdce","title":"Information-Theoretic Probing with Minimum Description Length"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"c44120f765fc43994c5cfb4e12e4f62999efeae6","title":"How Context Affects Language Models' Factual Predictions"},{"paperId":"15ad2b27c5248e7d1db5456794ca1ca8a8198f5d","title":"Transformers as Soft Reasoners over Language"},{"paperId":"80376bdec5f534be78ba82821f540590ebce5559","title":"How Much Knowledge Can You Pack into the Parameters of a Language Model?"},{"paperId":"4f03e69963b9649950ba29ae864a0de8c14f1f86","title":"K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"},{"paperId":"8ae9a17c87a4518b513e860683a0ef7824be994d","title":"Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"},{"paperId":"5a9001cdccdb8b1de227a45eccc503d32d1a2464","title":"What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge"},{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"04dd5ec6dcb2470adfb7d9398eb92e39530fd85e","title":"Textual Pre-Trained Models for Gender Identification Across Community Question-Answering Members"},{"paperId":"d2db40cf7de0eb23ae784dba292e15051f63a828","title":"Semantic HELM: An Interpretable Memory for Reinforcement Learning"},{"paperId":"4b8969091ab33583035b0fd6c0c0b6755f088338","title":"You are what you’re for: Essentialist categorization in large language models"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"483ef564512eaa8bdb261f8f498b22925de354a9","title":"Eliciting Affective Events from Language Models by Multiple View Co-prompting"},{"paperId":"fc80b1324187d5f7aa8466a7cb94d0d1640dfc2e","title":"Can Neural Networks Learn Implicit Logic from Physical Reasoning?"},{"paperId":"c25887ad78a4d9b3f7f4a1544121e1c2e1b58d85","title":"Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning"},{"paperId":"a38cdace4c3e08eb2919d67cd1b676f62cb5c7f7","title":"Using Wikidata for Enhancing Compositionality in Pretrained Language Models"},{"paperId":"c876fc25bab1e56ba59e7058b184ebca98f24db2","title":"Contextualized Sentiment Analysis using Large Language Models"},{"paperId":"0ff1b5c699ffdfa00ee4191e8959ba27cacc6713","title":"METAPROBE: A Representation- and Task-Agnostic Probe"},{"paperId":"414536eafa990570530433b2f112ad343c66b4b0","title":"Deep Learning Enabled Consumer Research for Product Development"},{"paperId":"cd54a839ba12e7417e321b4407788dc426ebaefc","title":"Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering"},{"paperId":"fed460648303afa32247e493847e4dc73dc1a5b3","title":"Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?"},{"paperId":"41237c1350eb2371363af543a6c6a30db9d09197","title":"AnaLog: Testing Analytical and Deductive Logic Learnability in Language Models"},{"paperId":"2aba5bba16dac5cd62683bab9de5d6faaaed0de1","title":"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach"},{"paperId":"b71c245e093568d8c95aa889f968ce72b18e3d8b","title":"The First Workshop on Commonsense Representation and Reasoning May 27 , 2022"},{"paperId":"82c6f5ffd4d2d43ae7684601f607eae26a759a5a","title":"Analytical Reasoning of Text"},{"paperId":"0ad58485bb1b0ff3cdcf0d2a087ce8c40de529e8","title":"Where Does Linguistic Information Emerge in Neural Language Models? Measuring Gains and Contributions across Layers"},{"paperId":"af7503c8afb957edbf6f196f3821a8cd79bfadf6","title":"Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation"},{"paperId":"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","title":"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning"},{"paperId":"b742233b83405d303f61b3378f70475e4efdf75b","title":"Analyzing Semantic Faithfulness of Language Models via Input Intervention on Conversational Question Answering"},{"paperId":"5c8cb5d39aa8531fcb5962821ac404e4a0e109d1","title":"Is It Smaller Than a Tennis Ball? Language Models Play the Game of Twenty Questions"},{"paperId":"151e9fb3f5cf636b3dca52c89d30e1cb6bd53508","title":"Keynote Talk: What kinds of questions have we been asking? A taxonomy for QA/RC benchmarks"},{"paperId":"1bbbe12b060f54a97a4e81fd23cbb7932ff9de53","title":"Language Models have a Moral Dimension"},{"paperId":"291a00d8433fecd2dd10f7f13b62dae8ce500043","title":"Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models"},{"paperId":"b7cd1d84baaa3a65be4216e39086e034b8432845","title":"Exploring the Use of Neural Transformers for Psycholinguistics"},{"paperId":"1db86e01300e2f30fd08b46e63ea11656cb6dcf5","title":"TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models"},{"paperId":"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","title":"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations"},{"paperId":"1b94ebedacda0c21a4b8a40a5a40afcea4cc719a","title":"When Combating Hype, Proceed with Caution"},{"paperId":"07f6f423950210e7eecc3affc2499a9b59d346df","title":"Test Harder than You Train: Probing with Extrapolation Splits"},{"paperId":"2af476f7c2a7c040fc9ab7750bf41a84f66aa947","title":"Knowledge Based Multilingual Language Model"},{"paperId":"593284010379e02f6ab57e7208b5511185ce8c0e","title":"Probing Pre-trained Language Models for Semantic Attributes and their Values"},{"paperId":"b5148f8324570a845a7ae9a5843102d0693d006f","title":"A Survey of Knowledge Enhanced Pre-trained Models"},{"paperId":"61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886","title":"Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI"},{"paperId":"42fffebf899440941faf558dfef001c85c5c442d","title":"BERT, XLNet or RoBERTa: The Best Transfer Learning Model to Detect Clickbaits"},{"paperId":"23446cc15226c46eae9e4414598f7375eacb6ea1","title":"Back to Square One: Bias Detection, Training and Commonsense Disentanglement in the Winograd Schema"},{"paperId":"db88987a25e036a9fd8b69f8575e0a75d63b260b","title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"},{"paperId":"170ec3ac79b81f21ac35247b7f8e73991a14ebac","title":"Can RoBERTa Reason? A Systematic Approach to Probe Logical Reasoning in Language Models"},{"paperId":"5414be4432fcfe1e83932127ee2463d107e5b61c","title":"A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"},{"paperId":"189b518d70ad34c8de6f613bf3bd5051077608bc","title":"Probing Language Models for Common Ground with Visual Representations"},{"paperId":"adc79a6040c49f048cf4ed81fbc441fef661d5ed","title":"End-to-end Dialog Systems with Numerical Slot Filling"},{"paperId":"2a195dbfc2763d6db6862dec38a494a393ee18c0","title":"Digital Commons @ University of Digital Commons @ University of South Florida South Florida"}],"references":[{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"68c1bf884f0fc0e86641466a1f1fa67e79f16a17","title":"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"},{"paperId":"83f54abd65634f34e68b30c6fff2a57de7a34f37","title":"Negated LAMA: Birds cannot fly"},{"paperId":"c2c165dd615d4fc31d4fef4b4acbcab1a1655983","title":"On Making Reading Comprehension More Comprehensive"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"4c9a8caf940627126aaa9bd3ac813d07065c86a0","title":"Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets"},{"paperId":"8e914370a043fd626e172d55816952bc477bd582","title":"Question Answering is a Format; When is it Useful?"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"681fbcd98acf20df3355eff3585994bd1f9008b7","title":"Probing Natural Language Inference Models through Semantic Fragments"},{"paperId":"199ff73d2f728e997f860b62a2322823d3e3d9e8","title":"Designing and Interpreting Probes with Control Tasks"},{"paperId":"0e4cd6bae6ac1017e7b1b9bd644375aee65b8372","title":"Show Your Work: Improved Reporting of Experimental Results"},{"paperId":"3cd331c997e90f737810aad6fcce4d993315189f","title":"Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"1f51e972f2aac9b14d2dcd0f88fc02feb1e7d790","title":"Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts"},{"paperId":"a0e49f65b6847437f262c59d0d399255101d0b75","title":"What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"afd110eace912c2b273e64851c6b4df2658622eb","title":"Visualizing and Measuring the Geometry of BERT"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"165d51a547cd920e6ac55660ad5c404dcb9562ed","title":"Open Sesame: Getting inside BERT’s Linguistic Knowledge"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1","title":"Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"},{"paperId":"c86b114be9e8c537f07b3a1d37f5a142c14e6d01","title":"Knowledge of animal appearance among sighted and blind adults"},{"paperId":"e2587eddd57bc4ba286d91b27c185083f16f40ee","title":"What do you learn from context? Probing for sentence structure in contextualized word representations"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"a5ec883e080d858b5df60f1b5d711d514459b1e4","title":"Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition"},{"paperId":"19281b9ecdb5c07a93423a506627ab9d9b0cf039","title":"Learning and Evaluating General Linguistic Intelligence"},{"paperId":"efeab0dcdb4c1cce5e537e57745d84774be99b9a","title":"Assessing BERT's Syntactic Abilities"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"ac11062f1f368d97f4c826c317bf50dcc13fdb59","title":"Dissecting Contextual Word Embeddings: Architecture and Representation"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"1536e8958697c5364f68b2e2448905dbbeb3a0ca","title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"c423c2e5f3a0a931f113da53c184542a0fc5a1db","title":"The Description Length of Deep Learning models"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"7d5cf22c70484fe217936c66741fb73b2a278bde","title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents"},{"paperId":"15bce8864a15c20cf483e18099f3601b94673565","title":"Premise Selection for Theorem Proving by Deep Graph Embedding"},{"paperId":"c9f343b492c170c726f607c255ec6c7177dc5800","title":"Verb Physics: Relative Physical Knowledge of Actions and Objects"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"3aa52436575cf6768a0a1a476601825f6a62e58f","title":"Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"},{"paperId":"83e7654d545fbbaaf2328df365a781fb67b841b4","title":"Enhanced LSTM for Natural Language Inference"},{"paperId":"e44da7d8c71edcc6e575fa7faadd5e75785a7901","title":"Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks"},{"paperId":"53d43ccc593bf44e9aa52e3971df1b9dd396e30d","title":"Probing for semantic evidence of composition by means of simple classification tasks"},{"paperId":"1c3884e43bba39c50e6172c403c057659ef3ce83","title":"Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects"},{"paperId":"8a4257052ead8da5773afeff3aa6b6291ca20f60","title":"Building a shared world: mapping distributional to model-theoretic semantic spaces"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","title":"Wikidata: a free collaborative knowledgebase"},{"paperId":"cceb698cbbb828537f2f195fb70b6fdc586d3327","title":"Reporting bias and knowledge acquisition"},{"paperId":"7be3afdb7b7894321027ec90ea0a990aa7a0f266","title":"Natural Language Understanding"},{"paperId":"53a139116437f716b17e60b0b8775708ea597ff5","title":"Formal semantics of Natural Language: Adverbs of quantification"},{"paperId":"d87ceda3042f781c341ac17109d1e94a717f5f60","title":"WordNet : an electronic lexical database"},{"paperId":"8862c1760476de1cf859e2a59998231e18e33317","title":"Generalized quantifiers and natural language"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":null,"title":"Hannaneh Hajishirzi, Alon Talmor, and Sewon Min"},{"paperId":"8eb9b2ea146e3a381a29b4a9314c36f61b71e367","title":"Extracting Commonsense Properties from Embeddings with Limited Human Guidance"},{"paperId":null,"title":"Semisupervised sequence learning"},{"paperId":"b3c41f269f77726e3f480915af017c4527d15f4f","title":"Donald Davidson's truth-theoretic semantics"}],"id":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","summary":"This work proposes eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition, and findings can help future work on designing new datasets, models, and objective functions for pre-training."},{"url":"https://www.semanticscholar.org/paper/4599f96dd1a2584e00d342953fc7e1361ffd6e1f","title":"Neural Status Registers","venue":"ArXiv","year":2020,"referenceCount":57,"citationCount":7,"influentialCitationCount":1,"publicationDate":"15/04/2020","authors":"Lukas Faber,Roger Wattenhofer","citations":[{"paperId":"4578747d52aa1b3537612287352a803a7b17e999","title":"Asynchronous Neural Networks for Learning in Graphs"},{"paperId":"0ce6a798f8222ed8f221326ca566311c648cb4dc","title":"Learning Division with Neural Arithmetic Logic Modules"},{"paperId":"def7c67c3688bc459fbef3ce50a466a0cbf411a9","title":"A Primer for Neural Arithmetic Logic Modules"},{"paperId":"30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0","title":"MC-LSTM: Mass-Conserving LSTM"},{"paperId":"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","title":"Neural Power Units"},{"paperId":"c36277b67814e0a522e786a38e1768612b0e63f2","title":"Exploring the Learning Mechanisms of Neural Division Modules"},{"paperId":"db4a017bba4b242bced04fd108df78e91c8624f6","title":"GwAC: GNNs with Asynchronous Communication"}],"references":[{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"def7c67c3688bc459fbef3ce50a466a0cbf411a9","title":"A Primer for Neural Arithmetic Logic Modules"},{"paperId":"2c0a266f9cb88bb914c138ece0deaab8cf528f78","title":"Neural Sequence-to-grid Module for Learning Symbolic Rules"},{"paperId":"6b989b8327db3a7212141c59c1569f0219775058","title":"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks"},{"paperId":"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","title":"Neural Power Units"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"e081dfa64e5ed12fe15fc885ec82ef1f35ab9830","title":"iNALU: Improved Neural Arithmetic Logic Unit"},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"a4a1a70b7cd477de254f88662787406dcd40d8bb","title":"Neural Execution of Graph Algorithms"},{"paperId":"112ac68ddb0f021517dd465e89918fa52755cc35","title":"Measuring Arithmetic Extrapolation Performance"},{"paperId":"da6ba9f19581bd7e28bc280c53385a1327eb09dd","title":"LSTM Networks Can Perform Dynamic Counting"},{"paperId":"271b31b41782d11c5176a260f0ecdf2611b21e77","title":"What Can Neural Networks Reason About?"},{"paperId":"640a08c0bf0f69cd659bae3fcee7ad359d8eee7c","title":"Neural Stored-program Memory"},{"paperId":"3a6447361b20c249f5306ae17dee43f645430e31","title":"Neural Logic Machines"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"f9717d29840f4d8f1cc19d1b1e80c5d12ec40608","title":"A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"},{"paperId":"e53fd0c9a04df127af80b2699f5f8f23a8a3e2af","title":"On Evaluating the Generalization of LSTM Models in Formal Languages"},{"paperId":"6f69e19348870552a7ab92d038c8b8d753fe6b60","title":"Neural Arithmetic Expression Calculator"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"acc43abe319bca7652a91f7d4ca6187049fb82e4","title":"Measuring abstract reasoning in neural networks"},{"paperId":"660f08f5aa355c19a851d848e0c1321c5c32c1ab","title":"Learning Equations for Extrapolation and Control"},{"paperId":"3a58efcc4558727cc5c131c44923635da4524f33","title":"Relational inductive biases, deep learning, and graph networks"},{"paperId":"06354570d5f6be803d4a79bf59ecbb097bca8755","title":"On the Practical Computational Power of Finite Precision RNNs for Language Recognition"},{"paperId":"21937ecd9d66567184b83eca3d3e09eb4e6fbd60","title":"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"},{"paperId":"4df7bbe3ca7806f39a490c99f17867a0ac299bc3","title":"Learning Explanatory Rules from Noisy Data"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"0c253bb9aee9aa1ae7909700eda845bd3124197f","title":"Neural Program Meta-Induction"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6b024162f81e8ff7aa34c3a43d601a912d012c78","title":"Making Neural Programming Architectures Generalize via Recursion"},{"paperId":"e24cdf73b3e7e590c2fe5ecac9ae8aa983801367","title":"Neural Message Passing for Quantum Chemistry"},{"paperId":"618784f1c811dd3df9a0fb95de9868575b38f7a4","title":"Improving the Neural GPU Architecture for Algorithm Learning"},{"paperId":"efc2ee2c7a9ec5e798720c29c54512407efe06d0","title":"Neural Functional Programming"},{"paperId":"15064b7e47b43a68b6661bc7c3eaaad207493191","title":"Neural Program Lattices"},{"paperId":"0bb487b9cfefd56e79be0a5be5f1e05742683301","title":"Extensions and Limitations of the Neural GPU"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"cdc90e091e64b796f5069bda7bbfdc5a04bd6365","title":"Extrapolation and learning equations"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"49e24b1fd415e20839c09b5dd68a01f61d1681d9","title":"Learning Simple Algorithms from Examples"},{"paperId":"b59d91e0699d4e1896a15bae13fd180bdaf77ea5","title":"Neural Programmer-Interpreters"},{"paperId":"e837b79de602c69395498c1fbbe39bbb4e6f75ad","title":"Learning to Transduce with Unbounded Memory"},{"paperId":"5259755f9c100e220ffaa7e08439c5d34be7757a","title":"Reinforcement Learning Neural Turing Machines - Revised"},{"paperId":"4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e","title":"End-To-End Memory Networks"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"c3823aacea60bc1f2cabb9283144690a3d015db5","title":"Neural Turing Machines"},{"paperId":"71ae756c75ac89e2d731c9c79649562b5768ff39","title":"Memory Networks"},{"paperId":"e15cf50aa89fee8535703b9f9512fca5bfc43327","title":"Going deeper with convolutions"},{"paperId":"13bc4e683075bdd6a3f0155241c276a772d4aa06","title":"Generative adversarial networks"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"7021a29fedec221babfa829de66f6379a5534b6a","title":"Lukás"},{"paperId":"8331c6613c175e98fb2226523e8aabfb36fb2408","title":"Towards Scale-Invariant Graph-related Problem Solving by Iterative Homogeneous GNNs"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Zero-shot learningthe good, the bad and the ugly"},{"paperId":null,"title":"Learning XOR: exploring the space of a classic problem. Department of Computing Science and Mathematics"}],"id":"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","summary":"The Neural Status Register is introduced, inspired by physical Status Registers, and at the heart of the NSR are arithmetic comparisons between inputs that allow end-to-end differentiation and learns such comparisons reliably."},{"url":"https://www.semanticscholar.org/paper/716efab73e1916fdc2a23727b581d200271ed499","title":"BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes","venue":"ArXiv","year":2020,"referenceCount":15,"citationCount":3,"influentialCitationCount":0,"publicationDate":"06/04/2020","authors":"Kushal Chawla,Gale M. Lucas,J. Gratch,Jonathan May","citations":[{"paperId":"67f23dec7687692660d8aa1315b9dbc8e1aacf22","title":"Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"04b748f940147bdd84a9047ad70ed1e9985d9770","title":"An Improvisational Approach to Acquire Social Interactions"}],"references":[{"paperId":"63748e59f4e106cbda6b65939b77589f40e48fcb","title":"Text Summarization with Pretrained Encoders"},{"paperId":"5a9b1a72c7ef52b756ce022176f1496279e9482c","title":"Intelligent Tutoring System for Negotiation Skills Training"},{"paperId":"b626754a0fd7de12c87e88165b2484ac5d98212a","title":"Decoupling Strategy and Generation in Negotiation Dialogues"},{"paperId":"112d8c462be072de121c13a523535f94f0256383","title":"How People Negotiate? From the Analysis of a Dialogue Corpus to a Dialogue System"},{"paperId":"bb669de2fce407df2f5cb2f8c51dedee3f467e04","title":"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6a8dbea5e40831bd6e987c03b76487f45ac49599","title":"Deal or No Deal? End-to-End Learning of Negotiation Dialogues"},{"paperId":"4d025da955a51350df56a54bbb59c1c1b7630e2b","title":"Discourse Structure and Dialogue Acts in Multiparty Dialogue: the STAC Corpus"},{"paperId":"34b4c12146e6aab1309bfe91a86ff3fa76d1677a","title":"Negotiation as a Challenge Problem for Virtual Humans"},{"paperId":"b8a6025630a0972f8a8bbf28376040951d109cfc","title":"GENIUS: AN INTEGRATED ENVIRONMENT FOR SUPPORTING THE DESIGN OF GENERIC AUTOMATED NEGOTIATORS"},{"paperId":"cb826a3899752b796f14df1c50378c64954a6b0a","title":"Statistical Significance Tests for Machine Translation Evaluation"},{"paperId":"395171da2b9c0617f2cc7adba9bbf13706effd9b","title":"Negotiation behavior when cultures collide: the United States and Japan."},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"bde21379dab0ccb52c96d1fb649fff4ed11f3e92","title":"IAMhaggler: A Negotiation Agent for Complex Environments"}],"id":"716efab73e1916fdc2a23727b581d200271ed499","summary":"This model is able to predict a negotiation's outcome within 10% for more than 70% of the cases, and suggests that rather than just being a way to realize a negotiation, natural language should be incorporated in the negotiation planning as well."},{"url":"https://www.semanticscholar.org/paper/8a5590978930070f50c5f9fbf61f67e5d95794f0","title":"Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning","venue":"AAAI Conference on Artificial Intelligence","year":2020,"referenceCount":60,"citationCount":17,"influentialCitationCount":2,"publicationDate":"03/04/2020","authors":"Wenhe Zhang,Chi Zhang,Yixin Zhu,Song-Chun Zhu","citations":[{"paperId":"2ad438f5b820df3ae7d7eabce1d95698b31e3769","title":"Mathematics Algorithms Gaps from Students Perspectives: A Case of Selected Community Secondary Schools"},{"paperId":"223886da4fd58a88930b033d7132cf59a038fbb4","title":"MEWL: Few-shot multimodal word learning with referential uncertainty"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"8a69778110274ff39c6197cfc800b43136ea0a2b","title":"Are Deep Neural Networks SMARTer Than Second Graders?"},{"paperId":"4c5108dddde056f430fd745eaeb661d54aa8a156","title":"Geoclidean: Few-Shot Generalization in Euclidean Geometry"},{"paperId":"a46a19ad0c3c5a88e6e8a03e5dcbccaf909f3319","title":"FACT: Learning Governing Abstractions Behind Integer Sequences"},{"paperId":"3c554b11d6c21799bb1687d5db916a3f963da640","title":"DETR++: Taming Your Multi-Scale Detection Transformer"},{"paperId":"02eae58e5ff7edb2f7cbb334e81c3af6b2768b59","title":"A Review of Emerging Research Directions in Abstract Visual Reasoning"},{"paperId":"59cb81d7763d8de23d9b4144c16287cb6808da1d","title":"Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices"},{"paperId":"03da1b6759f70c10e49b33a0ee914cb893d6f949","title":"Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning"},{"paperId":"40b065eb3aa5c5a54962aee78ebe30943beaabb1","title":"Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images"},{"paperId":"316c5d697f7caf92b419213e929a6063afaf253c","title":"ACRE: Abstract Causal REasoning Beyond Covariation"},{"paperId":"721379fe5d859f13442102a5681cf58952b5af14","title":"Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution"},{"paperId":"edaafec59b651d503ac7c8a86f8e2335273e1f7a","title":"Learning by Fixing: Solving Math Word Problems with Weak Supervision"},{"paperId":"862bca372c0c7475f9e5eb4951b9309f39cd4ace","title":"Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense"},{"paperId":"ff1b1f900e35d468c5ca5a065b46f442f80847dd","title":"A HINT from Arithmetic: On Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"e764dee4e50db01d77976e8f313fc092fc0eba85","title":"GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning"}],"references":[{"paperId":"0187141e52f6d5cd0cf812ced648960e8f610abe","title":"Learning Perceptual Inference by Contrasting"},{"paperId":"70336ddb0082a43cf6180a321cb0c0683ac6d59e","title":"Theory-based Causal Transfer: Integrating Instance-level Induction and Abstract-level Structure Learning"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"d0bfd3cb732471a0843a39d2d047caf60a844466","title":"RAVEN: A Dataset for Relational and Analogical Visual REasoNing"},{"paperId":"0d16b0fbe1e2bf6ef02aea2e058f2e13c3a83fa2","title":"Learning to Make Analogies by Contrasting Abstract Relational Structure"},{"paperId":"acc43abe319bca7652a91f7d4ca6187049fb82e4","title":"Measuring abstract reasoning in neural networks"},{"paperId":"b36a5bb1707bb9c70025294b3a310138aae8327a","title":"Automatic differentiation in PyTorch"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"007112213ece771be72cbecfd59f048209facabd","title":"A simple neural network module for relational reasoning"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"40d71f60c70527ba6007a0458c2004fa45753a3c","title":"Associations of non-symbolic and symbolic numerical magnitude processing with mathematical competence: a meta-analysis."},{"paperId":"ba84b8f16efc032945cd8ed960c121a9bcd064d5","title":"Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"815c84ab906e43f3e6322f2ca3fd5e1360c64285","title":"Human-level concept learning through probabilistic program induction"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"f9c990b1b5724e50e5632b94fdb7484ece8a6ce7","title":"Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"946dffe06446c38a397fafb8d54e48c764196712","title":"A Source Book Of Gestalt Psychology"},{"paperId":"a4ffb87c11a7d1a52777a99817e18640272606a4","title":"Flexible and adaptive use of strategies and representations in mathematics education"},{"paperId":"8b0401a2c013b07f8daea62cff00df1dc1c0ec70","title":"Solving Geometric Analogy Problems Through Two-Stage Analogical Mapping"},{"paperId":"6b34f82bc2a1f5a802bb47e8e8e176fa7cfa595d","title":"Representation of number in the brain."},{"paperId":"f75c694f31ae66fe9b5f4ae0345b3cc8a7cff1a6","title":"Oxford English Dictionary."},{"paperId":"9fc2448d9e38dd6a25fd236f8a78818f2ea758a5","title":"Exploring the microstructure of children's central conceptual structures in the domain of number."},{"paperId":"573ed869823a4f6c0730a8d24e6307cdfcf81517","title":"The ABC of cardinal and ordinal number representations"},{"paperId":"8c560202ceda574c65fcef53ba6a22c2e6640384","title":"The Organization of Learning"},{"paperId":"09699eae4b5f9ba6eb022c7381cc84a2e00c0ee7","title":"Using Measures of Number Sense to Screen for Difficulties in Mathematics: Preliminary Findings"},{"paperId":"09962320700f0ff1c71d353d406793531b6fc807","title":"Visual Analogy in Problem Solving"},{"paperId":"37e4180aaf2e4762cbdc9dd04e714643d86c219f","title":"Culture and systems of thought: holistic versus analytic cognition."},{"paperId":"b82e21162f76da0e082282a298e7519f9b0bdd81","title":"Constraint relaxation and chunk decomposition in insight problem solving"},{"paperId":"5385a494e90c6fc01195bf44d680c6e01cf687b5","title":"A System for Relational Reasoning in Human Prefrontal Cortex"},{"paperId":"1050141af39d7c64303e60b608db1cec23db528d","title":"Bayesian Modeling of Human Concept Learning"},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"249b235ef1fdc4de22032b60e6f12ed10adba0de","title":"Ordering of the numerosities 1 to 9 by monkeys."},{"paperId":"94518df3c15d4c88ec16cc4cc9f301895bca1af1","title":"Abstract representations of numbers in the animal and human brain"},{"paperId":"7e879518ec31255bc75c7f5a1586df9f39b231e6","title":"Brain mechanisms of quantity are similar in 5-year-old children and adults."},{"paperId":"6d9dc49325c8dd4edbf854c2a2eb174a52d9b126","title":"Addition and subtraction by human infants"},{"paperId":"eb45b073b52040d05aa9b462415cb881595b85fc","title":"Concept formation knowledge and experience in unsupervised learning"},{"paperId":"40e2df90808e9a38b342bff588b2583e66059b44","title":"THE NICOMACHEAN ETHICS"},{"paperId":"617b1261af3be2a9e72adc343e82c9edc9aedad3","title":"Numerical competence in animals: Definitional issues, current evidence, and a new research agenda"},{"paperId":"93151da5382d95b2b70e415d814b585871117988","title":"Perception of numbers by human infants."},{"paperId":"4978e96a066ab4e9189f0d117ef16378953d92e5","title":"Theory of fluid and crystallized intelligence: A critical experiment."},{"paperId":"06f2e16b9e19268d714b956af49649b2e8c79fc3","title":"Decomposing Human Causal Learning: Bottom-up Associative Learning and Top-down Schema Reasoning"},{"paperId":"49830f605c316fec8e9e365e7e64adeea47e89d8","title":"Modeling Visual Problem Solving as Analogical Reasoning"},{"paperId":null,"title":"and Forbus"},{"paperId":"7606136f5c67d8806cdc8750770db1a8c5073f10","title":"A Structure-Mapping Model of Raven's Progressive Matrices"},{"paperId":"d880bd87dea6c66de8c3326d2b2416947f15e6b8","title":"How much does number matter to a monkey (Macaca mulatta)?"},{"paperId":null,"title":"and Mumford"},{"paperId":null,"title":"A stochastic grammar of images"},{"paperId":"51391f0069c0491096971834d78a9ee29a6c811f","title":"AND CASE"},{"paperId":"0939cd35617482641d6e5e4f7619e68601e8048a","title":"Numbers, language, and the human mind"},{"paperId":"161889c6dfaadfda26a3834342354b52198ab433","title":"Psychological models for the development of mathematical understanding: rational numbers and functions"},{"paperId":"7847fa2a5a86ab30068d9d6b1757a96c386ae43c","title":"Visual Analogy: Consciousness as the Art of Connecting"},{"paperId":null,"title":"Oxford pyschologists Press. [Raven 1936] Raven, J. C. 1936. Mental tests used in genetic studies: The performance of related individuals on tests mainly educative and mainly reproductive"},{"paperId":null,"title":"Visualisation and the development of number sense with kindergarten children. Children's number learning: A research monograph of MERGA/AAMT Adelaide: Australian Association of Mathematics teachers"},{"paperId":"a64dddf69180e744f62a22d9be07736ad4b2d7d9","title":"Assessment of a problem-centered second-grade mathematics project."},{"paperId":"8aa8e60994006f803fd25f86f039e5a5b1e2542c","title":"Animal cognition: the representation of space, time and number."},{"paperId":"5ef6f2547e8a004a2549a1cfee26906879fe0025","title":"Oxford English dictionary"},{"paperId":"318cddbeb64fc022a482dfcb316f732af99e6d47","title":"Manual for Raven's progressive matrices and vocabulary scales"},{"paperId":"43a0503e38b4c47a5b794c04439299bd78310189","title":"Laws of organization in perceptual forms."}],"id":"8a5590978930070f50c5f9fbf61f67e5d95794f0","summary":"A dataset, Machine Number Sense (MNS), consisting of visual arithmetic problems automatically generated using a grammar model—And-Or Graph (AOG), called for attention in fusing the classic search-based algorithms with modern neural networks to discover the essential number concepts in future research."},{"url":"https://www.semanticscholar.org/paper/45e5d7637a585a87d967a4a357d17c5d89aecea2","title":"A Formal Hierarchy of RNN Architectures","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":30,"citationCount":49,"influentialCitationCount":1,"publicationDate":"01/04/2020","authors":"William Cooper Merrill,Gail Weiss,Yoav Goldberg,Roy Schwartz,Noah A. Smith,Eran Yahav","citations":[{"paperId":"9f64d655fc6815720b17b5b50838bd7cce4c27fd","title":"Spatial prediction of physical and chemical properties of soil using optical satellite imagery: a state-of-the-art hybridization of deep learning algorithm"},{"paperId":"21034467d4e1137802f610948c8ad26597400143","title":"On The Expressivity of Recurrent Neural Cascades"},{"paperId":"093df6958c8acf1ed2ba9dc3ca1abdfdbc601aa5","title":"A provably stable neural network Turing Machine with finite precision and time"},{"paperId":"6624065bd2825f500e93624671793d963187d066","title":"Transformers as Recognizers of Formal Languages: A Survey on Expressivity"},{"paperId":"1fbabc724cac5e500ca8328de39fa42a1dc58783","title":"Unraveling Feature Extraction Mechanisms in Neural Networks"},{"paperId":"9c71d178705989cd4371f8e760508f11b18a4bb4","title":"Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions"},{"paperId":"c169e5a40f4565693224b661370e85be940d49a4","title":"On the Representational Capacity of Recurrent Neural Language Models"},{"paperId":"ba79ab646311ef70e169a75078c05e5f6c589626","title":"Recurrent Neural Language Models as Probabilistic Finite-state Automata"},{"paperId":"e1112856e60ab89a5ebcf45cbb68ce4a2b69397c","title":"Refined Kolmogorov Complexity of Analog, Evolving and Stochastic Recurrent Neural Networks"},{"paperId":"dd3760fd99acf88a4c1e0401e87c4cc1e39bc4fd","title":"Augmenting Recurrent Graph Neural Networks with a Cache"},{"paperId":"aff3f5b090260646aad8c5abc61770640446a714","title":"DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification"},{"paperId":"8133ad3008aebcc734708f05580a21a0b948ed60","title":"Exploring the Promise and Limits of Real-Time Recurrent Learning"},{"paperId":"94c1da0e6bb1d9792986fa4badeb20af14737ebc","title":"Modeling rapid language learning by distilling Bayesian priors into artificial neural networks"},{"paperId":"e81602166f323d4eb8376bdc7b57c0744aac820f","title":"Empirical Analysis of the Inductive Bias of Recurrent Neural Networks by Discrete Fourier Transform of Output Sequences"},{"paperId":"221de85e2054d9f1762881fa6852f1061b53e399","title":"Modelling Concurrency Bugs Using Machine Learning"},{"paperId":"eb87c18dd5bfcc1106086a4c89653ecd02d71f33","title":"Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks"},{"paperId":"bab6f0823c3ca64f8c5c9f56f2467c27aabc80f6","title":"Exploring the Long-Term Generalization of Counting Behavior in RNNs"},{"paperId":"fb6d75a4f3b1af2058f59957116c178a47b56f05","title":"Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions"},{"paperId":"96414e6290ed0165e677e481b996044fd6c6b529","title":"Memory-Augmented Graph Neural Networks: A Brain-Inspired Review"},{"paperId":"9bd14955135aa1bf3927c6011232702d25394352","title":"Interaction of lexical strata in hybrid compound words through gradient phonotactics"},{"paperId":"c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617","title":"Neural Networks and the Chomsky Hierarchy"},{"paperId":"bab33a1a464bc237bfeccfaba9bd155d2b90d186","title":"Implicit n-grams Induced by Recurrence"},{"paperId":"0c7b82cef44cbdca4cbefe17a8ba7ca47fbffea9","title":"Exploring a diverse world of effector domains and amyloid signaling motifs in fungal NLR proteins"},{"paperId":"3fa611d27986ba9983df8c4013114b995c1793cc","title":"Extracting Finite Automata from RNNs Using State Merging"},{"paperId":"33382bb7352b4aabd33a8d2285f5b3ba73861208","title":"Training dynamics of neural language models"},{"paperId":"0735fb79bf34698c1df4461a05ed51c232c412e4","title":"Thinking Like Transformers"},{"paperId":"56f46451a8790a919c7048f869ee9aaac7ae2fd7","title":"Extracting Weighted Automata for Approximate Minimization in Language Modelling"},{"paperId":"26cf06dc782a7c484c5c2463b1c9c7482536c4e4","title":"Learning and Generalization in RNNs"},{"paperId":"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages"},{"paperId":"fdc1815b83a59c0667ac75c8696febf5f1fd59e8","title":"Lexical strata and phonotactic perplexity minimization"},{"paperId":"6a3aa7acaacd72fecb33e32d1c524883a45a18b7","title":"Formal Language Theory Meets Modern NLP"},{"paperId":"39928e0f5fd2d65aa92ea8fd03108b5d1f25f6f6","title":"Stronger Separation of Analog Neuron Hierarchy by Deterministic Context-Free Languages"},{"paperId":"2b0e5a699095184144023cce674e9c88994598a6","title":"Connecting Weighted Automata, Tensor Networks and Recurrent Neural Networks through Spectral Learning"},{"paperId":"f10a04a77fd1cd719792de374a60f3fd03f6b944","title":"Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent"},{"paperId":"9f13b84e05a0c6c717b06038ddbfd9af55b51806","title":"Parameter Norm Growth During Training of Transformers"},{"paperId":"d06e84ac9e912b415719f0e7f3163d59e0a329cd","title":"RNNs Can Generate Bounded Hierarchical Languages with Optimal Memory"},{"paperId":"52d87b59a1fa6561b90d5ea56f21180a9b1a505c","title":"How Can Self-Attention Networks Recognize Dyck-n Languages?"},{"paperId":"93a640dfdac5950bde902db9db31e3502e7fffe2","title":"Distillation of Weighted Automata from Recurrent Neural Networks using a Spectral Approach"},{"paperId":"0def0a8dfccd1e70bf0f0f8b1a2217252fa3f952","title":"On the Ability of Self-Attention Networks to Recognize Counter Languages"},{"paperId":"10c86505de83647c7b4157595ab10f64e97c94ef","title":"On the Ability and Limitations of Transformers to Recognize Formal Languages"},{"paperId":"032399b7fc693a9fc12bb26d6be8c02d77dd397a","title":"What they do when in doubt: a study of inductive biases in seq2seq learners"},{"paperId":"75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1","title":"On the Computational Power of Transformers and Its Implications in Sequence Modeling"},{"paperId":"ae807218c494fd4fb74690d3a48d18cdfbb44b1d","title":"Provably Stable Interpretable Encodings of Context Free Grammars in RNNs with a Differentiable Stack"},{"paperId":"299546bc706ee776aea9d58cb21c729ee4499bd2","title":"A provably stable neural network Turing Machine"},{"paperId":"0d317ae9d1cc285cfa90f44e0afbdb34b8b24d41","title":"Analog neuron hierarchy"},{"paperId":"1862ff140e0169c6b367dc3fd9e7c714dae38026","title":"N EURAL N ETWORKS AND THE C HOMSKY H IERARCHY"},{"paperId":"1781517a91be8248dd0febad65211a1b6614e199","title":"On the Power of Saturated Transformers: A View from Circuit Complexity"},{"paperId":"5bd9af165c7fe18aa6235df5032155befa522454","title":"Deep Sentiment Analysis: A Case Study on Stemmed Turkish Twitter Data"},{"paperId":"7e1c4d2f0b276bd30656f7577444c48210ab54ad","title":"Investigating Backpropagation Alternatives when Learning to Dynamically Count with Recurrent Neural Networks"}],"references":[{"paperId":"d15e715d99a52f1c8a1ae6ec0ca853b13e04bbc7","title":"On the Linguistic Capacity of Real-Time Counter Automata"},{"paperId":"a28ee1bea680c745636d7e09218fffda5d544ffe","title":"Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages"},{"paperId":"f0471ad6daecabbd2521c6cede46007a02260771","title":"RNN Architecture Learning with Sparse Regularization"},{"paperId":"b3564be8b79f25585acb035f3deaf4ae93c26d8f","title":"Theoretical Limitations of Self-Attention in Neural Sequence Models"},{"paperId":"da6ba9f19581bd7e28bc280c53385a1327eb09dd","title":"LSTM Networks Can Perform Dynamic Counting"},{"paperId":"a1b35b15a548819cc133e3e0e4cf9b01af80e35d","title":"Sequential Neural Networks as Automata"},{"paperId":"a56ebc39b8c527774be705cccdcb5f66c7302e0c","title":"Rational Recurrences"},{"paperId":"6bf2f1dc081b319ed55f2e185278c7ebfacd9e45","title":"Bridging CNNs, RNNs, and Weighted Finite-State Machines"},{"paperId":"06354570d5f6be803d4a79bf59ecbb097bca8755","title":"On the Practical Computational Power of Finite Precision RNNs for Language Recognition"},{"paperId":"93b4cc549a1bc4bc112189da36c318193d05d806","title":"AllenNLP: A Deep Semantic Natural Language Processing Platform"},{"paperId":"31b26b31f28988ebcfe7ff356e7fda7e17f1558c","title":"Recurrent Neural Networks as Weighted Language Recognizers"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2d876ed1dd2c58058d7197b734a8e4d349b8f231","title":"Quasi-Recurrent Neural Networks"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba","title":"Convolutional Neural Networks for Sentence Classification"},{"paperId":"c71325ea1e3391a688c70024c855b1b142854a2c","title":"Spectral learning of weighted automata"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"e72d20e5b6e21a2013d4b0d62001a194d86a210e","title":"Context-Free Recognition with Weighted Automata"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"93aaf126443643fb0835df896ab07b523f2c9613","title":"Analog computation via neural networks"},{"paperId":"17594df98c222217a11510dd454ba52a5a737378","title":"On the computational power of neural nets"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"},{"paperId":"c49838c33a6d190dea1981a04f54ee3443ac607b","title":"Realizations by Stochastic Finite Automata"},{"paperId":"31b1beb7a33feee7853fe195e5bc554d581154c6","title":"Counter machines and counter languages"},{"paperId":"19c278e06339c5a3e7dd94403ea70b7e919579bb","title":"Turing Machines with Restricted Memory Access"},{"paperId":null,"title":"2017) into the state expressiveness hierarchy. We consider a single-head self attention encoder"},{"paperId":"cb19a063ea91e716a2c84d7bd3ce8935a645ef8d","title":"Rational and Recognisable Power Series"},{"paperId":"6a4d840d64865ad887a4717a5b769e094a050c73","title":"The handbook of brain theory and neural networks"},{"paperId":null,"title":"Matrices de Hankel"},{"paperId":null,"title":"A CM is (Σ × Q ) -restricted iff u and δ depend only on the current input σ ∈ Σ and the current state q ∈ Q"}],"id":"45e5d7637a585a87d967a4a357d17c5d89aecea2","summary":"It is hypothesized that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy, and empirical results to support this conjecture are provided."},{"url":"https://www.semanticscholar.org/paper/72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks","venue":"ArXiv","year":2020,"referenceCount":46,"citationCount":10,"influentialCitationCount":1,"publicationDate":"18/05/2020","authors":"Swaroop Mishra,Arindam Mitra,Neeraj Varshney,Bhavdeep Singh Sachdeva,Chitta Baral","citations":[{"paperId":"18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6","title":"Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception"},{"paperId":"fb30166c218bef3597b0d9789ad340defc3989ca","title":"In-BoXBART: Get Instructions into Biomedical Multi-Task Learning"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"},{"paperId":"fb1af83fcf237692768a30a4f9eb0ab61c500c14","title":"Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems"},{"paperId":"dd75f713420cd8eb3e0b3ce870b2680cd93b39fd","title":"Sequence to General Tree: Knowledge-Guided Geometry Word Problem Solving"},{"paperId":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"884c0b6db564208d99cadf2548f0aa96dee5f859","title":"Commonsense Reasoning with Implicit Knowledge in Natural Language"}],"references":[{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"a87f0bac2ed58e8a79d33d0c1cf81c6407cd645f","title":"Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks"},{"paperId":"04f4e55e14150b7c48b0287ba77c7443df76ed45","title":"PIQA: Reasoning about Physical Commonsense in Natural Language"},{"paperId":"8ea4a76fc781d9dcbf792e47435092d485d60a56","title":"Comprehensive Multi-Dataset Evaluation of Reading Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"cc9afe82dd74486598023dcad37d4c8b810f15ac","title":"Tag-based Multi-Span Extraction in Reading Comprehension"},{"paperId":"efa23afbed4c95bb416b72e1bd477f6c27471baf","title":"Exploring ways to incorporate additional knowledge to improve Natural Language Commonsense Question Answering"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a4804394c362e7129312ae20fc094e60f56db3b0","title":"Careful Selection of Knowledge to Solve Open Book Question Answering"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"51004bc6461a572e1189a0e3b32b441155d760ce","title":"QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships"},{"paperId":"9784fbf77295860b2e412137b86356d70b25e3c0","title":"The Natural Language Decathlon: Multitask Learning as Question Answering"},{"paperId":"8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6","title":"Hypothesis Only Baselines in Natural Language Inference"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"8b7336f5dd13a45d4aab38428b4a88ce507ea310","title":"Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving"},{"paperId":"7b6f61e6067d3fe520e01b19e2941aa122fcd564","title":"Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"390808617b277987999b6e8e0aa5742aab54a6a0","title":"My Computer Is an Honor Student - but How Intelligent Is It? Standardized Tests as a Measure of AI"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"abb33d75dc297993fcc3fb75e0f4498f413eb4f6","title":"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"},{"paperId":"818b92bfad6e11d849bae552be60111579d91e91","title":"Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge!"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"128cb6b891aee1b5df099acb48e2efecfcff689f","title":"The Winograd Schema Challenge"},{"paperId":"75f9f2743951edd71369372d9cd6416cc822f063","title":"Number as a cognitive technology: Evidence from Pirahã language and cognition"},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"243e2140a14022531461c5f169ebbe14930efccc","title":"Two Cheers for Rebooting AI: Building Artificial Intelligence We Can Trust"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"4a98b6353d8492ed6ebe71c3a7df5f866f457173","title":"What’s Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"spacy 2: Natural language understanding with bloom embeddings"},{"paperId":null,"title":"Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning"},{"paperId":null,"title":"Honnibal and Montani, 2017] Matthew Honnibal and Ines Montani. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing"},{"paperId":"13167f9cd8c7906ca808b01d28dca6dd951da8a5","title":"of the Association for Computational Linguistics"}],"id":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","summary":"This work introduces NUMBERGAME, a multifaceted benchmark to evaluate model performance across numerical reasoning tasks of eight diverse formats, and takes forward the recent progress in generic system development, demonstrating the scope of under-explored tasks."},{"url":"https://www.semanticscholar.org/paper/016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":32,"citationCount":118,"influentialCitationCount":11,"publicationDate":"02/05/2020","authors":"Bill Yuchen Lin,Seyeon Lee,Rahul Khanna,Xiang Ren","citations":[{"paperId":"18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6","title":"Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception"},{"paperId":"557182159154a0478b50f19838767ebb1749db0d","title":"Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models"},{"paperId":"0960852f3e41b818065735bbcd87666f244cb201","title":"CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models"},{"paperId":"b6a90162584b50b8ef92950695901fd09cbea46e","title":"Explainable Authorship Identification in Cultural Heritage Applications: Analysis of a New Perspective"},{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"082d1415f36ab3cfb308892ade171ef5bef61020","title":"ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense"},{"paperId":"23cc6b2ed88872fcd3767cf054100e8eddcdb0a1","title":"CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks"},{"paperId":"d35e2ea704c7a1ba690cb1806891fcd6b84065ee","title":"GeoLLM: Extracting Geospatial Knowledge from Large Language Models"},{"paperId":"6db6e6e71cc54435265643e19fcbdc7f3ba4c772","title":"Crystal: Introspective Reasoners Reinforced with Self-Feedback"},{"paperId":"856c342606aca05434e48f2e53cdbd6f6b886802","title":"Pre‐trained language models: What do they know?"},{"paperId":"4ed50b9ff154f2725503306f3cf0a21e2681838f","title":"Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that Don't have a Definitive Answer?"},{"paperId":"33864bf364ab17b231054f5d8a199bc5d2225918","title":"TaskLAMA: Probing the Complex Task Understanding of Language Models"},{"paperId":"207f2ee22950d453b97ee79544d0a7226acc7b41","title":"Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts"},{"paperId":"88b4ed17e8881b4e420627f3650ddab181bb632b","title":"The Web Can Be Your Oyster for Improving Large Language Models"},{"paperId":"0b315e6d4d800e04caf2f587312ce163e748d10c","title":"Numeric Magnitude Comparison Effects in Large Language Models"},{"paperId":"c238c8df10846ef168a82a48d5e3208f1b617cb2","title":"Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey"},{"paperId":"b953a1527c8878698f9adb2691e425d5d206cae4","title":"Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements"},{"paperId":"3270c3054e6a14c5ee483f690ccda7367dd9a556","title":"CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population"},{"paperId":"be8f769f135b0ffcf829594b9421788781f38008","title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts"},{"paperId":"4d7571441f507f39133209e8afa7ad088da2199c","title":"ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models"},{"paperId":"12d16f426edc6ab248fb476007bd1646282d4d68","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"8018a68956d6751d7ea76110537d5a2e86ec05c4","title":"The Life Cycle of Knowledge in Big Language Models: A Survey"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"ce2e318d33b0e44fe371de981f072527abf2f6fb","title":"Class Cardinality Comparison as a Fermi Problem"},{"paperId":"06396c7cd5d223a1776abf8811359ec7bc05d420","title":"Knowledge-Augmented Methods for Natural Language Processing"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"9af3b6b3f8dcedbb02b88936c428e1cd02503a8a","title":"Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?"},{"paperId":"17a8b5e6fef1f69979d57021a8f30a5159e152c7","title":"Commonsense Reasoning for Conversational AI: A Survey of the State of the Art"},{"paperId":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey"},{"paperId":"83c2bb56f58d4ce63adb2faf073fc35c3515cda8","title":"Understanding Finetuning for Factual Knowledge Extraction from Language Models"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"71471561137956d6aeec39173db460a28d86c11b","title":"Analogical Math Word Problems Solving with Enhanced Problem-Solution Association"},{"paperId":"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","title":"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data"},{"paperId":"03c4ecc2796ecde6a93562fdd149cea10157f805","title":"Visually Grounded Commonsense Knowledge Acquisition"},{"paperId":"1d4d3bdbc70ab400d206ae209546f267f85817c4","title":"SocioProbe: What, When, and Where Language Models Learn about Sociodemographics"},{"paperId":"6681f0a0cc6ddaa70cdea109b941c47538caaa27","title":"Judicial knowledge-enhanced magnitude-aware reasoning for numerical legal judgment prediction"},{"paperId":"0cf5c61f367e3937dc07be634a43952a50b589a4","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"5137977bcbc4030a8c888cc5998aa047764535b0","title":"Improving Imbalanced Text Classification with Dynamic Curriculum Learning"},{"paperId":"d130837a5601f4f82380736897d07f3008de1fa4","title":"Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","title":"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility"},{"paperId":"e02dce6ee032a13b1653f69b034a35676e7d4dc2","title":"Can language representation models think in bets?"},{"paperId":"3b622664d44a57280d3a189fa6475e56b96f1add","title":"CIKQA: Learning Commonsense Inference with a Unified Knowledge-in-the-loop QA Paradigm"},{"paperId":"b47a3f7bcf540adb6fd97869c51449888d3160bb","title":"Can Language Models Be Specific? How?"},{"paperId":"2591c66c6006c9c275a3dc7108a487934bc1c06f","title":"Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering"},{"paperId":"15bacb240e2598457af4ded3039b6988aa9706f0","title":"Few-shot Adaptation Works with UnpredicTable Data"},{"paperId":"e852c5ed7f4f1a4375f0f3bce1b4e445c7684e7e","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP"},{"paperId":"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","title":"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"},{"paperId":"011095a0082e5e301f9bf30267b193c1c9e7e370","title":"Perturbation Augmentation for Fairer NLP"},{"paperId":"1da214f8f265445b5997f5d677452819b334bdfb","title":"Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts"},{"paperId":"e3dae33c5bdf397abdefd971ea34c48fb836dcc0","title":"GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models"},{"paperId":"d48b29889241551e1ee6622fa78c3fa4159255dd","title":"Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning"},{"paperId":"39f0f28848990f74eeb9019f579c6ebcc8ef3ea1","title":"TransTab: Learning Transferable Tabular Transformers Across Tables"},{"paperId":"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence"},{"paperId":"251c3afbaafcc9b5178534be9109f644bfc5e912","title":"Enhancing Knowledge Bases with Quantity Facts"},{"paperId":"591c54203cb06602d302230de1d6a60d753a4d8e","title":"What If: Generating Code to Answer Simulation Questions in Chemistry Texts"},{"paperId":"cf2453589ee98561c599694fce3220aa79e5c56b","title":"Probing Script Knowledge from Pre-Trained Models"},{"paperId":"682ee9c06f3eff3e3708b4d0419dc85ecf9c6c87","title":"What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured"},{"paperId":"706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding"},{"paperId":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"},{"paperId":"50f7d69ddd7f9b34f5121607fbdcc57236d65b8c","title":"Can Pre-trained Language Models Interpret Similes as Smart as Human?"},{"paperId":"8c62277dada489904a63de4dd87336c27c68fb5e","title":"Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models"},{"paperId":"7bebb48d34c219b119ca2d4ffc97d7fd4940c35c","title":"On the data requirements of probing"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"4497f3ed54ac520b50ffa05df04b37a59d4c1265","title":"Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET"},{"paperId":"7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"},{"paperId":"1ea78b1684371de0e859edf759e5e659152d31e3","title":"What do Large Language Models Learn about Scripts?"},{"paperId":"e02a757617c2c42eb62889cc4d4aee3765928303","title":"The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"47df3fd32d00220c85c2c51a571254fd99b2ecc7","title":"MetaICL: Learning to Learn In Context"},{"paperId":"12a763cb52f650710900790ca0bc43e5d5b88be6","title":"Generated Knowledge Prompting for Commonsense Reasoning"},{"paperId":"1360dc6d21ccbf2c23701c9ddd7c6ab8d04e4531","title":"The World of an Octopus: How Reporting Bias Influences a Language Model’s Perception of Color"},{"paperId":"d83c7aa12420d5d035f43d7737bfa6893e9e2c61","title":"Exploring Universal Intrinsic Task Subspace via Prompt Tuning"},{"paperId":"b15469d0ab3dc3a9dec037d761817b3fe546bed6","title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey"},{"paperId":"18ae4f4ccce2602fc09d5cf1ddcdfd9eaf416958","title":"Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP)"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0c4b2ae22f08425563a69527a3433516fc9737a1","title":"CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge"},{"paperId":"f640ef82eef63f16e9e732409e72aec638a53a3d","title":"Accurate, yet inconsistent? Consistency Analysis on Language Understanding Models"},{"paperId":"e248b22b7107ab057a0ea42a4dd075a5a4b2df26","title":"How to Query Language Models?"},{"paperId":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"},{"paperId":"40a1f266bb5ca853837355bfff272a55f0049c81","title":"Mining Numbers in Text: A Survey"},{"paperId":"e337ed6543c2e6e7e51c312c7d998798fc79fdde","title":"Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases"},{"paperId":"c6fd846b9b8f9eb0a492d6d6242fffce987c4580","title":"Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning"},{"paperId":"62953ca1252c9febe07c7007a10911726f37792d","title":"TIMEDIAL: Temporal Commonsense Reasoning in Dialog"},{"paperId":"5aab57cc0530560d82c74c055f664280619d7e81","title":"PROST: Physical Reasoning about Objects through Space and Time"},{"paperId":"6597d61bdb531051678c773526758a6dc113b9ce","title":"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences"},{"paperId":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"},{"paperId":"f80b837a211f71c6647c5755d4569559f0c2c0f7","title":"Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization"},{"paperId":"d3a1e7f060bfe7d97c2e430da9eb6967bbbe358a","title":"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"},{"paperId":"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"},{"paperId":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","title":"Relational World Knowledge Representation in Contextual Language Models: A Review"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"9f620ad41b4e506e777c0665681b839c89cd682a","title":"Dimensions of Commonsense Knowledge"},{"paperId":"71fab1ce3c66998ba681ab378484be77690327a9","title":"RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge"},{"paperId":"b042444e4e9501ab2fcae6b1f6b9d834146b3fc9","title":"Do Language Embeddings capture Scales?"},{"paperId":"011869f932f89d047ce2bd36d73a95cc04888193","title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms"},{"paperId":"64ad3e1fbbc9a4bec2a414fb31b05ee9a62d50cb","title":"tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation"},{"paperId":"60c11f02982bcfe1f8be25c87c82606aeef9758b","title":"Probing Power by Prompting: Harnessing Pre-trained Language Models for Power Connotation Framing"},{"paperId":"9815f586f329a098df83ca872799880bd5cb1a15","title":"Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes"},{"paperId":"a884bc8bc5b04e5ad096649856df5b7931fd3d23","title":"Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer"},{"paperId":"10f829a80a7ee0cdb16307f04e206133d75f81da","title":"Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Conflicts"},{"paperId":"c77feebb06b530079a34f04c64de359ae28c0a32","title":"Predicting Numerals in Text Using Nearest Neighbor Language Models"},{"paperId":"ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","title":"Numerical Correlation in Text"},{"paperId":"da20a595aee606dec221bd1c5757274b17f39c66","title":"Probing Measuring Skills in Pre-trained Language Models"},{"paperId":"7db7fb25a8753c9efc6b3722d178f94fcc1f82d3","title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP"},{"paperId":"2a83a92b08e0f3873d07162c73c67e533321112e","title":"Aligning Generative Language Models with Human Values"},{"paperId":"31662a7beb1acbdda51ecb6d8ec419fb9887a817","title":"Topicalization in Language Models: A Case Study on Japanese"},{"paperId":"a41b2d46b462b4af3dfada86c355a07814ae05db","title":"Is Self-Attention Powerful to Learn Code Syntax and Semantics?"},{"paperId":"1e336827a740f0e4a3def14d261e55d1bda26d83","title":"Probing Representations of Numbers in Vision and Language Models"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":"2b8c6c2ae778184f41db3467ed2cde5342aae676","title":"RiddleSense: Answering Riddle Questions as Commonsense Reasoning"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"884c0b6db564208d99cadf2548f0aa96dee5f859","title":"Commonsense Reasoning with Implicit Knowledge in Natural Language"},{"paperId":"080df61ee1c15ff3c8e5d0d82d60bfd80e372e38","title":"Probing Toxic Content in Large Pre-Trained Language Models"},{"paperId":"02d4e18bc3677be373ff3f31c4a5702cb4e1f520","title":"Pre-trained Language Models in Biomedical Domain: A Survey fromMultiscale Perspective"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":null,"title":"Later datasets increase in complexity and scale , incorporating reading comprehension"}],"references":[{"paperId":"35ca61aee1d58ccb641eec3804ffd96fdb968cb7","title":"GenericsKB: A Knowledge Base of Generic Statements"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"f67fcbb1aec92ae293998ddfd904f61a31bef334","title":"Inducing Relational Knowledge from BERT"},{"paperId":"01f2b214962997260020279bd1fd1f8f372249d4","title":"Evaluating Commonsense in Pre-trained Language Models"},{"paperId":"521b4e26df0f1cf5763dece14cbb218df152dc59","title":"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering"},{"paperId":"727c9d3846ebd80a9138d0e6c9e995d9afc1d312","title":"CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning"},{"paperId":"41094beddc498680d807e99e07efa41fec5d6724","title":"How Pre-trained Word Representations Capture Commonsense Physical Comparisons"},{"paperId":"136396e988dbbdaca433096e931b22732c3d95dd","title":"Text2Math: End-to-end Parsing Text into Math Expressions"},{"paperId":"bd2895717effe3cc52e77fbee3da8117cf1c01e1","title":"Measuring Numerical Common Sense: Is A Word Embedding Approach Effective?"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"710d183174844da5b7f392667f3cc25d2b098dde","title":"KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"f98e135986414cccf29aec593d547c0656e4d82c","title":"Commonsense Knowledge Mining from Pretrained Models"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"95a251513853c6032bdecebd4b74e15795662986","title":"What Does BERT Look at? An Analysis of BERT’s Attention"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796","title":"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"8cb592fa5e30e6fa5abe7041767768964f1f8cf4","title":"Do Language Models Have Common Sense"},{"paperId":"d7b6753a2d4a2b286c396854063bde3a91b75535","title":"A Simple Method for Commonsense Reasoning"},{"paperId":"c9f343b492c170c726f607c255ec6c7177dc5800","title":"Verb Physics: Relative Physical Knowledge of Actions and Objects"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"45a23651bcc5a6cc993d722e71b0d301a6dc9dee","title":"Open Mind Common Sense: Knowledge Acquisition from the General Public"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":"86e60b61a0f481876b5d44cedfee11c16456d788","title":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations"},{"paperId":"f7d95e8a91d7683e74ee642f20906e7751e47dfe","title":"Association for Computational Linguistics"}],"id":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","summary":"Investigating whether and to what extent one can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process finds that this may not work for numerical Commonsense knowledge."},{"url":"https://www.semanticscholar.org/paper/c3d487ca75f26adadd94ba7af4b57aed8d661ff3","title":"Automatic Knowledge Acquisition for Object-Oriented Expert Systems","venue":"ArXiv","year":2020,"referenceCount":9,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/05/2020","authors":"J. Colloc,D. Boulanger","citations":[{"paperId":"edd712436dbecd05161fdbe1caf96fd9f1739564","title":"Fvsoomm a Fuzzy Vectorial Space Model and Method of Personality, Cognitive Dissonance and Emotion in Decision Making"}],"references":[{"paperId":"92fc0736065d35fd089b86d60938930e5d18b7ab","title":"Dynamic Derivation of Personalized Views"},{"paperId":"8d7197d958b8d89a566e2c7469c0aea6461ec841","title":"Representational axes and temporal cooperative processes"},{"paperId":"61ed2ab133e48f34c59c14b4cc362cfa0f363392","title":"A Step Towards Unification of Syntactic and Statistical Pattern Recognition"},{"paperId":"5c0f93b5e4f64d833fa5aab15f8642b78f2dfb9f","title":"DETECTING HETEROGENEITY IN A MULTIDATABASE ENVIRONMENT THROUGH AN O-O MODEL"},{"paperId":null,"title":"Creating Expert-Systems for Business Industry"},{"paperId":null,"title":"Mécanismes de classification pour la recherche d'informations dans une base d'objets"},{"paperId":null,"title":"Object Oriented Model and Method Applied to Build an Expert System for Intoxication Diagnoses"},{"paperId":null,"title":"Automatic knowledge acquisition for object oriented expert systems AVIGNON'93"},{"paperId":null,"title":"Automatic knowledge acquisition for object oriented expert systems AVIGNON'93"}],"id":"c3d487ca75f26adadd94ba7af4b57aed8d661ff3","summary":"An Object Oriented Model for building Expert Systems is described and original algorithms which deal with total and partial structural similitude of objects to facilitate knowledge acquisition are proposed."},{"url":"https://www.semanticscholar.org/paper/8256f48f759cf85044db251cc512f965834945b3","title":"Rethinking Positional Encoding in Language Pre-training","venue":"International Conference on Learning Representations","year":2020,"referenceCount":34,"citationCount":199,"influentialCitationCount":22,"publicationDate":"28/06/2020","authors":"Guolin Ke,Di He,Tie-Yan Liu","citations":[{"paperId":"a9468d8bfa6bd016dfd3128c4e8408e30eb8549b","title":"LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning"},{"paperId":"3689b7ca7b07924b6135b8a71b9f1b7937b0a3d5","title":"Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding"},{"paperId":"dbaa101f96855451efc81559f4ada91e102696a2","title":"Algebraic Positional Encodings"},{"paperId":"488c486fef0d58259c46d7be42b285c1de118acb","title":"Long-MIL: Scaling Long Contextual Multiple Instance Learning for Histopathology Whole Slide Image Analysis"},{"paperId":"4dbed37665b7be64f05ad08820bc07fe48933fd7","title":"Exploring the Mosaic-like Tissue Architecture of Kidney Diseases Using Relation Equivariant Graph Neural Networks on Spatially Resolved Transcriptomics"},{"paperId":"434d751d355d7a7c20efa570e785c76286245e77","title":"Hierarchically Gated Recurrent Neural Network for Sequence Modeling"},{"paperId":"43480f896af42c9c80f36fe496fb75f3b03fe7a3","title":"Positional Encoding-based Resident Identification in Multi-resident Smart Homes"},{"paperId":"167d9487419f71a05b641272ae8af9d92f0907d2","title":"A Flash Attention Transformer for Multi-Behaviour Recommendation"},{"paperId":"2fd5de4b4f75234f980157b00e4db8af7a228578","title":"The Locality and Symmetry of Positional Encodings"},{"paperId":"4c50763846e82da2729e96d6b4a7ec3538d8641d","title":"Enhanced Transformer Architecture for Natural Language Processing"},{"paperId":"8c770082c31dd847024e59f66ca1478568e35706","title":"From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique"},{"paperId":"a8da1e22a371bfeca7ed93ad51e5ee75169fc6d2","title":"Fast-ELECTRA for Efficient Pre-training"},{"paperId":"42b877f7423fc727bff5b6e173ad336da33079a9","title":"Uncovering hidden geometry in Transformers via disentangling position and context"},{"paperId":"dc48bc1a4d81e0f37603013fd2a95644dc233bd0","title":"Functional Interpolation for Relative Positions Improves Long Context Transformers"},{"paperId":"f8a6a032718cce60a0dd6d5c75f42b97e644b34b","title":"Spherical Position Encoding for Transformers"},{"paperId":"16ec111fafd692fe786f7ee9e71536f1af18e56a","title":"Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness"},{"paperId":"c4d3b9a87295db0b9f6466b7f3ce2f175c6d3157","title":"GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length"},{"paperId":"e3aa232577bb427b1f3a34acbdef84bd85734042","title":"LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"},{"paperId":"fe5af00194dca1a0892e8df94bd660b821e724a1","title":"KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"},{"paperId":"7ac38c3398f2696754bec69f296468e7a8237a64","title":"FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"},{"paperId":"f3dbe1eb66e14dfc158ba251b976252ed9d1cb02","title":"Positional Encodings for Light Curve Transformers: Playing with Positions and Attention"},{"paperId":"f7f3a38bcb5ddacc4bf1ba39b923957c5c7f7bf9","title":"A Sequence-to-Sequence Approach with Mixed Pointers to Topic Segmentation and Segment Labeling"},{"paperId":"866f286d635e77aaa27474546ca4d0626d8e656c","title":"Customized Positional Encoding to Combine Static and Time-varying Data in Robust Representation Learning for Crop Yield Prediction"},{"paperId":"88695b5bb6462872ce1dd946cff00dd6ebabf2d9","title":"Scaling TransNormer to 175 Billion Parameters"},{"paperId":"8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8","title":"Linearized Relative Positional Encoding"},{"paperId":"5965a485cd26bb8840cf5b602c396e845530d1d1","title":"Frameless Graph Knowledge Distillation"},{"paperId":"3cc89f9e1a06960fd349cae901f4f5a1fb72a291","title":"Multimodal Molecular Pretraining via Modality Blending"},{"paperId":"66698153915f57fbf88c2e6a25155533955cd9fd","title":"Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface"},{"paperId":"db7145cb5f000fa39cec8d839c9ac15d897aa46b","title":"SPDER: Semiperiodic Damping-Enabled Object Representation"},{"paperId":"79efb734cef71f93ad4c875044f9fb4d2cd6079d","title":"Fine-Grained Position Helps Memorizing More, a Novel Music Compound Transformer Model with Feature Interaction Fusion"},{"paperId":"d6c2ce832157f54e1fe717bd3b1a9c4d4619c26e","title":"Random Walk Conformer: Learning Graph Representation from Long and Short Range"},{"paperId":"73c6533c2988eebb71cdaa758f38546cdf01f655","title":"Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset"},{"paperId":"01cc6983e74bcb047c9581305e0c231fde6b65a0","title":"Relational Temporal Graph Reasoning for Dual-Task Dialogue Language Understanding"},{"paperId":"345ba0a4fad9c762d87ef0268c3c047f774e9b5c","title":"SSIN: Self-Supervised Learning for Rainfall Spatial Interpolation"},{"paperId":"08e5d8d1a9d481204b4caff58490755f59b0e88f","title":"Monotonic Location Attention for Length Generalization"},{"paperId":"4487bdcf1eb42bdec83709ba0df5b32dcf388976","title":"What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"ab69a66fef4c4f489dbb6099c2d6e34fdb3c2ed5","title":"Spa-L Transformer: Sparse-self attention model of Long short-term memory positional encoding based on long text classification"},{"paperId":"9fe29c834afbe1848d9df713ae6e0ca3bd053605","title":"Probing the Role of Positional Information in Vision-Language Models"},{"paperId":"240bc60c98c9b860c27c6f962992618a6775cab1","title":"Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts"},{"paperId":"08ec444e77c43b318aa3999b32553f68f8a402fc","title":"Pre-training Language Model as a Multi-perspective Course Learner"},{"paperId":"9ea71e19ff2d831d14adf56ef6d9d5e416bb4039","title":"Revisiting the Encoding of Satellite Image Time Series"},{"paperId":"b750fd5ab13ea1191cfac4c7ee686b2db381a85c","title":"Transformer Tracker Based on Clipped SwinV2 Network"},{"paperId":"f20982cea07674eb779de507f09c26f65d445719","title":"HST-MRF: Heterogeneous Swin Transformer with Multi-Receptive Field for Medical Image Segmentation"},{"paperId":"81d49f8e4421feec17818e09c80801da4a790e6f","title":"EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation"},{"paperId":"0c0aabb2660797c6093d335935748f8db91c8b6b","title":"AutoMLP: Automated MLP for Sequential Recommendations"},{"paperId":"62fc770746f6a283fbc5cfc32275e23bfef82eb7","title":"Target-Aware Tracking with Long-term Context Attention"},{"paperId":"77786eda28e5f0b25d682c27846334d53daf43e4","title":"Complex-Valued Relative Positional Encodings for Transformer"},{"paperId":"55cd2d0a8f26c4dc458303f937af2b6fb8f8b693","title":"PolyFormer: Referring Image Segmentation as Sequential Polygon Generation"},{"paperId":"9e449d611bec60ff2e3faaec38dce3b79abac44e","title":"Enhancing Multivariate Time Series Classifiers through Self-Attention and Relative Positioning Infusion"},{"paperId":"d30c21212e0e1165b2e3a05f9bc31ce8b49e5f0a","title":"Knowledge Distillation in Vision Transformers: A Critical Review"},{"paperId":"7e96a8bc938b47cf805383ef8c079cd852bd64ba","title":"Representation Deficiency in Masked Language Modeling"},{"paperId":"1420b9ff9c7ecfc8c8b5fdce4517e6fc51cebf92","title":"Ankh ☥: Optimized Protein Language Model Unlocks General-Purpose Modelling"},{"paperId":"7389b6ebbf36f4d869a02e305e2ef52ad2c92264","title":"Applications of transformer-based language models in bioinformatics: a survey"},{"paperId":"86f3b14e3229be8e06060f08b6e65fd031efbc50","title":"Untied Positional Encodings for Efficient Transformer-Based Speech Recognition"},{"paperId":"4b308ba40e67b0b4b25c6fde17195d5a456a2f41","title":"Cramming: Training a Language Model on a Single GPU in One Day"},{"paperId":"f6d381941e2964872a283d5290a390126115ad03","title":"P-Transformer: Towards Better Document-to-Document Neural Machine Translation"},{"paperId":"bb1486013aedbbb4f9a960c83731fbb91a04b1f3","title":"Mitigation of Spatial Nonstationarity with Vision Transformers"},{"paperId":"0f107a8247983e494789ffd81663708dfbe483e6","title":"You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model"},{"paperId":"8e2930ac8ae9a758b367513cccb7d562f354afea","title":"Who Says Elephants Can’t Run: Bringing Large Scale MoE Models into Cloud Scale Production"},{"paperId":"e4679aaf696fcdd7fc0aab8b4b3af095d512b68f","title":"Pure Transformer with Integrated Experts for Scene Text Recognition"},{"paperId":"3ba812282de79a909c3c6074940e91f322b51db2","title":"AutoAttention: Automatic Field Pair Selection for Attention in User Behavior Modeling"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"e82e3f4347674b75c432cb80604d38ee630d4bf6","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"240300b1da360f22bf0b82c6817eacebba6deed4","title":"What Makes Convolutional Models Great on Long Sequence Modeling?"},{"paperId":"a0b4e5f73adb829f2b7c51eb373f0559c110e2ac","title":"Better Pre-Training by Reducing Representation Confusion"},{"paperId":"03b7d2e942eafabd14b229f8962fa6e943053f75","title":"Melody Infilling with User-Provided Structural Context"},{"paperId":"d4d07180764fc30cf31261ddd072175a4daee10b","title":"A Dual-Attention Learning Network with Word and Sentence Embedding for Medical Visual Question Answering"},{"paperId":"3eedf6e694a8d58c5faff3ac47c6a20daf05e48d","title":"Exploring Heterogeneous Feature Representation for Document Layout Understanding"},{"paperId":"e3486e1c1a3cac6ef2653ea409c86b1ee6a42ebd","title":"Husformer: A Multi-Modal Transformer for Multi-Modal Human State Recognition"},{"paperId":"70e91e16eb321067d9402710e14a40cf28311f73","title":"Mega: Moving Average Equipped Gated Attention"},{"paperId":"d6bf7e75662363c306c4c791d9ab1b4dd6a0b6f5","title":"Hyperspectral Image Classification via Spectral Pooling and Hybrid Transformer"},{"paperId":"1dd5beb70fe2a4ace695d3fc7f1fb0c53f757d87","title":"Pre-Training a Graph Recurrent Network for Language Representation"},{"paperId":"52e4f9232bdd5d298e49c75eb84dcd5c1445843e","title":"Global Positional Self-Attention for Skeleton-Based Action Recognition"},{"paperId":"369a564359d467eb3878d01f9f36e79703aa0374","title":"Learning Program Representations with a Tree-Structured Transformer"},{"paperId":"acd082f28074e047792691c5236819bb50132a7a","title":"Improving Micro-video Recommendation by Controlling Position Bias"},{"paperId":"076deb54a5d776cd21eabf2c40cdd839f53d6d77","title":"giMLPs: Gate with Inhibition Mechanism in MLPs"},{"paperId":"e9d1a851d4f7950db360d12fe7f96647aaf547da","title":"Generalized Attention Mechanism and Relative Position for Transformer"},{"paperId":"4299353235595391e2b4f7298baffd00b5acf9d1","title":"LordBERT: Embedding Long Text by Segment Ordering with BERT"},{"paperId":"4d80dae4039b4de6c1ba30227d5981fd1cbb4a1f","title":"Parameterization of Cross-token Relations with Relative Positional Encoding for Vision MLP"},{"paperId":"9692886ba8e2c9d8990b0505e9c67a696d9f28a7","title":"A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities"},{"paperId":"bc67a1d69d53bb6b5a4efa86721122f09532c97e","title":"Progressive Self-Attention Network with Unsymmetrical Positional Encoding for Sequential Recommendation"},{"paperId":"e28adeb4db46469df9f9bd653501871ddc5f4318","title":"MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization"},{"paperId":"efb8efd1cdf0f7b7ef04e7dc032485c2b15d209f","title":"Towards Robust Referring Video Object Segmentation with Cyclic Relational Consensus"},{"paperId":"b0c4438723c9193a4ab850f6cbb72bc5ffca3983","title":"Improving session-based recommendation with contrastive learning"},{"paperId":"b09420b30fc093d63fb2ee1aac26c71da81da437","title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"},{"paperId":"31a84391e1b47fa15f3a521c43d62385a7757637","title":"MetaTPTrans: A Meta Learning Approach for Multilingual Code Representation Learning"},{"paperId":"8e6ada0242dc9b3680ccf6ddeec9e7c3cd41766f","title":"Dynamically Relative Position Encoding-Based Transformer for Automatic Code Edit"},{"paperId":"e1461a3aa9f4bef86435fa2cc87b16d9f34d5ab0","title":"Do We Really Need Temporal Convolutions in Action Segmentation?"},{"paperId":"746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a","title":"Your Transformer May Not be as Powerful as You Expect"},{"paperId":"7a93df4da51f45e2af79856d417c77ca697016ab","title":"Neural Additive Models for Nowcasting"},{"paperId":"d6c5aab433d9871cabc01ffb1e5e1ea89141155b","title":"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"},{"paperId":"6e98a35c439c5c7387de6bffd96dea9b2943a548","title":"Trading Positional Complexity vs. Deepness in Coordinate Networks"},{"paperId":"9d03a164759bb5cc2fa6b575254b58f790ab6785","title":"Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction"},{"paperId":"7f9290803a6df981cc460894487868226e2131f3","title":"RM-Transformer: A Transformer-based Model for Mandarin Speech Recognition"},{"paperId":"b0773e4edee5a5f2bfc11ce793a7011a922ce743","title":"Spatial-Temporal Interval Aware Sequential POI Recommendation"},{"paperId":"501eb473c1ca97e66f7a0ff6379a00756d869d6c","title":"Decoupled Side Information Fusion for Sequential Recommendation"},{"paperId":"2193d898e556ad2ac14f0ababf02f90e6fdfe663","title":"DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks"},{"paperId":"633851a323990270461b49e8068a2c9ca1cea530","title":"Dynamic Position Encoding for Transformers"},{"paperId":"682ca4de7fc91ed44a86ce7762d74f58791f63e7","title":"3D Shuffle-Mixer: An Efficient Context-Aware Vision Learner of Transformer-MLP Paradigm for Dense Prediction in Medical Volume"},{"paperId":"b6ec1e8f18185b4b3d46201359a440404575460c","title":"METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals"},{"paperId":"02f36289e227595dd2786b32c3975ce7e61dff48","title":"Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators"},{"paperId":"838a2297b94f7bad96c4f8370a5f58487f194f44","title":"Visual Abductive Reasoning"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"4e9cd6be4a8fcad2ca562fcf41a1f882387a3167","title":"LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network"},{"paperId":"60c07da8f233dd3af214d4d5fbd2582a165797d8","title":"Probabilistic deep learning model as a tool for supporting the fast simulation of a thermal-hydraulic code"},{"paperId":"50af83ea20201b51014358534650213e6133650c","title":"FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks"},{"paperId":"5f104a804ed245c79847ad0593e8f86196d697b1","title":"Transformers in Time Series: A Survey"},{"paperId":"12809bcb734beafeb47876f42e7b438e27fe99fe","title":"General-purpose, long-context autoregressive modeling with Perceiver AR"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"c72a8f823f9e2d4c9820df5f78e3f6c4922910b6","title":"Improving Sample Efficiency of Value Based Models Using Attention and Vision Transformers"},{"paperId":"002390988f7157b425ac7e5dc42f3d06eca6ede7","title":"Rewiring with Positional Encodings for Graph Neural Networks"},{"paperId":"5fb29bd2eb585be9c7e2f8a483cc5fe8255d77c2","title":"SoK: Vehicle Orientation Representations for Deep Rotation Estimation"},{"paperId":"91a4cbae6553e975ddc3b2f6850ed725ff475307","title":"SwinTrack: A Simple and Strong Baseline for Transformer Tracking"},{"paperId":"be0fbb810583930c071d0b9b2c5187fe260783f5","title":"Swin Transformer V2: Scaling Up Capacity and Resolution"},{"paperId":"28a65397488d826e0063df6352f00e6a660d0a2f","title":"Theme Transformer: Symbolic Music Generation With Theme-Conditioned Transformer"},{"paperId":"63d70dba02c34e465f36fd8b123390efe7aa67e0","title":"Can Vision Transformers Perform Convolution?"},{"paperId":"2945348e0843b8a414cd54fe2588a45430249355","title":"Position-Augmented Transformers with Entity-Aligned Mesh for TextVQA"},{"paperId":"c501d6f1eecf3b0fdae7d428e1829bb6607a6a37","title":"Relative molecule self-attention transformer"},{"paperId":"87e879c2465b2414a58dd3a1184f8b346d48f3e7","title":"Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer"},{"paperId":"232acef2483a26fe95c70b619f88fa0b82c1a105","title":"Multiplicative Position-aware Transformer Models for Language Understanding"},{"paperId":"33c831326bb47b2ba2031fd7213b6918d23eb01e","title":"The Impact of Positional Encodings on Multilingual Compression"},{"paperId":"aec7e7143bfe082752f428fe01e4090dd7fc411c","title":"Ultra-high Resolution Image Segmentation via Locality-aware Context Fusion and Alternating Local Enhancement"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"51b5db5c679be0ce9a39a2ee21def42bca165efe","title":"Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning"},{"paperId":"1cd6b0f41d62aca38ba5a69db10e79c05e618c21","title":"Conditional DETR for Fast Training Convergence"},{"paperId":"79678d2f10bddf14b2aedf3427f8a4c39908931f","title":"Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding"},{"paperId":"87e823d2cb58e741230c0fa3b83f3459c7e32241","title":"PiSLTRc: Position-Informed Sign Language Transformer With Content-Aware Convolution"},{"paperId":"d3f51870f4da5dd9c2a08a55cfa8a380b8d49208","title":"Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation"},{"paperId":"e38dc0554dd89745bb17039a4d4ee9d714cf77f1","title":"SpectralFormer: Rethinking Hyperspectral Image Classification With Transformers"},{"paperId":"fbac64d617914ab8dac0682639fbd6012faed771","title":"Rethinking Positional Encoding"},{"paperId":"663251e39e301bb9a8bdaedcc9ab2489e1be7ef3","title":"Semi-Supervised Representation Learning for Segmentation on Medical Volumes and Sequences"},{"paperId":"0d508600d77d8a7e6a655cdb6d139779732f649f","title":"Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"},{"paperId":"df59d0098c1b2c1ee8995da802dd6b12d158c2b8","title":"Large-scale chemical language representations capture molecular structure and properties"},{"paperId":"1dbb523a6555d6e0c5727620e2b57daaa5b79dc0","title":"Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models"},{"paperId":"47ae807cd511b35e78a2cd4e198283dea6dafd41","title":"Do Transformers Really Perform Bad for Graph Representation?"},{"paperId":"d8d2e574965fe733eb1416e03df2b5c2914fc530","title":"A Survey of Transformers"},{"paperId":"7509c66a666e2e3f14bc8676b969b945ee6e136f","title":"CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"},{"paperId":"d8e7bad2681ce70277c900c77a22181d4b03d705","title":"The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"},{"paperId":"07e987364bf0be1949e379f976f8dea675977337","title":"MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens"},{"paperId":"b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea","title":"Self-Attention Networks Can Process Bounded Hierarchical Languages"},{"paperId":"08ffdec40291a2ccb5f8a6cc048b01247fb34b96","title":"Relative Positional Encoding for Transformers with Linear Complexity"},{"paperId":"52d01d9f71caf0d021fccb75b75b7d3dfc7460f5","title":"On Scalar Embedding of Relative Positions in Attention Models"},{"paperId":"b8cee43a51c44f8f4448e78e41ecf081987707cf","title":"Towards Robust Vision Transformer"},{"paperId":"fda805c6e85a03d10549acdc5489420ca8f3d405","title":"MuseMorphose: Full-Song and Fine-Grained Piano Music Style Transfer With One Transformer VAE"},{"paperId":"1c0752fb3e9ab5c9392f196225075422f26b5110","title":"How could Neural Networks understand Programs?"},{"paperId":"f9a61d89209da81df0b232f788c64cbf18adaba5","title":"Predicting functional effect of missense variants using graph attention neural networks"},{"paperId":"d76bf20d059359f9e05b342e56cfc7785af71bf4","title":"LANA: Towards Personalized Deep Knowledge Tracing Through Distinguishable Interactive Sequences"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"36938a8ccb323cdd45c304d4461a29aee3ebc28a","title":"SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization"},{"paperId":"db46b0de44c5113c47f0ec5392eb91d0726497bf","title":"A Simple and Effective Positional Encoding for Transformers"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview"},{"paperId":"1e9ff5f2e9aa3e6c01bc89c81ba16442f1b5938d","title":"Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder"},{"paperId":"19537be34dbadbcaa4fffcf028a8ada5095b1b5c","title":"COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining"},{"paperId":"594db112d65891fbaf45b27f17d2f9de88ddcd82","title":"Revisiting Language Encoding in Learning Multilingual Representations"},{"paperId":"eb0931c39904a40c6cb4aa35c9b21d5e3b7dc856","title":"Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs"},{"paperId":"0822f8d7e6a72a65e65f147d3a8d8fccd485da40","title":"Shortformer: Better Language Modeling using Shorter Inputs"},{"paperId":"5d76c2591334f56dc9155568f793b013df0f6613","title":"Focusing More on Conflicts with Mis-Predictions Helps Language Pre-Training"},{"paperId":"eb2fc03b8865b8e1b4cb933d917ea269ebe14584","title":"Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training"},{"paperId":"ac4ae352e2434d4a71c6a79bf5f93df5f600b058","title":"Increasing Learning Efficiency of Self-Attention Networks through Direct Position Interactions, Learnable Temperature, and Convoluted Attention"},{"paperId":"829580d6fc73fa601c4982e2b1b6832f2796270b","title":"Positional Artefacts Propagate Through Masked Language Model Embeddings"},{"paperId":"0971ad49ed7f063375d7a21bdfc2fab347d68ba8","title":"Transformer-based Arabic Dialect Identification"},{"paperId":"6482dbb26e6441c0cfdaa43c52b868751d16d10b","title":"Contextual BERT: Conditioning the Language Model Using a Global State"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"26ce5dd8221efc307fceef111e003bdb0ffe8ed5","title":"Positional Attention Guided Transformer-Like Architecture for Visual Question Answering"},{"paperId":"2b0d96dccd07ebe8feb90951fe90d3aa81741097","title":"Understand and Modularize Generator Optimization in ELECTRA-style Pretraining"},{"paperId":"11f42721f56f36a64638677ccde7784040829656","title":"Uni-Mol: A Universal 3D Molecular Representation Learning Framework"},{"paperId":"c0f5a27f26c93a5109d8a7103ac3536344cd164a","title":"Rethinking the Encoding of Satellite Image Time Series"},{"paperId":"fb45d31cc89207aec392dbac8908cc24db2df871","title":"Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting"},{"paperId":"ced170992b7ab2c9f69966f780996533b0f423b4","title":"Bridging the Gap between Position-Based and Content-Based Self-Attention for Neural Machine Translation"},{"paperId":"5cdb940cb0e8158bc5bc5aabcee84ffeee0c30fe","title":"Improve Transformer Pre-Training with Decoupled Directional Relative Position Encoding and Representation Differentiations"},{"paperId":"20333c34f892c8e0c2f4e6c37295a8b43ef35c02","title":"Rethinking Positional Encoding in Tree Transformer for Code Representation"},{"paperId":"2a98fd84fd7a1eadbeb4513ad322d13286ecb9ac","title":"Graph Hawkes Transformer for Extrapolated Reasoning on Temporal Knowledge Graphs"},{"paperId":"6410348a7b9da61ea5ca2979aaebb164d4b87b01","title":"Going “Deeper”: Structured Sememe Prediction via Transformer with Tree Attention"},{"paperId":"3884307cb95329275755baaf99600e7431be695d","title":"Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation"},{"paperId":"7865ae1c4d7ba0e3ba48aef54270c3fdf61ec11d","title":"Symbolic Data Augmentation for Assisted Neural Reasoning"},{"paperId":"8d47a9950a44b0d2f1172620966c4737908ad542","title":"R^2VOS: Robust Referring Video Object Segmentation via Relational Multimodal Cycle Consistency"},{"paperId":"08013183713f427258d36332b1bee9d6ce37cf30","title":"S TRUCTURE -A WARE T RANSFORMER P OLICY FOR I NHOMOGENEOUS M ULTI -T ASK R EINFORCEMENT L EARNING"},{"paperId":"5cea5af1822e7407c8393bd7c5d0208af26ea57e","title":"Priming Ancient Korean Neural Machine Translation"},{"paperId":"7adba2d365188a4514d695b9bc5e9901016c3f05","title":"CSiT: A Multiscale Vision Transformer for Hyperspectral Image Classification"},{"paperId":"97fcb73b687593059d5dd054183093a2bfe46cc1","title":"Hourglass Attention Network for Image Inpainting"},{"paperId":"6aded7d6f93fd93c5632b627b6520830a3999ad4","title":"Molformer: Large Scale Chemical Language Representations Capture Molecular Structure and Properties"},{"paperId":"3ee219ce028b78f6d0c099043f9e0a24106d8aa5","title":"R EWIRING WITH P OSITIONAL E NCODINGS FOR GNN S"},{"paperId":"a1ad15d2333cf9d9b55bbc97a3aacd244a8b9fdf","title":"Demystifying the Better Performance of Position Encoding Variants for Transformer"},{"paperId":"8835d7e27472e6041b640c869ce6b94021f8c851","title":"Do Large Scale Molecular Language Representations Capture Important Structural Information?"},{"paperId":"f5a2ce064ea0efc3d7f26acd91041824c3254cd8","title":"NRTSI: Non-Recurrent Time Series Imputation for Irregularly-sampled Data"},{"paperId":"dc3ee859157d163c7dd0ba497d8736a802fe59a0","title":"Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder"},{"paperId":"89bc3b21c15c0656643918488ea25af939d0881a","title":"MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with Just One Transformer VAE"},{"paperId":"976a609cf540d1ded373b872d34779f7164d840a","title":"Rethinking the Design Principles of Robust Vision Transformer"},{"paperId":"dc35daba3fb34b2e6a5b12530badb7b799262bbf","title":"On Position Embeddings in BERT"},{"paperId":"79a2ac8ec0c4d3bfc1762d330e3446af5f8f1922","title":"S2T: Learning Semantic-Spatial-Temporal Representation for Robust Active Object Tracking"},{"paperId":"2a405483796dfedf5d95483aa8880c57626e0e9f","title":"Integrating Tree Path in Transformer for Code Representation"},{"paperId":"8d6b1929b92211ad0eb3e14b2c2b41789ccf053a","title":"Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models"},{"paperId":"5be217c678916543884c354654263f27c0a6bd9f","title":"Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder"},{"paperId":"c687acfeaafb86ab045f9fcbb6e2ff691827a0c2","title":"On Position Embeddings in BERT O N P OSITION E MBEDDINGS IN BERT"},{"paperId":"7df56875cefeb9518e4a227bda4c9f64c68b76c7","title":"Transformer with Syntactic Position Encoding for Machine Translation"},{"paperId":"acf87283fa8ae426f1a4987b345b401bf2913f61","title":"Do Transformers Really Perform Badly for Graph Representation?"}],"references":[{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"e8984c6e6c24aab26c332728a5fff616dfb3adbb","title":"Learning to Encode Position for Transformer with Continuous Dynamical Model"},{"paperId":"f64e1d6bc13aae99aab5449fc9ae742a9ba7761e","title":"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b45d656ac8cc2e940609580cf291ee76ffcac20a","title":"On Layer Normalization in the Transformer Architecture"},{"paperId":"d0e28f5dc1feae19e41087a92a87992977fd85af","title":"Encoding word order in complex embeddings"},{"paperId":"37a23c43ddf09ea97b82b38e2827a2229cfae545","title":"Novel positional encodings to enable tree-based transformers"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"95a251513853c6032bdecebd4b74e15795662986","title":"What Does BERT Look at? An Analysis of BERT’s Attention"},{"paperId":"5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88","title":"Efficient Training of BERT by Progressively Stacking"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"b36a5bb1707bb9c70025294b3a310138aae8327a","title":"Automatic differentiation in PyTorch"},{"paperId":"e7fd6848cb29ca221a7e17d823e06fb566f1f135","title":"Mixed Precision Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5feb32a73dd1bd9e13f84a7b3344497a5545106b","title":"FastText.zip: Compressing text classification models"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"79d1b330f0ef51f63ecb9b291dd5a05de5a858c0","title":"Toeplitz and Circulant Matrices: A Review"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Electra: Pre-training text encoders as discriminators rather than generators"},{"paperId":"99e8d34817ae10d7304521e89c5fbf908b9d856b","title":"Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding"},{"paperId":"8f424d35b3e5af10b2e492f073b3347f1f5176f8","title":"Quantitative Linguistik / Quantitative Linguistics - Ein internationales Handbuch / An International Handbook"}],"id":"8256f48f759cf85044db251cc512f965834945b3","summary":"This work investigates the problems in the previous formulations and proposes a new positional encoding method for BERT called Transformer with Untied Positional Encoding (TUPE), which can achieve a higher score than baselines while only using 30% pre-training computational costs."},{"url":"https://www.semanticscholar.org/paper/0bfd4ed399054eae26c3cdaabc0aed80ca95e125","title":"Neural Power Units","venue":"Neural Information Processing Systems","year":2020,"referenceCount":25,"citationCount":8,"influentialCitationCount":3,"publicationDate":"02/06/2020","authors":"Niklas Heim,T. Pevný,V. Šmídl","citations":[{"paperId":"26e07b6ac9b0f04396e2e7f7ca518b6fbae67ce7","title":"Improving the Robustness of Neural Multiplication Units with Reversible Stochasticity"},{"paperId":"4578747d52aa1b3537612287352a803a7b17e999","title":"Asynchronous Neural Networks for Learning in Graphs"},{"paperId":"0ce6a798f8222ed8f221326ca566311c648cb4dc","title":"Learning Division with Neural Arithmetic Logic Modules"},{"paperId":"def7c67c3688bc459fbef3ce50a466a0cbf411a9","title":"A Primer for Neural Arithmetic Logic Modules"},{"paperId":"30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0","title":"MC-LSTM: Mass-Conserving LSTM"},{"paperId":"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","title":"Neural Status Registers"},{"paperId":"c36277b67814e0a522e786a38e1768612b0e63f2","title":"Exploring the Learning Mechanisms of Neural Division Modules"},{"paperId":"db4a017bba4b242bced04fd108df78e91c8624f6","title":"GwAC: GNNs with Asynchronous Communication"}],"references":[{"paperId":"4e181e7f1067ee2b24db5f21e6c4cc0b4875edd2","title":"Fractional SIR epidemiological models"},{"paperId":"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","title":"Neural Status Registers"},{"paperId":"e081dfa64e5ed12fe15fc885ec82ef1f35ab9830","title":"iNALU: Improved Neural Arithmetic Logic Unit"},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"696b388ee6221c6dbcfd647a06883b2bfee773d9","title":"Universal Differential Equations for Scientific Machine Learning"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"112ac68ddb0f021517dd465e89918fa52755cc35","title":"Measuring Arithmetic Extrapolation Performance"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"3c9961153493370500020c81527b3548c96f81e0","title":"Data-driven discovery of coordinates and governing equations"},{"paperId":"bc00ff34ec7772080c7039b17f7069a2f7df0889","title":"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead"},{"paperId":"e53fd0c9a04df127af80b2699f5f8f23a8a3e2af","title":"On Evaluating the Generalization of LSTM Models in Formal Languages"},{"paperId":"86ccab92ea6bd1f1cfae8d6d1cca3ea71e89bdc7","title":"Fashionable Modelling with Flux"},{"paperId":"6f69e19348870552a7ab92d038c8b8d753fe6b60","title":"Neural Arithmetic Expression Calculator"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"449310e3538b08b43227d660227dfd2875c3c3c1","title":"Neural Ordinary Differential Equations"},{"paperId":"0c10a2cafb715f5c7b1810f89dd2eb3fadc5a38e","title":"Finding numbers in the brain"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"34473be212082c64ee6c944758988fd64c9c9ef4","title":"DifferentialEquations.jl – A Performant and Feature-Rich Ecosystem for\n Solving Differential Equations in Julia"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"a2499dd426c46c645ee805d7594b6687547c72d4","title":"Neural Random Access Machines"},{"paperId":"5b791cd374c7109693aaddee2c12d659ae4e3ec0","title":"Grid Long Short-Term Memory"},{"paperId":"c3823aacea60bc1f2cabb9283144690a3d015db5","title":"Neural Turing Machines"},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"801257f13663c493851d55366b431fad0cc801a9","title":"A contribution to the mathematical theory of epidemics"},{"paperId":null,"title":"The Mythos of Model Interpretability. arXiv:1606.03490 [cs, stat], March 2017"}],"id":"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","summary":"The Neural Power Unit (NPU) is introduced that operates on the full domain of real numbers and is capable of learning arbitrary power functions in a single layer and fixes the shortcomings of existing arithmetic units and extends their expressivity."},{"url":"https://www.semanticscholar.org/paper/79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88","title":"Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge","venue":"Neural Information Processing Systems","year":2020,"referenceCount":42,"citationCount":75,"influentialCitationCount":10,"publicationDate":"11/06/2020","authors":"Alon Talmor,Oyvind Tafjord,Peter Clark,Yoav Goldberg,Jonathan Berant","citations":[{"paperId":"34ca51ce10e8d1d6c950ba519329714a0184d004","title":"KwaiYiiMath: Technical Report"},{"paperId":"f014f0ea9867c2912e625b54d57713f1937d124f","title":"Large Language Models can Learn Rules"},{"paperId":"3aee33831e0bdea1a1eaae21c7586e4f7c0396d6","title":"Making Retrieval-Augmented Language Models Robust to Irrelevant Context"},{"paperId":"f021aebbc4c8d38f55470ad11bfb1a2c59b788a7","title":"BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information"},{"paperId":"fcf36a99a8eddc4a29471507d4f3ab31580baa6c","title":"Causal interventions expose implicit situation models for commonsense language understanding"},{"paperId":"704cff9a00c7ea3358ac2710d5b213fe09c7f570","title":"Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification"},{"paperId":"74b05bba46db21e589a2cc0f916f81069b0368ef","title":"Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"},{"paperId":"c83e93b519acec9f9588dfb43a0d97fbc1222267","title":"Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems"},{"paperId":"c715914c388fa64dd8686cd8755e5adfebbf2388","title":"REFINER: Reasoning Feedback on Intermediate Representations"},{"paperId":"9cd398e75e89b9d8104837da44ad17e110a4e4f9","title":"Explicit Planning Helps Language Models in Logical Reasoning"},{"paperId":"63d0e5a8f195b1453006781d4d8a4eb7262652d9","title":"Logical Reasoning over Natural Language as Knowledge Representation: A Survey"},{"paperId":"2ae807688d5bae0a7331992793be066b93d7655f","title":"Does Deep Learning Learn to Abstract? A Systematic Probing Framework"},{"paperId":"35f5483fa6c1816739b604a5bb57719fadd79249","title":"Affective Faces for Goal-Driven Dyadic Communication"},{"paperId":"490d8006851b1562cfd9ec1f057471f2868289d1","title":"Rethinking with Retrieval: Faithful Large Language Model Inference"},{"paperId":"c7a4946eb49bc6a9c01eaa79e84a35316595bd5a","title":"Language Models as Inductive Reasoners"},{"paperId":"30977afbd4501249e1a320bd2e48581197914ab8","title":"A Short Survey of Systematic Generalization"},{"paperId":"8ee376114a43432399554be39a79c1a2b6c65d51","title":"Can Transformers Reason in Fragments of Natural Language?"},{"paperId":"61ddf932488405ab1c7b275460d2b3c5dfa274a0","title":"Fixing Model Bugs with Natural Language Patches"},{"paperId":"d400a649f0f0a3de22b89a268f48aff2dcb06a09","title":"Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"66f333c51e2bfa25380069f66500b491218da9c3","title":"Can Pretrained Language Models (Yet) Reason Deductively?"},{"paperId":"6fb0b072c4fcdc0c78218bfd1b181fd562f07cd2","title":"COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models"},{"paperId":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment"},{"paperId":"cb11d43d90aa5da2c0a8a682efc52e204dc3e0e0","title":"Reasoning over Logically Interacted Conditions for Question Answering"},{"paperId":"4b516216d7d150a081fd74993bddf36b6b22c118","title":"Chain of Thought Imitation with Procedure Cloning"},{"paperId":"e1943cbf4817605a1f988fe5fd785f6b707ca233","title":"METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation"},{"paperId":"0abcd69d87723b93bfc2074167418a4cf81a892e","title":"Logically Consistent Adversarial Attacks for Soft Theorem Provers"},{"paperId":"e7d75b80e0fa3ae190ff91676dbf18a006d3a311","title":"Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement"},{"paperId":"3f4d11971f2c64be9125a7fe99c019588bbebf16","title":"Iteratively Prompt Pre-trained Language Models for Chain of Thought"},{"paperId":"242eaa55cce5daad200850dd10a788a0f960cdd8","title":"Logical Reasoning for Task Oriented Dialogue Systems"},{"paperId":"bd44f34b47c8a4b6947695853fc2814ac69664a6","title":"Datasheet for the Pile"},{"paperId":"0b483b550b21ec42d693fc04a372dbb10dd07019","title":"Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge"},{"paperId":"188aa594c294f01fd18e0ff43eeb0d8430b39b90","title":"Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback"},{"paperId":"07d5bba7d2bc511c88eb143a926d3c297298ad15","title":"Interscript: A dataset for interactive learning of scripts through error feedback"},{"paperId":"a466d10b80dbdee3b130bef73ec62f3a89eb389b","title":"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples"},{"paperId":"3ecc1bb0171e7540410fbd454fed473b8b6fa437","title":"On Semantic Cognition, Inductive Generalization, and Language Models"},{"paperId":"ed02d4a08a50d23709b7ce5df9878a8bc34ac12c","title":"GNN is a Counter? Revisiting GNN for Question Answering"},{"paperId":"ecad7a322ede6de0013b46dc64429eed4c43e8af","title":"BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief"},{"paperId":"e55391a9406245584b3e5b3225dad2e171b9a06b","title":"RuleBERT: Teaching Soft Rules to Pre-Trained Language Models"},{"paperId":"d65a064eb837f838faf6ff67781b62450b92b159","title":"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification"},{"paperId":"f5ca46585818771e64ee9449c930748fbee35cba","title":"Improving Neural Model Performance through Natural Language Feedback on Their Explanations"},{"paperId":"911b7539e964782670e555930b291de16fa971c5","title":"Flexible Generation of Natural Language Deductions"},{"paperId":"f06ca547e5eaf0fd118f184b9ddd1cf7cd5c29e0","title":"Updater-Extractor Architecture for Inductive World State Representations"},{"paperId":"4ea544c849aee3e4f99e3d835ab3ea9ed685a5b9","title":"KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding"},{"paperId":"e0c4a2116ec0f3dc48457627bbd8bfe1f0026479","title":"Overcoming Poor Word Embeddings with Word Definitions"},{"paperId":"dcb67a471a3c6f782cf237ced1da28d3da23553c","title":"Concepts, Properties and an Approach for Compositional Generalization"},{"paperId":"5b6c582d51266be9aa7e32bfdc20891e5231eca4","title":"When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data"},{"paperId":"87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1","title":"ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language"},{"paperId":"18e5fb8cec55a75b288a499c57d77ede541dc049","title":"Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering"},{"paperId":"2db020e3398c06e3a22f12d8caffe76b0d9d1dda","title":"Knowledge-driven Self-supervision for Zero-shot Commonsense Question Answering"},{"paperId":"4bda9767c11a97a3d1a83577cb8ec94f16ceccb5","title":"A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English"},{"paperId":"d877020a6808f0d4baded6d6cd92d09bbc46bde2","title":"Constructing Taxonomies from Pretrained Language Models"},{"paperId":"58ba42ffc34dd24d6a77fe58c8973b3533c369cd","title":"Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views"},{"paperId":"f727f928e7e179307d8d4a1da2387393f2bd7915","title":"Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models"},{"paperId":"8fb90b215d3533cbb280e251d551b178c578080d","title":"MEERQAT-IRIT at SemEval-2023 Task 2: Leveraging Contextualized Tag Descriptors for Multilingual Named Entity Recognition"},{"paperId":"23f4cfc5695e13ebe32606475421d77a3999d2cb","title":"S CENARIO - BASED Q UESTION A NSWERING WITH I NTERACTING C ONTEXTUAL P ROPERTIES"},{"paperId":"39bf80c6455184192b307cb855698955d938aa7f","title":"A Theory for AI Computation?"},{"paperId":"6cb0efa435994a415bc090e5ab5a0ab5a6085897","title":"Toward the Reliability of Dialog Systems"},{"paperId":"2aba5bba16dac5cd62683bab9de5d6faaaed0de1","title":"Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach"},{"paperId":"10de447404ce566f08e6a3c1444739fb8261f009","title":"COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models"},{"paperId":"448e1493034dafe35699ae054ff4480b31dcf64a","title":"Memory-assisted prompt editing to improve GPT-3 after deployment"},{"paperId":"41a4482bb47de60fdc44492fa633701e425cb501","title":"SpARC: Sparsity Activation Regularization for Consistency"},{"paperId":"63cd10c4ca6733a8894d55cf1343636fa816cf7c","title":"Analyzing the Contribution of Commonsense Knowledge Sources for Why-Question Answering"},{"paperId":"c4487e6690173618bfec98751ce3590204e981ee","title":"Logical Reasoning with Span Predictions: Span-level Logical Atoms for Interpretable and Robust NLI Models"},{"paperId":"fa46f4ddd5c6e793a47c61db9c1ecde7ea1c82bc","title":"Dynamic Generation of Interpretable Inference Rules in a Neuro-Symbolic Expert System"},{"paperId":"fed460648303afa32247e493847e4dc73dc1a5b3","title":"Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?"},{"paperId":"7bb907e754942b832bacf7889ba1d6bd72945ca0","title":"Using Commonsense Knowledge to Answer Why-Questions"},{"paperId":"5de29c60071d65f0c00ab584b0738dcc56a1b158","title":"The First Workshop on Learning with Natural Language Supervision"},{"paperId":"bf9097371d7a88f45bbdc380680e5b2c98ffd793","title":"Identifying Physical Object Use in Sentences"},{"paperId":"f074ec057ae40bf8ea330bfa0df166a93e459439","title":"Flexible Operations for Natural Language Deduction"},{"paperId":"1b806d23d6e1daee1a7fa8df12b009e5c64bee59","title":"Are we there yet? Exploring clinical domain knowledge of BERT models"},{"paperId":"884c0b6db564208d99cadf2548f0aa96dee5f859","title":"Commonsense Reasoning with Implicit Knowledge in Natural Language"},{"paperId":"2af476f7c2a7c040fc9ab7750bf41a84f66aa947","title":"Knowledge Based Multilingual Language Model"},{"paperId":"f0ba384a13adfd4280ed10ff9974b787193c2531","title":"Improving scripts with a memory of natural feedback"},{"paperId":"751f55beac45cec14b0aff6174ed0139afb54b08","title":"Modelling Symbolic Knowledge Using Neural Representations"}],"references":[{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"15ad2b27c5248e7d1db5456794ca1ca8a8198f5d","title":"Transformers as Soft Reasoners over Language"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"5a9001cdccdb8b1de227a45eccc503d32d1a2464","title":"What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"521b4e26df0f1cf5763dece14cbb218df152dc59","title":"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering"},{"paperId":"c2c165dd615d4fc31d4fef4b4acbcab1a1655983","title":"On Making Reading Comprehension More Comprehensive"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"681fbcd98acf20df3355eff3585994bd1f9008b7","title":"Probing Natural Language Inference Models through Semantic Fragments"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"401dc39c2c8c910253d47980cfa3b4d2f7790d9b","title":"WinoGrande"},{"paperId":"95f2aedef1453e5d88af9d5fddb79e0e56223cb0","title":"Compositional Questions Do Not Necessitate Multi-hop Reasoning"},{"paperId":"fd4675526ee569196ad1698935b8f5a529b1f9ba","title":"Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA"},{"paperId":"636904d91d9dd1a641a595d9578ba7640f35aa74","title":"MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension"},{"paperId":"df2ce576eca0b3db177c83bc6cf0f9fe2c7714f0","title":"Dynamically Fused Graph Network for Multi-hop Reasoning"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"53e50a313ddfd222d958469edb6742f19458ec74","title":"Modeling Semantic Plausibility by Injecting World Knowledge"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"fbba8629ff9633ca57be1f2209d53d9bcfc2273c","title":"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference"},{"paperId":"7d5cf22c70484fe217936c66741fb73b2a278bde","title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"83e7654d545fbbaaf2328df365a781fb67b841b4","title":"Enhanced LSTM for Natural Language Inference"},{"paperId":"2cd8e8f510c89c7c18268e8ad51c061e459ad321","title":"A Decomposable Attention Model for Natural Language Inference"},{"paperId":"a9de2fc7ae3230652f27fd95e7175da8cf44f075","title":"Harnessing Deep Neural Networks with Logic Rules"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"0ba86604228b555475496e200f31878df3aabd6e","title":"Never-Ending Learning"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","title":"Wikidata: a free collaborative knowledgebase"},{"paperId":"3534ce1dc473d2f78024b2b3a6452d1e3335f6d4","title":"Book Reviews: Recognizing Textual Entailment: Models and Applications by Ido Dagan, Dan Roth, Mark Sammons and Fabio Massimo Zanzotto"},{"paperId":"c9c17279a2a6e564219cf573a74c6675cebb62e6","title":"True Knowledge: Open-Domain Question Answering Using Structured Knowledge and Inference"},{"paperId":"d87ceda3042f781c341ac17109d1e94a717f5f60","title":"WordNet : an electronic lexical database"},{"paperId":"563e0a5ab5e384a4afc7ae71bada34bd709498cd","title":"Interactive Transfer of Expertise: Acquisition of New Inference Rules"},{"paperId":"494aedf82da4755badc1fe74e4d21cf5fc029e9d","title":"Programs with common sense"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"}],"id":"79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88","summary":"This work provides a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements, and demonstrates that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting."},{"url":"https://www.semanticscholar.org/paper/63857190aaf5aab1d94b54bb257b7b03b8cb5a50","title":"GMAT: Global Memory Augmentation for Transformers","venue":"ArXiv","year":2020,"referenceCount":30,"citationCount":39,"influentialCitationCount":3,"publicationDate":"05/06/2020","authors":"Ankit Gupta,Jonathan Berant","citations":[{"paperId":"13261129251c9e8891cff02c3aee15c4df6a5630","title":"Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"},{"paperId":"e4c886fb04932bf6521d6ae4f3adc01ff2b18961","title":"Uncertainty Guided Global Memory Improves Multi-Hop Question Answering"},{"paperId":"8420fddf489bd7c5b822bd904aa11ff3742bfb78","title":"On the Long Range Abilities of Transformers"},{"paperId":"5e2cada56906540d31a73026a1e4d78cb8faf971","title":"Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems"},{"paperId":"93490a7cc9256c54554a66b223e1e6b6b1725062","title":"Heterogenous Memory Augmented Neural Networks"},{"paperId":"c3e9318b38662c942752b922b0df47fdd9f4de48","title":"SparseCoder: Advancing Source Code Analysis with Sparse Attention and Learned Token Pruning"},{"paperId":"daf83b63cd7c3f20f3b3283eb7e8b410209964f5","title":"Associative Transformer Is A Sparse Representation Learner"},{"paperId":"658869b820bfaa919d10378d0a019bff7e56db6c","title":"Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers"},{"paperId":"2d01b6afbc86cba1cb895dbcd9396b13952bf0e5","title":"Focus Your Attention (with Adaptive IIR Filters)"},{"paperId":"23005cd763b0ad723390f07c4a82693d58657919","title":"FIT: Far-reaching Interleaved Transformers"},{"paperId":"594d8e1696619f3cebb7c6bffdad8e0a5592f006","title":"Scaling Transformer to 1M tokens and beyond with RMT"},{"paperId":"5735e49e501c8e51e9be4079592e46e047747b03","title":"Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"},{"paperId":"980d1c3bf9d1a3c0ce68567e0efc1a72f203f12c","title":"Global memory transformer for processing long documents"},{"paperId":"ce750b62c1b090770aedd949fb289b96b03eedad","title":"SeDR: Segment Representation Learning for Long Documents Dense Retrieval"},{"paperId":"429803c4d7a88fccff5b24f6a9aaea8aabf7da81","title":"Simplified State Space Layers for Sequence Modeling"},{"paperId":"732e3faec4e5be4d144256f2c379b9dc49f0b227","title":"Efficient Long-Text Understanding with Short-Text Models"},{"paperId":"cfc5d3032c0c370892eb42687162afbb75455778","title":"Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes"},{"paperId":"a8cf0f7a20f886acfb332071c2daaf58ba86a5ca","title":"Recurrent Memory Transformer"},{"paperId":"5f3c2a31fc84d13a72008f70106163bd92f2f9d0","title":"kMaX-DeepLab: k-means Mask Transformer"},{"paperId":"eaef083b9d661f42cc0d89d9d8156218f33a91d9","title":"Long Range Language Modeling via Gated State Spaces"},{"paperId":"31a9744bd5421b3fbbad2ab38ce33bb2f352c77a","title":"CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation"},{"paperId":"b9a701c90f3d3df27366f5b29a97f798eb940ac7","title":"ChapterBreak: A Challenge Dataset for Long-Range Language Models"},{"paperId":"71e15a9a52dcafca57bff5f310b95e2c7d0cfc87","title":"Diagonal State Spaces are as Effective as Structured State Spaces"},{"paperId":"2f5a60cce19c6e5ca5a527f713d3b1545d64f0ef","title":"Socialformer: Social Network Inspired Long Document Modeling for Document Ranking"},{"paperId":"dbd684b259d6d7383ad4bb354ed53a0d12d38b63","title":"Leveraging Multi-view Inter-passage Interactions for Neural Document Ranking"},{"paperId":"73d64ecbe3e846394444dab6c5e89ba33e5daa49","title":"Memory transformer with hierarchical attention for long document processing"},{"paperId":"2ca44146b2ee3859d0a9f9e04e4bec0983e3f57b","title":"Interpretable Self-supervised Multi-task Learning for COVID-19 Information Retrieval and Extraction"},{"paperId":"c1a4278f969acfc6682a924e31b95e1ade9703ee","title":"Memory-efficient Transformers via Top-k Attention"},{"paperId":"af679d69fcc1d0fcf0f039aba937853bcb50a8de","title":"Luna: Linear Unified Nested Attention"},{"paperId":"69285c9040c1356272752499b7e1e53ef25ac008","title":"Beyond Paragraphs: NLP for Long Sequences"},{"paperId":"a4bd6a22dcdc740a9ff20af48fe2d828f0190b17","title":"Value-aware Approximate Attention"},{"paperId":"787119e3c3f819244c82b7d97779473773e60696","title":"MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"},{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","title":"Longformer: The Long-Document Transformer"},{"paperId":"5deef7fc161cbdc884aff15b9810f8a432c1489a","title":"k-means Mask Transformer"},{"paperId":"48ad536d00742a31eb8c6408c5d7ad96e654fe7a","title":"Receptive Field Alignment Enables Transformer Length Extrapolation"},{"paperId":"a8a048b862cf84a78e0a3f2e8ca8979b7c70ddb7","title":"An explainability analysis of a sentiment prediction task using a transformer-based attention filter"},{"paperId":"9b1434c63cec05b12500bc70e09fa6861e03a80d","title":"Topic Sentence Named Entity Recognition: A New Task with Its Dataset and Benchmarks"},{"paperId":"d828ec273669d7c3769d1f66ec37edfbc5de15f3","title":"TRANSFORMER-QL: A STEP TOWARDS MAKING TRANSFORMER NETWORK QUADRATICALLY LARGE"},{"paperId":"af07b1b454f16ed351c893376c85be11d6dc2b5c","title":"Appendix for: Convolutional State Space Models for Long-range 651 Spatiotemporal Modeling"}],"references":[{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","title":"Longformer: The Long-Document Transformer"},{"paperId":"b26f2037f769d5ffc5f7bdcec2de8da28ec14bee","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"657329c633709dd1ac34a30d57341b186b1a47c2","title":"Efficient Content-Based Sparse Attention with Routing Transformers"},{"paperId":"80376bdec5f534be78ba82821f540590ebce5559","title":"How Much Knowledge Can You Pack into the Parameters of a Language Model?"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"f51497f463566581874c941353dd9d80069c5b77","title":"Compressive Transformers for Long-Range Sequence Modelling"},{"paperId":"2e14e84ccec924ed770b58108ad1d9de6f0ca295","title":"BP-Transformer: Modelling Long-Range Context via Binary Partitioning"},{"paperId":"2cf3bd0cc1382f35384e259d99e4f9744eeaed28","title":"Blockwise Self-Attention for Long Document Understanding"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"d6dccb5d71fbb6f5765f89633ba3a8e6809a720d","title":"Stand-Alone Self-Attention in Vision Models"},{"paperId":"95f2aedef1453e5d88af9d5fddb79e0e56223cb0","title":"Compositional Questions Do Not Necessitate Multi-hop Reasoning"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"1db9bd18681b96473f3c82b21edc9240b44dc329","title":"Image Transformer"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a","title":"The NarrativeQA Reading Comprehension Challenge"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"d806d9fbc7e2f56bf0d1ccf430972a265089da6a","title":"High performance computing environment for multidimensional image analysis"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Understanding back-translation at scale"}],"id":"63857190aaf5aab1d94b54bb257b7b03b8cb5a50","summary":"This work proposes to augment sparse Transformer blocks with a dense attention-based $\\textit{global memory}$ of length $M$ ($\\ll L$) which provides an aggregate global view of the entire input sequence to each position, and empirically shows that this method leads to substantial improvement on a range of tasks."},{"url":"https://www.semanticscholar.org/paper/0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models","venue":"BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","year":2020,"referenceCount":9,"citationCount":12,"influentialCitationCount":0,"publicationDate":"13/10/2020","authors":"Devin J. Johnson,Denise Mak,Drew Barker,Lexi Loessberg-Zahl","citations":[{"paperId":"174a9b78350e9561555052bc6901cc44782f4c62","title":"Estimating Numbers without Regression"},{"paperId":"96c87efd6eef934979a2a2b083b17520ddf96e3f","title":"Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models"},{"paperId":"763f47b73d0273a93a52a8ee8a63cf19009df6e1","title":"Injecting the BM25 Score as Text Improves BERT-Based Re-rankers"},{"paperId":"1ec4ae64ded99846bcd8a1a3b16fe2875f55c7c1","title":"Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"03d73f3073ac0d73d60d5567fc1ed558367c8279","title":"Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"2efefcb2d48eecf308f856cab48f23621e4a88f1","title":"Scaling Neural ITN for Numbers and Temporal Expressions in Tamil: Findings for an Agglutinative Low-resource Language"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":"f3b1ad7986eea54d381f65abf4a0da5887129339","title":"A Multilingual Benchmark for Probing Negation-Awareness with Minimal Pairs"}],"references":[{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"a0e49f65b6847437f262c59d0d399255101d0b75","title":"What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"53d43ccc593bf44e9aa52e3971df1b9dd396e30d","title":"Probing for semantic evidence of composition by means of simple classification tasks"},{"paperId":"70ba4f5adfbb88e88b0dbfb60f3c9dfb97bf9940","title":"Preschool Origins of Cross-National Differences in Mathematical Competence: The Role of Number-Naming Systems"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}],"id":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","summary":"Novel multilingual probing tasks tested on DistilBERT, XLM, and BERT find evidence that the information encoded in these pretrained models’ embeddings is sufficient for grammaticality judgments but generally not for value comparisons."},{"url":"https://www.semanticscholar.org/paper/51c62d63c6204deecb24a1d3f9ea8e0a42d23817","title":"Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces","venue":"International Conference on Learning Representations","year":2020,"referenceCount":32,"citationCount":11,"influentialCitationCount":0,"publicationDate":"27/08/2020","authors":"Yatin Nandwani,Deepanshu Jindal,Mausam,Parag Singla","citations":[{"paperId":"c5fe40da42a4df3b58ab2a4204922dcf76368053","title":"Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods"},{"paperId":"fdb0f24fdf31b703921e4044ea1f55d82074b56f","title":"Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs"},{"paperId":"a12d2a66a52a4e45a2994f719f524f896265e6c1","title":"Scalable Coupling of Deep Learning with Logical Reasoning"},{"paperId":"4fb5641502fc47b0a54c06a0e79fd3192aea00d6","title":"Unsupervised Learning for Combinatorial Optimization Needs Meta-Learning"},{"paperId":"587450871bcc3f9211d2291037ed306e4c881988","title":"A Solver-Free Framework for Scalable Learning in Neural ILP Architectures"},{"paperId":"52f6003584f8b423abb0a8f2b890ae0eac660889","title":"Unsupervised Learning for Combinatorial Optimization with Principled Objective Relaxation"},{"paperId":"1cdcf5f527a887cb935eaf2a0109b40d2fc90fdb","title":"Conditional set generation using Seq2seq models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6294b2966523a264e99cf90dffaf31e40874c6cb","title":"Neural Models for Output-Space Invariance in Combinatorial Problems"},{"paperId":"0e282a0725ff485bd97a344668fe667027d26c93","title":"End-to-End Neuro-Symbolic Architecture for Image-to-Image Reasoning Tasks"},{"paperId":"bea00213becb832b2dd3ec6692d14145db168e8d","title":"Unsupervised Learning for Combinatorial Optimization with Principled Objective Design"}],"references":[{"paperId":"3cb4239741e5b7496b60ee7af24c08b4341af0a2","title":"Structured Prediction with Partial Labelling through the Infimum Loss"},{"paperId":"cb4571fa905abb70868d0bb9d4681f0a612c2d0f","title":"Differentiable Reasoning on Large Knowledge Bases and Natural Language"},{"paperId":"918fea871683bb51f7ca97540c767b067ef9e3c1","title":"Partial Label Learning via Label Enhancement"},{"paperId":"d3850595d3ae7c73e9488054c9b437f75511b569","title":"SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver"},{"paperId":"3a6447361b20c249f5306ae17dee43f645430e31","title":"Neural Logic Machines"},{"paperId":"0b86e5561f0de00047b96a1bb42a3e6379ed58c2","title":"Partial Label Learning with Self-Guided Retraining"},{"paperId":"fd4ae71916cf400bfd1490f275e91b154eb69160","title":"Relational recurrent neural networks"},{"paperId":"bbe56733113671455a9fee900bfca1a6d7f247bd","title":"Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning"},{"paperId":"b3dae9529f3caeeec9cc6872e94aa690418acb22","title":"Reinforcement Learning for Relation Classification From Noisy Data"},{"paperId":"de3b9eb697feed3d097e3f671afe395f48c1ab76","title":"Stochastic Video Generation with a Learned Prior"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"34273979fd2a62fd7b49ee6d14a925864ff94e74","title":"Recurrent Relational Networks"},{"paperId":"bfdb693df3a04fa9645233c444ccd8ec16c6c477","title":"Prediction Under Uncertainty with Error-Encoding Networks"},{"paperId":"4df7bbe3ca7806f39a490c99f17867a0ac299bc3","title":"Learning Explanatory Rules from Noisy Data"},{"paperId":"032274e57f7d8b456bd255fe76b909b2c1d7458e","title":"A Deep Reinforced Model for Abstractive Summarization"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"1bc49abe5145055f1fa259bd4e700b1eb6b7f08d","title":"SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents"},{"paperId":"62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e","title":"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge"},{"paperId":"bf55591e09b58ea9ce8d66110d6d3000ee804bdd","title":"Image Captioning with Semantic Attention"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"264184abb0a581701616f539336821182b077417","title":"There Is No 16-Clue Sudoku: Solving the Sudoku Minimum Number of Clues Problem via Hitting Set Enumeration"},{"paperId":"442f09ddb5bb7ba4e824c0795e37cad754967208","title":"Learning from Partial Labels"},{"paperId":"a6ccfe1ac31444fb5a0d32b58182e0fb1b17c0e4","title":"Multi-Label Classification: An Overview"},{"paperId":"cf376cd3d516e68dfbec2f08e552aa97d88e975b","title":"Zchaff2004: An Efficient SAT Solver"},{"paperId":"c25f5efa9e2d6ca04388052959d73e80163556f6","title":"7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019"},{"paperId":null,"title":"2018] 11 as the prediction network for this task. RRN uses a message passing based"},{"paperId":null,"title":"Can convolutional neural networks crack sudoku puzzles?"},{"paperId":"822f1ed9a76a57cc19d8fda7745365b97130b97a","title":"Injecting Logical Background Knowledge into Embeddings for Relation Extraction"},{"paperId":null,"title":"Minimum sudoku"},{"paperId":"89c3aed3e1219985c4a832f09adcd1df7096bf60","title":"Learning with Multiple Labels"},{"paperId":"754b96e00671c74de0add9df1ef60dcf21160483","title":"Local search strategies for satisfiability testing"}],"id":"51c62d63c6204deecb24a1d3f9ea8e0a42d23817","summary":"This paper formally defines the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku, and proposes an RL based approach to jointly train the selection module with the prediction network."},{"url":"https://www.semanticscholar.org/paper/2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of the Transformers with Simple Arithmetic Tasks","venue":"ArXiv","year":2021,"referenceCount":48,"citationCount":62,"influentialCitationCount":8,"publicationDate":"25/02/2021","authors":"Rodrigo Nogueira,Zhiying Jiang,Jimmy J. Li","citations":[{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"4726d1dc54851db99c29180127d840bd19f20afc","title":"Positional Description Matters for Transformers Arithmetic"},{"paperId":"dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills"},{"paperId":"1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b","title":"What Algorithms can Transformers Learn? A Study in Length Generalization"},{"paperId":"200fcd964f41efe0c35a3f888a520ede08a3269c","title":"From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers"},{"paperId":"174a9b78350e9561555052bc6901cc44782f4c62","title":"Estimating Numbers without Regression"},{"paperId":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners"},{"paperId":"0b778079946764292de3771a489d5ce9e1868a8b","title":"The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models"},{"paperId":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator"},{"paperId":"cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6","title":"Can transformers learn the greatest common divisor?"},{"paperId":"edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a","title":"It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models"},{"paperId":"9160673f89ecb326db8c64f8d09ab30f35a74e9d","title":"Arithmetic with Language Models: from Memorization to Computation"},{"paperId":"72a9187b489992cad3d54420611d5039eb6b9d86","title":"One Blade for One Purpose: Advancing Math Information Retrieval using Hybrid Search"},{"paperId":"8f1b0c247171510fc68da27a16a377456376a5a7","title":"Assessing GPT-3.5 and GPT-4 in Generating International Classification of Diseases Billing Codes"},{"paperId":"0db0af0cd3ceb0531a050a03e6ceb849580ff53b","title":"Teaching Arithmetic to Small Transformers"},{"paperId":"be3a74f9f6010889d049060eab2a4b09eb48bbfb","title":"The Value of Numbers in Clinical Text Classification"},{"paperId":"49d340b7d6108ce5ef2277a165ea83f254671763","title":"The use of weather nowcasting convolutional neural network extrapolators in cardiac PET imaging"},{"paperId":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers"},{"paperId":"b43383f10634f7e610f22badd4f42c93e5dcb947","title":"Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI"},{"paperId":"3e7a0dc5795dc108c78993bcf3624fc626a9f9cf","title":"Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks"},{"paperId":"d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","title":"Exposing Attention Glitches with Flip-Flop Language Modeling"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering"},{"paperId":"aec826ff336ca442697d5f908ab1668f1ea18987","title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","title":"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)"},{"paperId":"49e46e615747f258517248de5a736814fada17ee","title":"What is my math transformer doing? - Three results on interpretability and generalization"},{"paperId":"e82e3f4347674b75c432cb80604d38ee630d4bf6","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"2a7ae3e98357569c41424dacd60c62d3df78a0db","title":"Limitations of Language Models in Arithmetic and Symbolic Induction"},{"paperId":"40f73969cd3415e9c1b03796cbb5a50c79ebd448","title":"SALSA: Attacking Lattice Cryptography with Transformers"},{"paperId":"b09420b30fc093d63fb2ee1aac26c71da81da437","title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"},{"paperId":"d2018d1b0f69ca6805aa18a22be95cab9b5c44c7","title":"On Neural Architecture Inductive Biases for Relational Tasks"},{"paperId":"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence"},{"paperId":"7a25155364476839b6d1fc0653cd8611327ab9ba","title":"mGPT: Few-Shot Learners Go Multilingual"},{"paperId":"edd80013e8b9fcba9231cf99884f32e5236ff329","title":"AA-TransUNet: Attention Augmented TransUNet For Nowcasting Tasks"},{"paperId":"45ece6f3b0a319dba60c20b3013b5161dd49c58b","title":"Linear algebra with transformers"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"6c84fc8c5ec823342f34a020f8b92b7064e96ca2","title":"Towards Efficient and Robust Out-of-Distribution Deep Learning with Implicit Models"},{"paperId":"4f451ba06c4c9effd6c4ac0bae222495501a6200","title":"Innovations in Neural Data-to-text Generation"},{"paperId":"58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","title":"JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection"},{"paperId":"c27f3904490b8e5f9f39fe2b36722090c189e916","title":"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF"},{"paperId":"346accd49d1359aa3985c7b298bc2057ae642271","title":"One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development"},{"paperId":"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","title":"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning"},{"paperId":"33149f835f391119d287cc2c6b009464e7d14fe4","title":"On the Abilities of Mathematical Extrapolation with Implicit Models"},{"paperId":"5df55d94ab5026ff84ec01871592108fbadbddbe","title":"Measurement Extraction with Natural Language Processing: A Review"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"}],"references":[{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"4383a975c09b72ba2f1a77cd779bb6965dbfb2fb","title":"Scaling Laws for Transfer"},{"paperId":"02791e807dc9a91f854a1f3d5f6005122a546109","title":"Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"45cdda3f96ae32927c7b073ebbedf46f5e0fbdc5","title":"Enhancing the Numeracy of Word Embeddings: A Linear Algebraic Perspective"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"84476fdf6ead3553f4493dff8e02308439d6222b","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"8256f48f759cf85044db251cc512f965834945b3","title":"Rethinking Positional Encoding in Language Pre-training"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"ac67ec5b985a30239926c3362fa45a9a03f017af","title":"Learning to Generate Correct Numeric Values in News Headlines"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"d0e28f5dc1feae19e41087a92a87992977fd85af","title":"Encoding word order in complex embeddings"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"136396e988dbbdaca433096e931b22732c3d95dd","title":"Text2Math: End-to-end Parsing Text into Math Expressions"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"5435f997d98754f68492334eeb87d027047e60cb","title":"Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"0bb487b9cfefd56e79be0a5be5f1e05742683301","title":"Extensions and Limitations of the Neural GPU"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"5b791cd374c7109693aaddee2c12d659ae4e3ec0","title":"Grid Long Short-Term Memory"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"111fd833a4ae576cfdbb27d87d2f8fc0640af355","title":"Learning internal representations by error propagation"},{"paperId":"5414be4432fcfe1e83932127ee2463d107e5b61c","title":"A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":null,"title":"Character-level models such as ELMO"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"The architecture of the transformer"}],"id":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","summary":"It is found that how a number is represented in its surface form has a strong influence on the model’s accuracy, and this result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement."},{"url":"https://www.semanticscholar.org/paper/a20712b1b9779ee43ce143a19b3f67f0cacbbf57","title":"Neural Databases","venue":"ArXiv","year":2020,"referenceCount":59,"citationCount":8,"influentialCitationCount":0,"publicationDate":"14/10/2020","authors":"James Thorne,Majid Yazdani,Marzieh Saeidi,F. Silvestri,Sebastian Riedel,A. Halevy","citations":[{"paperId":"5c828011a508611df4d58cced9cc48d049dc4eb9","title":"A Data Source for Reasoning Embodied Agents"},{"paperId":"29cfffe7dcbcd78fbdaba938176be064b185d149","title":"Conversational Question Answering on Heterogeneous Sources"},{"paperId":"f49feb36a148fa0c5be0f2a2353583367e7cf4bd","title":"QuTE: Answering Quantity Queries from Web Tables"},{"paperId":"fe935caed47ef090a306d6d09240f76adc43a420","title":"Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins"},{"paperId":"d331de3b6bebb0f9af1fddf1b730ec057a7026d4","title":"Relational World Knowledge Representation in Contextual Language Models: A Review"},{"paperId":"73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19","title":"Measuring and Improving Consistency in Pretrained Language Models"},{"paperId":"7ef33fd5b0ef0c4de42cf0afdc9f7dfb0f430b20","title":"From Natural Language Processing to Neural Databases"},{"paperId":"d51bec1426494f23de57b2c4184c7e26583eda53","title":"Relational Pretrained Transformers towards Democratizing Data Preparation [Vision]"}],"references":[{"paperId":"0b09448f7543453cc066416f547292dc1e4471f6","title":"KILT: a Benchmark for Knowledge Intensive Language Tasks"},{"paperId":"ea8c46e193d5121e440daf96edfd15a47151c293","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"0544af8d6b6b6e9ce14be9c6425e520a638be380","title":"Photon: A Robust Cross-Domain Text-to-SQL System"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"b26f2037f769d5ffc5f7bdcec2de8da28ec14bee","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"512f34906ddaefe885af2e5eec9b2b3b50ffd377","title":"Deep entity matching with pre-trained language models"},{"paperId":"15ad2b27c5248e7d1db5456794ca1ca8a8198f5d","title":"Transformers as Soft Reasoners over Language"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"71c908529b12ef6ee8d735127a63d48b1fc5c43c","title":"Break It Down: A Question Understanding Benchmark"},{"paperId":"cb4571fa905abb70868d0bb9d4681f0a612c2d0f","title":"Differentiable Reasoning on Large Knowledge Bases and Natural Language"},{"paperId":"521b4e26df0f1cf5763dece14cbb218df152dc59","title":"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"f3fbcfbb396f6c0391674c8637a373b729e5e531","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"d3cacb4806886eb2fe59c90d4b6f822c24ff1822","title":"Visualizing and Understanding the Effectiveness of BERT"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"b9372e972997c5056bb79c70526230baed2e372b","title":"Multi-hop Reading Comprehension through Question Decomposition and Rescoring"},{"paperId":"e2587eddd57bc4ba286d91b27c185083f16f40ee","title":"What do you learn from context? Probing for sentence structure in contextualized word representations"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"cd832f7081ab7b83240140c4e5e58b4fb1f8e0e6","title":"Interpretation of Natural Language Rules in Conversational Machine Reading"},{"paperId":"ac11062f1f368d97f4c826c317bf50dcc13fdb59","title":"Dissecting Contextual Word Embeddings: Architecture and Representation"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"c0a29cb35c2965930566d6a407da043e18431eaa","title":"Deep Learning for Entity Matching: A Design Space Exploration"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"b1d24e8e08435b7c52335485a0d635abf9bc604c","title":"FEVER: a Large-scale Dataset for Fact Extraction and VERification"},{"paperId":"0539535989147bc7033f4a34931c7b8e17f1c650","title":"The Case for Learned Index Structures"},{"paperId":"7d5cf22c70484fe217936c66741fb73b2a278bde","title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5889e9afbcc3935867f9ae16fe46c71b9f2b071f","title":"End-to-end Differentiable Proving"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"4c41104e871bccbd56494350a71d77a7f1da5bb0","title":"Understanding Neural Networks through Representation Erasure"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"c0883f5930a232a9c1ad601c978caede29155979","title":"“Why Should I Trust You?”: Explaining the Predictions of Any Classifier"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","title":"Neural Module Networks"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"0fadc80c26e879d22cc3660783934c0e3158a167","title":"DB-IR integration using tight-coupling in the Odysseus DBMS"},{"paperId":"4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e","title":"End-To-End Memory Networks"},{"paperId":"abb33d75dc297993fcc3fb75e0f4498f413eb4f6","title":"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"},{"paperId":"c3823aacea60bc1f2cabb9283144690a3d015db5","title":"Neural Turing Machines"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","title":"Wikidata: a free collaborative knowledgebase"},{"paperId":"89de826b17bb3a8009c20b7573f85a184827e7fd","title":"Constructing an Interactive Natural Language Interface for Relational Databases"},{"paperId":"e925291a9358ed85b9ea1d2a767f1e657ea40ac8","title":"DB-IR integration using tight-coupling in the Odysseus DBMS"},{"paperId":"564257469fa44cdb57e4272f85253efb9acfd69d","title":"MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"672b3d3976aa106c72b845db7734cd8f194cc261","title":"ODYS: an approach to building a massively-parallel search engine using a DB-IR tightly-integrated parallel DBMS for higher-level functionality"},{"paperId":"6304131a1fb60b9883f1469072eeda8a0bd441f5","title":"DB&IR: both sides now"},{"paperId":"52e40e9466178129a15e6584f31d89d31800308f","title":"Report on the DB/IR panel at SIGMOD 2005"},{"paperId":"1df699c7da1b948bffd449ac44809017ee4206e0","title":"Natural language interfaces to databases – an introduction"},{"paperId":"03b7e51c52084ac1db5118342a00b5fbcfc587aa","title":"Q-learning"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"BERT and PALs: Projected attention layersforefficientadaptationinmulti-tasklearning"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"8d43a96e12a07b53014997f050005e09a62b7cef","title":"Crossing the Structure Chasm"},{"paperId":"c5f96d281b436ad4f1e0a3c37cbbcda4c5b17eea","title":"Models for Integrated Information Retrieval and Database Systems."}],"id":"a20712b1b9779ee43ce143a19b3f67f0cacbbf57","summary":"This paper describes NeuralDB, a database system with no pre-defined schema, in which updates and queries are given in natural language, and describes an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators."},{"url":"https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":29,"citationCount":293,"influentialCitationCount":31,"publicationDate":"11/03/2021","authors":"Arkil Patel,S. Bhattamishra,Navin Goyal","citations":[{"paperId":"4bebe389dfa85423e5cc089edf20b2c3f572f38c","title":"Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives"},{"paperId":"d4656bba3a424a25fcd9e1fbf3966f080ace9c2f","title":"LLM Harmony: Multi-Agent Communication for Problem Solving"},{"paperId":"fed7a1a2fc92b6d17b73d9f5d28a19ba87abaec0","title":"A&B == B&A: Triggering Logical Reasoning Failures in Large Language Models"},{"paperId":"7843287ffcd48bdf830f16465741a548c2ba8d66","title":"Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math"},{"paperId":"392d98aadc0ba0ac612ea48c2c60837b01fa849e","title":"Large Language Model for Causal Decision Making"},{"paperId":"a55e7ebf3b48fd7caeb1b2ec5b2d10bc0b390b24","title":"From Text to Multimodal: A Comprehensive Survey of Adversarial Example Generation in Question Answering Systems"},{"paperId":"b9ea4ea333eeb8875c3e23637a8dce99038be95c","title":"An In-depth Look at Gemini's Language Abilities"},{"paperId":"18e20944d1d64e73fc40321f65c3ddd0ef6a7aca","title":"Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models"},{"paperId":"1bd9466f0bb10d29a16f614943ec7823e13cb210","title":"Mixed Distillation Helps Smaller Language Model Better Reasoning"},{"paperId":"82730762d438b58a8af874d03e9903a73e18a39e","title":"TinyGSM: achieving>80% on GSM8k with small language models"},{"paperId":"55820f684fcd592edfa013633e5704a41b176d23","title":"Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning"},{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"efcb903bd962d1ffdd603fc540233971b5ce060f","title":"Get an A in Math: Progressive Rectification Prompting"},{"paperId":"76588eabeb1d712c6170fb9cbfe13b92d0809b69","title":"Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning"},{"paperId":"48cfb2e5d32dcfedd9acb8eb38f499d2b7202369","title":"Prompt Optimization via Adversarial In-Context Learning"},{"paperId":"a9e78765a4d49a50d67d0dacb033fb47f8d9f8c9","title":"Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication"},{"paperId":"4d4ebfc94cbbc6a8d8a27a634ebd2db973b88178","title":"ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions"},{"paperId":"73f02529d1878977d24ee0a906ad2b48a5ba022f","title":"Competition-Level Problems are Effective LLM Evaluators"},{"paperId":"51fb6598a3ebe36b371b096b4824d718e6e527fb","title":"The Efficiency Spectrum of Large Language Models: An Algorithmic Survey"},{"paperId":"44ffd3cd333bb41b900926556edb1c452759c398","title":"WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models"},{"paperId":"9690eda895336c02db338ffacb6a90c5bdeb5263","title":"Speak Like a Native: Prompting Large Language Models in a Native Style"},{"paperId":"6fa0677731184444df0e1fc8070938419cd6da47","title":"Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"},{"paperId":"d57ebdb4b5ed89ab4135c72f0c91dc1c4b347c4b","title":"KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains"},{"paperId":"eee5a8e06c32112d268e88a8d4c45592d7214244","title":"DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data"},{"paperId":"03f3801956fc4cc026860568670f9f65ed29b192","title":"Towards A Unified View of Answer Calibration for Multi-Step Reasoning"},{"paperId":"907cc13d907b2dfc39f6bb9e0c64cc69e71baefa","title":"Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models"},{"paperId":"bf11f01929afed0ad3ccfe1b5e0fd34d90ef2b4f","title":"Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"},{"paperId":"11b95e33f1a61079105e06090984b9dd8742887e","title":"Contrastive Chain-of-Thought Prompting"},{"paperId":"4014253368133c01bfc0383660c518d11afccad2","title":"Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration"},{"paperId":"c874aa93efe663ed31f2ec72d45a5dd4b4cdffba","title":"Plum: Prompt Learning using Metaheuristic"},{"paperId":"08a3e2c94707926fe543df69bdf6c3a9b71aab52","title":"Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios"},{"paperId":"2f806d52b30a29907341115b8fee466ebefe382f","title":"Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding"},{"paperId":"b6667ba4f586489f12587446c6daaa3f09cfc539","title":"Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset"},{"paperId":"7dd3b54233a71c532a15adc6faa7284af7c02f15","title":"Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs"},{"paperId":"ad402080a4aa66ef3c57a46ce4685a47a3cc0a61","title":"Quantifying Uncertainty in Natural Language Explanations of Large Language Models"},{"paperId":"027cd21f94a9c4bb7842c857e4641df463a55aa7","title":"Perturbation-based Active Learning for Question Answering"},{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"7215d95f6068013d3f06de8efd6a00e51b615731","title":"ATHENA: Mathematical Reasoning with Thought Expansion"},{"paperId":"5402c22369d0190d0a002b7a1222d403edae010a","title":"Defining a New NLP Playground"},{"paperId":"98b607e7cb84e1a5c87c8a49562ae35435e6722d","title":"Learning From Mistakes Makes LLM Better Reasoner"},{"paperId":"c1284ee1ddf29955a1a02bdc45abdaac63745017","title":"Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method"},{"paperId":"db7d339fc394f8db910a2dd12d2621e684094617","title":"In-Context Ability Transfer for Question Decomposition in Complex QA"},{"paperId":"4d2a0840abd2eef795a4e14538271e06f8536b07","title":"R3 Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context"},{"paperId":"ef9079f32e806e4d297cee28f36b2321acee9eb3","title":"MCC-KD: Multi-CoT Consistent Knowledge Distillation"},{"paperId":"f176d0d466d7c778a6435fe9a8d7e49508cb9059","title":"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts"},{"paperId":"b5f56f466c06d10100d8d1aac9e1f979c527b1cf","title":"Dual Process Theory for Large Language Models: An overview of using Psychology to address hallucination and reliability issues"},{"paperId":"be7dbac2bcaed4cd034a7371004a011933e1bdca","title":"Democratizing Reasoning Ability: Tailored Learning from Large Language Model"},{"paperId":"e1a10caa6571602980f488822e2f7e88e311f160","title":"Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning"},{"paperId":"4776de7b856d3b15eecc4f88666cdc13972df22e","title":"Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking"},{"paperId":"974f0e1a85c1ece2555718342ff2abb6bcb6a825","title":"KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models"},{"paperId":"a419e1d74cfc2b5ff400963476bda5c6ae66e172","title":"TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models"},{"paperId":"2179a13e78195fa78be5ccca2b5dcf9fad783ffc","title":"An Expression Tree Decoding Strategy for Mathematical Equation Generation"},{"paperId":"4e3e064e6613e2c40c1fa5dc2bd8fd934b410ccb","title":"Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models"},{"paperId":"b9e8b62bcc019f47a0a015568f70039b3b7c1196","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model"},{"paperId":"a83724fd55cd2bcf5583ca181373c34571ac1f73","title":"DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models"},{"paperId":"4ba40ae509a7b9a929bc47ac347865a3ffd9e103","title":"Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection"},{"paperId":"be9447ccc05a0e8a07321272778c7574173cf00e","title":"Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models"},{"paperId":"e754e647ce86774040b1f05706e9809f18929a6a","title":"Agent Instructs Large Language Models to be General Zero-Shot Reasoners"},{"paperId":"3a2492116f861a8fcd9db570151d80e6cbfd4a5e","title":"Redefining Digital Health Interfaces with Large Language Models"},{"paperId":"cddb552f6c3464a54a02b0b64b2d1af56c086606","title":"MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning"},{"paperId":"30b45e1d990c3e73fb328bbcfd0f616ac51f60b1","title":"DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning"},{"paperId":"8db1dcae055842f43ccac04182957b20d15bbe6b","title":"Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems"},{"paperId":"ff3a22641d21e9725efb5e79f22094300b689ab7","title":"Ask Again, Then Fail: Large Language Models' Vacillations in Judgement"},{"paperId":"1eb71a48256b3c5f5d8f7f39efbaa1b692b49860","title":"Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers"},{"paperId":"648fe0ddd5d6987d1b9b80234b9120764ea389e1","title":"SELF: Language-Driven Self-Evolution for Large Language Model"},{"paperId":"5076bbbf831a92174c9cc1b347bd0584560435fc","title":"Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning"},{"paperId":"bfeda6c7aa7899a80adb01894555b09d24756a59","title":"Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"},{"paperId":"053bdcbe3cf45c5f34a4e50b271a6016a960637a","title":"Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning"},{"paperId":"1c768b75c7c31428b57f627a5b68f663b391ac74","title":"DyVal: Graph-informed Dynamic Evaluation of Large Language Models"},{"paperId":"12db3efff4cc9e16822dd64bb1cad66f3f034f3b","title":"L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models"},{"paperId":"b272513916b45c8517d289d7abee4a53e6832187","title":"ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"},{"paperId":"7fe071ea76e49bc3e573beb53f07721630954247","title":"Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"},{"paperId":"31ae42394959fb1a336886379a5527bec5c9c9c4","title":"Stress Testing Chain-of-Thought Prompting for Large Language Models"},{"paperId":"11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"},{"paperId":"f148b3a0106cbaba356b9f099f1a10c072c0b3c5","title":"How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions"},{"paperId":"09c8e80e2edf2053bdc3c31b3fa9b83e3a4bd169","title":"Exploring the Benefits of Data Augmentation in Math Word Problem Solving"},{"paperId":"2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e","title":"Design of Chain-of-Thought in Math Problem Solving"},{"paperId":"e716e6e0b3dd5124268780dc9bed521a07f371b8","title":"Contrastive Decoding Improves Reasoning in Large Language Models"},{"paperId":"04e838c16f3d1fb8d69d34fe0a0a92c59717875b","title":"EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning"},{"paperId":"1f8d74abcb89ee21bf01e7133cea503d8c99fef7","title":"Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level"},{"paperId":"cd391facabf5005419b79997b2ef8473644a8192","title":"Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL"},{"paperId":"e7c85d7d58d4b1fde4be8a8f166e46c995dc0f1b","title":"Re-Reading Improves Reasoning in Language Models"},{"paperId":"a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","title":"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"},{"paperId":"3886f3bd2a0af9e75bf9fa5b7db4224969dbf346","title":"MathAttack: Attacking Large Language Models Towards Math Solving Ability"},{"paperId":"c01c7c1f903dfaa78812fb20a6cb2db25e4712e3","title":"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness"},{"paperId":"f221eccdd96122a42c5e65532373e6974b30c20c","title":"Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills"},{"paperId":"412fe1f135cb20c952962133ca1e534a71bfd27f","title":"When Do Program-of-Thoughts Work for Reasoning?"},{"paperId":"2dbe7fd41f23aa2a077c3d1e91b0b73f92e3d8d2","title":"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning"},{"paperId":"fb1c5598a43cb3a8d0e4b4f453ade718948b22fd","title":"A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability"},{"paperId":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"},{"paperId":"cc01b647e31fbbd308e9e57884d9b07b4f582702","title":"Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach"},{"paperId":"751c100802012764a2c45e17e41fa219867b12e5","title":"Better Zero-Shot Reasoning with Role-Play Prompting"},{"paperId":"801d7ba75fc833aa76ce4863dc1f79e30ee0c23f","title":"Forward-Backward Reasoning in Large Language Models for Mathematical Verification"},{"paperId":"506ad0e58f5d6b99bbcc5e8efb519ae6fa34d307","title":"Guiding Mathematical Reasoning via Mastering Commonsense Formula Knowledge"},{"paperId":"1e2eba005ccd8ab7a668a525c5b43245853bdaf1","title":"Reasoning in Large Language Models Through Symbolic Math Word Problems"},{"paperId":"91206346edbe28abb606d7b3425cd455d4019d4f","title":"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"},{"paperId":"c29dbfbc17fa190b787a2662d49f08a38c8bd166","title":"ARB: Advanced Reasoning Benchmark for Large Language Models"},{"paperId":"a53c8ba374d430d6c3786d13c04edb200d547750","title":"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"},{"paperId":"20d448a8712238ea34d9a18287e3bf05bc61dd2c","title":"Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa"},{"paperId":"3bd83ff979f3c0e9470f23c360a18333593dc5a1","title":"GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution"},{"paperId":"c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36","title":"MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"a79330d1069a86b7f8bf6f9fe0ef01d7b1379a91","title":"Chain of Thought Prompting Elicits Knowledge Augmentation"},{"paperId":"959e86a5678b5c2f4911cd75af8359acd325d4ad","title":"Solving Math Word Problems concerning Systems of Equations with GPT-3"},{"paperId":"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","title":"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements"},{"paperId":"8f7297454d7f44365b9bcda5ebb9439a43daf5e6","title":"Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs"},{"paperId":"c64c780427ee486568c919c3b7c2a314dfc89ecf","title":"DiversiGATE: A Comprehensive Framework for Reliable Large Language Models"},{"paperId":"ce51db215a08e83124b6845115984e488fc08eb3","title":"Solving Math Word Problems Following Logically Consistent Template"},{"paperId":"0ba49945649b40f205503dba3443e2bf550c7115","title":"Learning by Analogy: Diverse Questions Generation in Math Word Problem"},{"paperId":"ec9af8d8973fa4d302ccd53e5e0236651201351c","title":"Your Prompt is My Command: On Assessing the Human-Centred Generality of Multimodal Models"},{"paperId":"9efa81ec4954b0859c47dad8f42edfaf8bced69b","title":"Boosting Language Models Reasoning with Chain-of-Knowledge Prompting"},{"paperId":"4713dc19179cdd9083e47067fa9504751f8759c6","title":"Human-in-the-Loop through Chain-of-Thought"},{"paperId":"30ea884a5b8e4cffe18ebfef6c033e5c37b79eeb","title":"World Models for Math Story Problems"},{"paperId":"2d338cdd12091814dec11155d3f6f848d7bab4d8","title":"Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models"},{"paperId":"9d460930d9b5d12a65ff2b3efa23047ec75fbca1","title":"The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"d8244cf970b9019cfb7f813275eba10d17c47166","title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks"},{"paperId":"8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","title":"An Empirical Study on Challenging Math Problem Solving with GPT-4"},{"paperId":"d75d11d2c89c01cd284383546ae057cb827dc272","title":"Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning"},{"paperId":"0875651b68e6602d45ae08bee67cf63c02faa512","title":"Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models"},{"paperId":"6289de84a02f0c27734f295ada565603ac958948","title":"Tab-CoT: Zero-shot Tabular Chain of Thought"},{"paperId":"24a285b1e7ee9acfec9a82f7123563b62f532203","title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning"},{"paperId":"4cde406e8c6abe893c8f05e702d0c143c0c88ad1","title":"Neural Machine Translation for Mathematical Formulae"},{"paperId":"dbfd154190667087ed1cd6c7f75a81858c2f397e","title":"Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models"},{"paperId":"7283d616e40d7ab7422e3697218f3fc42f292bf2","title":"Getting MoRE out of Mixture of Language Model Reasoning Experts"},{"paperId":"c79a04da127d6ff28c19c5dc35feb01feaf3d2b9","title":"Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems"},{"paperId":"f29365305b2cd789019bafede8298d9731152f07","title":"GRACE: Discriminator-Guided Chain-of-Thought Reasoning"},{"paperId":"6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3","title":"Automatic Model Selection with Large Language Models for Reasoning"},{"paperId":"bb8f7fbec020675d269ccfa0e6e603f02b664c0d","title":"PaD: Program-aided Distillation Specializes Large Models in Reasoning"},{"paperId":"9a9b1e2968302eb882870537d4af6e2c722dfd1a","title":"Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement"},{"paperId":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models"},{"paperId":"7919cb1a1dcf70ed7803c43a71d43dba696ef149","title":"Making Language Models Better Tool Learners with Execution Feedback"},{"paperId":"073e6b91adc25c656d85002e3cb059e4530db20b","title":"Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation"},{"paperId":"a7307613447a828fc0fa2d233311b6f62fbadfb4","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents"},{"paperId":"4313b009cd30a469400d36b9fb20267620293d04","title":"A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"f576288b7f5ca4aff3a6492a9db1db5148b7616f","title":"Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?"},{"paperId":"a53b476e3236c2575e0d5aa451c2e17110715f42","title":"SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs"},{"paperId":"b4a6c010724f0459c9791018e34a982cf96987cf","title":"Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs"},{"paperId":"22d5459d1f47341b355feeb1becc37208d6ec365","title":"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"},{"paperId":"bcdaf6c98ddbd6809cf6241aa77200d7394db163","title":"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"},{"paperId":"df2beaae63e4d68ef8e762bcd4704c9f11f856d9","title":"Can Language Models Solve Graph Problems in Natural Language?"},{"paperId":"a39f960b28a627d554712a457efa5d3e3b7f8406","title":"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning"},{"paperId":"9ccb2beaec722232a84e9a7682c72dcf7de667df","title":"Non-Autoregressive Math Word Problem Solver with Unified Tree Structure"},{"paperId":"62176de125738e3b95850d1227bac81fd646b78e","title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models"},{"paperId":"3922365b7b40a447ecc57e027530cc90131e171e","title":"NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports"},{"paperId":"f11ea0fe307ce40fee1f18dfc3eef946a1c7a769","title":"SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data"},{"paperId":"aad167be3c902388ea625da4117fcae4325b8b7d","title":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"},{"paperId":"ef018d9fad6167cfddb7d6654c5422df1e953730","title":"Self-Evaluation Guided Beam Search for Reasoning"},{"paperId":"fce42753155280051ac64817404b4e1d3be6ebaa","title":"MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks"},{"paperId":"a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6","title":"Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"ac37accd7aedf1c25c3d54c7982579b297b3ff2b","title":"Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models"},{"paperId":"261549439aebdda72b648ecc462448fd24857ac1","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"90dd829f3d64dda19092b6e26909803bea5c37c1","title":"From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning"},{"paperId":"dca6c3927ade6481a1ae080f5c24decbfeced1be","title":"Boosted Prompt Ensembles for Large Language Models"},{"paperId":"0d502a1e300336ae628f5c8b99ee4d3766c8f60b","title":"Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"},{"paperId":"44b17e06fc7fbc87dd30e4a08f41507764f97e3e","title":"When do you need Chain-of-Thought Prompting for ChatGPT?"},{"paperId":"c715914c388fa64dd8686cd8755e5adfebbf2388","title":"REFINER: Reasoning Feedback on Intermediate Representations"},{"paperId":"bdb68c5e2369633b20e733774ac66eb4600c34d1","title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"},{"paperId":"bebede105aa69a81045bf79682272ee4cfb61475","title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks"},{"paperId":"c61d54644e9aedcfc756e5d6fe4cc8b78c87755d","title":"A Survey of Large Language Models"},{"paperId":"9a75e23639bfcc3a51da57a3b682a984d1d8ac0b","title":"Language Models can Solve Computer Tasks"},{"paperId":"00a9a469bb019bf33eeee438c110f704b71cda73","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"},{"paperId":"0d42221038c05cee8443c5b5af838505ee137dc3","title":"ART: Automatic multi-step reasoning and tool-use for large language models"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"e05483a41e8002e7024d39457e55a3fe533f5835","title":"How Many Demonstrations Do You Need for In-context Learning?"},{"paperId":"c9a9735216915e9afa0fc97b02b57148a0491bdd","title":"NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions"},{"paperId":"2ebd5df74980a37370b0bcdf16deff958289c041","title":"Foundation Models for Decision Making: Problems, Methods, and Opportunities"},{"paperId":"b626560f19f815808a289ef5c24a17c57320da70","title":"MathPrompter: Mathematical Reasoning using Large Language Models"},{"paperId":"fdacdbc6a00eeb42efe7f81848b0bc09be5ca997","title":"Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!"},{"paperId":"1462a0e5b7db47301bb0995db56426e1f4a0ac7d","title":"Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers"},{"paperId":"1358f90705b05cdb20ebe6799b02196205e7e9f0","title":"Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data"},{"paperId":"3fc3460c4554a28e489a0ea6ef067b79b7d301d9","title":"Active Prompting with Chain-of-Thought for Large Language Models"},{"paperId":"5938abafd61881f6b23a2ba318d2d3d0327402c0","title":"A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram"},{"paperId":"85996f9fc312777f487dd51bf9e96bb3704c2fb7","title":"On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)"},{"paperId":"0cc9d031ca2f85c1412d5eab9449416c47b8cacd","title":"Learning by Applying: A General Framework for Mathematical Reasoning via Enhancing Explicit Knowledge Learning"},{"paperId":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"07157c3cb9010acb84237e121dacc7ecba30f04b","title":"Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting"},{"paperId":"873a581320d928249609d3c07229d5af182a379c","title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"},{"paperId":"40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a","title":"Techniques to Improve Neural Math Word Problem Solvers"},{"paperId":"cd0988714ea326642d2b1bb18753e187fec71e42","title":"A Categorical Archive of ChatGPT Failures"},{"paperId":"69619a2a47faee7a29ec596db13172e2a42ff921","title":"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models"},{"paperId":"b115c1e1e9e51f8ad7d47b745bc04e29a654b84d","title":"Faithful Chain-of-Thought Reasoning"},{"paperId":"3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context"},{"paperId":"f2b0017ddd77fa38760a18145e63553105a1a236","title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"},{"paperId":"edc9bf11c4810a77f00ccb96130ff67ee578391e","title":"ThoughtSource: A central hub for large language model reasoning data"},{"paperId":"b1ae72169f750d2616b754576ed35b364007205b","title":"When Transformer models are more compositional than humans: The case of the depth charge illusion"},{"paperId":"1338b3771c27090dee722cc5b351ace179ebae76","title":"Mathematics, word problems, common sense, and artificial intelligence"},{"paperId":"27f0ce04403158b61328716ae4aaab5840c0d123","title":"Batch Prompting: Efficient Inference with Large Language Model APIs"},{"paperId":"9a9e68d400069f023f7dc9b982226c95159a509d","title":"Dissociating language and thought in large language models: a cognitive perspective"},{"paperId":"420a78c6382953c89d9c85be0580a4cb0c731099","title":"Solving Turkish math word problems by sequence-to-sequence encoder-decoder models"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","title":"Large Language Models Are Reasoning Teachers"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"7715ba5e75f5256e1061c7473afe61bb0dbb9065","title":"Large Language Models are Better Reasoners with Self-Verification"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"f0a136313ce7908a400dfb19ef6f9b9cc4a7b964","title":"Comparative Analysis of Problem Representation Learning in Math Word Problem Solving"},{"paperId":"5485c60dc4192cf118c4834d05da7b35a88e3928","title":"A Comparative Analysis of Math Word Problem Solving on Characterized Datasets"},{"paperId":"126a4776ff8315fd506766cb8f3c722cf746ad9e","title":"Teaching Small Language Models to Reason"},{"paperId":"2c08e45a4489937d6c38704d8884e127d8628ffe","title":"Explaining Math Word Problem Solvers"},{"paperId":"8fd462f6248d5e3f1b6602697c09489086b5655f","title":"Distilling Reasoning Capabilities into Smaller Language Models"},{"paperId":"5d10a5fcc943afab50bce6293eaaaf3d22815868","title":"Chaining Simultaneous Thoughts for Numerical Reasoning"},{"paperId":"3a3100001bdca4b094345263fd2aafebd5e93555","title":"Textual Enhanced Contrastive Learning for Solving Math Word Problems"},{"paperId":"e6745fb621481ccb0ed53c267a37292e499c1b42","title":"Automatic Generation of Socratic Subquestions for Teaching Math Word Problems"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"6681f0a0cc6ddaa70cdea109b941c47538caaa27","title":"Judicial knowledge-enhanced magnitude-aware reasoning for numerical legal judgment prediction"},{"paperId":"538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct"},{"paperId":"456de7821c97806d3d74559cb51d19e692883b79","title":"Self-consistent Reasoning For Solving Math Word Problems"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"b23a8493f384a52adf22d3c70c5827fd1a6ca42d","title":"Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem"},{"paperId":"3fa70115248377c3d1517c9f978791a296fbc1dd","title":"Large Language Models Can Self-Improve"},{"paperId":"6157e46e5be13225c86c5918bada3930565a97dc","title":"ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler"},{"paperId":"2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","title":"Mind's Eye: Grounded Language Model Reasoning through Simulation"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"2fe6060ced80c1c245a718e6188b6516207bf0a8","title":"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions"},{"paperId":"f557f3a32d309373e7d31bb93ca1b80b4a6e39e7","title":"Symbolic Math Reasoning with Language Models"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"e7f63186d9aa212ceb1b65f8d78d646b92960879","title":"Improving Compositional Generalization in Math Word Problem Solving"},{"paperId":"74cc3d340039c67bdabaef090d1386fe2c5376ca","title":"CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning"},{"paperId":"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","title":"Language models show human-like content effects on reasoning"},{"paperId":"2a0c444d14c4758ec37c32b20551f6048541f035","title":"Expression Syntax Information Bottleneck for Math Word Problems"},{"paperId":"e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc","title":"PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change"},{"paperId":"e826ac71dad8c4ce36d82fb7add43e3d306bb7e1","title":"Making Language Models Better Reasoners with Step-Aware Verifier"},{"paperId":"3ce8c07349d91bb3f022a211be36e98eef0e1046","title":"MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems"},{"paperId":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers"},{"paperId":"99752e255a866484291866a5ff5cf94e96d6bdc4","title":"Is a Question Decomposition Unit All We Need?"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e","title":"On the Paradox of Learning to Reason from Data"},{"paperId":"a4901628fc09ab1edbeaee7ebe771f442c41d006","title":"Tackling Math Word Problems with Fine-to-Coarse Abstracting and Reasoning"},{"paperId":"4feb4f49c5837cbb1c9c8e61d4e8056cafb102e6","title":"Unbiased Math Word Problems Benchmark for Mitigating Solving Bias"},{"paperId":"a560cb91061edac97053c9485f44eed176c1ed99","title":"LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning"},{"paperId":"b21670e8061a06ab97e7d6052c9345a326e84ff8","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"0232a71bd47b6410dea4b00559acb93755f65fff","title":"Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers"},{"paperId":"c9f48406851954cb098911eccb4124ea5f966675","title":"A Survey on Multi-hop Question Answering and Generation"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"a376e6b118a10a8cb8a2920f6b83a70f087579f5","title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"03488f1a193066b5ea8b9b800e119f07df5c1d9e","title":"Reasoning Like Program Executors"},{"paperId":"2ff977905770df1bb75b69cbbeb8e242c5ac46b7","title":"Improving a Graph-to-Tree Model for Solving Math Word Problems"},{"paperId":"64fef156c226b9fc812d80eb119fb074cf575279","title":"Evaluating State-of-the-Art Visual Question Answering Models Ability to Answer Complex Counting Questions"},{"paperId":"f729e16ee6765e2fd76772cbf9d3dd17b4d25438","title":"Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems"},{"paperId":"39df1a17da84f02bfcb8751de8965b798653a5ee","title":"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems"},{"paperId":"927efd299cffcfca3716efefcc904331b70c153e","title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset"},{"paperId":"68900bf4b9cc820f4b47608871eafcac65a33917","title":"Adversarial Examples for Evaluating Math Word Problem Solvers"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"668701cdb6f4f5a079ad60173885c172dde20f17","title":"Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving"},{"paperId":"c0f407e76b02da687524484435482b869b7f947c","title":"Evaluating and Enhancing the Robustness of Math Word Problem Solvers"},{"paperId":"f71234e3405d668b26d070d83274530a26caa2c6","title":"Toward Automatic Tutoring of Math Word Problems in Intelligent Tutoring Systems"},{"paperId":"5acb662fe3ee67a24f90ed0c5e1b33eb518ca533","title":"SpokenWOZ: A Large-Scale Speech-Text Dataset for Spoken Task-Oriented Dialogue in Multiple Domains"},{"paperId":"751a94c3f6ce75858c2023f0fdffa3572d535026","title":"Improving Mathematics Tutoring With A Code Scratchpad"},{"paperId":"0e06854a464d552007734b5d7672afa600edbedd","title":"B ACKWARD R EASONING IN L ARGE L ANGUAGE M OD - ELS FOR V ERIFICATION"},{"paperId":"a0eb6078463a8fd524bfa092429a6ab595845ffa","title":"It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations"},{"paperId":"cd059ebe0e3fa86062d50cb063c9c10d39f8f336","title":"Integrating Heterogeneous Graphs Using Graph Transformer Encoder for Solving Math Word Problems"},{"paperId":"cd6e3ede9f799db018897473e69cad9b02229940","title":"It Takes One to Tango but More Make Trouble? The Number of Demonstrations Needed for In-Context Learning"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"56e403ba5941cf4053a52fb752426e4a9b0255fd","title":"Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Dataset Augmented by ChatGPT"},{"paperId":"c059d251d8f0c2491892271eb40ea3cec4aea830","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue in Multiple Domains"},{"paperId":"520711a1e93e6c4221f2a7c97c27a508379e8e37","title":"Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts"},{"paperId":"322c9e4ef9dc7724935ba818c0ff38d2c3d11483","title":"Bridging the Resource Gap: Exploring the Efficacy of English and Multilingual LLMs for Swedish"},{"paperId":null,"title":"MIND’S EYE: GROUNDED LANGUAGE MODEL REA-"},{"paperId":"5b2d98622d793afacc6113028e4586d8ae0adfb2","title":"Evaluating the Robustness of Deep learning Models for NLP"},{"paperId":"9c46ade7da15032256b6cfe137ead958c416a14d","title":"DESCRIPT : Audio / Video ummarizer"},{"paperId":"064422a3713b96acc173d6bbcdfc6b6b15e3f5b7","title":": Leveraging Collective Human Intelligence to Study Large Language Models"},{"paperId":"1eac17ac733321c656b99de959c4e7a406ed8bbf","title":"Visual Information and Large Language Models: A Deeper Analysis"},{"paperId":"cb4f6823e25896d5919046696066d0d498cc4397","title":"Forward-Backward Reasoning in Large Language Models for Verification"},{"paperId":"2a4b854eb06b8c5a74eaf7494ad87322ba0f6c69","title":"Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment"},{"paperId":"9c6cecf409ca6257e717291b47c2cf8b75cf4a58","title":"Learning Multi-step Reasoning from Arithmetic Task"},{"paperId":"20f92345229a7155d851bbccc65ad21942ee7640","title":"B RIDGING THE G AP B ETWEEN F OUNDATION M ODELS AND H ETEROGENEOUS F EDERATED L EARNING"},{"paperId":"53c070ba3b02d01d80c4cead506580ecbdbc8442","title":"Analyzing ChatGPT’s Mathematical Deficiencies: Insights and Contributions"},{"paperId":"661ef7301c3c399130d3d8673098dd27f5696130","title":"Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs"},{"paperId":"760561c57f68044e2f1d089088df1da6c627b09a","title":"On the Advance of Making Language Models Better Reasoners"},{"paperId":"909003934f2e68527c5638fae1b60821eaf438f1","title":"HAWP: a Dataset for Hindi Arithmetic Word Problem Solving"},{"paperId":"f02e8f1c9b5ab12ddfb1977570f9f5445a99a973","title":"Large Language Models are reasoners with Self-Verification"},{"paperId":"4aca69be58a271b1be45ec7ebb3586569cec50b0","title":"ArMATH: a Dataset for Solving Arabic Math Word Problems"},{"paperId":"8d282bf295d655e38b63ddbc7cdc94b188df8bb2","title":"G ENERATING S EQUENCES BY L EARNING TO [S ELF -]C ORRECT"},{"paperId":"11eff7418c2e4171467aa48899d6cd3771c1e6e3","title":"Noun-MWP: Math Word Problems Meet Noun Answers"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":"f645347e06b430d32f916a3c673acfd348e23058","title":"Towards Interpretable Math Word Problem Solving with Grounded Linguistic Logic Reasoning"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"15e2520e7f3bd96ba9d9d6e967771b34e448a9f1","title":"Compositional Generalization and Neuro-Symbolic Architectures"},{"paperId":"3eae64c02e448e656305dde5d496f30db423683d","title":"MATHion: Solving Math Word Problems with Logically Consistent Problems"},{"paperId":"37cc867e3893f407b572113705ecd0f6efc8b9fc","title":"Improving Equation Set Problems with Label Augmentation"},{"paperId":"4befd752d21a6231a9d930b1946177bd4cba30cb","title":"Mathematical Word Problem Solution Evaluation via Data Preprocessing Approach"},{"paperId":null,"title":"Later datasets increase in complexity and scale , incorporating reading comprehension"},{"paperId":"a63c6e227a655573c27a01a7cf9477e052cf5eb7","title":"Competition-Level Problems Are Effective Evaluators of LLMs"}],"references":[{"paperId":"f9c07ed1d2113c858b38861790af6a26310b8465","title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"},{"paperId":"91bad6519095404998f4ce23592547b409cdb60a","title":"Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"33ec7eb2168e37e3007d1059aa96b9a63254b4da","title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"},{"paperId":"d0cda85c030711aaa5383c80d5928a4d22f8d3bf","title":"How Can We Accelerate Progress Towards Human-like Linguistic Generalization?"},{"paperId":"35e6783307f82d1faa39be0653431305abec7271","title":"Evaluating Models’ Local Decision Boundaries via Contrast Sets"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"ce177672b00ddf46e4906157a7e997ca9338b8b9","title":"Attention is not not Explanation"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6","title":"Hypothesis Only Baselines in Natural Language Inference"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"c20584e878efbc4d716710b9aa2f4954c4014638","title":"Learning Fine-Grained Expressions to Solve Math Word Problems"},{"paperId":"7220d7ec31f6dc6ad7030f601743b5392513a2d9","title":"Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks"},{"paperId":"624527d8b92bc6d423784113ded0b9fd639add00","title":"Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":null,"title":"Hyperparameters Scratch RoBERTa Scratch RoBERTa Scratch RoBERTa Scratch RoBERTa Embedding Size"},{"paperId":null,"title":"Apply the Invert Operation variation on the Base Example and on all the variations obtained so far"},{"paperId":null,"title":"Apply the Add relevant information variation on the Base Example . Then considering these variations as Base Examples , apply the Question Sensitivity variations"}],"id":"13c4e5a6122f3fa2663f63e49537091da6532f35","summary":"It is shown that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs, and models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy."},{"url":"https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":71,"citationCount":319,"influentialCitationCount":20,"publicationDate":"05/03/2021","authors":"Dan Hendrycks,Collin Burns,Saurav Kadavath,Akul Arora,Steven Basart,Eric Tang,D. Song,J. Steinhardt","citations":[{"paperId":"c3d1832ed0444f75d44116fabbdda891aebc4b01","title":"LLaMA Pro: Progressive LLaMA with Block Expansion"},{"paperId":"4bebe389dfa85423e5cc089edf20b2c3f572f38c","title":"Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives"},{"paperId":"efc5e94635a850ede9c1f8dbce65d5dc536f3bfb","title":"Understanding LLMs: A Comprehensive Overview from Training to Inference"},{"paperId":"fb5e1b25480e759e2f310aed6ff431de1ec23135","title":"Economics Arena for Large Language Models"},{"paperId":"fed7a1a2fc92b6d17b73d9f5d28a19ba87abaec0","title":"A&B == B&A: Triggering Logical Reasoning Failures in Large Language Models"},{"paperId":"7843287ffcd48bdf830f16465741a548c2ba8d66","title":"Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math"},{"paperId":"96372dd90192ca2f5947115a25482604499da476","title":"Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs"},{"paperId":"ab7d320cbae173aef86c31faa087780cba44551f","title":"SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling"},{"paperId":"3783c7c074513fcee62c851b2abb2e264c2cbde3","title":"YAYI 2: Multilingual Open-Source Large Language Models"},{"paperId":"a2d069eb8c772e51292d52e297a464a5eb51979f","title":"NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes"},{"paperId":"3df9189a464d7aa4550580af04dfc99ecc88e1a7","title":"Assessing the Impact of Prompting, Persona, and Chain of Thought Methods on ChatGPT's Arithmetic Capabilities"},{"paperId":"9529a6b2032bd08ea4e34954b7a7d2b8b5d5f66b","title":"Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data"},{"paperId":"608a2b333fd8262e8c918f36c5700bafd3ea3cdd","title":"GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning"},{"paperId":"874200f7c5062f32ccefc5e37796bd8cae6b8972","title":"Evaluating Language-Model Agents on Realistic Autonomous Tasks"},{"paperId":"18e20944d1d64e73fc40321f65c3ddd0ef6a7aca","title":"Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models"},{"paperId":"4318e4ab5e2f5e2a50469aa043fe66c0744370a4","title":"Catwalk: A Unified Language Model Evaluation Framework for Many Datasets"},{"paperId":"6b97aa78bcdb88548c44e7e1671c0ed37ed37976","title":"Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"},{"paperId":"7017c58e19f4db0c38040935cc9fb7b7090a466d","title":"Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent"},{"paperId":"82730762d438b58a8af874d03e9903a73e18a39e","title":"TinyGSM: achieving>80% on GSM8k with small language models"},{"paperId":"4ba57555bef02f988f2ed3bab2f102733dc55221","title":"Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations"},{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"cb39d6f1a2a4f6f3b8e942b04165a89c88b32ebf","title":"AI capabilities can be significantly improved without expensive retraining"},{"paperId":"f80c5c210574d5ed428cd31809baac362d7fea21","title":"ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity"},{"paperId":"6933570be05269a2ccf437fbcca860856ed93659","title":"EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models"},{"paperId":"48362b169a235ca650918c489c8cea4c597da645","title":"Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"},{"paperId":"9ad7b23972fa4969bf5cee050d501b7f3dc7f4d1","title":"Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning"},{"paperId":"f30b720e34d405f200270a6ef2d09e98585fb4d1","title":"CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models"},{"paperId":"36f71673d9337b432babc51da77ef38b2070b5ed","title":"An LLM Compiler for Parallel Function Calling"},{"paperId":"a987c21afbfb3bd746d114e248202c074b1c40ca","title":"Large Language Models for Mathematicians"},{"paperId":"d4346af837aa6c2bb4a341cfe9bd91862ea5910a","title":"Large Knowledge Model: Perspectives and Challenges"},{"paperId":"9d7945c07643fe7f96e2389c8b39846eafdcacb5","title":"Inherent limitations of LLMs regarding spatial information"},{"paperId":"4d4ebfc94cbbc6a8d8a27a634ebd2db973b88178","title":"ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions"},{"paperId":"0b760700b24736d67f3ec81ad64bafef358c789b","title":"Hashmarks: Privacy-Preserving Benchmarks for High-Stakes AI Evaluation"},{"paperId":"1e672bf4d38a93c4c140ee208216425444368fa6","title":"LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models"},{"paperId":"5f66d1a667eec13b5d337c3fc5619bcef95092bd","title":"Universal Self-Consistency for Large Language Model Generation"},{"paperId":"dab4f70d75a04e62553e583f2450d9bb1f0ead46","title":"CLOMO: Counterfactual Logical Modification with Large Language Models"},{"paperId":"936f7f0fa77efcd322805b93a8d74c48a4108290","title":"ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?"},{"paperId":"44ffd3cd333bb41b900926556edb1c452759c398","title":"WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models"},{"paperId":"7421e62ce80274458f5ebfcc8714530cb1827469","title":"YUAN 2.0: A Large Language Model with Localized Filtering-based Attention"},{"paperId":"ea9af64a79ffe964e43121e8f78883c146894910","title":"LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms"},{"paperId":"be130fe97c15048dd91cad438894fbed5a05365a","title":"Meta Prompting for AGI Systems"},{"paperId":"6fa0677731184444df0e1fc8070938419cd6da47","title":"Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"},{"paperId":"d57ebdb4b5ed89ab4135c72f0c91dc1c4b347c4b","title":"KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains"},{"paperId":"eee5a8e06c32112d268e88a8d4c45592d7214244","title":"DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data"},{"paperId":"bf11f01929afed0ad3ccfe1b5e0fd34d90ef2b4f","title":"Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"},{"paperId":"248c9663001cddba588709ac5fb67f2a549c01a0","title":"When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks"},{"paperId":"73bf4db44cc77ad2440d4aa429ffe010cc4f4805","title":"Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets"},{"paperId":"a1cbe3e24cdcb49541ef5da4f82d9ebc45bb6261","title":"StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving"},{"paperId":"ef72f4aae225529393264c2b2594e876bec3fd04","title":"Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains"},{"paperId":"526c02aa9f649da627f1631687c8b37a2a05868d","title":"Feature emergence via margin maximization: case studies in algebraic tasks"},{"paperId":"622b064bd56d5be022f6dae9f7656fa8b658e0cf","title":"From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models"},{"paperId":"df15b83986207d884a811ce572dcedb6654abc6f","title":"BizBench: A Quantitative Reasoning Benchmark for Business and Finance"},{"paperId":"6b631c1a7f24f8500e714ef3d89963ac62aca64b","title":"Let's Reinforce Step by Step"},{"paperId":"b6667ba4f586489f12587446c6daaa3f09cfc539","title":"Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset"},{"paperId":"227b5f8206b64858edeef6723b96af14133077e3","title":"Rethinking Benchmark and Contamination for Language Models with Rephrased Samples"},{"paperId":"490680ecfd920daed8147401e77c1ba074b7c8fb","title":"Ziya2: Data-centric Learning is All LLMs Need"},{"paperId":"c0230760f644f6b7538d93e4296a5e9aa7028e45","title":"Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch"},{"paperId":"027cd21f94a9c4bb7842c857e4641df463a55aa7","title":"Perturbation-based Active Learning for Question Answering"},{"paperId":"4411e6b32865933cab87696c2738cb7a204e4240","title":"Implicit Chain of Thought Reasoning via Knowledge Distillation"},{"paperId":"98b607e7cb84e1a5c87c8a49562ae35435e6722d","title":"Learning From Mistakes Makes LLM Better Reasoner"},{"paperId":"cd2f4aaf98bb1e020cff310000c8049d3460c54e","title":"NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark"},{"paperId":"4a830a6cba4ec8c87c10348955b6bb633f401c0b","title":"math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories"},{"paperId":"f176d0d466d7c778a6435fe9a8d7e49508cb9059","title":"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts"},{"paperId":"9015c9813b6ef8f05040a7b1340909c57d600e38","title":"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models"},{"paperId":"f9465e71697cae802d66a66eb307f0a809773cd3","title":"Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning"},{"paperId":"e6423c211fea2945aa71e1ac5ea24f8f595b4b0a","title":"Towards Understanding Sycophancy in Language Models"},{"paperId":"aac8cdd40b2bfd1b967f0e5ea6c01e93385169e7","title":"SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving"},{"paperId":"a8341286f6237feb8e29d3463938685a572c38ff","title":"Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education"},{"paperId":"66d927fdb6c2774131960c75275546fd5ee3dd72","title":"EvalCrafter: Benchmarking and Evaluating Large Video Generation Models"},{"paperId":"0f92d5a01baa449edc5592716dd639ec7868c44f","title":"Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations"},{"paperId":"44b506d9619b5f957dc2b5588801138f343c0308","title":"Let's reward step by step: Step-Level reward model as the Navigators for Reasoning"},{"paperId":"8868a6d452b06bf4ad33237d0f3952d895ca20e7","title":"Improving Large Language Model Fine-tuning for Solving Math Problems"},{"paperId":"b16c7d45183b9d595ab64301be019741b1528860","title":"Llemma: An Open Language Model For Mathematics"},{"paperId":"a419e1d74cfc2b5ff400963476bda5c6ae66e172","title":"TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models"},{"paperId":"4ea9ee6ff16e4c7da58d10f8a2322e6a5aaccdf5","title":"Autonomous Tree-search Ability of Large Language Models"},{"paperId":"3b918b15178bcc84fd22af5094fe1efbcd388e72","title":"Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration"},{"paperId":"c170d4c6c0f9a0d0f78d09dc78eaed51d8eb5376","title":"Exploration with Principles for Diverse AI Supervision"},{"paperId":"fef1919d3a55652004fd012e052fe5b8f726f4e9","title":"A New Approach Towards Autoformalization"},{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"},{"paperId":"e92d6b93371c5dc6f3f6fb917f925d6c2fae5492","title":"OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text"},{"paperId":"8147cec9245d34d13732a08e915c920a1a499bb5","title":"Lemur: Harmonizing Natural Language and Code for Language Agents"},{"paperId":"db633c6b1c286c0386f0078d8a2e6224e03a6227","title":"Mistral 7B"},{"paperId":"e93562137240873bf1262e769dd9d73c2dcba858","title":"Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization"},{"paperId":"24df244bf7a6e8c93c5f183d3f62d39c0f773c68","title":"SALMON: Self-Alignment with Principle-Following Reward Models"},{"paperId":"b29134737a0c81c13d31fc0263b3c4d4f05ccb78","title":"Guiding Language Model Reasoning with Planning Tokens"},{"paperId":"5088a04d1a9f42b967f3dcf791145e8aa367fc54","title":"How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"},{"paperId":"8c9b8ba4a44b9736ea9db94f11c3227d5bb91a09","title":"Do Large Language Models Know about Facts?"},{"paperId":"6f39442852656f8a9decc95854a2ed461b3a83ab","title":"Ada-Instruct: Adapting Instruction Generators for Complex Reasoning"},{"paperId":"603079ee89aafb0b0466c4089ef1edf678e47871","title":"Language Model-Based Player Goal Recognition in Open World Digital Games"},{"paperId":"9283b8c7e6ad6ae86be059a26595de0d7a427a10","title":"Talk like a Graph: Encoding Graphs for Large Language Models"},{"paperId":"a1071b30237b61c97aa86bdabdce211aaf4ca901","title":"Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models"},{"paperId":"cddb552f6c3464a54a02b0b64b2d1af56c086606","title":"MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning"},{"paperId":"9fcdbfdf28245010c875ce85502351fe05c04b49","title":"Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View"},{"paperId":"b97074e2f1407b349c0abbb8c689a23c02d1924d","title":"Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance"},{"paperId":"98ce7af921e7c52d81df64d632d34eb09522cd75","title":"Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization"},{"paperId":"518c3eae36d00ef612a623f335a595e8577655aa","title":"Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions"},{"paperId":"d4bf36cbc5855ea87235d7a64f406717ac6aa3c9","title":"Large Language Models as Analogical Reasoners"},{"paperId":"368fb35a07076eba01c2e4700499323cd4524513","title":"RA-DIT: Retrieval-Augmented Dual Instruction Tuning"},{"paperId":"4c8cc2383cec93bd9ea0758692f01b98a035215b","title":"UltraFeedback: Boosting Language Models with High-quality Feedback"},{"paperId":"20ae101289965d36dbd93e9b8c47ec9deab03ed0","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models"},{"paperId":"837a3c0417fb677d4f22c346b345a450ec417f2c","title":"FELM: Benchmarking Factuality Evaluation of Large Language Models"},{"paperId":"f8b5ee53c3410f20049e7def47bd52403fa388e3","title":"LEGO-Prover: Neural Theorem Proving with Growing Libraries"},{"paperId":"a1426b13b74dbad17b34606d25aabe1d61f6e11a","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets"},{"paperId":"b272513916b45c8517d289d7abee4a53e6832187","title":"ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"},{"paperId":"532430bfcedf0ca4d5ca695967b52fc21cb5b778","title":"GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond"},{"paperId":"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","title":"Qwen Technical Report"},{"paperId":"5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0","title":"Effective Long-Context Scaling of Foundation Models"},{"paperId":"b2608b0ab4207e1335df98853a23a68234233c1a","title":"Learning the Efficient Frontier"},{"paperId":"8d806a91e5f2166ee6823eb7e6e8e56826b6776d","title":"NLPBench: Evaluating Large Language Models on Solving NLP Problems"},{"paperId":"11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"},{"paperId":"1ddd474f8a7d596b7609d705f960d2d924ef8e00","title":"Lyra: Orchestrating Dual Correction in Automated Theorem Proving"},{"paperId":"0f45608ddc01b3e192f3490330f4c4b8de074f79","title":"Are Human-generated Demonstrations Necessary for In-context Learning?"},{"paperId":"77b1f1c6d1658d120456b9046667cf009ceb39ce","title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"},{"paperId":"d651380f8c99f2522ead2d86d60cb4af4413abfa","title":"How understanding large language models can inform the use of ChatGPT in physics education"},{"paperId":"20cb4e0bd8871d33d82fc72ea82a0aa1dd922810","title":"Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI"},{"paperId":"cf237f3a6ed3e8fd970c15bf1f0bdf94f34da4a9","title":"LPML: LLM-Prompting Markup Language for Mathematical Reasoning"},{"paperId":"2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e","title":"Design of Chain-of-Thought in Math Problem Solving"},{"paperId":"12b233752c7097ea6525622bed238ae2d2193c5a","title":"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback"},{"paperId":"c96297261467b5daa2d01227496a70d444602434","title":"Baichuan 2: Open Large-scale Language Models"},{"paperId":"e716e6e0b3dd5124268780dc9bed521a07f371b8","title":"Contrastive Decoding Improves Reasoning in Large Language Models"},{"paperId":"62b4e06f5249d22e4a153ec4a2dc934c6a014372","title":"OWL: A Large Language Model for IT Operations"},{"paperId":"e892a225c417fbac7545c3e31b45d1c42dc9c933","title":"Chain-of-Thought Reasoning is a Policy Improvement Operator"},{"paperId":"a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","title":"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"},{"paperId":"ef9a3b97f9743596237092f7a43304faa085b7dd","title":"Analysing Cross-Lingual Transfer in Low-Resourced African Named Entity Recognition"},{"paperId":"217ea09e54a80043ec06d7f5ec1e7f2f7eef5b43","title":"FIMO: A Challenge Formal Dataset for Automated Theorem Proving"},{"paperId":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator"},{"paperId":"040fcb3a7da3fb570b25e909f7ba4bc31593579f","title":"AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models"},{"paperId":"6d42d9c5e8e46d5f45201bd6fb95ecc5b0287839","title":"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks"},{"paperId":"537335d9aad0ddbaef93e7f88b0db096671ef6ec","title":"No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function"},{"paperId":"412fe1f135cb20c952962133ca1e534a71bfd27f","title":"When Do Program-of-Thoughts Work for Reasoning?"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"f53a955ea1812fb0481504fdfd8febcb2a553a45","title":"SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research"},{"paperId":"3b88526a0f0337e3a6b632b4af8fd0882eb4b470","title":"FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models"},{"paperId":"6f28ca1e6c007c46cbc30aad531d800b8e6bc405","title":"GameEval: Evaluating LLMs on Conversational Games"},{"paperId":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"},{"paperId":"5df24ed6fdf10d1e92885687abce7bd5e56f3f85","title":"CMB: A Comprehensive Medical Benchmark in Chinese"},{"paperId":"9ea0757c750ab1222a7442d3485a74d1c526b04c","title":"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation"},{"paperId":"1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc","title":"Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification"},{"paperId":"96f6ad72733599db609332987ec6b65e30f11d07","title":"Platypus: Quick, Cheap, and Powerful Refinement of LLMs"},{"paperId":"dd3fb89d1201d46fa80b6a9519114599c01c11ac","title":"#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"},{"paperId":"6c4b033fce7e4a2ce80252d98b581c51290e9ea8","title":"Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems"},{"paperId":"507acddb0b7f36b83fd7c8bff2f121eb506ac8fb","title":"Cumulative Reasoning with Large Language Models"},{"paperId":"cfb7948c8a09d0a64afecceb7efe3362318dbe17","title":"SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore"},{"paperId":"a58c5fec83bfaa48aec8637a9b86b89fbdcf33f4","title":"Gentopia: A Collaborative Platform for Tool-Augmented LLMs"},{"paperId":"506ad0e58f5d6b99bbcc5e8efb519ae6fa34d307","title":"Guiding Mathematical Reasoning via Mastering Commonsense Formula Knowledge"},{"paperId":"447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e","title":"Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models"},{"paperId":"1e6102c981b9464c632ef0b00dbd11dfb0564e4e","title":"SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning"},{"paperId":"4b4ba6a02148c9d6f78e95d8e0d927104c3e91a7","title":"Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias"},{"paperId":"599c24276441b9255b523eec917231b8b49a620e","title":"Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers"},{"paperId":"98032f95e274db30570727fb7196c15e325fb35a","title":"Three Bricks to Consolidate Watermarks for Large Language Models"},{"paperId":"c29dbfbc17fa190b787a2662d49f08a38c8bd166","title":"ARB: Advanced Reasoning Benchmark for Large Language Models"},{"paperId":"4993258852711c4e3d0011325ac3db680eae84f4","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":null,"title":"Go Beyond The Obvious: Probing the gap of INFORMAL reasoning ability between Humanity and LLMs by Detective Reasoning Puzzle Benchmark"},{"paperId":"50f9f33b284b7363fbd9b9d2da4939b989a1c7cd","title":"Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models"},{"paperId":"888728745dbb769e29ed475d4f7661eebe1a71cf","title":"A Survey on Evaluation of Large Language Models"},{"paperId":"5efec343015f9329c5cd56e2259f68f03c2ef8b5","title":"CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?"},{"paperId":"87875a07976c26f82705de1fc70041169e5d652b","title":"LeanDojo: Theorem Proving with Retrieval-Augmented Language Models"},{"paperId":"58d225fc5c13db9328c573209b51a2a7bf1340c8","title":"A Dataset for Learning University STEM Courses at Scale and Generating Questions at a Human Level"},{"paperId":"fae57797d357bfa3b39b220336d1a2e8deba5318","title":"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving"},{"paperId":"2ed07f3574fde65772b625153e853d716c2a6e14","title":"CMMLU: Measuring massive multitask language understanding in Chinese"},{"paperId":"97d9d728f924c1f6cc085844136a481cac07c4b0","title":"The ADAIO System at the BEA-2023 Shared Task: Shared Task Generating AI Teacher Responses in Educational Dialogues"},{"paperId":"a4929de687f3c6937dabbf733258af635781d3c4","title":"StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code"},{"paperId":"8236010c2ecc94d826be6010ff187fdc000e7df6","title":"Deductive Verification of Chain-of-Thought Reasoning"},{"paperId":"71faef5970a36cf3b0a4ef02cb6a9ed616d0299b","title":"Argument and explanation"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","title":"An Empirical Study on Challenging Math Problem Solving with GPT-4"},{"paperId":"d8244cf970b9019cfb7f813275eba10d17c47166","title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks"},{"paperId":"be8db99310602d66bba64bcf41a572c45816fbfc","title":"Let's Verify Step by Step"},{"paperId":"2e01fdebbc780d3667ec3bf87a44927f0d9c188a","title":"Decision-Oriented Dialogue for Human-AI Collaboration"},{"paperId":"d3060876d9ad4e4e50e1c88a8c04186df00f24e2","title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets"},{"paperId":"ea75117f34b168a20f2a4309ac2eb685ca6b1436","title":"Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance"},{"paperId":"b8fe9d7b5762f7c9c3789cff2bdbe968ff0f0ed6","title":"Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving"},{"paperId":"69335077fcacbff7a7cf25697da1949e6bdfa968","title":"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models"},{"paperId":"2cf1f6c723006f258599fd9f000bb616ae83387a","title":"Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models"},{"paperId":"c79a04da127d6ff28c19c5dc35feb01feaf3d2b9","title":"Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems"},{"paperId":"95ca67ba607d7859ee8eec457f4b59b115d69bf5","title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models"},{"paperId":"d9685ec1d83988d5b3b26a5576b5aec5cc7713b8","title":"Out-of-Distribution Generalization in Text Classification: Past, Present, and Future"},{"paperId":"aafa168f9756f42c4ff707f6577cdd2eccc62b12","title":"RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning"},{"paperId":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models"},{"paperId":"9a9b1e2968302eb882870537d4af6e2c722dfd1a","title":"Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"1563cff13dfa53e571641d108f2fec6b2bf77767","title":"VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models"},{"paperId":"b17db2508600f498cf36d8ea06716e238bebe3d7","title":"LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4"},{"paperId":"c218cd1772999517b137bbbc9872c4f67e540b7f","title":"OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models"},{"paperId":"0b315e6d4d800e04caf2f587312ce163e748d10c","title":"Numeric Magnitude Comparison Effects in Large Language Models"},{"paperId":"b6d6c33298b852cf63edac233deca70530d69a2a","title":"PaLM 2 Technical Report"},{"paperId":"236c7dafea3df7ecffb5f18ec780d12f2f27d4b0","title":"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"},{"paperId":"785bb49af762efd64d841e52aa82c708341a7c43","title":"Code Execution with Pre-trained Language Models"},{"paperId":"61aa5bfa47bc500690640e5a6fec4cdc82328c14","title":"Algebra Error Classification with Large Language Models"},{"paperId":"9ccb2beaec722232a84e9a7682c72dcf7de667df","title":"Non-Autoregressive Math Word Problem Solver with Unified Tree Structure"},{"paperId":"62176de125738e3b95850d1227bac81fd646b78e","title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models"},{"paperId":"36877d3608fb391bad5a22fabd81c6669e721e69","title":"Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering"},{"paperId":"69bef4ab1018cc956a77c3ccdcaa57b124ab9fcc","title":"ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness"},{"paperId":"27c16cca907aa43397cc226a182b73b396c5cf66","title":"Inducing anxiety in large language models increases exploration and bias"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"5c0f3b0e46e6125bf2f454bcd8565a6f3430a54c","title":"Learning to Plan with Natural Language"},{"paperId":"261549439aebdda72b648ecc462448fd24857ac1","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"57100e39d0413ee585b381ba9ab366e8a6cf2866","title":"Solving Math Word Problems by Combining Language Models With Symbolic Solvers"},{"paperId":"68c834c19cd126bbd6d25a3572d7205cfed76271","title":"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"},{"paperId":"dca6c3927ade6481a1ae080f5c24decbfeced1be","title":"Boosted Prompt Ensembles for Large Language Models"},{"paperId":"033275ccc2c7c5c38592ae893da0b5923cf90717","title":"Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases"},{"paperId":"12d16f426edc6ab248fb476007bd1646282d4d68","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"fc3fe217de5d8de1a5814daf94de52e0d941cf21","title":"Mind meets machine: Unravelling GPT-4's cognitive psychology"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"0c8a0e7ccb853946cc0c1f077d76dde6109a2984","title":"Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference"},{"paperId":"9f8ac6ee3760ab202e492c733362e5bfc6763934","title":"Baldur: Whole-Proof Generation and Repair with Large Language Models"},{"paperId":"d8c59c5a8a620729b9d282111a8491d4b0320a9b","title":"Probing the psychology of AI models"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"34e97ba23d856b1ebf99b5080c3f04c0342a3954","title":"Safety without alignment"},{"paperId":"272afc28d03890160b1f2808cc551c962ea9138c","title":"ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics"},{"paperId":"0cf694b8f85ab2e11d45595de211a15cfbadcd22","title":"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks"},{"paperId":"d2170504c4ad9403bea118ae8debdfda95978546","title":"The Wisdom of Hindsight Makes Language Models Better Instruction Followers"},{"paperId":"cd0988714ea326642d2b1bb18753e187fec71e42","title":"A Categorical Archive of ChatGPT Failures"},{"paperId":"3f2cb353c7528efafb847309ab1e1e95245740a4","title":"Mathematical Capabilities of ChatGPT"},{"paperId":"10422b7bcf17eae9b04d4a020a591b3a61b45593","title":"Towards Autoformalization of Mathematics and Code Correctness: Experiments with Elementary Proofs"},{"paperId":"18fa8810f5c5f0e9d3785f21be1415897eea08c6","title":"Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers"},{"paperId":"e965e93e76a9e6c4e4863d145b5c007b540d575d","title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"},{"paperId":"7cbc7aa08b96de770d9ce5c90d01e75e9df2caee","title":"Language models are better than humans at next-token prediction"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"03fb95e6be583ca954c3d00812a9e9a40f118e51","title":"LAMBADA: Backward Chaining for Automated Reasoning in Natural Language"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"95c11cc5820ba32c60d5f2671f6567b9914a4978","title":"ALERT: Adapt Language Models to Reasoning Tasks"},{"paperId":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers"},{"paperId":"391246ce9c59d61c94cca3f8bef56c95542a4708","title":"ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning"},{"paperId":"7d5175db1b99552491063d2d9581b0b51e1d2932","title":"Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety"},{"paperId":"0e1f80a3f52c9051026656f00e31c0b9c1428d7a","title":"Can language models automate data wrangling?"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","title":"Galactica: A Large Language Model for Science"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","title":"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)"},{"paperId":"0cf5c61f367e3937dc07be634a43952a50b589a4","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"4634362d75606287955260ef1788171286efbeaa","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"711d5e8ddbb840ad31a9ffa3d38590603ba69a92","title":"Prompting GPT-3 To Be Reliable"},{"paperId":"e02dce6ee032a13b1653f69b034a35676e7d4dc2","title":"Can language representation models think in bets?"},{"paperId":"6d5555348f453bac901c5b57e8a4eeb3074b4071","title":"Learning to Reason With Relational Abstractions"},{"paperId":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment"},{"paperId":"2fe6060ced80c1c245a718e6188b6516207bf0a8","title":"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions"},{"paperId":"465dfa563e7d5ddef794f90ad3b0b19cf46d91a7","title":"Compute Like Humans: Interpretable Step-by-step Symbolic Computation with Deep Neural Network"},{"paperId":"53b3b15e8cbe2baf82cd0de3709e0a1e6f677415","title":"Limits of an AI program for solving college math problems"},{"paperId":"0a9df881784009cbb8efcd037d82ae222440aade","title":"An Interpretability Evaluation Benchmark for Pre-trained Language Models"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"041edc8b14bdd0e5627377956fd0e6c6c011146a","title":"Machine Learning Model Sizes and the Parameter Gap"},{"paperId":"2a85c44afeeeb336c5eafcd4001ccb033d3d1f1c","title":"Forecasting Future World Events with Neural Networks"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"e6ecdbbceff06cc1e667e3261596fd0fa6b32c4b","title":"How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild"},{"paperId":"23af448f60ddbad785402055e96366b65464fa8f","title":"Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation"},{"paperId":"d253beffd28d88cc3150c9e80511a6187ea6613b","title":"Unveiling Transformers with LEGO: a synthetic reasoning task"},{"paperId":"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","title":"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"},{"paperId":"c28e95a06dfcf13fc65a1cac83722f53e34f12a5","title":"Autoformalization with Large Language Models"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"354bf043179e3e9f05df73e3f04517e53c326d1f","title":"TALM: Tool Augmented Language Models"},{"paperId":"d072b46a0504ac023d5035d8ec0c7876151245c4","title":"Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models"},{"paperId":"c9f48406851954cb098911eccb4124ea5f966675","title":"A Survey on Multi-hop Question Answering and Generation"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"004d6ce718b0edb5b999f26710c5ae80b04bc900","title":"GPT-based Open-Ended Knowledge Tracing"},{"paperId":"04ff95e0edc3759fc5d18a1b929b3ccf79b032b2","title":"Deconstructing Distributions: A Pointwise Framework of Learning"},{"paperId":"916a06a6d51aa93de27aac2f3e14faed08dd6706","title":"Formal Mathematics Statement Curriculum Learning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"739715750d8eacbf04f16e36770089afad358cfa","title":"Towards More Robust Natural Language Understanding"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"f729e16ee6765e2fd76772cbf9d3dd17b4d25438","title":"Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"3ceca3cc39cd90515ccd73afe6539cc953762140","title":"How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI"},{"paperId":"21f455ca0bbf9a355611bc0593dd1cf8a8d32584","title":"The Computational Complexity of Finding Arithmetic Expressions With and Without Parentheses"},{"paperId":"320c1c6647a5b975c901347f71638c881888686b","title":"NQuAD: 70,000+ Questions for Machine Comprehension of the Numerals in Text"},{"paperId":"0cc771ca7859b395728c428b2a707007c4d69416","title":"Pretrained Language Models are Symbolic Mathematics Solvers too!"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"7ba98b00a224094c09676090f5d6d69498f5b299","title":"MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics"},{"paperId":"9140afbf83ac5fa07b3d4672e2329012812ca0fc","title":"Systematic human learning and generalization from a brief tutorial with explanatory feedback"},{"paperId":"d2824febb0c0c1fbfedc35e1a4d0532017603753","title":"Solving Machine Learning Problems"},{"paperId":"a2a97c944a5a987435cbe8249693b63a3a2c2745","title":"Effect of pre-training scale on intra- and inter-domain, full and few-shot transfer learning for natural and X-Ray chest images"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"4cc1fb128fa3abf6f90d567744767e8fd6315e1d","title":"NaturalProofs: Mathematical Theorem Proving in Natural Language"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"6a1b25f7a67395ad1e676027322913acbb0a0635","title":"CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"3457222ca38472a41c0c76346afbdf58bf68a28f","title":"Scalable and Explainable Automated Scoring for Open-Ended Constructed Response Math Word Problems"},{"paperId":"46b294941c397699fde0ee7e7fc441f6a755f671","title":"D RAFT , S KETCH , AND P ROVE : G UIDING F ORMAL T HEOREM P ROVERS WITH I NFORMAL P ROOFS"},{"paperId":"11a05b7e2bc79a9e0cc1d1b83ec978da8ee13bc9","title":"LogiQA 2.0—An Improved Dataset for Logical Reasoning in Natural Language Understanding"},{"paperId":"1a4c6856292b8c64d19a812a77f0aa6fd47cb96c","title":"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework"},{"paperId":"9c6cecf409ca6257e717291b47c2cf8b75cf4a58","title":"Learning Multi-step Reasoning from Arithmetic Task"},{"paperId":"6c0a3927005c8cde1e9df52e7aee686b59ca5bd0","title":"Beyond the Obvious: Evaluating the Reasoning Ability In Real-life Scenarios of Language Models on Life Scapes Reasoning Benchmark~(LSR-Benchmark)"},{"paperId":"1b04ac51ffeed0d31fbdbab0e4ed65ffe8c35df6","title":"AI Safety: Model Trojaning and Benchmarking"},{"paperId":"df9e675bc5ca7d57f5c218b12e672b3cb3ad44ac","title":"LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering"},{"paperId":"e8e3a5f23f336c64100960c45a28a730b72beec1","title":"Out-of-Distribution Generalization in Natural Language Processing: Past, Present, and Future"},{"paperId":"dc4da699b547a38e7f28ae6b39a779338b701131","title":"Structural information in mathematical formulas for exercise difficulty prediction: a comparison of NLP representations"},{"paperId":"c8e727c4e2bbdbdfbc77610215e8c9e9b09ce63b","title":"Transformer-Encoder and Decoder Models for Questions on Math"},{"paperId":"e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","title":"Solving Math Word Problems with Process-based and Outcome-based Feedback"},{"paperId":"d1bcb86bab0906bd631c4eb5ab81ce342b5927d0","title":"Open-Ended Knowledge Tracing"},{"paperId":"15789db234a9338e57aff9709868dcbd290727d3","title":"Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network"},{"paperId":"15e767aa26a14455da95a2b2f11e3d59f2c250f6","title":"Autoformalization for Neural Theorem Proving"},{"paperId":"cc8417aa578016203cb52efc63592bba64b08bb3","title":"FinMath: Injecting a Tree-structured Solver for Question Answering over Financial Reports"},{"paperId":"c27f3904490b8e5f9f39fe2b36722090c189e916","title":"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF"},{"paperId":"8eabf511ce56c56b99dd79ac3e9935624313eaac","title":"Analyzing the Effects of Annotator Gender across NLP Tasks"},{"paperId":"9529ef8c65a148ab0542ff3b7e9b81d73ac3d8b2","title":"Hybrid Tokenization and Datasets for Solving Mathematics and Science Problems Using Transformers"},{"paperId":"7b5aa186ca8abc585607c5ec91562e127a398601","title":"Open-ended Knowledge Tracing for Computer Science Education"},{"paperId":"8a4dce5735a101ff8f64c2b676afb8c24950a5d8","title":"Zero-shot Mathematical Problem Solving via Generative Pre-trained Transformers"},{"paperId":"bfda912457074fc7e1ccc10a250d4664e8efd2ec","title":"Analyzing Korean Math Word Problem Data Classification Difficulty Level Using the KoEPT Model"},{"paperId":"5eadc68b7a6a049eacd5449a8047405a3f6d3893","title":"Extracting Operator Trees from Model Embeddings"},{"paperId":"d9c05c32b7935dc8f7a048f79c2ce2f45558ddc8","title":"ProofNet: A Benchmark for Autoformalizing and Formally Proving Undergraduate-Level Mathematics Problems"},{"paperId":"f1362000a1561924a3a07d7b9ab3d8cc3fd4e96d","title":"Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"096cffc20d3bc89e8bc337607435a67e79d888d9","title":"Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images"},{"paperId":"4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc","title":"Learning Methods for Solving Astronomy Course Problems"},{"paperId":"cfe14dc505f4e3fd4574b9d1b4fafdc64583cc03","title":"Modern Approaches in Natural Language Processing"},{"paperId":"4befd752d21a6231a9d930b1946177bd4cba30cb","title":"Mathematical Word Problem Solution Evaluation via Data Preprocessing Approach"},{"paperId":"4bbafce8de5301222f236784dfa23217636963e4","title":"Overleaf Example"},{"paperId":"5b7fddf4a66a981249933be868e816127c5b3322","title":"TUNING CHATGPT MATHEMATICAL REASONING LIMITATIONS AND FAILURES WITH PROCESS SUPERVISION"},{"paperId":"e277bef6465198c2ea2786dbb832834cd9a0dfc3","title":"M ATH -S HEPHERD : A L ABEL -F REE S TEP - BY -S TEP V ERIFIER FOR LLM S IN M ATHEMATICAL R EASONING"}],"references":[{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"4383a975c09b72ba2f1a77cd779bb6965dbfb2fb","title":"Scaling Laws for Transfer"},{"paperId":"58c74cec28f3416b9a1d308bb2d6519d21d53ab0","title":"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"4e561318668f0ae190217ffe82bf44c9c33b9c0d","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","title":"Measuring Massive Multitask Language Understanding"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"da97bd6d2d0a2f11bb011b9925585e086010cff0","title":"A Promising Path Towards Autoformalization and General Artificial Intelligence"},{"paperId":"00974f862a2cbef54a87ca47413c90884bddd5c1","title":"LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"53ebd9e0ef2a6a088a5f977f331e434de256ad26","title":"Modelling High-Level Mathematical Reasoning in Mechanised Declarative Proofs"},{"paperId":"45bb43cdc35324fea4350ed335c500d4a5fd6ef5","title":"Mathematical Reasoning via Self-supervised Skip-tree Training"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"e02b0fc9255d58cae698121a5935bd9793c1883c","title":"MathZero, The Classification Problem, and Set-Theoretic Type Theory"},{"paperId":"bd49e66af9755e6138967eba6aeb37d8190d2b4f","title":"ExpBERT: Representation Engineering with Natural Language Explanations"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"36efa56d011bc3dc84c2499bd0dfcdcba55cfefb","title":"Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"9ef2e09a9e16e176e19c3fdc3b6ee22c5d3f3c97","title":"HOList: An Environment for Machine Learning of Higher-Order Theorem Proving (extended version)"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"87c425f23bcac2f082968abda64a971f91522f73","title":"GamePad: A Learning Environment for Theorem Proving"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"6ff2a434578ff2746b9283e45abf296887f48a2d","title":"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"8388f1be26329fa45e5807e968a641ce170ea078","title":"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"687684749814cbe54672d3b53ea08df6c66382bb","title":"Beyond the Turing Test"},{"paperId":"8e8ec502208f29ee9f78ded19226578e027ecd16","title":"Universal Intelligence: A Definition of Machine Intelligence"},{"paperId":null,"title":"Uniﬁedqa"},{"paperId":null,"title":"for symbolic"},{"paperId":null,"title":"2020) introduce BART, which has a bidirectional encoder and unidirectional decoder"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"395cc693f69e61bb5212f77015560570a97a1b20","title":"Solve it."},{"paperId":"3ac7cae59622f1f91833dbd0d9a8313f21b97614","title":"The Unreasonable Effectiveness of Mathematics in the Natural Sciences"},{"paperId":"6e51d8e92729366721de4b886ce3c26b124c9e58","title":"A Formal Definition of Intelligence Based on an Intensional Variant of Algorithmic Complexity"},{"paperId":"7b8faf606bab25f2205af0b881ab179cff2fb9d8","title":"George Pólya"},{"paperId":null,"title":"multiplying polynomials; multiplying polynomials 0.5; multiplying positive and negative fractions"},{"paperId":null,"title":"code for reproducing results"},{"paperId":null,"title":"Let S be the set of complex numbers of the form a + bi , where a and b are integers. We say that z ∈ S is a unit if there exists a w ∈ S such that zw = 1"},{"paperId":null,"title":"The length of a rectangle is 3 x + 10 feet and its width is x + 12 feet. If the perimeter of the rectangle is 76 feet, how many square feet are in the area of the rectangle?"},{"paperId":null,"title":"integration using completing the square; integration using long division; integration using trigonometric identities"},{"paperId":null,"title":"integration with partial fractions; intercepts from an equation; interpret quadratic models; interval of convergence; inverse of a 3x3 matrix; inverses of functions"},{"paperId":null,"title":"If n gives a remainder of 3 when divided by 7 , then what remainder does 2 n + 1 give when divided by 7 ? 16"},{"paperId":null,"title":"probability with permutations and combinations; problems involving definite integrals (algebraic)"},{"paperId":null,"title":"Khan Academy Modules (4/4): properties of exponents (rational exponents); proportion word problems; pythagorean identities; pythagorean theorem"},{"paperId":null,"title":"We have a triangle (cid:52) ABC where AC = 17 , BC = 15 , and AB = 8 . Let M be the midpoint of AB"},{"paperId":null,"title":"integration by parts; integration by parts: definite integrals"},{"paperId":null,"title":"multiplying rational numbers; multivariable chain rule; multivariable chain rule intro; negative exponents"},{"paperId":null,"title":"parametric curve arc length; parametric equations differentiation; parametric velocity and speed"},{"paperId":null,"title":"special right triangles; square and cube challenge; square roots of perfect squares"},{"paperId":null,"title":"Khan Academy Modules (3/4): integrate and differentiate power series"},{"paperId":null,"title":"A European train compartment has six seats"},{"paperId":null,"title":"hopital's rule: 0/0; l'hopital's rule: ∞/∞; lagrange error bound"},{"paperId":null,"title":"Let p ( x ) be a cubic polynomial such that p (2) = 0 , p ( − 1) = 0 , p (4) = 6 , and p (5) = 8"},{"paperId":null,"title":"Let H be the hyperbola with foci at ( ± 5 , 0) and vertices at ( ± 3 , 0) , and let C be the circle with center (0 , 0) and radius 4 . Given that H and C intersect at four points"},{"paperId":null,"title":"relative minima and maxima; remainder theorem; remainder theorem and factors"},{"paperId":null,"title":"systems of equations with substitution; systems of equations word problems; tangents to polar curves"},{"paperId":null,"title":"-digit divisors); divide with remainders (2-digit by 1-digit); dividing complex numbers"},{"paperId":null,"title":"number of solutions of quadratic equations; one step equations; one step equations with multiplication"},{"paperId":null,"title":"Find the remainder when 1 + 2 + 2 2 + 2 3 + · · · + 2 100 is divided by 7"}],"id":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","summary":"This work introduces MATH, a new dataset of 12, 500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations, and shows that accuracy remains relatively low, even with enormous Transformer models."},{"url":"https://www.semanticscholar.org/paper/d898662fb0519f00f5ccc87f06294fa7322715b4","title":"LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training","venue":"ArXiv","year":2022,"referenceCount":19,"citationCount":5,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Hongwei Han,Jialiang Xu,Mengyuan Zhou,Yijia Shao,Shi Han,Dongmei Zhang","citations":[{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"f1fff40fff36d546873705ee564f4d59b88bed59","title":"FATA-Trans: Field And Time-Aware Transformer for Sequential Tabular Data"},{"paperId":"f88e229de74d51a242060e3046633fe4612e318d","title":"Towards Foundation Models for Learning on Tabular Data"},{"paperId":"fce42753155280051ac64817404b4e1d3be6ebaa","title":"MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks"},{"paperId":"2ebfa7db6e0899041fff28b9f7fd212581d4f7e8","title":"One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data"}],"references":[{"paperId":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"1b055049c568be70d6a762679cdb93f630d5d6e6","title":"Tabular Transformers for Modeling Multivariate Time Series"},{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","title":"Pre-trained models for natural language processing: A survey"},{"paperId":"ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96","title":"TabFact: A Large-scale Dataset for Table-based Fact Verification"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"10ffbbcf23923ffef2b1ae78b516ec4329c78727","title":"Time2Vec: Learning a Vector Representation of Time"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"1f22f4ffcc6d7dce094e62a77d1a8866bf4ef4ee","title":"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"table cells contain at least 2 numbers."}],"id":"d898662fb0519f00f5ccc87f06294fa7322715b4","summary":"The LUNA framework is proposed which improves the numerical reasoning and calculation capabilities of transformer-based language models and bridges the gap between number and vocabulary embeddings with number pre-training and model distillation."},{"url":"https://www.semanticscholar.org/paper/d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning","venue":"ArXiv","year":2022,"referenceCount":233,"citationCount":48,"influentialCitationCount":1,"publicationDate":"20/12/2022","authors":"Pan Lu,Liang Qiu,Wenhao Yu,S. Welleck,Kai-Wei Chang","citations":[{"paperId":"608a2b333fd8262e8c918f36c5700bafd3ea3cdd","title":"GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning"},{"paperId":"7017c58e19f4db0c38040935cc9fb7b7090a466d","title":"Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent"},{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"e99de66608a3b060d54548b9e9a7c39961872cd7","title":"LANS: A Layout-Aware Neural Solver for Plane Geometry Problem"},{"paperId":"89ed7fd00319d45269906a9b05e10c8680bf9cec","title":"FinanceBench: A New Benchmark for Financial Question Answering"},{"paperId":"6fa0677731184444df0e1fc8070938419cd6da47","title":"Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"},{"paperId":"b6667ba4f586489f12587446c6daaa3f09cfc539","title":"Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"c22cc4e2eed78b4b31e50d94ea35da0405aabb87","title":"Multi-Operational Mathematical Derivations in Latent Space"},{"paperId":"f176d0d466d7c778a6435fe9a8d7e49508cb9059","title":"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts"},{"paperId":"b16c7d45183b9d595ab64301be019741b1528860","title":"Llemma: An Open Language Model For Mathematics"},{"paperId":"44b506d9619b5f957dc2b5588801138f343c0308","title":"Let's reward step by step: Step-Level reward model as the Navigators for Reasoning"},{"paperId":"b9e8b62bcc019f47a0a015568f70039b3b7c1196","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model"},{"paperId":"8db1dcae055842f43ccac04182957b20d15bbe6b","title":"Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems"},{"paperId":"287a30043ad1c3e349095af7e3e42d3be3b6c0c9","title":"SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training"},{"paperId":"5076bbbf831a92174c9cc1b347bd0584560435fc","title":"Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning"},{"paperId":"e61a96cf602ebff6683929aaf916e25614a475bc","title":"UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities"},{"paperId":"8d806a91e5f2166ee6823eb7e6e8e56826b6776d","title":"NLPBench: Evaluating Large Language Models on Solving NLP Problems"},{"paperId":"856c342606aca05434e48f2e53cdbd6f6b886802","title":"Pre‐trained language models: What do they know?"},{"paperId":"2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e","title":"Design of Chain-of-Thought in Math Problem Solving"},{"paperId":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners"},{"paperId":"537335d9aad0ddbaef93e7f88b0db096671ef6ec","title":"No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function"},{"paperId":"2dbe7fd41f23aa2a077c3d1e91b0b73f92e3d8d2","title":"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning"},{"paperId":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"},{"paperId":"692902404c282de936249aef68ce9f974815128c","title":"CLEVA: Chinese Language Models EVAluation Platform"},{"paperId":"ca4d1d9c61e6496bbb0f361b55413f265d81de6e","title":"Prompting Large Language Models for Malicious Webpage Detection"},{"paperId":"4993258852711c4e3d0011325ac3db680eae84f4","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"},{"paperId":"87875a07976c26f82705de1fc70041169e5d652b","title":"LeanDojo: Theorem Proving with Retrieval-Augmented Language Models"},{"paperId":"fae57797d357bfa3b39b220336d1a2e8deba5318","title":"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"7eb044170c11b7e2193b8df35f606edcfc7f2585","title":"Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"95ca67ba607d7859ee8eec457f4b59b115d69bf5","title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models"},{"paperId":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"7a6dc7071891cb3d658c93418801942a4c6ed373","title":"Autonomous GIS: the next-generation AI-powered GIS"},{"paperId":"0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","title":"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs"},{"paperId":"7e4f5589327b6b574cc950a03fd1d6236e9e6128","title":"Answering Questions by Meta-Reasoning over Multiple Chains of Thought"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"c61d54644e9aedcfc756e5d6fe4cc8b78c87755d","title":"A Survey of Large Language Models"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"5938abafd61881f6b23a2ba318d2d3d0327402c0","title":"A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram"},{"paperId":"3f2cb353c7528efafb847309ab1e1e95245740a4","title":"Mathematical Capabilities of ChatGPT"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"668701cdb6f4f5a079ad60173885c172dde20f17","title":"Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving"},{"paperId":"a3bbe282f1666900a6604991f96e1637c6946253","title":"MATHVISTA: EVALUATING MATHEMATICAL REASON-"},{"paperId":"bba47079d385bab0facf4527d5da35b7ff0a5c3d","title":"Basic Arithmetic Properties in the Space of Language Model Prompts"}],"references":[{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"72fce949725b20428e5f56247fef5c6bd1ce6154","title":"UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"75f7e9e2b59fb640ef9d1dff94097175daf46c4d","title":"Large Language Models Struggle to Learn Long-Tail Knowledge"},{"paperId":"538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct"},{"paperId":"4634362d75606287955260ef1788171286efbeaa","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"d96997265f8146e93b4c9350f19d55e46d1317f0","title":"ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering"},{"paperId":"07955e96cbd778d0ae2a68f09d073b866dd84c2a","title":"Decomposed Prompting: A Modular Approach for Solving Complex Tasks"},{"paperId":"c88cafa3e980765a64febe369ceb7c2aa7261d2a","title":"Complexity-Based Prompting for Multi-Step Reasoning"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"b2542a738b75ee9b7ce1a13d8b78f9095d212412","title":"Generate rather than Retrieve: Large Language Models are Strong Context Generators"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"d4f42cf69b91f08e86acc36b6f9f47bd1aac4a2a","title":"Insights into Pre-training via Simpler Synthetic Tasks"},{"paperId":"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","title":"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"},{"paperId":"3464bf8d2fa50e8b5ddd07cfb10790bbfe6dfa2f","title":"A Survey in Mathematical Language Processing"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"c28e95a06dfcf13fc65a1cac83722f53e34f12a5","title":"Autoformalization with Large Language Models"},{"paperId":"50b0c6ee2b3d53ba5af69d6c00b5d60888a9026f","title":"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"65b4b25272c50dc376f5c018338931bfd349e532","title":"HyperTree Proof Search for Neural Theorem Proving"},{"paperId":"c2d574f7c6a9e3bafe396ecb4ab639179d6fd92c","title":"Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"f08cf3957d6f4ca66296cfd361d30fea08bccf65","title":"PGDP5K: A Diagram Parsing Dataset for Plane Geometry Problems"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"a376e6b118a10a8cb8a2920f6b83a70f087579f5","title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"},{"paperId":"b2e8906d0c2999e3b6cbb5bfd7dc687f1a0a751c","title":"Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"916a06a6d51aa93de27aac2f3e14faed08dd6706","title":"Formal Mathematics Statement Curriculum Learning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"f9838a3be5c94bb2674a0e224de349b50e18f3c4","title":"Learning To Retrieve Prompts for In-Context Learning"},{"paperId":"3af37400f1f9a4f4f211c4a472e18963edc2b34f","title":"ValueNet: A New Dataset for Human Value Driven Dialogue System"},{"paperId":"ab15163d8c57fa5706d399c8fb62e489d57b22b7","title":"Injecting Numerical Reasoning Skills into Knowledge Base Question Answering Models"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"45ece6f3b0a319dba60c20b3013b5161dd49c58b","title":"Linear algebra with transformers"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"3ceca3cc39cd90515ccd73afe6539cc953762140","title":"How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI"},{"paperId":"9bcf3b43f2323a194036cc52c6878a9b1dc7e058","title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning"},{"paperId":"39df1a17da84f02bfcb8751de8965b798653a5ee","title":"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems"},{"paperId":"a7353543dd7cc2dc8396e8e8d2a1b60e9d38985c","title":"Improving Fractal Pre-training"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"927efd299cffcfca3716efefcc904331b70c153e","title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset"},{"paperId":"3d33c71af053b42c14ad8d476c9df9cf6dfc1e16","title":"Does Pretraining for Summarization Require Knowledge Transfer?"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"7ba98b00a224094c09676090f5d6d69498f5b299","title":"MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"102ebe229df18c8733ea1b8def56cd79996e2178","title":"A Survey of Human-in-the-loop for Machine Learning"},{"paperId":"f8e0f7babe7cd858aecfafb03375bf578fe161f7","title":"MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving"},{"paperId":"2406cf39805c70264c4226b7325a09b506c70921","title":"TAPEX: Table Pre-training via Learning a Neural SQL Executor"},{"paperId":"5cfdc75bcd204c94eb979c5b17fdb86d33b3c184","title":"Solving arithmetic word problems by scoring equations with recursive neural networks"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7654dbd372d8b65e730e3bd477ff9fec96c16dc5","title":"Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"291133a657498920451481d3bf784ebbafda8d6e","title":"GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning"},{"paperId":"4ae6b5ce971c58c1280bc971a1879e6d547c5f8c","title":"A Bottom-Up DAG Structure Extraction Model for Math Word Problems"},{"paperId":"680b0467861a70be41c31e4f2415fe5e2958fbc0","title":"HMS: A Hierarchical Solver with Dependency-Enhanced Understanding for Math Word Problem"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"fb1c90806fc5ec72987f58110aa255edbce6620d","title":"Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning"},{"paperId":"593499b654360101682edec1dd711fa7c09f6971","title":"IsarStep: a Benchmark for High-level Mathematical Reasoning"},{"paperId":"0adec918885dff698acf359988ed79a543157f80","title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"},{"paperId":"4cc1fb128fa3abf6f90d567744767e8fd6315e1d","title":"NaturalProofs: Mathematical Theorem Proving in Natural Language"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"3baaabdd47c466df1f728deb31ae509aacf289c4","title":"Towards Socially Intelligent Agents with Mental State Transition and Human Value"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"9231927bc0a9ed10de64cad05640587893eba4b1","title":"Proof Artifact Co-training for Theorem Proving with Language Models"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"cb596bffc5c5042c254058b62317a57fa156fea4","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"58c74cec28f3416b9a1d308bb2d6519d21d53ab0","title":"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"593ca71119fb6ee560926d9b304bde095267432d","title":"SMART: A Situation Model for Algebra Story Problems via Attributed Grammar"},{"paperId":"edaafec59b651d503ac7c8a86f8e2335273e1f7a","title":"Learning by Fixing: Solving Math Word Problems with Weak Supervision"},{"paperId":"53953a19e2fdf9c5ad5ff445c07ce36cc70f551b","title":"Solving Math Word Problems with Multi-Encoders and Multi-Decoders"},{"paperId":"24ed85ad966823868c1694a19385d01c6ad71008","title":"A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving"},{"paperId":"3938fe72ccfe4fe92387258874cb1cbe66194d4f","title":"Point to the Expression: Solving Algebraic Word Problems Using the Expression-Pointer Transformer Model"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"f9c07ed1d2113c858b38861790af6a26310b8465","title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"},{"paperId":"eb2f1480eca6195fe8dbab5de8bbac8749e9be58","title":"RODA: Reverse Operation Based Data Augmentation for Solving Math Word Problems"},{"paperId":"44507bde6e9caf60b41c60d703e7972b520d48a6","title":"Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems"},{"paperId":"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","title":"Measuring Massive Multitask Language Understanding"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"016863a86189c4e8ccecf9a36c4406c439a8a84c","title":"INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"33ba3001cdbc1c1f59fefe5368300245c05560c5","title":"Teacher-Student Networks with Multiple Decoders for Solving Math Word Problem"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"cdcdc7ab1f5b6e86146b5c0224cba7d8cd35142c","title":"What Does BERT with Vision Look At?"},{"paperId":"ab38bbb36ba38047c5bb556694d148225971957f","title":"Premise Selection in Natural Language Mathematical Texts"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"97f08c1ae8ca5ddf5948c66bfbbc0546ac154807","title":"Pretrained Transformers Improve Out-of-Distribution Robustness"},{"paperId":"750bf158dd5060dfac0c6e1734654060df1f6374","title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem"},{"paperId":"52cb05d721688cb766c6e282e9d55c3b8e3dc0cf","title":"TaPas: Weakly Supervised Table Parsing via Pre-training"},{"paperId":"8a5590978930070f50c5f9fbf61f67e5d95794f0","title":"Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"1180c1d2b2fad2527e98c0065319ea8150a3b888","title":"Natural Language Premise Selection: Finding Supporting Statements for Mathematical Text"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","title":"Pre-trained models for natural language processing: A survey"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"388e2fcdcefbe0834e153ab2a0be127092f9674d","title":"DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"9df88155d6b65e14e06f8dcd851d31b7753659b1","title":"The lean mathematical library"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"81b4920ad488affaee27389ff9540b7fea90a4ce","title":"“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding"},{"paperId":"007bc04c97f9f8bcec0487699e197315418f22e7","title":"From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the Aristo Project"},{"paperId":"20fa66cb21d8a1b7436306af4b0bedddd3bb470b","title":"Solving Math Word Problems with Double-Decoder Transformer"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"9ff3afa6577d5469cd68c4991476f426efdf6bca","title":"Template-Based Math Word Problem Solvers with Recursive Neural Networks"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"80c4185d5311841cbbcbb9f1d3555b61081ca5d8","title":"HOList: An Environment for Machine Learning of Higher Order Logic Theorem Proving"},{"paperId":"a596f03145285cd05a6ca57a4e25418b23b24976","title":"Learning to Prove Theorems via Interacting with Proof Assistants"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"6d6c91485aa0527195c256997be54c188f7640ad","title":"Survey on Mathematical Word Problem Solving Using Natural Language Processing"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"6ff68b34a5f78bdd14437fe5a79aebbc42c26467","title":"DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"e9b13731027418ed38103d1dfc8a70f6881bc684","title":"Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering"},{"paperId":"51004bc6461a572e1189a0e3b32b441155d760ce","title":"QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships"},{"paperId":"6605bba6e0caabda06b090d67698a5683eba4dfa","title":"Translating a Math Word Problem to an Expression Tree"},{"paperId":"f67ede490780f939103d4cc4e9c6866b83ee59b3","title":"Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"caeb950e503872a903e18a3b259424e3cc3c6006","title":"Neural Math Word Problem Solver with Reinforcement Learning"},{"paperId":"de95601d9e3b20ec51aa33e1f27b1880d2c44ef2","title":"CBAM: Convolutional Block Attention Module"},{"paperId":"87c425f23bcac2f082968abda64a971f91522f73","title":"GamePad: A Learning Environment for Theorem Proving"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"a396effc8987cc0938a4057d9d9008ff2a5452b6","title":"Data-Driven Methods for Solving Algebra Word Problems"},{"paperId":"835c6b524b90b1639aba28742f7161137ddf4397","title":"MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning"},{"paperId":"b1b88c6d1f05913be18750ff00e06edd5e1f49ef","title":"TacticToe: Learning to Prove with Tactics"},{"paperId":"7289a240c9425bc7cad87b3b835e5f0cac22f488","title":"DVQA: Understanding Data Visualizations via Question Answering"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"cb6be69c67b0b15ebbda89a126f4dd62a4d32958","title":"FigureQA: An Annotated Figure Dataset for Visual Reasoning"},{"paperId":"7cfa5c97164129ce3630511f639040d28db1d4b7","title":"FiLM: Visual Reasoning with a General Conditioning Layer"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"81f466a535cdec4957989999f9ca381bc4fe14e9","title":"From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems"},{"paperId":"c20584e878efbc4d716710b9aa2f4954c4014638","title":"Learning Fine-Grained Expressions to Solve Math Word Problems"},{"paperId":"7220d7ec31f6dc6ad7030f601743b5392513a2d9","title":"Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"38bdca75587ed41bb7e1e6673fe5118aa25efe89","title":"A Survey of Question Answering for Math and Science Problem"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"f1318d15d7d8ab626b92d0c70dbdc5b5d37e223f","title":"HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving"},{"paperId":"e77e00286a63a32dafb9629cd79f6a77bddb1941","title":"Deep Network Guided Proof Search"},{"paperId":"8b7336f5dd13a45d4aab38428b4a88ce507ea310","title":"Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving"},{"paperId":"3d3b4ec7789f634e0752d50484dad7d2ea2460d5","title":"Dialogue Learning With Human-In-The-Loop"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"ba84b8f16efc032945cd8ed960c121a9bcd064d5","title":"Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems"},{"paperId":"2ab3c53b08d4180c8642207ee09738f8df193b92","title":"Holophrasm: a neural Automated Theorem Prover for higher-order logic"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"f8909e8b359c72bc0e3257dadcc48a8787bca74b","title":"DeepMath - Deep Sequence Models for Premise Selection"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"0da2fc91eb560f4245d8e7b6dff9edd0a5727dff","title":"DRAW: A Challenging and Diverse Algebra Word Problem Set"},{"paperId":"6a9533730bc2070b222b056df9bf0ee16ba7a509","title":"Four Decades of Mizar"},{"paperId":"acfe603d8a90ee3d4187e426b26c4d7bf394b4c8","title":"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning"},{"paperId":"c87dccf7c21e67679389f23f86f039cd96720c3f","title":"Solving Geometry Problems: Combining Text and Diagram Interpretation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"2a441a46e228ed0ea2251a4e61be6c7025b45766","title":"The Lean Theorem Prover (System Description)"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"32de44f01a96d4473d21099d15e25bc2b9f08e2f","title":"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"9f9d16e6cf929c66be59fd56d9870196c19410c1","title":"Distributed Asynchronous Online Learning for Natural Language Processing"},{"paperId":"226966243877f186d346be01047cf71cee1b5ec4","title":"Online EM for Unsupervised Models"},{"paperId":"1bc38b2c99f58a9199771c124248eaad2ab82671","title":"MPTP 0.2: Design, Implementation, and Initial Experiments"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"8a61ce3920931123d08dce297350c50ecb292d66","title":"Automated generation of readable proofs with geometric invariants"},{"paperId":"7ee54cd7419f1f90912508fdbdc37fdd47dc0947","title":"Isabelle: A Generic Theorem Prover"},{"paperId":"38a5fc91e0366bbd5121154aeb498a1a83573dd5","title":"Basic principles of mechanical theorem proving in elementary geometries"},{"paperId":"b9124861e4e874bbc477b4b726adf94f7d2ecdc4","title":"Learning"},{"paperId":"9fc77941297522cc420ce9292193dd04ed2ed1af","title":"Natural Language Input for a Computer Problem Solving System"},{"paperId":"7b202896ca3151fabb4c4197c329b530491c2b3e","title":"Empirical explorations of the logic theory machine: a case study in heuristic"},{"paperId":"9b7856e0b9b3187085567212471eee412eef8131","title":"Empirical Expiorations of the Geometry Theorem Machine"},{"paperId":"ecb6cad427818163b27bff2241edd1c8a7eb5946","title":"An Augmented Benchmark Dataset for Geometric Question Answering through Dual Parallel Text Encoding"},{"paperId":"760561c57f68044e2f1d089088df1da6c627b09a","title":"On the Advance of Making Language Models Better Reasoners"},{"paperId":"52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"4aca69be58a271b1be45ec7ebb3586569cec50b0","title":"ArMATH: a Dataset for Solving Arabic Math Word Problems"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":null,"title":"2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales"},{"paperId":null,"title":"An emerging area of research aims to combine elements of informal and formal theorem proving"},{"paperId":null,"title":"language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever"},{"paperId":null,"title":"Enhancing selfconsistency and performance of pretrained language models with nli"},{"paperId":"ec15ff1fc5c9780fd91902f286a9e3fcd00b890d","title":"Math Word Problem Solving with Explicit Numerical Values"},{"paperId":"494a102164b790a63dc99385b0e56d1e80e0a93c","title":"An Edge-Enhanced Hierarchical Graph-to-Tree Network for Math Word Problem Solving"},{"paperId":"a2fff5686bdf8e25003097107516d6ac13bf8b8e","title":"Improving Math Word Problems with Pre-trained Knowledge and Hierarchical Reasoning"},{"paperId":"adc61e21eafecfbf6ebecc570f9f913659a2bfb2","title":"Deep Learning Based Text Classification: A Comprehensive Review"},{"paperId":null,"title":"2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond"},{"paperId":null,"title":"Lisa: Language models of isabelle proofs"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":null,"title":"2022b). To address this issue, new benchmarks are proposed from various aspects"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Meta-math: A Computer Language for Mathematical Proofs"},{"paperId":"998bcc21375a5639e1b65bb6eb42e8cf5c30907a","title":"Synthesis of Solutions for Shaded Area Geometry Problems"},{"paperId":"eb1179c1a8514fee677d9f23ab112367adef6321","title":"The coq proof assistant reference manual"},{"paperId":"162d958ff885f1462aeda91cd72582323fd6a1f4","title":"Gradient-based learning applied to document recognition"},{"paperId":"4a59af12923633416e5d79f4a85285f154375d26","title":"Isabelle"},{"paperId":"78fc3b741828124d8a79667466516a544143bf68","title":"Computers and Thought"},{"paperId":"a4eb8d6ee85af4d82ace0a1989f5af7baeecc747","title":"An Introduction to Java Geometry Expert"},{"paperId":null,"title":"Tianlong Ma, and Liang He. 2022a. A survey of human-in-the-loop for machine learning. Future Generation Computer Systems"},{"paperId":null,"title":"2022. Learning to understand plane geometry diagram"},{"paperId":null,"title":"2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)"},{"paperId":null,"title":"2022a. Holistic evaluation of language models"},{"paperId":null,"title":"2022. Estimating numbers without regression"}],"id":"d3a7a4543d83f568f79d1febe8379465ff0140c9","summary":"This survey paper reviews the key tasks, datasets, and methods at the intersec-tion of mathematical reasoning and deep learning over the past decade, and evaluates existing benchmarks and methods and discusses future research directions."},{"url":"https://www.semanticscholar.org/paper/37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":18,"citationCount":15,"influentialCitationCount":1,"publicationDate":"10/09/2021","authors":"K. Pal,Chitta Baral","citations":[{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"5dc15ac1c92ab7492f121471823fb13a95d273ba","title":"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis"},{"paperId":"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"da345b189e4faaaa489f7319640868a37a3932a1","title":"Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"d69521c1521bfa3fefb32b2106da38b1821e975d","title":"NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"87a31b729295a3357949683276a2625288fdd0f0","title":"PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance"},{"paperId":"9a3f1a51ab2b3816655e4c4f58a022421ea7b34b","title":"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"0605f95aadf5d47af1bcd53fc053fc2eb642b8fb","title":"Generating Academic Abstracts: Controlled Text Generation Using Metrics from a Target Text Allows for Transparency in the Absence of Specialized Knowledge"},{"paperId":"cd54a839ba12e7417e321b4407788dc426ebaefc","title":"Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering"}],"references":[{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"ad46feec0cefb78dafa60821598c052cf0b9b7bd","title":"NT5?! Training T5 to Perform Numerical Reasoning"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"}],"id":"37588705a2af7d5b24d901dd33ade1ff293aabdd","summary":"This work investigates the ability of text-to-text transfer learning model (T5), which has outperformed its predecessors in the conventional NLP tasks, to learn numeracy, and considers four numeracy tasks."},{"url":"https://www.semanticscholar.org/paper/c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals","venue":"Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out","year":2021,"referenceCount":23,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/06/2021","authors":"Taku Sakamoto,Akiko Aizawa","citations":[{"paperId":"c77feebb06b530079a34f04c64de359ae28c0a32","title":"Predicting Numerals in Text Using Nearest Neighbor Language Models"},{"paperId":"c97bcf9e605938c2bef54f12b2156c678f1a2373","title":"Introducing Semantic Information for Numerical Attribute Prediction over Knowledge Graphs"}],"references":[{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"f67fcbb1aec92ae293998ddfd904f61a31bef334","title":"Inducing Relational Knowledge from BERT"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"bd2895717effe3cc52e77fbee3da8117cf1c01e1","title":"Measuring Numerical Common Sense: Is A Word Embedding Approach Effective?"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"f98e135986414cccf29aec593d547c0656e4d82c","title":"Commonsense Knowledge Mining from Pretrained Models"},{"paperId":"b285c067f2da04bf5647beb8853bfddf6d9b4e1b","title":"A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"7e1eea0235e1902b6e3219fd9de9105e2f08fd65","title":"Numerically Grounded Language Models for Semantic Error Correction"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"93c20e38c85b69fc2d2eb314b3c1217913f7db11","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Evaluating commonsense in pretrained language models"},{"paperId":"f7d95e8a91d7683e74ee642f20906e7751e47dfe","title":"Association for Computational Linguistics"}],"id":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","summary":"This paper measures the NCS acquired by existing neural language models using a masked numeral prediction task as an evaluation task and proposes methods to reflect not only the symbolic aspect but also the quantitative aspect of numerals in the training of language models, using a loss function that depends on the magnitudes of the numerals and a regression model for the masked numal prediction task."},{"url":"https://www.semanticscholar.org/paper/ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":26,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Sungjin Park,Seung-kook Ryu,E. Choi","citations":[{"paperId":"18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6","title":"Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception"},{"paperId":"d505d66769c76b5ade7f65df89188f7d3b4b343a","title":"A Numeral and Affective Knowledge Enhanced Network for Aspect-based Financial Sentiment Analysis"}],"references":[{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"ad46feec0cefb78dafa60821598c052cf0b9b7bd","title":"NT5?! Training T5 to Perform Numerical Reasoning"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"02809fc23aecf33e3ed95b83d1d03b54fb5c3d0a","title":"An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"c9f343b492c170c726f607c255ec6c7177dc5800","title":"Verb Physics: Relative Physical Knowledge of Actions and Objects"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"ec15ff1fc5c9780fd91902f286a9e3fcd00b890d","title":"Math Word Problem Solving with Explicit Numerical Values"},{"paperId":null,"title":"2021), we included synonyms as an answer to make the prompt diverse. We used the website https://www.wordhippo"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Test-set results on different sets of prompts. We report the classification accuracy and the performance difference (∆)"}],"id":"ff8f3dfd9e2f4a92310999722abefab202935521","summary":"This study shows that PLMs lack the capability required for reasoning over measurements, and proposes a simple embedding strategy to better distinguish between numbers and units, which leads to a significant improvement in the probing tasks."},{"url":"https://www.semanticscholar.org/paper/f729e16ee6765e2fd76772cbf9d3dd17b4d25438","title":"Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems","venue":"ArXiv","year":2021,"referenceCount":73,"citationCount":11,"influentialCitationCount":2,"publicationDate":"29/10/2021","authors":"Keyur Faldu,A. Sheth,Prashant Kikani,Manas Gaur,Aditi Avasthi","citations":[{"paperId":"75903522d49c383647e938e8d3ced18176e69c6e","title":"Vector Relation Acquisition and Scene Knowledge for Solving Arithmetic Word Problems"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"40bb02398e6a4ed04d9ff7acc2dee88cd7898a5a","title":"Techniques to Improve Neural Math Word Problem Solvers"},{"paperId":"81481de5cd3dd4f170559958a7110f734d3dcdd0","title":"Prompt-Based Missing Entity Recovery for Solving Arithmetic Word Problems"},{"paperId":"71471561137956d6aeec39173db460a28d86c11b","title":"Analogical Math Word Problems Solving with Enhanced Problem-Solution Association"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"3d9703a974184ca8a1f3c97e1fb8acff1a50a18d","title":"Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing"},{"paperId":"b23a8493f384a52adf22d3c70c5827fd1a6ca42d","title":"Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem"},{"paperId":"c7d7f1c220412988695079677ab7700c4b4d67e3","title":"Solving arithmetic word problems by synergizing syntax-semantics extractor for explicit relations and neural network miner for implicit relations"},{"paperId":"3ce8c07349d91bb3f022a211be36e98eef0e1046","title":"MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems"},{"paperId":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers"}],"references":[{"paperId":"19f4fb23d15172306544d3ab35c4a0faa7f648e9","title":"Knowledge-Intensive Language Understanding for Explainable AI"},{"paperId":"d0dae92c4d37520ae20c072ec64fdb718874bfd0","title":"A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis"},{"paperId":"7654dbd372d8b65e730e3bd477ff9fec96c16dc5","title":"Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks"},{"paperId":"cdd961a274f42a0c4435367745b01f3da6491513","title":"Solving Arithmetic Word Problems with Transformers and Preprocessing of Problem Text"},{"paperId":"4ea544c849aee3e4f99e3d835ab3ea9ed685a5b9","title":"KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"593ca71119fb6ee560926d9b304bde095267432d","title":"SMART: A Situation Model for Algebra Story Problems via Attributed Grammar"},{"paperId":"edaafec59b651d503ac7c8a86f8e2335273e1f7a","title":"Learning by Fixing: Solving Math Word Problems with Weak Supervision"},{"paperId":"04124bc9f1f04e52e6b44385b985d9598613f0b4","title":"Semantics of the Black-Box: Can Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Explainable?"},{"paperId":"f9c07ed1d2113c858b38861790af6a26310b8465","title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"},{"paperId":"99287a5d6c822edf50bf3e9eeb17acf152d6a7c4","title":"A framework for predicting, interpreting, and improving Learning Outcomes"},{"paperId":"eb2f1480eca6195fe8dbab5de8bbac8749e9be58","title":"RODA: Reverse Operation Based Data Augmentation for Solving Math Word Problems"},{"paperId":"44507bde6e9caf60b41c60d703e7972b520d48a6","title":"Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"19af1457c7cb744ed5c6ca12d03e295599ee0d1a","title":"Auto generation of diagnostic assessments and their quality evaluation"},{"paperId":"45f952c21130d655090058e864c2772358a1de72","title":"Commonsense Reasoning for Natural Language Processing"},{"paperId":"b2a839e3ee68e81b863b73ee08c6626c94477fef","title":"WT5?! Training Text-to-Text Models to Explain their Predictions"},{"paperId":"4f03e69963b9649950ba29ae864a0de8c14f1f86","title":"K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"b20b96862dc0b3cf22bbb6246faa173dcec0f608","title":"A Deep Reinforcement Learning Approach to First-Order Logic Theorem Proving"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"b1832b749528755dfcbe462717f4f5afc07243b8","title":"Commonsense Reasoning for Natural Language Understanding: A Survey of Benchmarks, Resources, and Approaches"},{"paperId":"b93daf00cff1e3c703316e759caeaef1dbded6e6","title":"On the Capabilities and Limitations of Reasoning for Natural Language Understanding"},{"paperId":"caeb950e503872a903e18a3b259424e3cc3c6006","title":"Neural Math Word Problem Solver with Reinforcement Learning"},{"paperId":"e889e2dd1647fdfc28edb9abe08a863b704f08be","title":"Using Intermediate Representations to Solve Math Word Problems"},{"paperId":"920febb03475b068286a855c10ea09b968fe7ee3","title":"Reinforcement Learning of Theorem Proving"},{"paperId":"a396effc8987cc0938a4057d9d9008ff2a5452b6","title":"Data-Driven Methods for Solving Algebra Word Problems"},{"paperId":"835c6b524b90b1639aba28742f7161137ddf4397","title":"MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"8f1ddf500dc30a845bb9ff98eb9751c5490fed26","title":"A Meaning-Based Statistical English Math Word Problem Solver"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"3500e99072744ecbeac891eec6683ec867427400","title":"Thinking fast and slow: Optimization decomposition across timescales"},{"paperId":"8b7336f5dd13a45d4aab38428b4a88ce507ea310","title":"Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving"},{"paperId":"f6b5335f27b9583dd152d8cd4ea9134e24bd297b","title":"Learning To Use Formulas To Solve Simple Arithmetic Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"0da2fc91eb560f4245d8e7b6dff9edd0a5727dff","title":"DRAW: A Challenging and Diverse Algebra Word Problem Set"},{"paperId":"bb34a79ac2afa68dd9096d5246cf46b8f39bcde8","title":"Designing a Tag-Based Statistical Math Word Problem Solver with Reasoning and Explanation"},{"paperId":"acfe603d8a90ee3d4187e426b26c4d7bf394b4c8","title":"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning"},{"paperId":"414483f5d80802284885003d6b0bfc8a10f61d42","title":"Learn to Solve Algebra Word Problems Using Quadratic Programming"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"cdddbbdea32cad593b4a8f6225deeb494d792b63","title":"Modeling Math Word Problems with Augmented Semantic Networks"},{"paperId":"299af69ba394a46f2125a6571a7af7f4c66d1165","title":"Frame-Based Calculus of Solving Arithmetic Multi-Step Addition and Subtraction Word Problems"},{"paperId":"03c712cb6d4f14f4dd3eb4740d3934daacc04674","title":"Robust Understanding of Word Problems with Extraneous Information"},{"paperId":"c321e6238cbb5933c0c1166f07f5033eee98b7ae","title":"A computer simulation of children’s arithmetic word-problem solving"},{"paperId":"43489fda8a0c6276a3aa8f068892e31ebfd63c13","title":"Understanding and solving arithmetic word problems: A computer simulation"},{"paperId":"239f0416319da5782d933501c59457c22ddda9af","title":"觀察學習（Observational Learning）"},{"paperId":"045c3fec7574bbdab16c16f0af5be11696aa9b47","title":"An integrated model of skill in solving elementary word problems cognition and instruction"},{"paperId":"9fc77941297522cc420ce9292193dd04ed2ed1af","title":"Natural Language Input for a Computer Problem Solving System"},{"paperId":"8461dde4f8644517259067de63d9c8b5e589fc70","title":"MWP-BERT: A Strong Baseline for Math Word Problems"},{"paperId":"ec15ff1fc5c9780fd91902f286a9e3fcd00b890d","title":"Math Word Problem Solving with Explicit Numerical Values"},{"paperId":null,"title":"System and method for recommending personalized content using contextualized knowledge base"},{"paperId":null,"title":"Adaptive learning machine for score improvement and parts thereof"},{"paperId":"d0f3423fcea593e693ff4e20f9addc2e454ac563","title":"Mathematical Reasoning Analogies Metaphors And Images"},{"paperId":"a0d7cd519aeffd6b131137b490337ed4d6393dc3","title":"Machine-guided Solution to Mathematical Word Problems"},{"paperId":"c9d0ffbce02bc8ac9432d7df859d59e215421c0f","title":"Extended-HowNet: A Representational Framework for Concepts"},{"paperId":null,"title":"Approximation with artificial neural networks"},{"paperId":"c1c8c48a3c603262209fbcd7a95d1d03f0b8ad36","title":"Mathematical reasoning : analogies, metaphors, and images"},{"paperId":null,"title":"Calculus word problems"},{"paperId":"78fc3b741828124d8a79667466516a544143bf68","title":"Computers and Thought"}],"id":"f729e16ee6765e2fd76772cbf9d3dd17b4d25438","summary":"This work inspects non-neural and neural methods to solve math word problems narrated in a natural language, and highlights the ability of these methods to be generalizable, mathematically reasonable, interpretable, and explainable."},{"url":"https://www.semanticscholar.org/paper/62953ca1252c9febe07c7007a10911726f37792d","title":"TIMEDIAL: Temporal Commonsense Reasoning in Dialog","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":47,"citationCount":43,"influentialCitationCount":3,"publicationDate":"08/06/2021","authors":"Lianhui Qin,Aditya Gupta,Shyam Upadhyay,Luheng He,Yejin Choi,Manaal Faruqui","citations":[{"paperId":"8d3d792c28ad8b72899193449a2603efec24275d","title":"Temporal Validity Change Prediction"},{"paperId":"9085e72053b8e1295e1641276682f0bc1463d8b1","title":"Reverse Multi-Choice Dialogue Commonsense Inference with Graph-of-Thought"},{"paperId":"0960852f3e41b818065735bbcd87666f244cb201","title":"CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models"},{"paperId":"f37d1ef3c4fd85f608439d239306a3b3302e3add","title":"TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models"},{"paperId":"dab4f70d75a04e62553e583f2450d9bb1f0ead46","title":"CLOMO: Counterfactual Logical Modification with Large Language Models"},{"paperId":"ec5f1c1bc361ca4c53932e3a3a14d6f3f814e268","title":"MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document"},{"paperId":"07b2f2f705288fa456a8e4a49ce421304fbc24aa","title":"Once Upon a Time in Graph: Relative-Time Pretraining for Complex Temporal Reasoning"},{"paperId":"23cc6b2ed88872fcd3767cf054100e8eddcdb0a1","title":"CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks"},{"paperId":"99f7bd216214b22c22974142fd558ce554f518b8","title":"Contrastive Learning for Inference in Dialogue"},{"paperId":"4776de7b856d3b15eecc4f88666cdc13972df22e","title":"Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking"},{"paperId":"19a0777498ec3ef1e11e8349df3eb336cc19698d","title":"Syndicom: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback"},{"paperId":"cb1b43d25f18ad37d7ca1110dcdb87287b4186c6","title":"An Overview Of Temporal Commonsense Reasoning and Acquisition"},{"paperId":"031970e341dfd1385ce0ac9f4a21dc8d20e85645","title":"Self-Supervised Logic Induction for Explainable Fuzzy Temporal Commonsense Reasoning"},{"paperId":"60b4b7514ac3a3d9fbebf10f935dd89267c93621","title":"SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning"},{"paperId":"de5e0a70af551df17e693223d29a1840900d6b74","title":"Mitigating Temporal Misalignment by Discarding Outdated Facts"},{"paperId":"d3d8f37a809a0e3efe1cf6419033ed36424ffc8a","title":"ReTAG: Reasoning Aware Table to Analytic Text Generation"},{"paperId":"24ab6356b355b29a3770db56dd1f2200cdd987fa","title":"Salient Span Masking for Temporal Understanding"},{"paperId":"12d16f426edc6ab248fb476007bd1646282d4d68","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"17a8b5e6fef1f69979d57021a8f30a5159e152c7","title":"Commonsense Reasoning for Conversational AI: A Survey of the State of the Art"},{"paperId":"2e86c8cab0135ad7e363bb64aacef4eb8c639e6a","title":"Benchmarks for Automated Commonsense Reasoning: A Survey"},{"paperId":"bf8491bef353df126e2306ad2fe4b898697b906a","title":"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity"},{"paperId":"bf53543b21819c6d7d397183c38015540cb5f8af","title":"Time expression recognition and normalization: a survey"},{"paperId":"ee153a2c91d36b034dc86c945aee9859b79da812","title":"Test of Time: Instilling Video-Language Models with a Sense of Time"},{"paperId":"77fd0d22b6d361680b364c5561e9288ca4559c85","title":"A Probabilistic-Logic based Commonsense Representation Framework for Modelling Inferences with Multiple Antecedents and Varying Likelihoods"},{"paperId":"1a5f48161df983a0e9485425495121201902433b","title":"DiscoSense: Commonsense Reasoning with Discourse Connectives"},{"paperId":"d0ba95d3c7766038ea47fda8a13377cf3ee1c8e3","title":"Multiview Contextual Commonsense Inference: A New Dataset and Task"},{"paperId":"8f926c0c3f1557a9241b7e75609082a1f207a75e","title":"InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning"},{"paperId":"7ba28d214d98f2a9c2e37e6cdf294d0d4e2a1e50","title":"Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling"},{"paperId":"706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding"},{"paperId":"7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"},{"paperId":"16bf88a6d172699cb9a26a6936efb4941e3f3c13","title":"An Application of Pseudo-Log-Likelihoods to Natural Language Scoring"},{"paperId":"0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4","title":"QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"},{"paperId":"281b4a7e7fb057d8266ec0610888905c46fd715d","title":"Advances in Multi-turn Dialogue Comprehension: A Survey"},{"paperId":"331a87762228ecca2eebf309b859189f033414ba","title":"NarrativeTime: Dense Temporal Annotation on a Timeline"},{"paperId":"856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","title":"Interpreting docstrings without using common sense: the private science of very large language models∗"},{"paperId":"a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b","title":"Good Night at 4 pm?! Time Expressions in Different Cultures"},{"paperId":"f4ebe3bfd4b67595ca35fbfb0eaf3bf934c8b1b9","title":"A Unifying View On Task-oriented Dialogue Annotation"},{"paperId":"867915b9587c046ce8e4b71ab4dee2a1d8bf0b48","title":"DocTime: A Document-level Temporal Dependency Graph Parser"},{"paperId":"2a83a92b08e0f3873d07162c73c67e533321112e","title":"Aligning Generative Language Models with Human Values"},{"paperId":"8b0b5b998d65add5c036b45c36afa3f9df96f4b0","title":"Toward Building a Language Model for Understanding Temporal Commonsense"},{"paperId":"8424082e3bf4792462eb112d7ebcecf5b0dc3613","title":"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"},{"paperId":"cd0f142d51f8da5a626672954478bdaf9f27fa7f","title":"Time Expression Normalization with Meta Time Information"}],"references":[{"paperId":"3d2035edd4dd48e1e638279409e11bf689c461e1","title":"Temporal Reasoning in Natural Language Inference"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"36d6c8895bbc755964b8b2136c6fd6087a7af089","title":"TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions"},{"paperId":"1b04936c2599e59b120f743fbb30df2eed3fd782","title":"Shortcut learning in deep neural networks"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"22d834f7983fbd7cf2418978571f23efcd224bd9","title":"Adversarial Filters of Dataset Biases"},{"paperId":"d08463bd665589d04619f04dbde84183ffcf2e63","title":"Towards a Human-like Open-Domain Chatbot"},{"paperId":"f67fcbb1aec92ae293998ddfd904f61a31bef334","title":"Inducing Relational Knowledge from BERT"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"4d16457cded23bce6eaa91cd17aefd22af2279f0","title":"Counterfactual Story Reasoning and Generation"},{"paperId":"81b4920ad488affaee27389ff9540b7fea90a4ce","title":"“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding"},{"paperId":"f98e135986414cccf29aec593d547c0656e4d82c","title":"Commonsense Knowledge Mining from Pretrained Models"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"27033b8f72bf8cb7662c9f92b3ccb3c476db7135","title":"Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading"},{"paperId":"227458886343b86bd15adf58c769be326b4b058a","title":"Wizard of Wikipedia: Knowledge-Powered Conversational agents"},{"paperId":"bbad0b301561c9b44a43f2880b29f143dc7297ba","title":"Temporal Information Extraction by Predicting Relative Time-lines"},{"paperId":"34901b737b11aa51b688fa18c2eab47639d7b8c6","title":"Joint Reasoning for Temporal and Causal Relations"},{"paperId":"18dacf9ead8dfefcd36ecb765509ebfb6f88abbd","title":"Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge"},{"paperId":"a6ced6341e8bf2d819cbc7de73b869752019afdd","title":"Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"3108f96f80d129036f53684344f4058257b37c4b","title":"DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset"},{"paperId":"a39ffa57ef8e538b3c6a6c2bbc0b641f7cdc60dc","title":"Who did What: A Large-Scale Person-Centered Cloze Dataset"},{"paperId":"85b68477a6e031d88b963833e15a4b4fc6855264","title":"A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories"},{"paperId":"7b82383b09bedb765ab9c5c2153978035aacd830","title":"Context-dependent Semantic Parsing for Time Expressions"},{"paperId":"0d9d8be5ee0c1cda47beafea0ef0b14722cbd908","title":"SemEval-2013 Task 1: TempEval-3: Evaluating Time Expressions, Events, and Temporal Relations"},{"paperId":"39c230241d51b1435472115aaa8c62b94ab9927d","title":"Joint Inference for Event Timeline Construction"},{"paperId":"e95353d65cdad3ca73e6e70634f1964bb14e31de","title":"Parsing Time: Learning to Interpret Time Expressions"},{"paperId":"fe80746646f6ee819205ca9a8476292ea6b83e66","title":"SUTime: A library for recognizing and normalizing time expressions"},{"paperId":"407314c1b491511564c089d26e7e92c0c93af754","title":"Learning Temporal Information for States and Events"},{"paperId":"d50806c7de06356fa207f299dacc7feb1f58dd47","title":"Classifying Temporal Relations Between Events"},{"paperId":"b06cc9e40a6e98db86275f1852eba447b9a7ec25","title":"Learning Sentence-internal Temporal Relations"},{"paperId":"b0c06a9d9adf535d5066f7a7d6dcecd0292ebbac","title":"Temporal and Event Information in Natural Language Text"},{"paperId":"ace31a10ee17fb9676efb47b55007d1c996d6122","title":"Annotating Events and Temporal Information in Newswire Texts"},{"paperId":"d36afe59ad1b706e020f55b54740bc9cddf25dcd","title":"Towards a General Theory of Action and Time"},{"paperId":"f4d642d674aa63aafc11562c2557cb4772946147","title":"Maintaining knowledge about temporal intervals"},{"paperId":"cc2b719c7ff4c7105f30015c3b129480e16c6fa4","title":"Mechanizing Temporal Knowledge"},{"paperId":"7e7343a5608fff1c68c5259db0c77b9193f1546d","title":"The measurement of observer agreement for categorical data."},{"paperId":"766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd","title":"“Cloze Procedure”: A New Tool for Measuring Readability"},{"paperId":null,"title":"Backpropagation-based Decoding for Unsupervised Counterfactual and Abductive Reasoning"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9264010159e6de0e88dc883bd06797bc7f23d0ed","title":"ISO-TimeML and the Annotation of Temporal Information"},{"paperId":null,"title":"Event cognition"},{"paperId":"ad06d0a4b7bff0e86a6b38cfad4a82c023401901","title":"A Model for Temporal References and Its Application in a Question Answering Program"}],"id":"62953ca1252c9febe07c7007a10911726f37792d","summary":"This paper presents the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set, TimeDial, and reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context."},{"url":"https://www.semanticscholar.org/paper/598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":219,"influentialCitationCount":5,"publicationDate":"22/11/2022","authors":"Wenhu Chen,Xueguang Ma,Xinyi Wang,William W. Cohen","citations":[{"paperId":"c3d1832ed0444f75d44116fabbdda891aebc4b01","title":"LLaMA Pro: Progressive LLaMA with Block Expansion"},{"paperId":"4bebe389dfa85423e5cc089edf20b2c3f572f38c","title":"Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives"},{"paperId":"fc7feeaddc5a38c0d6f0d793737584e5f0bb7519","title":"Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers"},{"paperId":"a06d3e9e90008c64c45a0029d580541d5f646771","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents"},{"paperId":"96372dd90192ca2f5947115a25482604499da476","title":"Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs"},{"paperId":"13261129251c9e8891cff02c3aee15c4df6a5630","title":"Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"},{"paperId":"0540dd805f040fd414dc16735fcb0c63111c3b1e","title":"Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs. SQL for No-Code Access to Relational Databases"},{"paperId":"218a9c9f836e31be1e71ba50728d24a478ebcddd","title":"kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest Neighbor In-Context Learning"},{"paperId":"1bd9466f0bb10d29a16f614943ec7823e13cb210","title":"Mixed Distillation Helps Smaller Language Model Better Reasoning"},{"paperId":"55820f684fcd592edfa013633e5704a41b176d23","title":"Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning"},{"paperId":"559ec2f23e7b65825b614346bdbabdfd8b56a667","title":"Assessing GPT4-V on Structured Reasoning Tasks"},{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"378f828f992458cd0b2eb2fd66adb979297534be","title":"Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization"},{"paperId":"76588eabeb1d712c6170fb9cbfe13b92d0809b69","title":"Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning"},{"paperId":"3a56bc074b8f3f985599627404b70e16fc5bce1b","title":"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator"},{"paperId":"36f71673d9337b432babc51da77ef38b2070b5ed","title":"An LLM Compiler for Parallel Function Calling"},{"paperId":"d4346af837aa6c2bb4a341cfe9bd91862ea5910a","title":"Large Knowledge Model: Perspectives and Challenges"},{"paperId":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models"},{"paperId":"0d8da8431bee9b2c2d40b605a754c5c840833323","title":"Recursive Visual Programming"},{"paperId":"a9e78765a4d49a50d67d0dacb033fb47f8d9f8c9","title":"Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication"},{"paperId":"c3bc547fc48ab9557457be003feea036adec1b4b","title":"D-Bot: Database Diagnosis System using Large Language Models"},{"paperId":"ccb055aba5f2f76f23598de0d101844a1a14fb84","title":"On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs"},{"paperId":"21c224b1604da6718cde07f8543b276f583d927e","title":"Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses"},{"paperId":"db6a4a5f22de7e6527a2aed9acc19c40aeeab97b","title":"Prompting in Autoregressive Large Language Models"},{"paperId":"e1b7f13051bec4a713c6226c177b63599d49e3e9","title":"LLM-Assisted Code Cleaning For Training Accurate Code Generators"},{"paperId":"dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills"},{"paperId":"6fa0677731184444df0e1fc8070938419cd6da47","title":"Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"},{"paperId":"be130fe97c15048dd91cad438894fbed5a05365a","title":"Meta Prompting for AGI Systems"},{"paperId":"6ea0d369103c5786fc555c0da05f81eb013392d0","title":"Zero-Shot Question Answering over Financial Documents using Large Language Models"},{"paperId":"76f6142d2b62972de89cb8651ea036d0dd6be68b","title":"Program-Aided Reasoners (better) Know What They Know"},{"paperId":"d57ebdb4b5ed89ab4135c72f0c91dc1c4b347c4b","title":"KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains"},{"paperId":"eee5a8e06c32112d268e88a8d4c45592d7214244","title":"DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data"},{"paperId":"e56aa728aaa32c087c8f7bc56a7eb225675dd8ae","title":"Structured Chemistry Reasoning with Large Language Models"},{"paperId":"f9ba0caf7a13844710815a615dd29e96c89f6b27","title":"Efficient End-to-End Visual Document Understanding with Rationale Distillation"},{"paperId":"1486a3310e945b69f3dcf605f09e777187a0189d","title":"Leveraging Code to Improve In-context Learning for Semantic Parsing"},{"paperId":"722aa3bb6e426afc40f05c42a2fc0623adb51af9","title":"Towards Verifiable Text Generation with Symbolic References"},{"paperId":"03f3801956fc4cc026860568670f9f65ed29b192","title":"Towards A Unified View of Answer Calibration for Multi-Step Reasoning"},{"paperId":"a1cbe3e24cdcb49541ef5da4f82d9ebc45bb6261","title":"StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving"},{"paperId":"f12d4adcdbc9f60cc0fcb4fc58302f55faf65a69","title":"Language Models are Better Bug Detector Through Code-Pair Classification"},{"paperId":"6a28a2c2282c23e173cb3feefea27566eb2c6377","title":"Past as a Guide: Leveraging Retrospective Learning for Python Code Completion"},{"paperId":"df15b83986207d884a811ce572dcedb6654abc6f","title":"BizBench: A Quantitative Reasoning Benchmark for Business and Finance"},{"paperId":"a2ccffe67a4ccfb10279dc3f0167fe65ae01e471","title":"Language Models can be Logical Solvers"},{"paperId":"9ddd1a8705cd4af1b193a8a554efe0671d9f0aa9","title":"$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation"},{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"c22cc4e2eed78b4b31e50d94ea35da0405aabb87","title":"Multi-Operational Mathematical Derivations in Latent Space"},{"paperId":"4411e6b32865933cab87696c2738cb7a204e4240","title":"Implicit Chain of Thought Reasoning via Knowledge Distillation"},{"paperId":"5402c22369d0190d0a002b7a1222d403edae010a","title":"Defining a New NLP Playground"},{"paperId":"f8b8f926bbfa327c86c40796131fe2695db81126","title":"DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models"},{"paperId":"133777180e326dfa53523bf53b0a969bbdccb0ee","title":"API-Assisted Code Generation for Question Answering on Varied Table Structures"},{"paperId":"f176d0d466d7c778a6435fe9a8d7e49508cb9059","title":"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts"},{"paperId":"66d98dc2aad17c03532dbae21d05f098257cc2e2","title":"LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers"},{"paperId":"729fc01274cc26798654a318d1a95e73c61f99a3","title":"Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models"},{"paperId":"8a1792a03c4b0ced9650940ca6e9ea495922ace2","title":"Prompt Engineering Through the Lens of Optimal Control"},{"paperId":"465443c398652061873cb5cf767e9f5805a6fde1","title":"Semantic Decomposition of Question and SQL for Text-to-SQL Parsing"},{"paperId":"79e7ead8f59b17431de2b86af10dc0c30a1f5a2b","title":"ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search"},{"paperId":"20eecb9ead20ffe49a66588a9662336eefb20a54","title":"MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models"},{"paperId":"aac8cdd40b2bfd1b967f0e5ea6c01e93385169e7","title":"SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving"},{"paperId":"bb9f3c9c8bcf78814740559ae6fb82ca1012ea90","title":"A Comprehensive Evaluation of Tool-Assisted Generation Strategies"},{"paperId":"3bddda884c5264d2d3ae7087c2d570243dbe1db4","title":"MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities"},{"paperId":"8868a6d452b06bf4ad33237d0f3952d895ca20e7","title":"Improving Large Language Model Fine-tuning for Solving Math Problems"},{"paperId":"4941b37136cdf1836b78ddb6cee65a28c3ce45f0","title":"ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models"},{"paperId":"0d31d1c2174ae18a7e2bdc560f4de45d2b9dc8c5","title":"Formally Specifying the High-Level Behavior of LLM-Based Agents"},{"paperId":"0885471c0215b3c0d31c82518066913f7f738128","title":"Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement"},{"paperId":"f6007270b8aba3a69eac7c98375c99b7ca26c9b0","title":"Diversity of Thought Improves Reasoning Abilities of Large Language Models"},{"paperId":"34ca51ce10e8d1d6c950ba519329714a0184d004","title":"KwaiYiiMath: Technical Report"},{"paperId":"6a3c67d35f3a9c10c8fde6c325a0535c03876068","title":"SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA"},{"paperId":"8147cec9245d34d13732a08e915c920a1a499bb5","title":"Lemur: Harmonizing Natural Language and Code for Language Agents"},{"paperId":"0947cbc83b72fefa536423114883ddb6627625f7","title":"Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding"},{"paperId":"b29134737a0c81c13d31fc0263b3c4d4f05ccb78","title":"Guiding Language Model Reasoning with Planning Tokens"},{"paperId":"174a9b78350e9561555052bc6901cc44782f4c62","title":"Estimating Numbers without Regression"},{"paperId":"12a4c41b087629548b07d0dadb9da05147fa4f81","title":"Towards Better Chain-of-Thought Prompting Strategies: A Survey"},{"paperId":"b9e8b62bcc019f47a0a015568f70039b3b7c1196","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model"},{"paperId":"ff2eecb21972eb287064f98db1a4487c62bd7566","title":"MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models"},{"paperId":"79429814fd4d967b9277af2805c53f370e52ebb5","title":"Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations"},{"paperId":"700bd9681f1b9e9e2212e10415d27b11c7e6836b","title":"Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models"},{"paperId":"cddb552f6c3464a54a02b0b64b2d1af56c086606","title":"MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning"},{"paperId":"2069aaaa281eb13bcd9330fc4d43f24f6b436a53","title":"DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines"},{"paperId":"1d8472bbf71ccf60550126d136b53699a2f4f685","title":"DOMINO: A Dual-System for Multi-step Visual Language Reasoning"},{"paperId":"00cccb9065f0a59e845d5b4d360ce31cf25036be","title":"Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning"},{"paperId":"3fe940a1f121f083cb90c568fc6fa2951bb27dda","title":"Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation"},{"paperId":"d4bf36cbc5855ea87235d7a64f406717ac6aa3c9","title":"Large Language Models as Analogical Reasoners"},{"paperId":"a5d27bf7a2155d4ca016565a78b52ee90f81624c","title":"Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning"},{"paperId":"5076bbbf831a92174c9cc1b347bd0584560435fc","title":"Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning"},{"paperId":"bfeda6c7aa7899a80adb01894555b09d24756a59","title":"Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"},{"paperId":"e61a96cf602ebff6683929aaf916e25614a475bc","title":"UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities"},{"paperId":"12db3efff4cc9e16822dd64bb1cad66f3f034f3b","title":"L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models"},{"paperId":"a1426b13b74dbad17b34606d25aabe1d61f6e11a","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets"},{"paperId":"b272513916b45c8517d289d7abee4a53e6832187","title":"ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"},{"paperId":"7fe071ea76e49bc3e573beb53f07721630954247","title":"Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"},{"paperId":"11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"},{"paperId":"77b1f1c6d1658d120456b9046667cf009ceb39ce","title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"},{"paperId":"cf237f3a6ed3e8fd970c15bf1f0bdf94f34da4a9","title":"LPML: LLM-Prompting Markup Language for Mathematical Reasoning"},{"paperId":"5558226fa14f7467d25690b97febdf0e8299e432","title":"Code Soliloquies for Accurate Calculations in Large Language Models"},{"paperId":"2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e","title":"Design of Chain-of-Thought in Math Problem Solving"},{"paperId":"5d5dbba58aaf5b3fa4044cc3ffc71a3fe2b8c654","title":"SCREWS: A Modular Framework for Reasoning with Revisions"},{"paperId":"94d878ba7eeba4abc4d5e42b4c2c4c98d4e575ce","title":"Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning"},{"paperId":"12b233752c7097ea6525622bed238ae2d2193c5a","title":"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback"},{"paperId":"1ecf2955a2f4f2039f36b0334e2c376a5c901d6c","title":"LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models"},{"paperId":"62b4e06f5249d22e4a153ec4a2dc934c6a014372","title":"OWL: A Large Language Model for IT Operations"},{"paperId":"396305230ddcf915b19a19683a89e34d76321a33","title":"Cognitive Mirage: A Review of Hallucinations in Large Language Models"},{"paperId":"32890c0c431e66e40680c21f24a84b097f81e422","title":"Evaluation on ChatGPT for Chinese Language Understanding"},{"paperId":"a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","title":"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"},{"paperId":"4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f","title":"Hypothesis Search: Inductive Reasoning with Language Models"},{"paperId":"280353fd7a7a3e49c415c443e1b7ccf7de9c2b4e","title":"Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models"},{"paperId":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator"},{"paperId":"3886f3bd2a0af9e75bf9fa5b7db4224969dbf346","title":"MathAttack: Attacking Large Language Models Towards Math Solving Ability"},{"paperId":"c237a22698223e4060d83027f399f4fb2aa24291","title":"Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"},{"paperId":"412fe1f135cb20c952962133ca1e534a71bfd27f","title":"When Do Program-of-Thoughts Work for Reasoning?"},{"paperId":"2dbe7fd41f23aa2a077c3d1e91b0b73f92e3d8d2","title":"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning"},{"paperId":"49408c5e1ac75854f1580e561384df2be870d559","title":"KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases"},{"paperId":"1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc","title":"Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification"},{"paperId":"507acddb0b7f36b83fd7c8bff2f121eb506ac8fb","title":"Cumulative Reasoning with Large Language Models"},{"paperId":"377d4d6c1be01b9df32edfd94b2c5946971b0108","title":"Flows: Building Blocks of Reasoning and Collaborating AI"},{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"3d473cbb7a377cf960abff31748a1a39bb6c7d7c","title":"Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"},{"paperId":"4993258852711c4e3d0011325ac3db680eae84f4","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"},{"paperId":"4a5af57b2056c4cc0a768d830d5427f0d1bdae33","title":"Large Language Models Perform Diagnostic Reasoning"},{"paperId":"d85d18eefa8d70985a48f77b31ef74d5990d148f","title":"Explaining Competitive-Level Programming Solutions using LLMs"},{"paperId":"50f9f33b284b7363fbd9b9d2da4939b989a1c7cd","title":"Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models"},{"paperId":"f8e99be4f9a01761fab74bade2c3c18de9fc686b","title":"Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks"},{"paperId":"0b94b999fdd9488e1a0914d37f8fb3ea7e9ea0fd","title":"Comparative Analysis of GPT-4 and Human Graders in Evaluating Human Tutors Giving Praise to Students"},{"paperId":"8568d7bd9dfb5ba0b91940b938b44a88fafdf95b","title":"Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models"},{"paperId":"f94c040b02bdd6cf1b85f374e3912630c66861c3","title":"InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback"},{"paperId":"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","title":"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements"},{"paperId":"fae57797d357bfa3b39b220336d1a2e8deba5318","title":"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving"},{"paperId":"7011bf9aa7e68fabaa1df498da6d2dd8a950f037","title":"Pushing the Limits of ChatGPT on NLP Tasks"},{"paperId":"9efa81ec4954b0859c47dad8f42edfaf8bced69b","title":"Boosting Language Models Reasoning with Chain-of-Knowledge Prompting"},{"paperId":"455866ca838f356b53a7e3e5b344834f9e93dbbc","title":"ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases"},{"paperId":"5bbc3b014f7c2dd151dc6b3cfb183889c44e772d","title":"Natural Language Commanding via Program Synthesis"},{"paperId":"50f44ef10335d59cec145b15effae20ff22c1fdb","title":"ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory"},{"paperId":"8236010c2ecc94d826be6010ff187fdc000e7df6","title":"Deductive Verification of Chain-of-Thought Reasoning"},{"paperId":"1a3ba6662ef1c5aebd3b343d3d9f77a8543e474d","title":"Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"8353dfd0cb9f8e8a3adcfb88cec53303ae7ae27c","title":"An Empirical Study on Challenging Math Problem Solving with GPT-4"},{"paperId":"a563fa042b16533d23829d76e7e17bf19a05891c","title":"Large Language Models Are Not Strong Abstract Reasoners"},{"paperId":"6847b9658f287f430098199cd81bf26308da13f9","title":"Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"498d1406fc4cddb05cd46477793f2e726a6fe238","title":"The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code"},{"paperId":"0875651b68e6602d45ae08bee67cf63c02faa512","title":"Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models"},{"paperId":"eb971944bccf9793ac463c3e2f4d4251d4e8e071","title":"Do Large Language Models Know What They Don't Know?"},{"paperId":"32dcd0887537cece54e214f531d2c384470b023f","title":"Large Language Models as Tool Makers"},{"paperId":"8e37dc1215681aa153a51c07078ba8befd6a6e01","title":"AdaPlanner: Adaptive Planning from Feedback with Language Models"},{"paperId":"21e0a1324522b39e5cec94885501e906942c43d0","title":"InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction"},{"paperId":"7283d616e40d7ab7422e3697218f3fc42f292bf2","title":"Getting MoRE out of Mixture of Language Model Reasoning Experts"},{"paperId":"152d9a231c00d4495c9bc4a466f42165ce2e2164","title":"Evaluating Factual Consistency of Summaries with Large Language Models"},{"paperId":"6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3","title":"Automatic Model Selection with Large Language Models for Reasoning"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"de0593223973eccfee04d598a68bc55784c7fc17","title":"LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models"},{"paperId":"ae80c69872d4af5814d9d7dfa771c794a93697d3","title":"Question Answering as Programming for Solving Time-Sensitive Questions"},{"paperId":"4ee96f0757e517928590a2300af5d40ba768a5a7","title":"PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents"},{"paperId":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models"},{"paperId":"c20b18d6b919695a69e416debf8bf1ffeac03992","title":"SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables"},{"paperId":"8793066d170b6a742c4fcdb478d4f100c1e4bf17","title":"Fact-Checking Complex Claims with Program-Guided Reasoning"},{"paperId":"073e6b91adc25c656d85002e3cb059e4530db20b","title":"Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"4313b009cd30a469400d36b9fb20267620293d04","title":"A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers"},{"paperId":"327e0290fd71609bfc1a30478a95f690668fe622","title":"Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies"},{"paperId":"9e9e4df2996bac794c4f04cb887df3e553bae4fd","title":"Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning"},{"paperId":"bcdaf6c98ddbd6809cf6241aa77200d7394db163","title":"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"},{"paperId":"22d5459d1f47341b355feeb1becc37208d6ec365","title":"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"},{"paperId":"d3d8f37a809a0e3efe1cf6419033ed36424ffc8a","title":"ReTAG: Reasoning Aware Table to Analytic Text Generation"},{"paperId":"c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c","title":"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings"},{"paperId":"972501b057e2b84d6ce6506f70bcac697bab7872","title":"LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation"},{"paperId":"f27f6d1d521d189e78f5623098ced0deea613d33","title":"Satisfiability-Aided Language Models Using Declarative Prompting"},{"paperId":"8efbd687804a13762f135db30b6077a1b171ae01","title":"Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media"},{"paperId":"62176de125738e3b95850d1227bac81fd646b78e","title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models"},{"paperId":"3f758a13d3703b02bdf977f9189230276064da42","title":"T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering"},{"paperId":"ab9485a6564cdbe5b5ce2233d5b00c64ceaa518c","title":"From Words to Code: Harnessing Data for Program Synthesis from Natural Language"},{"paperId":"0139e689add40a61c9454674edac4e93702aa5fc","title":"Few-shot In-context Learning on Knowledge Base Question Answering"},{"paperId":"ef018d9fad6167cfddb7d6654c5422df1e953730","title":"Self-Evaluation Guided Beam Search for Reasoning"},{"paperId":"6898f302acab95f30822c948c484af5c6e99827b","title":"Exploring the Curious Case of Code Prompts"},{"paperId":"003ef1cd670d01af05afa0d3c72d72228f494432","title":"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"261549439aebdda72b648ecc462448fd24857ac1","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"c4e4b72da211dbf2ab2fd5263d453cf22ee0cf44","title":"CodeKGC: Code Language Model for Generative Knowledge Graph Construction"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","title":"Tool Learning with Foundation Models"},{"paperId":"9e3c493fb09dcd61bb05e8c5659f23327b7b6340","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"bebede105aa69a81045bf79682272ee4cfb61475","title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks"},{"paperId":"27f32dc1aab7919eb7039deca067c3fbdc719c2a","title":"Koala: An Index for Quantifying Overlaps with Pre-training Corpora"},{"paperId":"033275ccc2c7c5c38592ae893da0b5923cf90717","title":"Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases"},{"paperId":"00a9a469bb019bf33eeee438c110f704b71cda73","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"},{"paperId":"0d42221038c05cee8443c5b5af838505ee137dc3","title":"ART: Automatic multi-step reasoning and tool-use for large language models"},{"paperId":"0100785773b8217c44606ab260e3212f93b0a4fd","title":"Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"47c3b8dd2c8a9326249ac98900b2c3fc71f46ab1","title":"Self-planning Code Generation with Large Language Model"},{"paperId":"197022486b2e2584302bd9b6442e44d15bf3e351","title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"59fe7cb560651281cfc5db6b8940da0e3ba9dea6","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"2029349c55c1dba3493c5b3bd25152f18ba21ae2","title":"Augmented Language Models: a Survey"},{"paperId":"873a581320d928249609d3c07229d5af182a379c","title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"},{"paperId":"b549716038bb167fcc65e3f9f725013d8b2ea649","title":"Reliable Natural Language Understanding with Large Language Models and Answer Set Programming"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"69619a2a47faee7a29ec596db13172e2a42ff921","title":"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models"},{"paperId":"b115c1e1e9e51f8ad7d47b745bc04e29a654b84d","title":"Faithful Chain-of-Thought Reasoning"},{"paperId":"5988806996e8d12f5d4aa911960d842cf7be0c24","title":"Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning"},{"paperId":"3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context"},{"paperId":"a23e0cf74f10b6a3061ab71497fe2c8c476fecd1","title":"Conversational Automated Program Repair"},{"paperId":"29bd550d0ab53296790ceba31dfe0a06754bcdde","title":"Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning"},{"paperId":"c43a4a7b7ea4f4889de051321cb0073fd577f843","title":"Causal Reasoning of Entities and Events in Procedural Texts"},{"paperId":"27f0ce04403158b61328716ae4aaab5840c0d123","title":"Batch Prompting: Efficient Inference with Large Language Model APIs"},{"paperId":"4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8","title":"DePlot: One-shot visual language reasoning by plot-to-table translation"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"35922cd0d6b17e45320917338e9f98cb5c1a4f6f","title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"95e184e2668ad49b5ea7c1c185727ef2d2c30de6","title":"MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering"},{"paperId":"5791c2b41dd23310c53d6738a4c0d587107c2dc8","title":"MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation"},{"paperId":"c2329c685f11efa25c562f97be71ff03103423fd","title":"Prompting Is Programming: A Query Language for Large Language Models"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"285d13bf3cbe6a8a0f164f584d84f8b74067271f","title":"Towards Faithful Model Explanation in NLP: A Survey"},{"paperId":"68abbfebe8ff4ebe538bf20621223da2854b8684","title":"A Computational Approach to Understand the Human Thought Process"},{"paperId":"30b81e62bbbbe9b9afe304171dab463eb0991c0f","title":"Operation-Augmented Numerical Reasoning for Question Answering"},{"paperId":"75c08892179fc478f87d7020b5daff9fca4f3389","title":"Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models"},{"paperId":"d04ab82edfec9718997f609f3466bd9bc6491d15","title":"Does ChatGPT Comprehend the Place Value in Numbers When Solving Math Word Problems?"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"064422a3713b96acc173d6bbcdfc6b6b15e3f5b7","title":": Leveraging Collective Human Intelligence to Study Large Language Models"},{"paperId":"a3bbe282f1666900a6604991f96e1637c6946253","title":"MATHVISTA: EVALUATING MATHEMATICAL REASON-"},{"paperId":"cc5abf7957e0038028b17f5fa5e8e8562f6369f0","title":"Chain-of-Thought Reasoning in Tabular Language Models"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"bcdaf6c98ddbd6809cf6241aa77200d7394db163","title":"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"},{"paperId":"9ada8fa11b1cdece31f253acae50b62df8d5f823","title":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation"},{"paperId":"62176de125738e3b95850d1227bac81fd646b78e","title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models"},{"paperId":"0d42221038c05cee8443c5b5af838505ee137dc3","title":"ART: Automatic multi-step reasoning and tool-use for large language models"},{"paperId":"ace2b6367a067898f66a33fca19581ebe71fccc5","title":"GPT-4 Technical Report"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"e66f0f822d4c4853b39b27daaafa2993005fd55e","title":"Large Language Models are few(1)-shot Table Reasoners"},{"paperId":"d96997265f8146e93b4c9350f19d55e46d1317f0","title":"ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"d84643c9f0289df5f9373480025da8f76f6df79c","title":"Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder"},{"paperId":"dd43b2dacded3fbe089085f920f6ff9a0f33d5f6","title":"A Numerical Reasoning Question Answering System with Fine-grained Retriever and the Ensemble of Multiple Generators for FinQA"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"10bd4160b44803ada6a3d2e366c44b7e2a4ffe90","title":"An Explanation of In-context Learning as Implicit Bayesian Inference"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":null,"title":"Decomposition enhances reasoning via self-evaluation guided decoding"}],"id":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","summary":"Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoT performance on all math problem datasets and near-SoTA performance on ﬁnancial datasets."},{"url":"https://www.semanticscholar.org/paper/a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":23,"citationCount":14,"influentialCitationCount":0,"publicationDate":2022,"authors":"Matteo Muffo,A. Cocco,Enrico Bertino","citations":[{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners"},{"paperId":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator"},{"paperId":"a37d5620210276e47cf0c9dd2898c2a82c9d0422","title":"Simple synthetic data reduces sycophancy in large language models"},{"paperId":"9160673f89ecb326db8c64f8d09ab30f35a74e9d","title":"Arithmetic with Language Models: from Memorization to Computation"},{"paperId":"7ef0cc95ff71c1098414cd61e88ac373ea2db4c4","title":"Performance of Generative Large Language Models on Ophthalmology Board Style Questions."},{"paperId":"24a285b1e7ee9acfec9a82f7123563b62f532203","title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"ade6d2808283ab8372db701ede6ee145a5445a95","title":"SLaDe: A Portable Small Language Model Decompiler for Optimized Assembler"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models"},{"paperId":"763f47b73d0273a93a52a8ee8a63cf19009df6e1","title":"Injecting the BM25 Score as Text Improves BERT-Based Re-rankers"},{"paperId":"651dac86d8bf847ec6780a878cb1e04d3d41f356","title":"GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities"}],"references":[{"paperId":"ac34c70ee85b048ad97328713c790f389656e4eb","title":"Ecco: An Open Source Library for the Explainability of Transformer Language Models"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1a2118bed729579528deb51e745d58dd3629baf6","title":"Learning Important Features Through Propagating Activation Differences"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"a9075f6332542e12b2bf3cdbdb3a6ed44733fb41","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"13167f9cd8c7906ca808b01d28dca6dd951da8a5","title":"of the Association for Computational Linguistics"},{"paperId":null,"title":"3374–3380, Florence, Italy,"}],"id":"a5808ccc50f77083bd3be926fb2af05cf34563ff","summary":"The ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on is evaluated."},{"url":"https://www.semanticscholar.org/paper/1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":49,"citationCount":3,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Jialiang Xu,Mengyu Zhou,Xinyi He,Shi Han,Dongmei Zhang","citations":[{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"a37a0ac532442873b425e47296faa8d24c1c7d01","title":"Creativity index calculation with question answering system using BERT model"},{"paperId":"a39f960b28a627d554712a457efa5d3e3b7f8406","title":"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning"}],"references":[{"paperId":"e262e1aa4d1efb909705a2dfcf9df53d76cb4e12","title":"Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation"},{"paperId":"d9055c52b9454d91be09c197b11ed3f60ee7bb0a","title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"03488f1a193066b5ea8b9b800e119f07df5c1d9e","title":"Reasoning Like Program Executors"},{"paperId":"49f4b4ca86e574c7ec688cfd45d2e17ff079c313","title":"Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks"},{"paperId":"5d9ac727b4a4e1044b3ba464df3be66eb8568127","title":"Numerical reasoning in machine reading comprehension tasks: are we there yet?"},{"paperId":"68900bf4b9cc820f4b47608871eafcac65a33917","title":"Adversarial Examples for Evaluating Math Word Problem Solvers"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"2406cf39805c70264c4226b7325a09b506c70921","title":"TAPEX: Table Pre-training via Learning a Neural SQL Executor"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"386bfd0e411dee4f512a8737c55dd84846981182","title":"TABBIE: Pretrained Representations of Tabular Data"},{"paperId":"0f08f4458dcf7618263405cbf31e6a48684bc1fa","title":"Discrete Reasoning Templates for Natural Language Understanding"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"53953a19e2fdf9c5ad5ff445c07ce36cc70f551b","title":"Solving Math Word Problems with Multi-Encoders and Multi-Decoders"},{"paperId":"1b055049c568be70d6a762679cdb93f630d5d6e6","title":"Tabular Transformers for Modeling Multivariate Time Series"},{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"f9c07ed1d2113c858b38861790af6a26310b8465","title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"a5b1d1cab073cb746a990b37d42dc7b67763f881","title":"TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data"},{"paperId":"52cb05d721688cb766c6e282e9d55c3b8e3dc0cf","title":"TaPas: Weakly Supervised Table Parsing via Pre-training"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96","title":"TabFact: A Large-scale Dataset for Table-based Fact Verification"},{"paperId":"b285c067f2da04bf5647beb8853bfddf6d9b4e1b","title":"A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"f67ede490780f939103d4cc4e9c6866b83ee59b3","title":"Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"cbd569036fc72ae7ff747350b91816440282596b","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"ba84b8f16efc032945cd8ed960c121a9bcd064d5","title":"Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":"8461dde4f8644517259067de63d9c8b5e589fc70","title":"MWP-BERT: A Strong Baseline for Math Word Problems"},{"paperId":null,"title":"the semantic of the paragraph and will also confuse humans"},{"paperId":null,"title":"Computational Linguistics: EMNLP 2021"},{"paperId":null,"title":"International Committee on Computational Linguistics"}],"id":"1a174b63d294f96568517b91f2c8d6c9362118b5","summary":"This paper proposes to conduct numerical capability diagnosis on a series of Numerical Question Answering systems and datasets and investigates the effectiveness of applying perturbations as data augmentation to relieve systems’ lack of robust numerical capabilities."},{"url":"https://www.semanticscholar.org/paper/40a1f266bb5ca853837355bfff272a55f0049c81","title":"Mining Numbers in Text: A Survey","venue":"Information Systems","year":2021,"referenceCount":42,"citationCount":3,"influentialCitationCount":0,"publicationDate":"06/07/2021","authors":"Minoru Yoshida,K. Kita","citations":[{"paperId":"5d0523abccf830984595899fd8e59a521cf761d6","title":"Using Machine Learning to Classify Information Related to Child Rearing of Infants from Twitter"},{"paperId":"e605f32b628b3a36b72cab0e9189aacac0b4f780","title":"Text2Struct: A Machine Learning Pipeline for Mining Structured Data from Text"},{"paperId":"5df55d94ab5026ff84ec01871592108fbadbddbe","title":"Measurement Extraction with Natural Language Processing: A Review"}],"references":[{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"d5fcad8a3b183642fcf609519a4dbbda9c3541ff","title":"Learning Numeral Embedding"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"4c9a8caf940627126aaa9bd3ac813d07065c86a0","title":"Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"2a0fc534ebf1b22b5851f5b1fb781dd707f7c530","title":"Numeral Attachment with Auxiliary Tasks"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1","title":"Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"dfe50d03bcd8377a03e0ea642146476828ea65c9","title":"Table Topic Models for Hidden Unit Estimation"},{"paperId":"c825b92effcf400ab1679071cef8233a5a6a18ba","title":"How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"1c3884e43bba39c50e6172c403c057659ef3ce83","title":"Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"bc51700cfae5ad693530331875b04ab4b62fc03e","title":"Open-domain quantity queries on web tables: annotation, response, and consensus models"},{"paperId":"ea68d467d08252c8f9002512dce6dfae360e5157","title":"Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web"},{"paperId":"ff6a34e67bb78105ed599d375932e0f580bad384","title":"SCAD: collective discovery of attribute values"},{"paperId":"6c93b11ce414776553ed5cb9eb5d3cffa2a5fc6b","title":"Extraction and Approximation of Numerical Attributes from the Web"},{"paperId":"d4ad37d742179fb6ad7fe4852f4d05faf1690aac","title":"Mining Numbers in Text Using Suffix Arrays and Clustering Based on Dirichlet Process Mixture Models"},{"paperId":"c25e24384cfa66659db8803428c6ba02a157137e","title":"UAIC: Participation in CLEF-IP Track"},{"paperId":"c0873cfd6c19e903b139bc6f465d06da5972aece","title":"Learning to rank for quantity consensus queries"},{"paperId":"217499e1559ddac51ca882a0d2f5a7e5eb5d2b2c","title":"UTH: SVM-based Semantic Relation Classification using Physical Sizes"},{"paperId":"4d1e1b15ba023d3a033c02fa34ca0e893b709696","title":"Numerical Data Integration for Cooperative Question-Answering"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":null,"title":"Table Topic 13 Mining Numbers in Text: A Survey DOI: http://dx.doi.org/10.5772/intechopen.98540 Models for Hidden Unit Estimation"},{"paperId":"d70aedeafc16123085b1ac08b4b94288222942d6","title":"Estimating Numerical Attributes by Bringing Together Fragmentary Clues"},{"paperId":"d89d4a1fc9ce7003cbc18ca835860c114e78d5ae","title":"Syntactic Difference Based Approach for NTCIR-9 RITE Task"},{"paperId":"97cd342bee2377756f003eec0508f0a710225012","title":"UAIC Participation at RTE-7"},{"paperId":"47790162c24c6bfbd856d0398365a338848a9a6d","title":"UAIC Participation at RTE4"}],"id":"40a1f266bb5ca853837355bfff272a55f0049c81","summary":"A quick overview of the history and recent advances of the research of mining such relations between numerals and words found in text data is provided."},{"url":"https://www.semanticscholar.org/paper/24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context","venue":"NAACL-HLT","year":2021,"referenceCount":33,"citationCount":5,"influentialCitationCount":1,"publicationDate":"16/12/2021","authors":"Daniel M. Spokoyny,Ivan Lee,Zhao Jin,Taylor Berg-Kirkpatrick","citations":[{"paperId":"5dc15ac1c92ab7492f121471823fb13a95d273ba","title":"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis"},{"paperId":"be8f769f135b0ffcf829594b9421788781f38008","title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","title":"Numerical Correlation in Text"},{"paperId":"5df55d94ab5026ff84ec01871592108fbadbddbe","title":"Measurement Extraction with Natural Language Processing: A Review"}],"references":[{"paperId":"7aacad59c63520b234923fc25eb14146c788f3f9","title":"Solving Price Per Unit Problem Around the World: Formulating Fact Extraction as Question Answering"},{"paperId":"3ceca3cc39cd90515ccd73afe6539cc953762140","title":"How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"58df3ea12952d35980ce2f7145c8d4669be343a5","title":"Learning to Reason for Text Generation from Scientific Tables"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"99f1898b9466fcf3f13061402a6ac70bb9f42c10","title":"Qsearch: Answering Quantity Queries from Text"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96","title":"TabFact: A Large-scale Dataset for Table-based Fact Verification"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"d34e6c84119cef8eb9149a27d6b4903131407ea6","title":"How Large Are Lions? Inducing Distributions over Quantitative Attributes"},{"paperId":"f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1","title":"Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"c825b92effcf400ab1679071cef8233a5a6a18ba","title":"How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"bc51700cfae5ad693530331875b04ab4b62fc03e","title":"Open-domain quantity queries on web tables: annotation, response, and consensus models"},{"paperId":"a6109618caa42b7318884b5418cec4c8c06a96fd","title":"“Ask Not What Textual Entailment Can Do for You...”"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":null,"title":"Reasoning about quantities in nat - 575 ural language “ ask not what textual entailment can 579 do for you . . . ”"},{"paperId":null,"title":"2021a), the units in WiCo are heavily biased"},{"paperId":null,"title":"2021a) constructed WiCo with the intent that it be used to further numeracy NLP research"}],"id":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","summary":"This work introduces a novel task, Masked Measurement Prediction (MMP), where a model learns to reconstruct a number 010 together with its associated unit given masked 011 text and introduces a new GeMM model that jointly learns to predict numbers along with 017 their units."},{"url":"https://www.semanticscholar.org/paper/320c1c6647a5b975c901347f71638c881888686b","title":"NQuAD: 70,000+ Questions for Machine Comprehension of the Numerals in Text","venue":"International Conference on Information and Knowledge Management","year":2021,"referenceCount":30,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Chung-Chi Chen,Hen-Hsen Huang,Hsin-Hsi Chen","citations":[{"paperId":"712f6dfcee099ee38d6d09af23e8bc0a7e82bb72","title":"Fake Alignment: Are LLMs Really Aligned Well?"},{"paperId":"d1cbec486a54e846107db42eb6a3a89188c8b06a","title":"Improving Numeracy by Input Reframing and Quantitative Pre-Finetuning Task"}],"references":[{"paperId":"01f68682c88c23376dcbf194c846232cba5f28a6","title":"Distilling Numeral Information for Volatility Forecasting"},{"paperId":"6563251e69e4378c189d0a0c94d8d19508d552c8","title":"MathBERT: A Pre-Trained Model for Mathematical Formula Understanding"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"0772212420238b48233256a59912f8e6a31cde3d","title":"NumClaim: Investor's Fine-grained Claim Detection"},{"paperId":"50b9efabca6e7f5d7885f5ee69ad593bdebcf98b","title":"Data-Driven Market-Making via Model-Free Learning"},{"paperId":"ac67ec5b985a30239926c3362fa45a9a03f017af","title":"Learning to Generate Correct Numeric Values in News Headlines"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"2a0fc534ebf1b22b5851f5b1fb781dd707f7c530","title":"Numeral Attachment with Auxiliary Tasks"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"bab85059937299a18919b6580feee2c4398e5665","title":"Bridging Quantities in Tables and Text"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting"},{"paperId":"2be63d6b7dc137aa202907a471243e91deab258d","title":"Textual Analogy Parsing: What’s Shared and What’s Compared among Analogous Facts"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"d3406f220c16a20b877f10f5609fa589b629cc91","title":"Leveraging Context Information for Natural Question Generation"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"88bb0a28bb58d847183ec505dda89b63771bb495","title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"},{"paperId":"eedc76b6754e3cc17373dfac604e82ab385460a9","title":"MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge"},{"paperId":"eac21b22b2bba3a2311820c3f98702fa1d380ad5","title":"Question Generation for Question Answering"},{"paperId":"636a79420d838eabe4af7fb25d6437de45ab64e8","title":"RACE: Large-scale ReAding Comprehension Dataset From Examinations"},{"paperId":"564257469fa44cdb57e4272f85253efb9acfd69d","title":"MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text"},{"paperId":"b68bb98aa5075934d92b1d34387be31e2449faf8","title":"From Opinion Mining to Financial Argument Mining"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"2db6b8e36bb5a3594b2200b2d4f931268d6279db","title":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"},{"paperId":"8ce1512e77fa6a646513a60d78e0081afe870c07","title":"Dataset for the First Evaluation on Chinese Machine Reading Comprehension"},{"paperId":null,"title":"Join Mail List for Latest News  Feel free to contact us if you have any questions"}],"id":"320c1c6647a5b975c901347f71638c881888686b","summary":"A Numeral-related Question Answering Dataset, NQuAD, for fine-grained numeracy, is presented, and several baselines for future works are proposed and it is shown that N QuAD is more challenging than the numeral- related questions in other datasets."},{"url":"https://www.semanticscholar.org/paper/31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":20,"citationCount":18,"influentialCitationCount":1,"publicationDate":2021,"authors":"Avijit Thawani,J. Pujara,F. Ilievski","citations":[{"paperId":"18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6","title":"Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception"},{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"174a9b78350e9561555052bc6901cc44782f4c62","title":"Estimating Numbers without Regression"},{"paperId":"d96e1c2a0b66effa2b00ef117c0df3fcb2cd9928","title":"Comparing scalable strategies for generating numerical perspectives"},{"paperId":"fc097b3dfa6b8dfbcf7275e0014a2a9e39704b2d","title":"Image-to-Text Translation for Interactive Image Recognition: A Comparative User Study with Non-Expert Users"},{"paperId":"be8f769f135b0ffcf829594b9421788781f38008","title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts"},{"paperId":"1f84b6075ffa2d4ed685f31668ea1324bfba4fe5","title":"A Survey on Medical Document Summarization"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"efcd9c1438559d908efd702333232078fd251a0f","title":"Phenotyping in clinical text with unsupervised numerical reasoning for patient stratification"},{"paperId":"cbccb1201eee6432020276762a44ebfbf8f981a0","title":"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"4ef578869c957f5fb969fa9c164dca0433f48042","title":"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining"},{"paperId":"c77feebb06b530079a34f04c64de359ae28c0a32","title":"Predicting Numerals in Text Using Nearest Neighbor Language Models"},{"paperId":"c97bcf9e605938c2bef54f12b2156c678f1a2373","title":"Introducing Semantic Information for Numerical Attribute Prediction over Knowledge Graphs"},{"paperId":"ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","title":"Numerical Correlation in Text"},{"paperId":"5df55d94ab5026ff84ec01871592108fbadbddbe","title":"Measurement Extraction with Natural Language Processing: A Review"},{"paperId":"b16d916ce411be0b1c7b6139317c652927984f46","title":"Tokenization on the Number Line is All You Need"}],"references":[{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"d5fcad8a3b183642fcf609519a4dbbda9c3541ff","title":"Learning Numeral Embedding"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"70c0bbece96ab013050e10ece778ef857858bde6","title":"Wikicorpus: A Word-Sense Disambiguated Multilingual Wikipedia Corpus"},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"09437cba3abec8d7befd5358c0049281eb2afd69","title":"Dataset for Evaluation of Mathematical Reasoning Abilities in Russian"},{"paperId":null,"title":"Pytorch lightning"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"English gigaword. Linguistic Data Consortium"},{"paperId":null,"title":"of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages"}],"id":"31699d03a49e38295298f1b1a53185644abba12e","summary":"A significant improvement in MWP for sentences containing numbers is found, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers."},{"url":"https://www.semanticscholar.org/paper/b16d916ce411be0b1c7b6139317c652927984f46","title":"Tokenization on the Number Line is All You Need","venue":"","year":2021,"referenceCount":10,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"","citations":[],"references":[{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":null,"title":"Japanese 346 and korean voice search"},{"paperId":null,"title":"320 Injecting numerical reasoning skills into language 321 models How much coffee was consumed during 328 EMNLP 2019 ? fermi problems : A new reason - 329 ing challenge for AI"},{"paperId":null,"title":"BERT : Pre - training of 305 deep bidirectional transformers for language under - 306 standing"},{"paperId":null,"title":"Szekely , and Filip 363 Ilievski . 2021 b . Representing numbers in NLP : a 364 survey and a vision"},{"paperId":null,"title":"Have 336 you seen that number ? investigating extrapolation 337 in question answering models"},{"paperId":null,"title":"2019. BERT: Pre-training"},{"paperId":null,"title":"Neural machine translation of rare words 351 with subword units"},{"paperId":null,"title":"The number sense: How the"},{"paperId":null,"title":"2020 . 296 An empirical investigation of contextualized number 297 prediction"}],"id":"b16d916ce411be0b1c7b6139317c652927984f46","summary":"A carefully designed tokenization scheme is found to be both the simplest to implement and sufﬁcient to implement, and changes at the 032 tokenization level achieve near state-of-the-art results while requiring minimal resources compared to other number representation schemes."},{"url":"https://www.semanticscholar.org/paper/c2d50f17ea6769f6f5663ccac37a9627a0543184","title":"I NVESTIGATING THE L IMITATIONS OF T RANSFORM ERS WITH S IMPLE A RITHMETIC T ASKS","venue":"","year":null,"referenceCount":48,"citationCount":62,"influentialCitationCount":0,"publicationDate":null,"authors":"","citations":[{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"4726d1dc54851db99c29180127d840bd19f20afc","title":"Positional Description Matters for Transformers Arithmetic"},{"paperId":"dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills"},{"paperId":"1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b","title":"What Algorithms can Transformers Learn? A Study in Length Generalization"},{"paperId":"200fcd964f41efe0c35a3f888a520ede08a3269c","title":"From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers"},{"paperId":"174a9b78350e9561555052bc6901cc44782f4c62","title":"Estimating Numbers without Regression"},{"paperId":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners"},{"paperId":"0b778079946764292de3771a489d5ce9e1868a8b","title":"The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models"},{"paperId":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator"},{"paperId":"cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6","title":"Can transformers learn the greatest common divisor?"},{"paperId":"edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a","title":"It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models"},{"paperId":"9160673f89ecb326db8c64f8d09ab30f35a74e9d","title":"Arithmetic with Language Models: from Memorization to Computation"},{"paperId":"72a9187b489992cad3d54420611d5039eb6b9d86","title":"One Blade for One Purpose: Advancing Math Information Retrieval using Hybrid Search"},{"paperId":"8f1b0c247171510fc68da27a16a377456376a5a7","title":"Assessing GPT-3.5 and GPT-4 in Generating International Classification of Diseases Billing Codes"},{"paperId":"0db0af0cd3ceb0531a050a03e6ceb849580ff53b","title":"Teaching Arithmetic to Small Transformers"},{"paperId":"be3a74f9f6010889d049060eab2a4b09eb48bbfb","title":"The Value of Numbers in Clinical Text Classification"},{"paperId":"49d340b7d6108ce5ef2277a165ea83f254671763","title":"The use of weather nowcasting convolutional neural network extrapolators in cardiac PET imaging"},{"paperId":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers"},{"paperId":"b43383f10634f7e610f22badd4f42c93e5dcb947","title":"Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI"},{"paperId":"3e7a0dc5795dc108c78993bcf3624fc626a9f9cf","title":"Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks"},{"paperId":"d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","title":"Exposing Attention Glitches with Flip-Flop Language Modeling"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62","title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering"},{"paperId":"aec826ff336ca442697d5f908ab1668f1ea18987","title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"1a174b63d294f96568517b91f2c8d6c9362118b5","title":"Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems"},{"paperId":"1f036b092a74f0c2f7eef37daa16eeb0f5954d9b","title":"Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)"},{"paperId":"49e46e615747f258517248de5a736814fada17ee","title":"What is my math transformer doing? - Three results on interpretability and generalization"},{"paperId":"e82e3f4347674b75c432cb80604d38ee630d4bf6","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"2a7ae3e98357569c41424dacd60c62d3df78a0db","title":"Limitations of Language Models in Arithmetic and Symbolic Induction"},{"paperId":"40f73969cd3415e9c1b03796cbb5a50c79ebd448","title":"SALSA: Attacking Lattice Cryptography with Transformers"},{"paperId":"b09420b30fc093d63fb2ee1aac26c71da81da437","title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"},{"paperId":"d2018d1b0f69ca6805aa18a22be95cab9b5c44c7","title":"On Neural Architecture Inductive Biases for Relational Tasks"},{"paperId":"4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80","title":"Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence"},{"paperId":"7a25155364476839b6d1fc0653cd8611327ab9ba","title":"mGPT: Few-Shot Learners Go Multilingual"},{"paperId":"edd80013e8b9fcba9231cf99884f32e5236ff329","title":"AA-TransUNet: Attention Augmented TransUNet For Nowcasting Tasks"},{"paperId":"45ece6f3b0a319dba60c20b3013b5161dd49c58b","title":"Linear algebra with transformers"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"1d6ca8d8401e14bbb764d35ba13a9505af0dcc2f","title":"Controlling Neural Network Smoothness for Neural Algorithmic Reasoning"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"6c84fc8c5ec823342f34a020f8b92b7064e96ca2","title":"Towards Efficient and Robust Out-of-Distribution Deep Learning with Implicit Models"},{"paperId":"4f451ba06c4c9effd6c4ac0bae222495501a6200","title":"Innovations in Neural Data-to-text Generation"},{"paperId":"58d5e4cbb5f3a944bae0bcc2bff93f06696f8196","title":"JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection"},{"paperId":"c27f3904490b8e5f9f39fe2b36722090c189e916","title":"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF"},{"paperId":"346accd49d1359aa3985c7b298bc2057ae642271","title":"One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development"},{"paperId":"f8a36a6c10b4f598f7a936f09c58de31c9e10bcd","title":"Controlling Neural Network Smoothness for Algorithmic Neural Reasoning"},{"paperId":"33149f835f391119d287cc2c6b009464e7d14fe4","title":"On the Abilities of Mathematical Extrapolation with Implicit Models"},{"paperId":"5df55d94ab5026ff84ec01871592108fbadbddbe","title":"Measurement Extraction with Natural Language Processing: A Review"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"}],"references":[{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"4383a975c09b72ba2f1a77cd779bb6965dbfb2fb","title":"Scaling Laws for Transfer"},{"paperId":"02791e807dc9a91f854a1f3d5f6005122a546109","title":"Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"45cdda3f96ae32927c7b073ebbedf46f5e0fbdc5","title":"Enhancing the Numeracy of Word Embeddings: A Linear Algebraic Perspective"},{"paperId":"0c7f81e26ae77b4be5e5e6ab96641effe219ab1c","title":"Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"},{"paperId":"84476fdf6ead3553f4493dff8e02308439d6222b","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"8256f48f759cf85044db251cc512f965834945b3","title":"Rethinking Positional Encoding in Language Pre-training"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"72e6b14c081ad3e6a1c295b60e5c834837e6b304","title":"Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"ac67ec5b985a30239926c3362fa45a9a03f017af","title":"Learning to Generate Correct Numeric Values in News Headlines"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"d0e28f5dc1feae19e41087a92a87992977fd85af","title":"Encoding word order in complex embeddings"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"136396e988dbbdaca433096e931b22732c3d95dd","title":"Text2Math: End-to-end Parsing Text into Math Expressions"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"5435f997d98754f68492334eeb87d027047e60cb","title":"Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"0bb487b9cfefd56e79be0a5be5f1e05742683301","title":"Extensions and Limitations of the Neural GPU"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"5b791cd374c7109693aaddee2c12d659ae4e3ec0","title":"Grid Long Short-Term Memory"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"111fd833a4ae576cfdbb27d87d2f8fc0640af355","title":"Learning internal representations by error propagation"},{"paperId":"5414be4432fcfe1e83932127ee2463d107e5b61c","title":"A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":null,"title":"Character-level models such as ELMO"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"The architecture of the transformer"}],"id":"c2d50f17ea6769f6f5663ccac37a9627a0543184","summary":"It is concluded that modern pretrained language models can easily learn arithmetic from very few examples, as long as they use the proper surface representation, which bolsters evidence that subword tokenizers and positional encodings are compo-nents in current transformer designs that might need improvement."},{"url":"https://www.semanticscholar.org/paper/108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models","venue":"MATHNLP","year":2022,"referenceCount":66,"citationCount":8,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"M. Bueno,Carlos Gemmel,Jeffrey Stephen Dalton,R. Lotufo,Rodrigo Nogueira","citations":[{"paperId":"dc48bc1a4d81e0f37603013fd2a95644dc233bd0","title":"Functional Interpolation for Relative Positions Improves Long Context Transformers"},{"paperId":"e3aa232577bb427b1f3a34acbdef84bd85734042","title":"LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"},{"paperId":"c92516b47444b2b18108b8a6e5785d263173789d","title":"ExaRanker: Synthetic Explanations Improve Neural Rankers"},{"paperId":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers"},{"paperId":"6f6e2e0311589a9af045f6acd00b7dee6d19fce4","title":"The Impact of Positional Encoding on Length Generalization in Transformers"},{"paperId":"7d97c17a75beb89f938eaac1d3ca60ac2245fb2e","title":"Faith and Fate: Limits of Transformers on Compositionality"},{"paperId":"80b99ed94e991a941ffeabf74826a9ee8c995d6d","title":"Generate, Transform, Answer: Question Specific Tool Synthesis for Tabular Data"},{"paperId":"988ca520327c2ead08d29c4907130b8a8833d769","title":"ExaRanker: Explanation-Augmented Neural Ranker"}],"references":[{"paperId":"07955e96cbd778d0ae2a68f09d073b866dd84c2a","title":"Decomposed Prompting: A Modular Approach for Solving Complex Tasks"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"37e2c7c89325a1a4685a46ff830fe7ecca8f1f80","title":"Learning to Scaffold: Optimizing Model Explanations for Teaching"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"23dd78e424d32f6a48660dcd67ce994b8a7db8be","title":"STaR: Bootstrapping Reasoning With Reasoning"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"0f2199296f01694ee46b6059879260fb80a84fa6","title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration"},{"paperId":"9ca329408813d209b1dcb36936f7f9cba82506bd","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"10c86505de83647c7b4157595ab10f64e97c94ef","title":"On the Ability and Limitations of Transformers to Recognize Formal Languages"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"e8984c6e6c24aab26c332728a5fff616dfb3adbb","title":"Learning to Encode Position for Transformer with Continuous Dynamical Model"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"136396e988dbbdaca433096e931b22732c3d95dd","title":"Text2Math: End-to-end Parsing Text into Math Expressions"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"5435f997d98754f68492334eeb87d027047e60cb","title":"Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"5e2bb96c47ccaa16a4e7192e8fadb3b3e1c3acdc","title":"Deep Learning: A Critical Appraisal"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7","title":"Modular Multitask Reinforcement Learning with Policy Sketches"},{"paperId":"0bb487b9cfefd56e79be0a5be5f1e05742683301","title":"Extensions and Limitations of the Neural GPU"},{"paperId":"98ea4abc9bf0e30eb020db2075c9c8a039a848a3","title":"Learning to Compose Neural Networks for Question Answering"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"5b791cd374c7109693aaddee2c12d659ae4e3ec0","title":"Grid Long Short-Term Memory"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"08dc7b19e679539f0f93db0192a8e8d11538b3dd","title":"Rethinking Eliminative Connectionism"},{"paperId":"fc5f5fe38d0188a94c11aec228cc5e756120589c","title":"Have you seen this number?"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":"8e9b51c87fab57313b6b30ed9b08b02a7abb5031","title":"Inferring Implicit Relations with Language Models"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":null,"title":"and Berant"},{"paperId":"5414be4432fcfe1e83932127ee2463d107e5b61c","title":"A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":null,"title":"and Sutskever"},{"paperId":null,"title":"and Mikolov"},{"paperId":"087d5ea60b0b15fb9d3396ad321a7f941f88e720","title":"Transfer of learning by composing solutions of elemental sequential tasks"},{"paperId":"30110856f45fde473f1903f686aa365cf70ed4c7","title":"Learning Context-free Grammars: Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory (cid:3)"},{"paperId":"550e406b84e46cf8c77baa70a61704e93fd963bc","title":"Towards Compositional Learning in Dynamic NetworksTechnical Report"}],"id":"108c25905be36b2a7a0fc7256ac314985ecd9699","summary":"This work demonstrates that large language models can succeed in extrapolation without modifying their architecture or training procedure, and shows how generating step-by-step rationales and introducing marker tokens are both required for effective extrapolation."},{"url":"https://www.semanticscholar.org/paper/c97bcf9e605938c2bef54f12b2156c678f1a2373","title":"Introducing Semantic Information for Numerical Attribute Prediction over Knowledge Graphs","venue":"International Workshop on the Semantic Web","year":2022,"referenceCount":53,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Bingcong Xue,Yanzeng Li,Lei Zou","citations":[{"paperId":"3a7e223cf580b61fdc5a92b3c34e91ca5696ab0c","title":"Attribute Enhancement using Aligned Entities between Knowledge Graphs"},{"paperId":"68684537f964262d207ba8454d7e35e1a9b5c52e","title":"AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction"}],"references":[{"paperId":"e3034935bf12190465f7dbee1c05d74abbbd7767","title":"End-to-End Learning on Multimodal Knowledge Graphs"},{"paperId":"ae5b587fb5ff55c074a770acf81c27d9d3046748","title":"Knowledge Graph Quality Management: A Comprehensive Survey"},{"paperId":"2a561c9650d27218054aa2c87474f0121ba3f33b","title":"SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models"},{"paperId":"e468efa59a621f49709ded07ba18ca7751ab91e5","title":"PGE: Robust Product Graph Embedding Learning for Error Detection"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"c9343c26a0e604f7afd94b7290bbdf8d96cd65b6","title":"Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"75f1c7dadb4ed733fb4d3a4cc47b9cbde9ad98cc","title":"Combining pre-trained language models and structured knowledge"},{"paperId":"b1e6aa78db5478be5eaa47697382241c2b7aab1f","title":"Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models"},{"paperId":"a9027974b23e5208baabd3f9d78fbc10e9e3c395","title":"Node Attribute Completion in Knowledge Graphs with Multi-Relational Propagation"},{"paperId":"5b2e955de772ae4d128b8a78ebff57982a6ca06d","title":"Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83","title":"LibKGE - A knowledge graph embedding library for reproducible research"},{"paperId":"7b12198a77972508689a904c544010b2a406ed20","title":"YAGO 4: A Reason-able Knowledge Base"},{"paperId":"5bb28ae9adf604c5acc56fd51d0be3ea392048ce","title":"The Value of Paraphrase for Knowledge Base Predicates"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"80376bdec5f534be78ba82821f540590ebce5559","title":"How Much Knowledge Can You Pack into the Parameters of a Language Model?"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"cda5af1f5516a6920f818714ec570952101afe07","title":"A Survey on Knowledge Graph Embeddings with Literals: Which model links better Literal-ly?"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"bfeb827d06c1a3583b5cc6d25241203a81f6af09","title":"Knowledge Enhanced Contextual Word Representations"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"e1b41796b752d6fb6032bbad2f8998302209d79b","title":"A survey on ensemble learning"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"73f303a28d4abcd48593274a90aeecb05085fa52","title":"Learning Numerical Attributes in Knowledge Bases"},{"paperId":"d593a5830a7e7d84443473c3912b59165056d45a","title":"MMKG: Multi-Modal Knowledge Graphs"},{"paperId":"8f096071a09701012c9c279aee2a88143a295935","title":"RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space"},{"paperId":"bca4a782116e663dfd0119b6176a3c228c651bda","title":"Embedding Multimodal Relational Data for Knowledge Base Completion"},{"paperId":"71245f9d9ba0317f78151698dc1ddba7583a3afd","title":"Knowledge Graph Embedding with Numeric Attributes of Entities"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"3448e6a5039417dc1ae890efeca3bef5390ace7c","title":"xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems"},{"paperId":"995f33b2359966dd364d2cf4bcf5e8d339cfecce","title":"Incorporating Literals into Knowledge Graph Embeddings"},{"paperId":"c14347fa745a1f113fdbe8bf1c5ccfb71b5da296","title":"KBlrn: End-to-End Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features"},{"paperId":"729fbc6664890c4e78cbcb2686a9b5e895255485","title":"Multi-Task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs"},{"paperId":"2218e2e1df2c3adfb70e0def2e326a39928aacfc","title":"Complex Embeddings for Simple Link Prediction"},{"paperId":"96acb1c882ad655c6b8459c2cd331803801446ca","title":"Representation Learning of Knowledge Graphs with Entity Descriptions"},{"paperId":"adcfebbe2a1960e5a23243d1a5f3837832109ff1","title":"Distributional vectors encode referential attributes"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","title":"Wikidata: a free collaborative knowledgebase"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"f6764d853a14b0c34df1d2283e76277aead40fde","title":"A Three-Way Model for Collective Learning on Multi-Relational Data"},{"paperId":"168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74","title":"Scikit-learn: Machine Learning in Python"},{"paperId":"6c93b11ce414776553ed5cb9eb5d3cffa2a5fc6b","title":"Extraction and Approximation of Numerical Attributes from the Web"},{"paperId":"8c93f3cecf79bd9f8d021f589d095305e281dd2f","title":"Knowledge Graph Embedding for Link Prediction: A Comparative Analysis"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":"9aca07d7688e87e996c4a117fc28e200e469b820","title":"Leveraging Literals for Knowledge Graph Embeddings"},{"paperId":"d0d9a32181e3e4b68e7c32d9ffbef86c12d9609b","title":"Leveraging Multilingual Descriptions for Link Prediction: Initial Experiments"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"d2946a868682e4141beabc288d79253ae254c6e1","title":"DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia"},{"paperId":null,"title":"Do language embeddings capture scales? arXiv preprint arXiv:2010.05345"}],"id":"c97bcf9e605938c2bef54f12b2156c678f1a2373","summary":"This paper re-examines the numerical attribute prediction task over KGs, and introduces several novel methods to explore and utilize the rich semantic knowledge of language models (LMs) for this task."},{"url":"https://www.semanticscholar.org/paper/833a2f1817cb9aeb292620454889cae78e26dda4","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":110,"influentialCitationCount":4,"publicationDate":"15/02/2022","authors":"Yasaman Razeghi,Robert L Logan IV,Matt Gardner,Sameer Singh","citations":[{"paperId":"b23894806efcc29cb145db2df3767f4a4e6057d8","title":"GPT for RCTs?: Using AI to measure adherence to reporting guidelines"},{"paperId":"2878d5d0922ff55f22e2e6cad3e8036eed072809","title":"Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games"},{"paperId":"11e56b35fa81ddb00dc96329c077c5b39c8f9e75","title":"A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia"},{"paperId":"d2bf6100142592974ab43a7b0ea7ceac1453d82d","title":"How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks"},{"paperId":"4dc74f6a95f94d44ca0cf629634b511581710957","title":"ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification"},{"paperId":"6d68b5c1eaf03aba857476a9825acf3e48edd840","title":"Hijacking Large Language Models via Adversarial In-Context Learning"},{"paperId":"422e4b666cbd4dd287a458b8b559cd2426a4f60d","title":"Safer-Instruct: Aligning Language Models with Automated Preference Data"},{"paperId":"31282fc5a46f4410450bb4324d47aad0a37d4a36","title":"In Search of the Long-Tail: Systematic Generation of Long-Tail Knowledge via Logical Rule Guided Search"},{"paperId":"02186caef0f02305f85ceaf188f2ed8773e39217","title":"A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models"},{"paperId":"f44c1d2f1764bbf186c95af1b3b5a624b9801d18","title":"Characterizing Mechanisms for Factual Recall in Language Models"},{"paperId":"ef9df346acb04d97fdeb3d0046d8920e54b02351","title":"How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations"},{"paperId":"bd2bbaa226be8fe6564e878e262942dd54c4fdad","title":"Generative Calibration for In-context Learning"},{"paperId":"a44dd81e42c690f6b0fe86f6142722491ae36278","title":"Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning"},{"paperId":"a9d460f8eb9001b1bed11b7fb2af555185c70fcf","title":"Do pretrained Transformers Really Learn In-context by Gradient Descent?"},{"paperId":"174a9b78350e9561555052bc6901cc44782f4c62","title":"Estimating Numbers without Regression"},{"paperId":"d4c1517ca5e550ce43515b54e475082fba80bd56","title":"Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions"},{"paperId":"0f3868e735c727e5b2096667544b50b7d9762853","title":"Improving web element localization by using a large language model"},{"paperId":"19d79592cee5d8df68f833280b991b6a81b8cc91","title":"A taxonomy and review of generalization research in NLP"},{"paperId":"d3ca116177369bf6fbe27de64506a2f401aca996","title":"Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency"},{"paperId":"50e8ab900d2ca4d83da120bbfe5338ee93dbe741","title":"Generative Speech Recognition Error Correction with Large Language Models and Task-Activating Prompting"},{"paperId":"460fc460695bce86e66da0f1e4f7a7a7a3b1481a","title":"GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models"},{"paperId":"b5baedd5b7c270903e6861bebbfda81b10d59419","title":"In-Context Learning for Text Classification with Many Labels"},{"paperId":"e3aa232577bb427b1f3a34acbdef84bd85734042","title":"LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"},{"paperId":"d6d0fd994f37b630f35945736b5e1bb198148404","title":"Time Travel in LLMs: Tracing Data Contamination in Large Language Models"},{"paperId":"cf7368f38cc1f0861d4b35db1307776c7f3f237d","title":"In-Context Learning Learns Label Relationships but Is Not Conventional Learning"},{"paperId":"8f1b0c247171510fc68da27a16a377456376a5a7","title":"Assessing GPT-3.5 and GPT-4 in Generating International Classification of Diseases Billing Codes"},{"paperId":"0db0af0cd3ceb0531a050a03e6ceb849580ff53b","title":"Teaching Arithmetic to Small Transformers"},{"paperId":"5bac7d00035bc1e246a34f9ee3152b290f97bb92","title":"Supervised Pretraining Can Learn In-Context Reinforcement Learning"},{"paperId":"36f7bc27c9a37eb337c35df4ae86f148e13d4e9a","title":"Understanding In-Context Learning via Supportive Pretraining Data"},{"paperId":"a4c0144062d8e36485bad438968894cbf49ab998","title":"Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding"},{"paperId":"f4d543ff431359947bf41152ac01233b8062221f","title":"In-Context Learning through the Bayesian Prism"},{"paperId":"d8244cf970b9019cfb7f813275eba10d17c47166","title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks"},{"paperId":"11ae58636a5daf0ea1297f1c4ee94042fcebefa8","title":"Birth of a Transformer: A Memory Viewpoint"},{"paperId":"62729cff7dda7614f648a84e8967076d8878a5ff","title":"ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing"},{"paperId":"28a7ced384549eaae74ea9ad3ee21189a0625afe","title":"Mitigating Label Biases for In-context Learning"},{"paperId":"5412e54947f6574095174d7b85da67a5bfba4e46","title":"A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification"},{"paperId":"fe425e341cf646689e42adead17f14eeac5d03e6","title":"Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations"},{"paperId":"3fb0731538c59f8520a309996a0567b58965f0fe","title":"Pre-Training to Learn in Context"},{"paperId":"393262e1345e71fbd6eb454a4f76eecf2da6634f","title":"Can Publicly Available Models Understand NATO Language? A Named Entity Recognition Case Study"},{"paperId":"9e1ba67d5f443a8bd42a8b856534f50c429baf11","title":"Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency"},{"paperId":"a33daa434b0adcc2cefe5293765f39e4c5b673ed","title":"Towards Mode Balancing of Generative Models via Diversity Weights"},{"paperId":"70ece7b4ba8f3b67f5a797daed544fb6a0b627bf","title":"A Latent Space Theory for Emergent Abilities in Large Language Models"},{"paperId":"bebede105aa69a81045bf79682272ee4cfb61475","title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"27f32dc1aab7919eb7039deca067c3fbdc719c2a","title":"Koala: An Index for Quantifying Overlaps with Pre-training Corpora"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"b6207fe49e29c77402f8dbab052e949990949609","title":"In-context Example Selection with Influences"},{"paperId":"74013b7cfa0fc524803350fca51341004565eb22","title":"Data Selection for Language Models via Importance Resampling"},{"paperId":"06edda0310b4ec7c5012d012349252a3a77521b6","title":"Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners"},{"paperId":"c6ee979c2da4b55a8486abae4cd720422ab09b26","title":"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","title":"Large Language Models Are Reasoning Teachers"},{"paperId":"db4ab91d5675c37795e719e997a2827d3d83cd45","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"35922cd0d6b17e45320917338e9f98cb5c1a4f6f","title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters"},{"paperId":"0942bd8fad71282994ff4e9a779c09745da68edc","title":"Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations"},{"paperId":"34bc28087e1d6f047e2736791f79d769293f447c","title":"Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale"},{"paperId":"ad5573cb25fd403f7620332f363ae87327c69a49","title":"The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning"},{"paperId":"eecb45aa040064cbc0b37fd100706c02e7dc880e","title":"Structured Prompting: Scaling In-Context Learning to 1, 000 Examples"},{"paperId":"7d5175db1b99552491063d2d9581b0b51e1d2932","title":"Despite \"super-human\" performance, current LLMs are unsuited for decisions about ethics and safety"},{"paperId":"d03a9b2a0e090cc9fd2ba0a457ecea35372f1018","title":"Demystifying Prompts in Language Models via Perplexity Estimation"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","title":"Galactica: A Large Language Model for Science"},{"paperId":"75f7e9e2b59fb640ef9d1dff94097175daf46c4d","title":"Large Language Models Struggle to Learn Long-Tail Knowledge"},{"paperId":"0a67a5e3f4125445ed84f2db3c92429010aad68a","title":"Prompting Language Models for Linguistic Structure"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"2fa6dbd2f147b6fa34d22c80aa225cc2cad0d96e","title":"Complex Reading Comprehension Through Question Decomposition"},{"paperId":"0cf5c61f367e3937dc07be634a43952a50b589a4","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"e3cd9f01f87a601b274b4ef6513a84c8cde03214","title":"Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning"},{"paperId":"990026128083f5a47b061f6237b8135b2d3a41a9","title":"Don’t Prompt, Search! Mining-based Zero-Shot Learning with Language Models"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"0c56a799e47ffa3fc96a6e85f53e992f1df41381","title":"Rationale Aware Contrastive Learning Based Approach to Classify and Summarize Crisis-Related Microblogs"},{"paperId":"4bdcfb8bc450987c4f91c1cb6598fab04a96cf5c","title":"Towards relation extraction from speech"},{"paperId":"c84bb36ab145cf903c1ad404008e0250f688c162","title":"Behavior Cloned Transformers are Neurosymbolic Reasoners"},{"paperId":"49aec6fb44ab52181960512a6067eded0ce4182b","title":"Benchmarking Long-tail Generalization with Likelihood Splits"},{"paperId":"2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8","title":"Mind's Eye: Grounded Language Model Reasoning through Simulation"},{"paperId":"712f21411526e8450036d7199637808590be3579","title":"Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"48c487ccae7fc6085b7edde4d135040f58d79a5d","title":"ChemAlgebra: Algebraic Reasoning on Chemical Reactions"},{"paperId":"e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a","title":"Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought"},{"paperId":"a5378175d31d3dd8fa004037df663aa00f236a0b","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks"},{"paperId":"7ce0c89a452e3c2917b63847495533865697c79c","title":"Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts"},{"paperId":"de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9","title":"What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"},{"paperId":"60240322dd39ad4c22fed2dc884e65a20e9e61f6","title":"Symbols and mental programs: a hypothesis about human singularity"},{"paperId":"4721dd8c4f2681e231040cc5deebdbf938f58392","title":"Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions"},{"paperId":"290732e9fb08a29af8892a7c1f73c9d2a1b9d7db","title":"Language models show human-like content effects on reasoning"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7","title":"The Curious Case of Control"},{"paperId":"6a483cd1cbecd66150c9bbcd01606723950281bc","title":"Prototypical Calibration for Few-shot Learning of Language Models"},{"paperId":"146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd","title":"Data Distributional Properties Drive Emergent In-Context Learning in Transformers"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"c37d0b258386293097fa3f71f971dc5dfceb4684","title":"Data Contamination: From Memorization to Exploitation"},{"paperId":"cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e","title":"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models"},{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"8a920123a7c435e60b0912b6a49de40bfefd967f","title":"ClueReader: Heterogeneous Graph Attention Network for Multi-Hop Machine Reading Comprehension"},{"paperId":"9c6cecf409ca6257e717291b47c2cf8b75cf4a58","title":"Learning Multi-step Reasoning from Arithmetic Task"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"79dd9e62f0e3ba35ce54d98b32750933e91a972a","title":"Representations and Computations in Transformers that Support Generalization on Structured Tasks"},{"paperId":null,"title":"MIND’S EYE: GROUNDED LANGUAGE MODEL REA-"},{"paperId":"ca0dcc09e69732b7fc582d8a610488f86ad07e36","title":"In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning"},{"paperId":"414a476f83634e3b452b243ed7460c9ef3d1aaa4","title":"Benchmarking Progress to Infant-Level Physical Reasoning in AI"},{"paperId":"a2087860d3a8948ba4b8290f8c9394eb59ee899e","title":"A Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension"},{"paperId":"a8394722079c251d360fbbc53137753849a64c63","title":"Careful Data Curation Stabilizes In-context Learning"},{"paperId":"5c8cb5d39aa8531fcb5962821ac404e4a0e109d1","title":"Is It Smaller Than a Tennis Ball? Language Models Play the Game of Twenty Questions"},{"paperId":"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","title":"20Q: Overlap-Free World Knowledge Benchmark for Language Models"},{"paperId":"7b0f98f51040700aae3cd9f0e3432dedcd69fb30","title":"When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories"},{"paperId":"0b18c6f168e1a6d2f16eaa317747748c5c4c5b63","title":"Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers"},{"paperId":"cb15518b8a848dc68d1c5fab02414d205ccdcb67","title":"Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers"},{"paperId":"d652ccccca9f9dc653a9b3b5bae7aa67df9fcd83","title":"Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers"}],"references":[{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"39c77e29a232a9fb62b3a3c89c50f487d73e27ce","title":"Counterfactual Memorization in Neural Language Models"},{"paperId":"3962f108081b22c7e54b413f47ba6f2c16f2cc05","title":"Frequency Effects on Syntactic Rule Learning in Transformers"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"4291fe672cf6dc73e237ca0942fa49beb8c98711","title":"Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP"},{"paperId":"7571ed4cf1bbdcf891b576a0da12c910b1f0c72f","title":"Concealed Data Poisoning Attacks on NLP Models"},{"paperId":"5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec","title":"What’s in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus"},{"paperId":"1adadbfa95e43a70fcd17e6ce947a0652b86bfc3","title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus"},{"paperId":"023fc86c932fbc36702a6ad11c94ba419e1d8d88","title":"Competency Problems: On Finding and Removing Artifacts in Language Data"},{"paperId":"4e00843bc5f60d2b9116abc4320af6d184422291","title":"Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"ca2f1088d3e581b2c6c75cf0ebc96506d620f64d","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"9eea59c34f139f3d2153226c8cf026e975622074","title":"Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"b68b2e81ae2de647394ec05ee62ecf108bf2b50a","title":"Eliciting Knowledge from Language Models Using Automatically Generated Prompts"},{"paperId":"cc3725e66aa600eb12b244e82880f8e0bd225065","title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"15e1ee23b52e2b9f5706e603856062bc26fb9f88","title":"What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation"},{"paperId":"cb58542c94ce83b09f5d3809e69518ba52709c92","title":"Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets"},{"paperId":"6f2b90ee5a0feea87264148c25a874f84bae20a0","title":"Are Pretrained Language Models Symbolic Reasoners over Knowledge?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b9485d1e2c66c3ae452ec4903c2a157caef4d2ed","title":"Temporal Common Sense Acquisition with Minimal Supervision"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"d0fcdf47561ff742c9a72495102f16646eca43b7","title":"We Need To Talk About Random Splits"},{"paperId":"774319233a107a29622003a115aa6c79f4a7b37f","title":"Probing Neural Language Models for Human Tacit Assumptions"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"68c7a76c8bbab5a3e5901ed03ac084c3e0b5d4b3","title":"Know thy Corpus! Robust Methods for Digital Curation of Web corpora"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"72a1d0256b38dea6c3e7d10a63eacc51abdc96da","title":"End-to-End Bias Mitigation by Modelling Biases in Corpora"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"f98e135986414cccf29aec593d547c0656e4d82c","title":"Commonsense Knowledge Mining from Pretrained Models"},{"paperId":"94befec2a6d96e3a60fb8b77f2e161666743c1a5","title":"We Need to Talk about Standard Splits"},{"paperId":"0ada4f23141e774dcac63cb8569730ddacda75c4","title":"Does learning require memorization? a short tale about a long tail"},{"paperId":"2e282bca209a3bad3ee0e35ea03b24056af7975c","title":"What’s in a Name? Reducing Bias in Bios without Access to Protected Attributes"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"ffca61ff581efb26d52be9189ba028ccb10eccd1","title":"Auditing Data Provenance in Text-Generation Models"},{"paperId":"1d97b7aaf42c798225283987a1cd7df09c5d9b31","title":"What Makes Reading Comprehension Questions Easier?"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6","title":"Hypothesis Only Baselines in Natural Language Inference"},{"paperId":"9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7","title":"Gender Bias in Coreference Resolution"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for datasets"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"520ec00dc35475e0554dbb72f27bd2eeb6f4191d","title":"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks"},{"paperId":"ffb949d3493c3b2f3c9acf9c75cb03938933ddf0","title":"Adversarial Examples for Evaluating Reading Comprehension Systems"},{"paperId":"b1e20420982a4f923c08652941666b189b11b7fe","title":"A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task"},{"paperId":"d0d384cbab4679fc604bbdc2c6912bb342d83315","title":"Mental models and human reasoning"},{"paperId":"a84f4fe31fcfb4ad92c995dba0fc09ed8fe6a4f4","title":"Exploiting Machine Learning to Subvert Your Spam Filter"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":null,"title":"Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Openwebtext corpus"},{"paperId":"f7d95e8a91d7683e74ee642f20906e7751e47dfe","title":"Association for Computational Linguistics"}],"id":"833a2f1817cb9aeb292620454889cae78e26dda4","summary":"Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, the results raise the question of how much models actually generalize beyond pretraining data, and researchers are encouraged to take thepretraining data into account when interpreting evaluation results."},{"url":"https://www.semanticscholar.org/paper/ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","title":"Numerical Correlation in Text","venue":"MATHNLP","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Daniel M. Spokoyny,Chien-Sheng Wu,Caiming Xiong","citations":[],"references":[{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"72c447f305265f8c030a2736a35fd1a3e8dae505","title":"Improving Downstream Task Performance by Treating Numbers as Entities"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"5f277d2605a272c321615373254097939aae4fac","title":"“When Numbers Matter!”: Detecting Sarcasm in Numerical Portions of Text"},{"paperId":"f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1","title":"Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"}],"id":"ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9","summary":"A new task, Numerical Correlation in Text, which requires models to identify the correlation between two numbers in a sentence is proposed and a new dataset is introduced, which contains over 2,000 Wikipedia sentences with two numbers and their correlation labels."},{"url":"https://www.semanticscholar.org/paper/0b7e9b6b588baa3f24fdde06feec26a067aa74bd","title":"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":66,"citationCount":36,"influentialCitationCount":4,"publicationDate":"03/06/2022","authors":"Yilun Zhao,Yunxiang Li,Chenying Li,Rui Zhang","citations":[{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"eee5a8e06c32112d268e88a8d4c45592d7214244","title":"DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data"},{"paperId":"d57ebdb4b5ed89ab4135c72f0c91dc1c4b347c4b","title":"KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains"},{"paperId":"df15b83986207d884a811ce572dcedb6654abc6f","title":"BizBench: A Quantitative Reasoning Benchmark for Business and Finance"},{"paperId":"cb886bc9674d689d3a1f23713826374279894557","title":"EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images"},{"paperId":"6a3c67d35f3a9c10c8fde6c325a0535c03876068","title":"SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA"},{"paperId":"74acec7abaa6193107c6e3dfc630758ad9474af5","title":"HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering"},{"paperId":"27d6d02e24de259e3aa38e556a81f89ec505816e","title":"MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images"},{"paperId":"520628a4d5b0d609586292c78871ab6b9504a501","title":"Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset"},{"paperId":"beeaa3b353f0afeb2387f8336c052042a9b78cc0","title":"QTSumm: A New Benchmark for Query-Focused Table Summarization"},{"paperId":"2236dca1be9f73c634d4d865e3c78b9639bfcd98","title":"QTSumm: Query-Focused Summarization over Tabular Data"},{"paperId":"d3d8f37a809a0e3efe1cf6419033ed36424ffc8a","title":"ReTAG: Reasoning Aware Table to Analytic Text Generation"},{"paperId":"0ea6b7371017721d87f9c5b32b084bd1ca762532","title":"S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering"},{"paperId":"a39f960b28a627d554712a457efa5d3e3b7f8406","title":"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning"},{"paperId":"5bcb7579011ec43b5a60a587f4434c756eddd85d","title":"Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question"},{"paperId":"0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","title":"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs"},{"paperId":"2504035356a92cf2c8ad2beafd361719ac8aa6da","title":"SoarGraph: Numerical Reasoning over Financial Table-Text Data via Semantic-Oriented Hierarchical Graphs"},{"paperId":"6fd4e1c761b32e7f22e6377d7b97d272e4b05831","title":"cTBLS: Augmenting Large Language Models with Conversational Tables"},{"paperId":"05c3a61c5106793dbc0ca4c6d81be888234a13d8","title":"LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control"},{"paperId":"62f7f806a0e914c360f0640d247f4df1974c1f3b","title":"Semi-Structured Object Sequence Encoders"},{"paperId":"1278dc2e5077b9a0fe5266ae6850a59d8231b41f","title":"A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"9cc5c25517c3a78183a052e8e93a44e85bb17432","title":"Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations"},{"paperId":"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","title":"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data"},{"paperId":"d69521c1521bfa3fefb32b2106da38b1821e975d","title":"NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering"},{"paperId":"e1801f68d42f4a1a96d580b73706733883bf6af8","title":"ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"d84643c9f0289df5f9373480025da8f76f6df79c","title":"Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder"},{"paperId":"52b3087525b262f6f467453e22fdfa843353d40c","title":"TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data"},{"paperId":"117e1323677cb5d78ece0fd07b5cfa81618f4866","title":"Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning"},{"paperId":"1aa2b8d2a2dd685b8d5318eb20cffee093412603","title":"IM-TQA: A Chinese Table Question Answering Dataset with Implicit and Multi-type Table Structures"},{"paperId":"a0ee62ff9a6a00d29634a5146f9ffb91ec730e32","title":"OpenRT: An Open-source Framework for Reasoning Over Tabular Data"},{"paperId":"19856469e29d13b63adde4d198d4785a65204997","title":"cTBL: Augmenting Large Language Models for Conversational Tables"},{"paperId":"3ff6796b954c26b39a94666428bd40ce4b8f89eb","title":"Reasoning arithmetic word problems entailing implicit relations based on the chain-of-thought model"},{"paperId":"834fd676114c0b6e077c735ddffd9fa14ee856d2","title":"Improving compositional generalization for multi-step quantitative reasoning in question answering"},{"paperId":"cc8417aa578016203cb52efc63592bba64b08bb3","title":"FinMath: Injecting a Tree-structured Solver for Question Answering over Financial Reports"}],"references":[{"paperId":"2e43501a14b831744999355c177321709659aff1","title":"TableFormer: Robust Transformer Modeling for Table-Text Encoding"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"927efd299cffcfca3716efefcc904331b70c153e","title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset"},{"paperId":"4ef578869c957f5fb969fa9c164dca0433f48042","title":"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"a1364257028332760208827cd0c7af08d91e058b","title":"HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation"},{"paperId":"bea8d75b7e78ba8becc691d89eb3b52a674272f0","title":"AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"d365978adf0a5c9c6028820857e015617856256b","title":"MultiModalQA: Complex Question Answering over Text, Tables and Images"},{"paperId":"388513e8e09ad60f619054361f4d2cdf5a146bc8","title":"FeTaQA: Free-form Table Question Answering"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"084c5afc5b16b0c50c53390f550a13f4ed4c7d3c","title":"TSQA: Tabular Scenario Based Question Answering"},{"paperId":"edaafec59b651d503ac7c8a86f8e2335273e1f7a","title":"Learning by Fixing: Solving Math Word Problems with Weak Supervision"},{"paperId":"24a12899ce97bd4a56f7c6b49d3979b9465f0190","title":"TUTA: Tree-based Transformers for Generally Structured Table Pre-training"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"65be695739d0fa35212e49ccccd129535e6d9e15","title":"Understanding tables with intermediate pre-training"},{"paperId":"3578a7792904e6af3db8ffefdff86ab6a387c7c3","title":"FinBERT: A Pretrained Language Model for Financial Communications"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"73a906a988e54defee536a120125f957059d595e","title":"Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context"},{"paperId":"27db72a2f643f9dfebc0cc2e8b98a9db307f0f07","title":"HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data"},{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","title":"Longformer: The Long-Document Transformer"},{"paperId":"52cb05d721688cb766c6e282e9d55c3b8e3dc0cf","title":"TaPas: Weakly Supervised Table Parsing via Pre-training"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"55a3b36fd21dbbe9384ab3ba1bcf901235d95f47","title":"Unsupervised Question Decomposition for Question Answering"},{"paperId":"f352298f760c710a6021a66c549f7c65f0866eb7","title":"A Time Series Analysis of Emotional Loading in Central Bank Statements"},{"paperId":"26f8c13adc064a3833437d22ebfe3f89412b9760","title":"Forecasting Firm Material Events from 8-K Reports"},{"paperId":"6bb20fe9099fbee39df252fbff1289d0a8ec3cf2","title":"Complaint Analysis and Classification for Economic and Food Safety"},{"paperId":"b393ece4c0e7f8ff696b10b952a9fa420b419a05","title":"Financial Event Extraction Using Wikipedia-Based Weak Supervision"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"ba783d92d0eaf6a7bff6ced7660150ce38016bbc","title":"Don’t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases"},{"paperId":"7102bb3fe73bd057ff161d9db5214a267c1ef312","title":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"},{"paperId":"5e11a9817db945e4bd965b26fc7060b4afe5574d","title":"A framework for anomaly detection using language modeling, and its applications to finance"},{"paperId":"a77df5291ee644022067f34eec790ea31380792b","title":"Are You for Real? Detecting Identity Fraud via Dialogue Interactions"},{"paperId":"b285c067f2da04bf5647beb8853bfddf6d9b4e1b","title":"A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"fd4675526ee569196ad1698935b8f5a529b1f9ba","title":"Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"0c4fc6680682910f5c0608c5475b323ac94cbb8c","title":"Deep learning models for bankruptcy prediction using textual disclosures"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"e0c66240239263f16159eef166a391d3939ae2d5","title":"How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks"},{"paperId":"1e75b03bce78e73b7d0e707912f394585262cff0","title":"Leveraging News Sentiment to Improve Microblog Sentiment Classification in the Financial Domain"},{"paperId":"f2d742704a32c7ebef768bd74d1438522269a4c2","title":"NextGen AML: Distributed Deep Learning based Language Technologies to Augment Anti Money Laundering Investigation"},{"paperId":"7191680b572ee7145f1a9d95ff11ab1ff44259f3","title":"WWW'18 Open Challenge: Financial Opinion Mining and Question Answering"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"cbd569036fc72ae7ff747350b91816440282596b","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"2d2b1ce365b879dc8f337c293b69e9c0d62bfc91","title":"Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base"},{"paperId":"d1505c6123c102e53eb19dff312cb25cea840b72","title":"Teaching Machines to Read and Comprehend"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"799b424118419cf7fc78ed1bc407e3af03aba01f","title":"Free-Marginal Multirater Kappa (multirater K[free]): An Alternative to Fleiss' Fixed-Marginal Multirater Kappa."},{"paperId":"0ce43def1c95802a464449f055e78c08d6d99dc2","title":"Towards Table-to-Text Generation with Numerical Reasoning"},{"paperId":"1893f9875fe6a5b40b82838aa3a4259f5763d7f0","title":"SciGen: a Dataset for Reasoning-Aware Text Generation from Scientific Tables"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8f90c243cd7d69c4c92cee1ac924e74592b8df99","title":"Utilizing Pre-Trained Word Embeddings to Learn Classification Lexicons with Little Supervision"},{"paperId":"238ecc4956656d4aaecfa79d8239dd8f10b3d83a","title":"Causality Analysis of Twitter Sentiments and Stock Market Returns"},{"paperId":"1778d275965921e75e93fda7bf40d922071682ff","title":"Word Embeddings-Based Uncertainty Detection in Financial Disclosures"},{"paperId":"dc64a09b1d58f65a6fc4784684a54bed6fe7857d","title":"A Simple End-to-End Question Answering Model for Product Information"},{"paperId":null,"title":"SIGKDD Conference on Knowledge Discovery amp; Data Mining ,"}],"id":"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","summary":"A new large-scale benchmark, MultiHiertt, with QA pairs over Multi Hierarchical Tabular and Textual data is constructed and a novel QA model termed MT2Net is introduced, which first applies facts retrieving to extract relevant supporting facts from both tables and text and then uses a reasoning module to perform symbolic reasoning over retrieved facts."},{"url":"https://www.semanticscholar.org/paper/cd54a839ba12e7417e321b4407788dc426ebaefc","title":"Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering","venue":"NAACL-HLT","year":2022,"referenceCount":24,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jeonghwan Kim,Junmo Kang,Kyung-min Kim,Giwon Hong,Sung-Hyon Myaeng","citations":[{"paperId":"60be11b0c34038d9ee156cbec6c4df5ae5db68b8","title":"Pretrained Language Model based Web Search Ranking: From Relevance to Satisfaction"},{"paperId":"704332d287bc78ac95fea3d90ec3945e9cec9ab3","title":"A survey on complex factual question answering"},{"paperId":"6fa180f4ce4be2715f3cbe13dc5dc6d198dc7249","title":"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data"},{"paperId":"d69521c1521bfa3fefb32b2106da38b1821e975d","title":"NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering"}],"references":[{"paperId":"5d9ac727b4a4e1044b3ba464df3be66eb8568127","title":"Numerical reasoning in machine reading comprehension tasks: are we there yet?"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"238deab37e201c57505a4a47bb854e462af79bd7","title":"Entity-Based Knowledge Conflicts in Question Answering"},{"paperId":"3122a2d7799ba585b993e432b3deb47659b3f3c1","title":"Hurdles to Progress in Long-form Question Answering"},{"paperId":"ca2f1088d3e581b2c6c75cf0ebc96506d620f64d","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"a48b1b5390d7c8621c0dbb55d6e675da83a2027a","title":"Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"5b6d03ed66473599ee31872b3cd5ad2ce282371f","title":"Roles and Utilization of Attention Heads in Transformer-based Neural Language Models"},{"paperId":"b9a5aa5db8836744ff2073e8368520b7a614049f","title":"Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"641a9749fe546a02bbab9a86bfc91492db1c3bc5","title":"Stanza: A Python Natural Language Processing Toolkit for Many Human Languages"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"2d08cdc45bbed8cea283d92132b4b88a507a7303","title":"Neural Module Networks for Reasoning over Text"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":null,"title":"Methods for numeracypreserving word embeddings"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}],"id":"cd54a839ba12e7417e321b4407788dc426ebaefc","summary":"This work proposes a novel attention masked reasoning model, the NC-BERT, that learns to leverage the number-related contextual knowledge to alleviate the over-reliance on parametric knowledge and enhance the numerical reasoning capabilities of the QA model."},{"url":"https://www.semanticscholar.org/paper/58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models","venue":"ArXiv","year":2021,"referenceCount":30,"citationCount":14,"influentialCitationCount":1,"publicationDate":"07/09/2021","authors":"Zhihua Jin,Xin Jiang,Xingbo Wang,Qun Liu,Yong Wang,Xiaozhe Ren,Huamin Qu","citations":[{"paperId":"5be5619fc22300ef356ec4ef729d567ce7116c57","title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data"},{"paperId":"1364e3c277e5833299f73b618c021b38510207cd","title":"ML-LJP: Multi-Law Aware Legal Judgment Prediction"},{"paperId":"bc0ee1c62b7864afd211e085fb3d58263c84ccfe","title":"Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"efa9fc7d5b6b244d8ae3a9c3db98418ec39aa7a3","title":"Precision information extraction for rare disease epidemiology at scale"},{"paperId":"eb9b60db6832b00b9932584663bec104d6d415dd","title":"Tree-Based Representation and Generation of Natural and Mathematical Language"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"efcd9c1438559d908efd702333232078fd251a0f","title":"Phenotyping in clinical text with unsupervised numerical reasoning for patient stratification"},{"paperId":"cbccb1201eee6432020276762a44ebfbf8f981a0","title":"Unsupervised Numerical Reasoning to Extract Phenotypes from Clinical Text by Leveraging External Knowledge"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"4ef578869c957f5fb969fa9c164dca0433f48042","title":"FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining"},{"paperId":"c77feebb06b530079a34f04c64de359ae28c0a32","title":"Predicting Numerals in Text Using Nearest Neighbor Language Models"},{"paperId":"c97bcf9e605938c2bef54f12b2156c678f1a2373","title":"Introducing Semantic Information for Numerical Attribute Prediction over Knowledge Graphs"}],"references":[{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"6456d905a63d2741c6d896f59eb6372ceedcecce","title":"Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"f67ede490780f939103d4cc4e9c6866b83ee59b3","title":"Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"caeb950e503872a903e18a3b259424e3cc3c6006","title":"Neural Math Word Problem Solver with Reinforcement Learning"},{"paperId":"835c6b524b90b1639aba28742f7161137ddf4397","title":"MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"b23903d90f9930f032622c895e88f1b7751229d4","title":"Scales"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"}],"id":"58c7a31bf47948d936de937b1cf7b49463608557","summary":"The experiment results show that NumGPT outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as measurement estimation, number comparison, math word problems, and magnitude classification."},{"url":"https://www.semanticscholar.org/paper/cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/03/2023","authors":"Alberto Testolin","citations":[{"paperId":"ce78fe8b536f5a5c438f6ce31d64311cdc824a30","title":"Calabi-Yau Four/Five/Six-folds as $\\mathbb{P}^n_\\textbf{w}$ Hypersurfaces: Machine Learning, Approximation, and Generation"},{"paperId":"4726d1dc54851db99c29180127d840bd19f20afc","title":"Positional Description Matters for Transformers Arithmetic"},{"paperId":"70ffc32ecb69740bd508a7863a2d303d4eb4f208","title":"xVal: A Continuous Number Encoding for Large Language Models"},{"paperId":"fecbf14c6afa8d17a0cae8697db8abbadb7abe7d","title":"A Hybrid System for Systematic Generalization in Simple Arithmetic Problems"}],"references":[{"paperId":"3f2cb353c7528efafb847309ab1e1e95245740a4","title":"Mathematical Capabilities of ChatGPT"},{"paperId":"1338b3771c27090dee722cc5b351ace179ebae76","title":"Mathematics, word problems, common sense, and artificial intelligence"},{"paperId":"68cde862fefcea9508b0a835799f8b02716a3420","title":"Learning to solve arithmetic problems with a virtual abacus"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"442ab95eb9cfbc03bb17a27b52313b5d25eaa738","title":"Discovering faster matrix multiplication algorithms with reinforcement learning"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"7691c40975cf25fae4b7a55278a65d2dcb361646","title":"Transformers discover an elementary calculation system exploiting local attention and grid-like problem representation"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"fddf8eff2e8758a3c49f8dd26f059fcf8fc6c2bc","title":"Self-Communicating Deep Reinforcement Learning Agents Develop External Number Representations"},{"paperId":"354bf043179e3e9f05df73e3f04517e53c326d1f","title":"TALM: Tool Augmented Language Models"},{"paperId":"1bcde55995a957b3e8a595d536b816cb8989cf1d","title":"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"257aee73d83a87921fd2d56b524de394dcf6a264","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"642bc6776f6032db4695cc8158caef4b24529436","title":"Neuro-Symbolic Artificial Intelligence: The State of the Art"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"c0cd4b4844c31a27a7900a754d0a91b160b00e55","title":"Advancing mathematics by guiding human intuition with AI"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"f729e16ee6765e2fd76772cbf9d3dd17b4d25438","title":"Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"6356f71e9e7f89d5e0da52bfcc52cc93ad67affa","title":"7 Revealing Ways AIs Fail: Neural Networks can be Disastrously Brittle, Forgetful, and Surprisingly Bad at Math"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"b350be3836c3d183464642815b26b061f24e8314","title":"Learning Mathematical Properties of Integers"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"2fd1312b8507aae41bace1dd89712754a81fbc49","title":"PonderNet: Learning to Ponder"},{"paperId":"5e8618c19b619c95ca098bb135e4b816c05ac4b7","title":"Learning Numerosity Representations with Transformers: Number Generation Tasks and Out-of-Distribution Generalization"},{"paperId":"4e699db94d550f1d698706024af1cba05de6080c","title":"The cultural origins of symbolic number."},{"paperId":"ad46feec0cefb78dafa60821598c052cf0b9b7bd","title":"NT5?! Training T5 to Perform Numerical Reasoning"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"be09ed6cd73654a23f78416433a1b23ea623ea79","title":"Symbolic Behaviour in Artificial Intelligence"},{"paperId":"2c0a266f9cb88bb914c138ece0deaab8cf528f78","title":"Neural Sequence-to-grid Module for Learning Symbolic Rules"},{"paperId":"cfa097ad84c0056e8e37bdf3285543eb85a063c5","title":"Methods for Numeracy-Preserving Word Embeddings"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"251a27e00b9838c8c315f0152cb52762db766aee","title":"Visual sense of number vs. sense of magnitude in humans and machines"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"1e5ae63dae90c573313b9d9a6d481015bc03850c","title":"The Challenge of Modeling the Acquisition of Mathematical Concepts"},{"paperId":"601a469f3609cc6cf6a932b6dfd4c699d436c7a1","title":"Learning to Prove Theorems by Learning to Generate Theorems"},{"paperId":"499914837b096342af5d381cf3ff406b1bfcb8ff","title":"Numerosity discrimination in deep neural networks: Initial competence, developmental refinement and experience statistics."},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"d5fcad8a3b183642fcf609519a4dbbda9c3541ff","title":"Learning Numeral Embedding"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"730043364aed106241ef18ab3e3b5e316802a254","title":"NumNet: Machine Reading Comprehension with Numerical Reasoning"},{"paperId":"84a2a78fbbcde2258af267dd66f8b80c21ff03a7","title":"Mathematical Reasoning in Latent Space"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"dde6c4667c6a84c6479531ca949b26f0717b6fca","title":"Ontogenetic Origins of Human Integer Representations"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"6605bba6e0caabda06b090d67698a5683eba4dfa","title":"Translating a Math Word Problem to an Expression Tree"},{"paperId":"186b6ce02ab2c2927df096bcb93da858fbf341ff","title":"Bias and Generalization in Deep Generative Models: An Empirical Study"},{"paperId":"c4c1ced7a200a565419c1c216333e9b7d038d187","title":"The Cultural Challenge in Mathematical Cognition"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"9481d7c6b2e7eb4b782d27fb9b1cdb59a3b93c05","title":"From number sense to number symbols. An archaeological perspective"},{"paperId":"5e2bb96c47ccaa16a4e7192e8fadb3b3e1c3acdc","title":"Deep Learning: A Critical Appraisal"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"2e9f4e687a14b96f74ef37cd042d35d6ddabb186","title":"Is There Really an Evolved Capacity for Number?"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"3904315e2eca50d0086e4b7273f7fd707c652230","title":"Meta-Learning with Memory-Augmented Neural Networks"},{"paperId":"e5497dd7e6f34de75416d86ce32f593df84225e2","title":"The role of materiality in numerical cognition"},{"paperId":"0409af1451fe463d5811b61d08048ca1f6c33b1e","title":"The neuronal code for number"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"f7c7c5c7013541ee68143141af7615cf173c4ecd","title":"Early Numeracy Assessment: The Development of the Preschool Early Numeracy Scales"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"625b803b63acadf93352347b5b0a2f0dfa90d0f9","title":"How Important is Where you Start? Early Mathematics Knowledge and Later School Success"},{"paperId":"36bf449f880caf48d13c8afd3ed73b3a7c7bc05b","title":"Nature and culture of finger counting: Diversity and representational effects of an embodied cognitive tool"},{"paperId":"6c84f43caf334a0fa9861148aa36c9e2961cf7f9","title":"Home and preschool learning environments and their relations to the development of early numeracy skills"},{"paperId":"23c98518cea776c7ea142841de79c708a981dabc","title":"Emergence of a 'visual number sense' in hierarchical generative models"},{"paperId":"379e90567ab508ebb461c48fee9cfb919c4b2328","title":"Neurocognitive start-up tools for symbolic number representations"},{"paperId":"e5da4d52a7d89bfe3482836bc59ec32dd7b57e31","title":"Perceptual Learning Modules in Mathematics: Enhancing Students' Pattern Recognition, Structure Extraction, and Fluency"},{"paperId":"34fb621cf73b7bad25d77ff1229467a34f736474","title":"Early math matters: kindergarten number competence and later mathematics outcomes."},{"paperId":"d7595c7af245e474c7ab173faf2389d5a06458db","title":"Development of a measure of early mathematics achievement using the Rasch model: the Research‐Based Early Maths Assessment"},{"paperId":"01d84d35394f22b8b904ba30abfe9def2aea7d87","title":"Number and language: how are they related?"},{"paperId":"b3df5811486bc147f5f57777aa2011d0ac5dbab3","title":"Number Processing and Calculation – Normative Data from Healthy Adults"},{"paperId":"a9386eb6808b41238381c708f2642bcb7dc34b29","title":"Where mathematics comes from : how the embodied mind brings mathematics into being"},{"paperId":"8de194704879008cb5e15a7a0e9f26df73394ccb","title":"Developing Conceptual Understanding and Procedural Skill in Mathematics: An Iterative Process."},{"paperId":"a3b7e80260891dcd3844b1835df8dee3a1cd67c7","title":"The Number Sense: How the Mind Creates Mathematics."},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"4106342a079eecadd4a4a1d39eabd4a50e0a7cd1","title":"Symbol grounding problem"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":"4ace8a3b40c0a6245fc0814f508303f6ae83b655","title":"The Computer Modelling of Mathematical Reasoning"},{"paperId":"bfa035c0e723f8f540500db038ca6e26d599029d","title":"Minds, brains, and programs"},{"paperId":"df1b7ec5e3feca2174fbd301e4915909c2fcae04","title":"The logic theory machine-A complex information processing system"},{"paperId":"4b2137280915ccc0e06e97b604778b05876a34ad","title":"Evaluating Large Language Models"},{"paperId":"52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"4ae538370ee94957421bdce45f254f9df017e314","title":"Which Preschool Mathematics Competencies Are Most Predictive of Fifth Grade Achievement?"},{"paperId":null,"title":"Early Childhood Mathematics Education Research"},{"paperId":null,"title":"Impact of Pretraining Term"},{"paperId":null,"title":"Origins of mathematical intuitions: the case of arithmetic"}],"id":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","summary":"This survey critically examines the recent literature, concluding that even state-of-the-art architectures often fall short when probed with relatively simple tasks designed to test basic numerical and arithmetic knowledge."},{"url":"https://www.semanticscholar.org/paper/817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":40,"influentialCitationCount":0,"publicationDate":"16/03/2023","authors":"Zheng Yuan,Hongyi Yuan,Chuanqi Tan,Wei Wang,Songfang Huang","citations":[{"paperId":"4bebe389dfa85423e5cc089edf20b2c3f572f38c","title":"Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives"},{"paperId":"e58d6db2de725732ade38355d0349f078ea6368d","title":"Evaluating AI Vocational Skills Through Professional Testing"},{"paperId":"f1847c5693f7dac9ba89b3e0571ee27cfca720dc","title":"Measurement in the Age of LLMs: An Application to Ideological Scaling"},{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"40f40bbaf75d7b063d4a06f07b384bda7a8bcccd","title":"Generating Interpretable Networks using Hypernetworks"},{"paperId":"6d3da24a9bcef989900d646aaea23b446ffd39e1","title":"Understanding Users' Dissatisfaction with ChatGPT Responses: Types, Resolving Tactics, and the Effect of Knowledge Level"},{"paperId":"3d13935886627982fc98971baa33d2f9f3115bff","title":"Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer"},{"paperId":"a768256f92d911a00389627e0d1fabaa2e0d1c32","title":"PhoGPT: Generative Pre-training for Vietnamese"},{"paperId":"123acfbccca0460171b6b06a4012dbb991cde55b","title":"Large Language Models Are Zero-Shot Time Series Forecasters"},{"paperId":"e93562137240873bf1262e769dd9d73c2dcba858","title":"Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization"},{"paperId":"37a30e97ae043075f09738984a59991ff20ecf0c","title":"OptiMUS: Optimization Modeling Using MIP Solvers and large language models"},{"paperId":"b1c9c455acb1393f1bf60ec4095ee343531e404e","title":"MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use"},{"paperId":"c02cded20074fff4310d7bc943d0b8bfff305d58","title":"Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench"},{"paperId":"02bc90e9fb4a681b048c6652720afe439d16e6cd","title":"Scaling Experiments in Self-Supervised Cross-Table Representation Learning"},{"paperId":"5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0","title":"Qwen Technical Report"},{"paperId":"d47a8522eadb9414cce5b1d1f043b9761c510dd8","title":"ChatGPT & Mechanical Engineering: Examining performance on the FE Mechanical Engineering and Undergraduate Exams"},{"paperId":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator"},{"paperId":"e29414150d604191df0f59c30d8c39fca438d3c2","title":"Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain"},{"paperId":"d4fc988c6510420a5290dfe8d1a991ca4878d696","title":"Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries"},{"paperId":"629f44f5fb78ec390ef66633dc627f1d04f3eb85","title":"Knowledge Graph Prompting for Multi-Document Question Answering"},{"paperId":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"},{"paperId":"91206346edbe28abb606d7b3425cd455d4019d4f","title":"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"},{"paperId":"9160673f89ecb326db8c64f8d09ab30f35a74e9d","title":"Arithmetic with Language Models: from Memorization to Computation"},{"paperId":"4703cbd3743ff81297c64007db0109d96dec98c0","title":"Generating Mathematical Derivations with Large Language Models"},{"paperId":"8f1b0c247171510fc68da27a16a377456376a5a7","title":"Assessing GPT-3.5 and GPT-4 in Generating International Classification of Diseases Billing Codes"},{"paperId":"0db0af0cd3ceb0531a050a03e6ceb849580ff53b","title":"Teaching Arithmetic to Small Transformers"},{"paperId":"888728745dbb769e29ed475d4f7661eebe1a71cf","title":"A Survey on Evaluation of Large Language Models"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"fdf16ba99ddeb9a82311b7565143cf9ccfd004a8","title":"Dissecting Chain-of-Thought: A Study on Compositional In-Context Learning of MLPs"},{"paperId":"714740e38dbb0642475ff3eae5681ae0a4103670","title":"GPT4GEO: How a Language Model Sees the World's Geography"},{"paperId":"19c63eade265d8a47d160098d97194b3b83d3770","title":"In-Context Impersonation Reveals Large Language Models' Strengths and Biases"},{"paperId":"6dd44624ac912fb50c21c691806ee52d27e73abb","title":"Large Language Models are Few-Shot Health Learners"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"08a3986c595dc38147edf6425641de1fe0d796b3","title":"Mobile-Env: An Evaluation Platform and Benchmark for Interactive Agents in LLM Era"},{"paperId":"7a6dc7071891cb3d658c93418801942a4c6ed373","title":"Autonomous GIS: the next-generation AI-powered GIS"},{"paperId":"5cbd3c6a70cf4f1a29634c23b28af5101db882a4","title":"Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models"},{"paperId":"8fb645e51d74244ab93b41788570c958fdcad06a","title":"AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays"},{"paperId":"748698bd4387afd08594e0dc8150c2afa210d9ae","title":"RRHF: Rank Responses to Align Language Models with Human Feedback without tears"},{"paperId":"a3ee94dbcd0af8a02e937660ece777e21d5019ed","title":"Leveraging ChatGPT Capabilities in Vietnamese High School Mathematics Education"},{"paperId":"69bfa665e507fcee4a8d003933998eb89f336c9f","title":"Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning"}],"references":[{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"754ed0deccc8c05e7dd05d5df376f205e569a841","title":"An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP)"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"3f2cb353c7528efafb847309ab1e1e95245740a4","title":"Mathematical Capabilities of ChatGPT"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","title":"Galactica: A Large Language Model for Science"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"f557f3a32d309373e7d31bb93ca1b80b4a6e39e7","title":"Symbolic Math Reasoning with Language Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"c28e95a06dfcf13fc65a1cac83722f53e34f12a5","title":"Autoformalization with Large Language Models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"0cc771ca7859b395728c428b2a707007c4d69416","title":"Pretrained Language Models are Symbolic Mathematics Solvers too!"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"4972b88f8f324a4fa18e921f62a9857af2b5fc7b","title":"Crosslingual Generalization through Multitask Finetuning"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":null,"title":"OpenAI. 2023."},{"paperId":null,"title":"2022. Scaling instruction-ﬁnetuned language models"}],"id":"817e52b815560f95171d8fa60f78dd965e885a65","summary":"This work proposes an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatG PT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provides a detailed analysis of the ability of largelanguage models."},{"url":"https://www.semanticscholar.org/paper/8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":23,"influentialCitationCount":3,"publicationDate":"23/05/2023","authors":"Tiedong Liu,K. H. Low","citations":[{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"d58721cc0d12ac43df9419c5ae25e1ddef87f0f5","title":"Adapting Foundation Models for Operator Data Analytics"},{"paperId":"dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills"},{"paperId":"1cc88712fc6d30b95ad05edc6ed49ac31b2dc9dc","title":"Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks"},{"paperId":"f7a0501a246882103bc84cbc4f7270d1e7e428a8","title":"Enabling Large Language Models to Learn from Rules"},{"paperId":"4c86f91116483d946bde77684aa10844f932d5f5","title":"DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations"},{"paperId":"3b918b15178bcc84fd22af5094fe1efbcd388e72","title":"Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration"},{"paperId":"123acfbccca0460171b6b06a4012dbb991cde55b","title":"Large Language Models Are Zero-Shot Time Series Forecasters"},{"paperId":"90ff14a19419a0b6bb0965f0ab5e359462556172","title":"How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances"},{"paperId":"34ca51ce10e8d1d6c950ba519329714a0184d004","title":"KwaiYiiMath: Technical Report"},{"paperId":"b9e8b62bcc019f47a0a015568f70039b3b7c1196","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model"},{"paperId":"fa820299a00542eedb8624756016f7b14f4a383e","title":"The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning"},{"paperId":"ffd97533ac66b7d02ca58c1d5951d5427da0ffd6","title":"Improving Length-Generalization in Transformers via Task Hinting"},{"paperId":"e892a225c417fbac7545c3e31b45d1c42dc9c933","title":"Chain-of-Thought Reasoning is a Policy Improvement Operator"},{"paperId":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners"},{"paperId":"0b778079946764292de3771a489d5ce9e1868a8b","title":"The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models"},{"paperId":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"2dbe7fd41f23aa2a077c3d1e91b0b73f92e3d8d2","title":"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning"},{"paperId":"a68dc9208aae7578e8ee384caa8ccbcf34e539e8","title":"Mini-Giants: \"Small\" Language Models and Open Source Win-Win"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"5dc15ac1c92ab7492f121471823fb13a95d273ba","title":"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis"}],"references":[{"paperId":"29c7f009df21d0112c48dec254ff80cc45fac3af","title":"Are Emergent Abilities of Large Language Models a Mirage?"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"9e8cb8c91a0acb6e661b58ad724aa758490f2bea","title":"Instruction Tuning with GPT-4"},{"paperId":"42e741e0be43954ae684d14333e4074f4d0ae961","title":"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"574beee702be3856d60aa482ec725168fe64fc99","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"e65b346d442e9962a4276dc1c1af2956d9d5f1eb","title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"2a7ae3e98357569c41424dacd60c62d3df78a0db","title":"Limitations of Language Models in Arithmetic and Symbolic Induction"},{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"6d86f08a5d936780a4785acfad92f5f3e82004ad","title":"Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"c9dff8253b2e776abf363d0a4836abcaf64ee327","title":"ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge"},{"paperId":null,"title":"Koala: A dialogue model for academic research"},{"paperId":null,"title":"Model Number Tokenization LLaMA 74815"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":null,"title":"2023. Stanford alpaca: An instruction-following llama model"},{"paperId":null,"title":"Hyperparameter Value batch size 128 learning rate 0.0003 lora r 64 lora alpha 64"},{"paperId":null,"title":"OpenAI. 2023."},{"paperId":null,"title":"2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}],"id":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","summary":"Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks, is introduced and an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks into a series of learnable tasks by leveraging basic arithmetic principles is proposed."},{"url":"https://www.semanticscholar.org/paper/d8244cf970b9019cfb7f813275eba10d17c47166","title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":40,"citationCount":5,"influentialCitationCount":0,"publicationDate":"02/06/2023","authors":"Tianduo Wang,Wei Lu","citations":[{"paperId":"dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills"},{"paperId":"2179a13e78195fa78be5ccca2b5dcf9fad783ffc","title":"An Expression Tree Decoding Strategy for Mathematical Equation Generation"},{"paperId":"28b8aff02f3338d60b590907872961d067002944","title":"Adaptivity and Modularity for Efficient Generalization Over Task Complexity"},{"paperId":"f313a8599e6a85ec97126e9a99f4b3dc04a428a5","title":"FootGPT : A Large Language Model Development Experiment on a Minimal Setting"},{"paperId":"d75d11d2c89c01cd284383546ae057cb827dc272","title":"Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning"}],"references":[{"paperId":"d75d11d2c89c01cd284383546ae057cb827dc272","title":"Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"3e73561daf893074ab2b7e198e9dd7f1fa4f1263","title":"Differentiable Data Augmentation for Contrastive Sentence Representation Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"a376e6b118a10a8cb8a2920f6b83a70f087579f5","title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"03488f1a193066b5ea8b9b800e119f07df5c1d9e","title":"Reasoning Like Program Executors"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"f8e0f7babe7cd858aecfafb03375bf578fe161f7","title":"MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"ec15ff1fc5c9780fd91902f286a9e3fcd00b890d","title":"Math Word Problem Solving with Explicit Numerical Values"},{"paperId":null,"title":"2021), we train our models over 3,138 training examples from a combination"},{"paperId":null,"title":"Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?"},{"paperId":null,"title":"Input sequence construction We start by preparing a set of equation templates and each equation template contains no more than 3 binary operators"},{"paperId":null,"title":"A.1 Construction of M S AT The proposed M S AT is a synthetic Seq2Seq where the inputs describe arithmetic questions outputs are the solutions represented by a"},{"paperId":null,"title":"the equality hold and a variable name selected from the capitalized letters"},{"paperId":null,"title":"the resulting input arithmetic question"},{"paperId":null,"title":"A Additional information about datasets In this section, we provide additional details about the datasets that we used in the experiments"},{"paperId":null,"title":"equation and a question variable"},{"paperId":null,"title":"solution accordingly"},{"paperId":null,"title":"construct an input arithmetic question is to instantiate an equation from an equation template"},{"paperId":null,"title":"outputs of M S AT can be generated To construct an example of M S AT, we"},{"paperId":null,"title":"7: An illustration of the \"tree inversion\" algo-rithm that produces an output expression from an arithmetic question"}],"id":"d8244cf970b9019cfb7f813275eba10d17c47166","summary":"This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning by continually pre-training LMs on a synthetic dataset MsAT which is composed of Multi-step Arithmetic Tasks."},{"url":"https://www.semanticscholar.org/paper/0b315e6d4d800e04caf2f587312ce163e748d10c","title":"Numeric Magnitude Comparison Effects in Large Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Raj Sanjay Shah,Vijay Marupudi,Reba Koenen,Khushi Bhardwaj,S. Varma","citations":[],"references":[{"paperId":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","title":"Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"13c9554a8aff011351fabefaa96c5bd43b9a8afe","title":"Transformer networks of human conceptual knowledge."},{"paperId":"e3661f6a474e24a8919bffb8dfcfda6c79d47fc0","title":"Measurement of Text Similarity: A Survey"},{"paperId":"19f4404f640f850521cc97bc75ddbe8f482fc85e","title":"Word meaning in minds and machines"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"9e9857375834d5fe6c792769f5a5f37f51bf6e7f","title":"Similarity Judgment Within and Across Categories: A Comprehensive Model Comparison"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"4eb8ac1f091befb9eebb60d5b80fdac5667fd758","title":"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"},{"paperId":"06354570d5f6be803d4a79bf59ecbb097bca8755","title":"On the Practical Computational Power of Finite Precision RNNs for Language Recognition"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"f212042a090433eb5a4d06239ff7f33e60442ddf","title":"Fundamentals of Applied Multidimensional Scaling for Educational and Psychological Research"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"3e1c23ec31bec412161bb9e3078b5e4c72af173d","title":"Math, monkeys, and the developing brain"},{"paperId":"f5fbcbfc59ca491cf8745a1722e58c514639d217","title":"Using eye tracking to study numerical cognition: the case of the ratio effect"},{"paperId":"6b34f82bc2a1f5a802bb47e8e8e176fa7cfa595d","title":"Representation of number in the brain."},{"paperId":"9026755081d838609c62878c3deab601090c67da","title":"Individual differences in non-verbal number acuity correlate with maths achievement"},{"paperId":"5ba8564f1900b75927c961f7c412916e0a653425","title":"Are numbers special? An overview of chronometric, neuroimaging, developmental and comparative studies of magnitude representation"},{"paperId":"2e9fad91a799e945d2d06f74548da17082c7f6fa","title":"Neural correlates of symbolic number processing in children and adults"},{"paperId":"2ae0a2e56e2e5c3b565e27c785c89e7245a8a6bd","title":"Numerical Cognition Without Words: Evidence from Amazonia"},{"paperId":"8310db57008b1b32076a89b061f0979bd2012067","title":"Exact and Approximate Arithmetic in an Amazonian Indigene Group"},{"paperId":"db246ad3476897b900e3b6dd61530ac9f8cf4f09","title":"Coding of Cognitive Magnitude Compressed Scaling of Numerical Information in the Primate Prefrontal Cortex"},{"paperId":"e240b067902209a217236ab6ed31543e64c47ebd","title":"Decade breaks in the mental number line? Putting the tens and units back in different bins"},{"paperId":"e854885e41b6754177d0892129a8215b7e467c23","title":"Modern Multidimensional Scaling: Theory and Applications"},{"paperId":"0ecd3e2f6115c793487ccde57c582eaf35b35edb","title":"The language user as an arithmetician"},{"paperId":"90061477c9cb4e4f45170e9af0d79d3b06464ec0","title":"Cross-linguistic regularities in the frequency of number words"},{"paperId":"4106342a079eecadd4a4a1d39eabd4a50e0a7cd1","title":"Symbol grounding problem"},{"paperId":"1ef96e3e52abc7ef4882c7f337f9b07dcd4898fe","title":"Temporal aspects of digit and letter inequality judgments."},{"paperId":"2ef2fd185a3a118c4aa03a1928148e8ffbc629eb","title":"Some statistical considerations in multidimensional scaling"},{"paperId":"cda3ff1dfca7f7fe8af5a11246f5039e30872020","title":"Time required for Judgements of Numerical Inequality"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"62b8ec3401d4309c9a304bdca7cd929472223547","title":"To honor Fechner and obey Stevens: relationships between psychophysical and neural nonlinearities."},{"paperId":null,"title":"Wikipedia contributors"},{"paperId":"d05cdb8a04ed602409c28ca21b0f01f9ff8ce5a1","title":"Elements of psychophysics"},{"paperId":null,"title":"How do model behaviors change when using larger variants (more parameters) of the same architecture?"}],"id":"0b315e6d4d800e04caf2f587312ce163e748d10c","summary":""},{"url":"https://www.semanticscholar.org/paper/ecf0cb0725de18659f9ba25a8cf65a1085564006","title":"Understanding Arithmetic Reasoning in Language Models using Causal Mediation Analysis","venue":"arXiv.org","year":2023,"referenceCount":51,"citationCount":1,"influentialCitationCount":1,"publicationDate":"24/05/2023","authors":"Alessandro Stolfo,Yonatan Belinkov,Mrinmaya Sachan","citations":[{"paperId":"c16c05ca0a3d24519405849fd24604fc1ce47751","title":"Towards Best Practices of Activation Patching in Language Models: Metrics and Methods"}],"references":[{"paperId":"405f8f5f1c6df1b3343c812832479aad5180b65f","title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"12910786da7a34c9ee26798fd81b0ed7b0e38789","title":"Finding Neurons in a Haystack: Case Studies with Sparse Probing"},{"paperId":"133b97e40017a9bbbadd10bcd7f13088a97ca3cc","title":"Dissecting Recall of Factual Associations in Auto-Regressive Language Models"},{"paperId":"be55e8ec4213868db08f2c3168ae666001bea4b8","title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"574beee702be3856d60aa482ec725168fe64fc99","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"dcd8dff20f4f490acd5f94001d34c774167f053e","title":"Jump to Conclusions: Short-Cutting Transformers With Linear Transformations"},{"paperId":"762ca2711eb167f19b79e39c175708ca15e1f5d7","title":"Eliciting Latent Predictions from Transformers with the Tuned Lens"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"f680d47a51a0e470fcb228bf0110c026535ead1b","title":"Progress measures for grokking via mechanistic interpretability"},{"paperId":"89c3bd70ad33c4f8832f00ab98872b77861ee0ec","title":"Discovering Latent Knowledge in Language Models Without Supervision"},{"paperId":"6edd112383ad494f5f2eba72b6f4ffae122ce61f","title":"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"},{"paperId":"9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models"},{"paperId":"c90a99eeb57019732a6cc996bb9eaf13faedf00f","title":"In-context Learning and Induction Heads"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"1bcde55995a957b3e8a595d536b816cb8989cf1d","title":"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cf36236015c9f93f15bfafbf282f69e08bdc9c16","title":"Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"},{"paperId":"996445d847f06e99b0bd259345408a0cf1bce87e","title":"Locating and Editing Factual Associations in GPT"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"aaf3ebaf12baeb366ce6ff32aa36d608a7eab583","title":"On the Pitfalls of Analyzing Individual Neurons in Language Models"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"488de57dde884402d88ccecfd02347dd7e4d01a1","title":"Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models"},{"paperId":"57a1258571a21817d89197dc84c986861fb6e580","title":"Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning."},{"paperId":"5951ede73a1dbb9c5ea8b4d95f31c5ed646aacbd","title":"Causal Abstractions of Neural Networks"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"b5ba72aaaef1ae5dccb313c64a5cfb5de3e2b442","title":"Multimodal Neurons in Artificial Neural Networks"},{"paperId":"9558d5637955bc444b45ee867a2959d98b0d3e6c","title":"Visualizing Weights"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"4a54d58a4b20e4f3af25cea3c188a12082a95e02","title":"Transformer Feed-Forward Layers Are Key-Value Memories"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"54a13bcc9613dcaa76fb25fbe96572f376cfcca9","title":"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b8aa08f1ae7ffa568b3ea925af121d629e862b99","title":"The Building Blocks of Inter-operability"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"40be3888daa5c2e5af4d36ae22f690bcc8caf600","title":"Visualizing and Understanding Recurrent Networks"},{"paperId":"d8574a62874312b81347438b1566cdb1c6d5abe5","title":"Direct and Indirect Effects"},{"paperId":"fca0e8bb93a8242c153ef966cefade5e19e5ed21","title":"Feature Visualization"},{"paperId":"a0117b72a4f68d0a134e24f674ca7fd0b42663b7","title":"Causality"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":null,"title":"A mathematical framework for transformer circuits"},{"paperId":"78d08b8ab4132defffe98ec7f80a51452203f70d","title":"Investigating Gender Bias in Language Models Using Causal Mediation Analysis"},{"paperId":null,"title":"2023. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks"},{"paperId":null,"title":"We measure the causal effect of the intervention on variables m ( l ) t and a ( l ) t on the model’s prediction by computing the change in the probability values assigned to the results r and r ′"},{"paperId":null,"title":"During the forward pass with input p 1 , we store the activation values ¯ m ( l ) t := MLP ( l ) ( h ( l − 1) t ) , and ¯ a ( l ) t := A ( l ) ( h ( l − 1) 1 , . . . , h ( l − 1) t )"},{"paperId":null,"title":"f O"},{"paperId":null,"title":"Natural Language Processing (Volume 1"},{"paperId":null,"title":"2022b. Chain of thought prompting elicits reasoning in large language models"}],"id":"ecf0cb0725de18659f9ba25a8cf65a1085564006","summary":"A mechanistic interpretation of LLMs for arithmetic-based questions using a causal mediation analysis framework, which identifies the subset of parameters responsible for specific predictions and investigates the role of the attention mechanism."},{"url":"https://www.semanticscholar.org/paper/24a285b1e7ee9acfec9a82f7123563b62f532203","title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/05/2023","authors":"Jasivan Sivakumar,N. Moosavi","citations":[{"paperId":"18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6","title":"Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception"}],"references":[{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"0232a71bd47b6410dea4b00559acb93755f65fff","title":"Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"2ca4b9bca494619818fcf6f1a2b41956949fbee0","title":"Solving Arithmetic Word Problems Using Natural Language Processing and Rule-Based Classification"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"68900bf4b9cc820f4b47608871eafcac65a33917","title":"Adversarial Examples for Evaluating Math Word Problem Solvers"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"f8e0f7babe7cd858aecfafb03375bf578fe161f7","title":"MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"ad46feec0cefb78dafa60821598c052cf0b9b7bd","title":"NT5?! Training T5 to Perform Numerical Reasoning"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"3684491d62db5c3e5602375271e4b339bbf416ee","title":"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"07c1c2429b63fefdae41eb546c31b40de2a880f7","title":"INFOTABS: Inference on Tables as Semi-structured Data"},{"paperId":"33ec7eb2168e37e3007d1059aa96b9a63254b4da","title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"81b4920ad488affaee27389ff9540b7fea90a4ce","title":"“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding"},{"paperId":"0c3c4c88c7b07596221ac640c7b7102686e3eae3","title":"PubMedQA: A Dataset for Biomedical Research Question Answering"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"51004bc6461a572e1189a0e3b32b441155d760ce","title":"QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships"},{"paperId":"8f1c9b656157b1d851563fb42129245701d83175","title":"Transforming Question Answering Datasets Into Natural Language Inference Datasets"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"038a496d70f8edc2a3bf6c8e112a9dd4f9a3ea22","title":"Illinois Math Solver: Math Reasoning on the Web"},{"paperId":"446f1eaec22a90574670491073cd5b03bfa1e273","title":"Identification and Verification of Simple Claims about Statistical Properties"},{"paperId":"52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"8f90451a627c0b5d0cdff5f6e11b4cdf2fe008db","title":"Improving the Numerical Reasoning Skills of Pretrained Language Models"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":"0ce43def1c95802a464449f055e78c08d6d99dc2","title":"Towards Table-to-Text Generation with Numerical Reasoning"},{"paperId":"1893f9875fe6a5b40b82838aa3a4259f5763d7f0","title":"SciGen: a Dataset for Reasoning-Aware Text Generation from Scientific Tables"},{"paperId":"3838387ea8dd1bb8c2306be5a63c1c120075c5a2","title":"Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning"},{"paperId":"72324e26cddab0792986dc8357e2459afec7ae5e","title":"Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics"},{"paperId":null,"title":"d) One Number : at least one number is seen with none of the operations"},{"paperId":null,"title":"The templates’ operation distribution is given by Table 4"},{"paperId":null,"title":"Table 3: Distribution of the mathematical operations the Original set."}],"id":"24a285b1e7ee9acfec9a82f7123563b62f532203","summary":"This work introduces a multi-view evaluation set for numerical reasoning in English, called FERMAT, which evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency."},{"url":"https://www.semanticscholar.org/paper/857d4589b62085e900805fd5432f496e8fc07bd9","title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model","venue":"arXiv.org","year":2023,"referenceCount":53,"citationCount":19,"influentialCitationCount":2,"publicationDate":"30/04/2023","authors":"Michael Hanna,Ollie Liu,Alexandre Variengien","citations":[{"paperId":"07c2c3b4af7e4ff1577f36f47f1c93398a6df648","title":"Successor Heads: Recurring, Interpretable Attention Heads In The Wild"},{"paperId":"96706639ed81471fc37dc0d130b8db679a6d173c","title":"Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks in Language Models"},{"paperId":"e2bc390cf21dc319ea5aa9a7c3a223911dbf2012","title":"Interpretability Illusions in the Generalization of Simplified Models"},{"paperId":"4726d1dc54851db99c29180127d840bd19f20afc","title":"Positional Description Matters for Transformers Arithmetic"},{"paperId":"458642a7695013cd128bb3070540970be814e50d","title":"Locating Cross-Task Sequence Continuation Circuits in Transformers"},{"paperId":"40c34e85dc4558d26bfef7fb54327dbd4a0bebc3","title":"Uncovering Intermediate Variables in Transformers using Circuit Probing"},{"paperId":"af365d54e237fb213d980b2dc0c2ef1a4280bbd7","title":"In-Context Learning Dynamics with Random Binary Sequences"},{"paperId":"1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b","title":"What Algorithms can Transformers Learn? A Study in Length Generalization"},{"paperId":"df02a738a0daa6fbdabe9144762b9ed1ef9d4cf8","title":"Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model"},{"paperId":"7b1c53e392dc322e7f9525b5c7fb8cdf0e357f8a","title":"Instilling Inductive Biases with Subnetworks"},{"paperId":"27f7aa77bf343fefd3984c6b23265af672bcc0a3","title":"Attribution Patching Outperforms Automated Circuit Discovery"},{"paperId":"0b349cac39f1c521af21fd6aac7f9b7372fa8a66","title":"Interpretable Diffusion via Information Decomposition"},{"paperId":"c0d9a48547d728dd320b453b01a0ab1ce2f96098","title":"Circuit Component Reuse Across Tasks in Transformer Language Models"},{"paperId":"0eae83a76b2db97547d8ed42bb048c3eaaf78027","title":"Copy Suppression: Comprehensively Understanding an Attention Head"},{"paperId":"740c783ac07039cf30b6d8a8f95e775b3297c79e","title":"Language Models Represent Space and Time"},{"paperId":"c16c05ca0a3d24519405849fd24604fc1ce47751","title":"Towards Best Practices of Activation Patching in Language Models: Metrics and Methods"},{"paperId":"110804428354df709b3693f9efc81946a9036ebf","title":"Neurons in Large Language Models: Dead, N-gram, Positional"},{"paperId":"0db0af0cd3ceb0531a050a03e6ceb849580ff53b","title":"Teaching Arithmetic to Small Transformers"},{"paperId":"eefbd8b384a58f464827b19e30a6920ba976def9","title":"Towards Automated Circuit Discovery for Mechanistic Interpretability"}],"references":[{"paperId":"8d8fc878bf4c7005546c866824a72d0c46ca91a3","title":"Localizing Model Behavior with Path Patching"},{"paperId":"762ca2711eb167f19b79e39c175708ca15e1f5d7","title":"Eliciting Latent Predictions from Transformers with the Tuned Lens"},{"paperId":"f680d47a51a0e470fcb228bf0110c026535ead1b","title":"Progress measures for grokking via mechanistic interpretability"},{"paperId":"6edd112383ad494f5f2eba72b6f4ffae122ce61f","title":"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"},{"paperId":"b34a7d4f40d7653b15f4e2e04405cf59b8821d5d","title":"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task"},{"paperId":"2a7ae3e98357569c41424dacd60c62d3df78a0db","title":"Limitations of Language Models in Arithmetic and Symbolic Induction"},{"paperId":"2c709ef6186bd607494a3344c903552ea500e449","title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"8b293973061026d9d0eed90e71e30928e029171e","title":"Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models"},{"paperId":"20de79ec4fe682b68930eb4dcd91b1801b8d4731","title":"Towards Understanding Grokking: An Effective Theory of Representation Learning"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"cf36236015c9f93f15bfafbf282f69e08bdc9c16","title":"Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"},{"paperId":"28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d","title":"Quantifying Memorization Across Neural Language Models"},{"paperId":"996445d847f06e99b0bd259345408a0cf1bce87e","title":"Locating and Editing Factual Associations in GPT"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"6cc9484612ab146c9fed9f7dce283c815af3cbc8","title":"Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids’ Representations"},{"paperId":"ff56dfdbc8b86decbc6119d96c1097c0fef56ecd","title":"Neuron-level Interpretation of Deep NLP Models: A Survey"},{"paperId":"f51e164e4b116846867ca9f78c5ff23583e4fd09","title":"Roles"},{"paperId":"5951ede73a1dbb9c5ea8b4d95f31c5ed646aacbd","title":"Causal Abstractions of Neural Networks"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"3dcfa05a1c162e6cab927c5b08d0444f7b6691f4","title":"Probing Classifiers: Promises, Shortcomings, and Advances"},{"paperId":"4a54d58a4b20e4f3af25cea3c188a12082a95e02","title":"Transformer Feed-Forward Layers Are Key-Value Memories"},{"paperId":"1d7f3297924a9dd90cfc0df522ebe9138c28b46f","title":"Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals"},{"paperId":"9f0272bb258506fdc0ee7d8951593914d4f9c39d","title":"Analyzing Individual Neurons in Pre-trained Language Models"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"922535984641f052d2530c8b68f9ebd7de38dfa5","title":"Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?"},{"paperId":"ca8d228cc20829fa95a95bfd5f7e92b2d13f9f5a","title":"How Do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"a0cfd36e6c7abf070f492ae52a35af895a1c5592","title":"Zoom In: An Introduction to Circuits"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"a5dde0a71fb0465d33b95496c4bb64914b3d62d9","title":"Exploring Numeracy in Word Embeddings"},{"paperId":"95a251513853c6032bdecebd4b74e15795662986","title":"What Does BERT Look at? An Analysis of BERT’s Attention"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"cca6f4177921be0dd0ea2794d9d788f265b44da0","title":"The emergence of number and syntax units in LSTM language models"},{"paperId":"9c2156bc35c6f8e68aa21d4b2f339134a4d28708","title":"What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models"},{"paperId":"c5489d244bfc1e9b0d8c94bf6dd774ee1aca2def","title":"Identifying and Controlling Important Neurons in Neural Machine Translation"},{"paperId":"7fedd981f2769bd009f749a3dff7044d8378c9b4","title":"Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"f170fed9acd71bd5feb20901c7ec1fe395f3fae5","title":"Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"547f23597f9ec8a93f66cedaa6fbfb73960426b1","title":"The Berkeley FrameNet Project"},{"paperId":"a0117b72a4f68d0a134e24f674ca7fd0b42663b7","title":"Causality"},{"paperId":"c095cb3c28ba6eb4bd90ed8bee3460bfc2f10a19","title":"The Functional Relevance of Probed Information: A Case Study"},{"paperId":null,"title":"Scaling language modeling with pathways, 2022"},{"paperId":null,"title":"A mathematical framework for transformer circuits"},{"paperId":"78d08b8ab4132defffe98ec7f80a51452203f70d","title":"Investigating Gender Bias in Language Models Using Causal Mediation Analysis"},{"paperId":null,"title":"Nostalgebrist"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"We see this weakly by viewing the top-10 neurons' contributions, but more strongly in the top-100 or 200 (of 3072) neurons (Figure 20)"}],"id":"857d4589b62085e900805fd5432f496e8fc07bd9","summary":"The basic mathematical abilities often acquired by pre-trained language models are investigated, and mechanistic interpretability techniques are used to explain the (limited) mathematical abilities of GPT-2 small."},{"url":"https://www.semanticscholar.org/paper/073e6b91adc25c656d85002e3cb059e4530db20b","title":"Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":8,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Zhenwen Liang,W. Yu,Tanmay Rajpurohit,Peter Clark,Xiangliang Zhang,Ashwin Kaylan","citations":[{"paperId":"d43587743abd006be30756b67efd87f61671412b","title":"Tartare: Automatic Generation of C Pointer Statements and Feedback"},{"paperId":"04cc5744fe8b436fb0f8b2763ed8ac7e4a60f7d1","title":"Combining GPT and Colab as learning tools for students to explore the numerical solutions of difference equations"},{"paperId":"0f7240d1c43b19c88eebe5708700ebdfc17221f9","title":"Knowledge Distillation of LLM for Education"},{"paperId":"1a6bd4f70bceb26ddb722ce98c0eae2a64147048","title":"Knowledge-Based and Generative-AI-Driven Pedagogical Conversational Agents: A Comparative Study of Grice’s Cooperative Principles and Trust"},{"paperId":"98b607e7cb84e1a5c87c8a49562ae35435e6722d","title":"Learning From Mistakes Makes LLM Better Reasoner"},{"paperId":"e1414fc1e1a6752524a1807a29ee406e8d808849","title":"Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models"},{"paperId":"c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36","title":"MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning"},{"paperId":"0b47c8eae44c8c3c5f474d1b9869ba90a5199eae","title":"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements"}],"references":[{"paperId":"3fc3460c4554a28e489a0ea6ef067b79b7d301d9","title":"Active Prompting with Chain-of-Thought for Large Language Models"},{"paperId":"69619a2a47faee7a29ec596db13172e2a42ff921","title":"Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models"},{"paperId":"a9e3e5dd7b30890553b7ae1c41f932e99192bb44","title":"Large Language Models Are Reasoning Teachers"},{"paperId":"126a4776ff8315fd506766cb8f3c722cf746ad9e","title":"Teaching Small Language Models to Reason"},{"paperId":"71471561137956d6aeec39173db460a28d86c11b","title":"Analogical Math Word Problems Solving with Enhanced Problem-Solution Association"},{"paperId":"e6745fb621481ccb0ed53c267a37292e499c1b42","title":"Automatic Generation of Socratic Subquestions for Teaching Math Word Problems"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"95d54e3ce577f7d91ab4b2c52c73b501245e484d","title":"ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback"},{"paperId":"7d29a84a589aa5655e5d3fed8d725ea472816599","title":"Explanations from Large Language Models Make Small Reasoners Better"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"4feb4f49c5837cbb1c9c8e61d4e8056cafb102e6","title":"Unbiased Math Word Problems Benchmark for Mitigating Solving Bias"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"a376e6b118a10a8cb8a2920f6b83a70f087579f5","title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"},{"paperId":"7955b46e640fe460d9c781263c498a19a460bb9b","title":"PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks"},{"paperId":"2145fcceeb69385e108bf1796d52f974854d4c0b","title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"5e8d3c2dc0fc53949794fc00600e25558c4a2441","title":"WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation"},{"paperId":"110c359de78cf1071a3d7a1cd61396b3b8f2722e","title":"Knowledge Tracing: A Survey"},{"paperId":"39df1a17da84f02bfcb8751de8965b798653a5ee","title":"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems"},{"paperId":"521ccc898395a2818fced22b4cf371b0e5121f94","title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models"},{"paperId":"81c241c2cb8e86a5574797424bc281e323cd831b","title":"Recall and Learn: A Memory-augmented Solver for Math Word Problems"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"f8e0f7babe7cd858aecfafb03375bf578fe161f7","title":"MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"eb2f1480eca6195fe8dbab5de8bbac8749e9be58","title":"RODA: Reverse Operation Based Data Augmentation for Solving Math Word Problems"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"6605bba6e0caabda06b090d67698a5683eba4dfa","title":"Translating a Math Word Problem to an Expression Tree"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"c20584e878efbc4d716710b9aa2f4954c4014638","title":"Learning Fine-Grained Expressions to Solve Math Word Problems"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"f6b5335f27b9583dd152d8cd4ea9134e24bd297b","title":"Learning To Use Formulas To Solve Simple Arithmetic Problems"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"acfe603d8a90ee3d4187e426b26c4d7bf394b4c8","title":"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"e2694d343993482626163fa9e4f3d3d0ba5f8efd","title":"A review of methods for automatic understanding of natural language mathematical problems"},{"paperId":"5b3ffcbd38b130e54e161b22e513302ad3f186ce","title":"The Real Story Behind Story Problems: Effects of Representations on Quantitative Reasoning"},{"paperId":"34c0410aaaed15350e3c576ad2bc327e8a444c65","title":"The power of feedback."},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"ca1a53c7cadfca7e3201568ba84bba490e6077b1","title":"Comprehension of arithmetic word problems: A comparison of successful and unsuccessful problem solvers."},{"paperId":"43489fda8a0c6276a3aa8f068892e31ebfd63c13","title":"Understanding and solving arithmetic word problems: A computer simulation"},{"paperId":"72123a86eae2cb5c4eae8650f43524039d48875d","title":"Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions"},{"paperId":"25eba922c7076e1a8a0a86fd0eed2bfcdbe0bbc3","title":"AugESC: Large-scale Data Augmentation for Emotional Support Conversation with Pre-trained Language Models"},{"paperId":"5926be5b2832d69e97f50588e1f5005b892743a7","title":"Solving Math Word Problems with Teacher Supervision"},{"paperId":null,"title":"Gpt-j-6b: A 6 billion parameter autoregressive language model"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Personalized learning: A guide for engaging students with technology"},{"paperId":"645b2c28c28bd28eaa187a2faafa5ec12bc12e3a","title":"Knowledge tracing: Modeling the acquisition of procedural knowledge"}],"id":"073e6b91adc25c656d85002e3cb059e4530db20b","summary":"A novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles."},{"url":"https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":233,"citationCount":48,"influentialCitationCount":3,"publicationDate":"20/12/2022","authors":"Pan Lu,Liang Qiu,Wenhao Yu,S. Welleck,Kai-Wei Chang","citations":[{"paperId":"608a2b333fd8262e8c918f36c5700bafd3ea3cdd","title":"GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning"},{"paperId":"7017c58e19f4db0c38040935cc9fb7b7090a466d","title":"Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent"},{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"e99de66608a3b060d54548b9e9a7c39961872cd7","title":"LANS: A Layout-Aware Neural Solver for Plane Geometry Problem"},{"paperId":"89ed7fd00319d45269906a9b05e10c8680bf9cec","title":"FinanceBench: A New Benchmark for Financial Question Answering"},{"paperId":"6fa0677731184444df0e1fc8070938419cd6da47","title":"Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"},{"paperId":"b6667ba4f586489f12587446c6daaa3f09cfc539","title":"Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"c22cc4e2eed78b4b31e50d94ea35da0405aabb87","title":"Multi-Operational Mathematical Derivations in Latent Space"},{"paperId":"f176d0d466d7c778a6435fe9a8d7e49508cb9059","title":"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts"},{"paperId":"b16c7d45183b9d595ab64301be019741b1528860","title":"Llemma: An Open Language Model For Mathematics"},{"paperId":"44b506d9619b5f957dc2b5588801138f343c0308","title":"Let's reward step by step: Step-Level reward model as the Navigators for Reasoning"},{"paperId":"b9e8b62bcc019f47a0a015568f70039b3b7c1196","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model"},{"paperId":"8db1dcae055842f43ccac04182957b20d15bbe6b","title":"Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems"},{"paperId":"287a30043ad1c3e349095af7e3e42d3be3b6c0c9","title":"SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training"},{"paperId":"5076bbbf831a92174c9cc1b347bd0584560435fc","title":"Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning"},{"paperId":"e61a96cf602ebff6683929aaf916e25614a475bc","title":"UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities"},{"paperId":"8d806a91e5f2166ee6823eb7e6e8e56826b6776d","title":"NLPBench: Evaluating Large Language Models on Solving NLP Problems"},{"paperId":"856c342606aca05434e48f2e53cdbd6f6b886802","title":"Pre‐trained language models: What do they know?"},{"paperId":"2d4ee2195cf1932eb6b2d940c241f4f4443c0a6e","title":"Design of Chain-of-Thought in Math Problem Solving"},{"paperId":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners"},{"paperId":"537335d9aad0ddbaef93e7f88b0db096671ef6ec","title":"No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function"},{"paperId":"2dbe7fd41f23aa2a077c3d1e91b0b73f92e3d8d2","title":"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning"},{"paperId":"dd18782960f9ee4c66b79e1518b342ad3f8d19e7","title":"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"},{"paperId":"692902404c282de936249aef68ce9f974815128c","title":"CLEVA: Chinese Language Models EVAluation Platform"},{"paperId":"ca4d1d9c61e6496bbb0f361b55413f265d81de6e","title":"Prompting Large Language Models for Malicious Webpage Detection"},{"paperId":"4993258852711c4e3d0011325ac3db680eae84f4","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"},{"paperId":"87875a07976c26f82705de1fc70041169e5d652b","title":"LeanDojo: Theorem Proving with Retrieval-Augmented Language Models"},{"paperId":"fae57797d357bfa3b39b220336d1a2e8deba5318","title":"JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving"},{"paperId":"587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning"},{"paperId":"7eb044170c11b7e2193b8df35f606edcfc7f2585","title":"Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"95ca67ba607d7859ee8eec457f4b59b115d69bf5","title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models"},{"paperId":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"7a6dc7071891cb3d658c93418801942a4c6ed373","title":"Autonomous GIS: the next-generation AI-powered GIS"},{"paperId":"0ed565e9c2ddb80e3d6cc54c921e08f95e569eb0","title":"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs"},{"paperId":"7e4f5589327b6b574cc950a03fd1d6236e9e6128","title":"Answering Questions by Meta-Reasoning over Multiple Chains of Thought"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"c61d54644e9aedcfc756e5d6fe4cc8b78c87755d","title":"A Survey of Large Language Models"},{"paperId":"cd854d3d7a1231b1dfbbfa09a697ac064026be51","title":"Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models"},{"paperId":"5938abafd61881f6b23a2ba318d2d3d0327402c0","title":"A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram"},{"paperId":"3f2cb353c7528efafb847309ab1e1e95245740a4","title":"Mathematical Capabilities of ChatGPT"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"668701cdb6f4f5a079ad60173885c172dde20f17","title":"Number-enhanced representation with hierarchical recursive tree decoding for math word problem solving"},{"paperId":"a3bbe282f1666900a6604991f96e1637c6946253","title":"MATHVISTA: EVALUATING MATHEMATICAL REASON-"},{"paperId":"bba47079d385bab0facf4527d5da35b7ff0a5c3d","title":"Basic Arithmetic Properties in the Space of Language Model Prompts"}],"references":[{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"72fce949725b20428e5f56247fef5c6bd1ce6154","title":"UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"75f7e9e2b59fb640ef9d1dff94097175daf46c4d","title":"Large Language Models Struggle to Learn Long-Tail Knowledge"},{"paperId":"538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct"},{"paperId":"4634362d75606287955260ef1788171286efbeaa","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"d96997265f8146e93b4c9350f19d55e46d1317f0","title":"ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering"},{"paperId":"07955e96cbd778d0ae2a68f09d073b866dd84c2a","title":"Decomposed Prompting: A Modular Approach for Solving Complex Tasks"},{"paperId":"c88cafa3e980765a64febe369ceb7c2aa7261d2a","title":"Complexity-Based Prompting for Multi-Step Reasoning"},{"paperId":"3e565c544a8639cc9c7568833e484d7610f5e5d4","title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"b2542a738b75ee9b7ce1a13d8b78f9095d212412","title":"Generate rather than Retrieve: Large Language Models are Strong Context Generators"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"d4f42cf69b91f08e86acc36b6f9f47bd1aac4a2a","title":"Insights into Pre-training via Simpler Synthetic Tasks"},{"paperId":"0b7e9b6b588baa3f24fdde06feec26a067aa74bd","title":"MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"},{"paperId":"3464bf8d2fa50e8b5ddd07cfb10790bbfe6dfa2f","title":"A Survey in Mathematical Language Processing"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"c28e95a06dfcf13fc65a1cac83722f53e34f12a5","title":"Autoformalization with Large Language Models"},{"paperId":"50b0c6ee2b3d53ba5af69d6c00b5d60888a9026f","title":"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"65b4b25272c50dc376f5c018338931bfd349e532","title":"HyperTree Proof Search for Neural Theorem Proving"},{"paperId":"c2d574f7c6a9e3bafe396ecb4ab639179d6fd92c","title":"Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"f08cf3957d6f4ca66296cfd361d30fea08bccf65","title":"PGDP5K: A Diagram Parsing Dataset for Plane Geometry Problems"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"a376e6b118a10a8cb8a2920f6b83a70f087579f5","title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"},{"paperId":"b2e8906d0c2999e3b6cbb5bfd7dc687f1a0a751c","title":"Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"916a06a6d51aa93de27aac2f3e14faed08dd6706","title":"Formal Mathematics Statement Curriculum Learning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"f9838a3be5c94bb2674a0e224de349b50e18f3c4","title":"Learning To Retrieve Prompts for In-Context Learning"},{"paperId":"3af37400f1f9a4f4f211c4a472e18963edc2b34f","title":"ValueNet: A New Dataset for Human Value Driven Dialogue System"},{"paperId":"ab15163d8c57fa5706d399c8fb62e489d57b22b7","title":"Injecting Numerical Reasoning Skills into Knowledge Base Question Answering Models"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"45ece6f3b0a319dba60c20b3013b5161dd49c58b","title":"Linear algebra with transformers"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"3ceca3cc39cd90515ccd73afe6539cc953762140","title":"How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI"},{"paperId":"9bcf3b43f2323a194036cc52c6878a9b1dc7e058","title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning"},{"paperId":"39df1a17da84f02bfcb8751de8965b798653a5ee","title":"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems"},{"paperId":"a7353543dd7cc2dc8396e8e8d2a1b60e9d38985c","title":"Improving Fractal Pre-training"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"927efd299cffcfca3716efefcc904331b70c153e","title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset"},{"paperId":"3d33c71af053b42c14ad8d476c9df9cf6dfc1e16","title":"Does Pretraining for Summarization Require Knowledge Transfer?"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"934bdb14923510a2dedd3682a3f2c89f7e1ab364","title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"7ba98b00a224094c09676090f5d6d69498f5b299","title":"MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"102ebe229df18c8733ea1b8def56cd79996e2178","title":"A Survey of Human-in-the-loop for Machine Learning"},{"paperId":"f8e0f7babe7cd858aecfafb03375bf578fe161f7","title":"MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving"},{"paperId":"2406cf39805c70264c4226b7325a09b506c70921","title":"TAPEX: Table Pre-training via Learning a Neural SQL Executor"},{"paperId":"5cfdc75bcd204c94eb979c5b17fdb86d33b3c184","title":"Solving arithmetic word problems by scoring equations with recursive neural networks"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7654dbd372d8b65e730e3bd477ff9fec96c16dc5","title":"Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"291133a657498920451481d3bf784ebbafda8d6e","title":"GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning"},{"paperId":"4ae6b5ce971c58c1280bc971a1879e6d547c5f8c","title":"A Bottom-Up DAG Structure Extraction Model for Math Word Problems"},{"paperId":"680b0467861a70be41c31e4f2415fe5e2958fbc0","title":"HMS: A Hierarchical Solver with Dependency-Enhanced Understanding for Math Word Problem"},{"paperId":"b3213c84a6ff7a2f11099de783c93166e4fc02a4","title":"TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance"},{"paperId":"fb1c90806fc5ec72987f58110aa255edbce6620d","title":"Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning"},{"paperId":"593499b654360101682edec1dd711fa7c09f6971","title":"IsarStep: a Benchmark for High-level Mathematical Reasoning"},{"paperId":"0adec918885dff698acf359988ed79a543157f80","title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"},{"paperId":"4cc1fb128fa3abf6f90d567744767e8fd6315e1d","title":"NaturalProofs: Mathematical Theorem Proving in Natural Language"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"3baaabdd47c466df1f728deb31ae509aacf289c4","title":"Towards Socially Intelligent Agents with Mental State Transition and Human Value"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"9231927bc0a9ed10de64cad05640587893eba4b1","title":"Proof Artifact Co-training for Theorem Proving with Language Models"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"cb596bffc5c5042c254058b62317a57fa156fea4","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"58c74cec28f3416b9a1d308bb2d6519d21d53ab0","title":"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"593ca71119fb6ee560926d9b304bde095267432d","title":"SMART: A Situation Model for Algebra Story Problems via Attributed Grammar"},{"paperId":"edaafec59b651d503ac7c8a86f8e2335273e1f7a","title":"Learning by Fixing: Solving Math Word Problems with Weak Supervision"},{"paperId":"53953a19e2fdf9c5ad5ff445c07ce36cc70f551b","title":"Solving Math Word Problems with Multi-Encoders and Multi-Decoders"},{"paperId":"24ed85ad966823868c1694a19385d01c6ad71008","title":"A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving"},{"paperId":"3938fe72ccfe4fe92387258874cb1cbe66194d4f","title":"Point to the Expression: Solving Algebraic Word Problems Using the Expression-Pointer Transformer Model"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"f9c07ed1d2113c858b38861790af6a26310b8465","title":"Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"},{"paperId":"eb2f1480eca6195fe8dbab5de8bbac8749e9be58","title":"RODA: Reverse Operation Based Data Augmentation for Solving Math Word Problems"},{"paperId":"44507bde6e9caf60b41c60d703e7972b520d48a6","title":"Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems"},{"paperId":"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","title":"Measuring Massive Multitask Language Understanding"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"016863a86189c4e8ccecf9a36c4406c439a8a84c","title":"INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"33ba3001cdbc1c1f59fefe5368300245c05560c5","title":"Teacher-Student Networks with Multiple Decoders for Solving Math Word Problem"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"cdcdc7ab1f5b6e86146b5c0224cba7d8cd35142c","title":"What Does BERT with Vision Look At?"},{"paperId":"ab38bbb36ba38047c5bb556694d148225971957f","title":"Premise Selection in Natural Language Mathematical Texts"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"97f08c1ae8ca5ddf5948c66bfbbc0546ac154807","title":"Pretrained Transformers Improve Out-of-Distribution Robustness"},{"paperId":"750bf158dd5060dfac0c6e1734654060df1f6374","title":"Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem"},{"paperId":"52cb05d721688cb766c6e282e9d55c3b8e3dc0cf","title":"TaPas: Weakly Supervised Table Parsing via Pre-training"},{"paperId":"8a5590978930070f50c5f9fbf61f67e5d95794f0","title":"Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"1180c1d2b2fad2527e98c0065319ea8150a3b888","title":"Natural Language Premise Selection: Finding Supporting Statements for Mathematical Text"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","title":"Pre-trained models for natural language processing: A survey"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"b39eed03d345f5c244eac12fd1315d26eba77d62","title":"Deep Learning for Symbolic Mathematics"},{"paperId":"388e2fcdcefbe0834e153ab2a0be127092f9674d","title":"DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation"},{"paperId":"cd36b981e39edc5517d2a508e879b4d9bdc1b1c6","title":"Tree-structured Decoding for Solving Math Word Problems"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"9df88155d6b65e14e06f8dcd851d31b7753659b1","title":"The lean mathematical library"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"81b4920ad488affaee27389ff9540b7fea90a4ce","title":"“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding"},{"paperId":"007bc04c97f9f8bcec0487699e197315418f22e7","title":"From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the Aristo Project"},{"paperId":"20fa66cb21d8a1b7436306af4b0bedddd3bb470b","title":"Solving Math Word Problems with Double-Decoder Transformer"},{"paperId":"5bf6ca86a4f21d07014ce60f3cd09345ad3414e2","title":"A Goal-Driven Tree-Structured Neural Model for Math Word Problems"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"9ff3afa6577d5469cd68c4991476f426efdf6bca","title":"Template-Based Math Word Problem Solvers with Recursive Neural Networks"},{"paperId":"865f5167c4353d2b120f0469ed1c298bc92794fa","title":"Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"80c4185d5311841cbbcbb9f1d3555b61081ca5d8","title":"HOList: An Environment for Machine Learning of Higher Order Logic Theorem Proving"},{"paperId":"a596f03145285cd05a6ca57a4e25418b23b24976","title":"Learning to Prove Theorems via Interacting with Proof Assistants"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"6d6c91485aa0527195c256997be54c188f7640ad","title":"Survey on Mathematical Word Problem Solving Using Natural Language Processing"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"6ff68b34a5f78bdd14437fe5a79aebbc42c26467","title":"DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension"},{"paperId":"1eff94b44432a8e6af29288b8234494516579ad3","title":"EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"},{"paperId":"e9b13731027418ed38103d1dfc8a70f6881bc684","title":"Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering"},{"paperId":"51004bc6461a572e1189a0e3b32b441155d760ce","title":"QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships"},{"paperId":"6605bba6e0caabda06b090d67698a5683eba4dfa","title":"Translating a Math Word Problem to an Expression Tree"},{"paperId":"f67ede490780f939103d4cc4e9c6866b83ee59b3","title":"Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"caeb950e503872a903e18a3b259424e3cc3c6006","title":"Neural Math Word Problem Solver with Reinforcement Learning"},{"paperId":"de95601d9e3b20ec51aa33e1f27b1880d2c44ef2","title":"CBAM: Convolutional Block Attention Module"},{"paperId":"87c425f23bcac2f082968abda64a971f91522f73","title":"GamePad: A Learning Environment for Theorem Proving"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"a396effc8987cc0938a4057d9d9008ff2a5452b6","title":"Data-Driven Methods for Solving Algebra Word Problems"},{"paperId":"835c6b524b90b1639aba28742f7161137ddf4397","title":"MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning"},{"paperId":"b1b88c6d1f05913be18750ff00e06edd5e1f49ef","title":"TacticToe: Learning to Prove with Tactics"},{"paperId":"7289a240c9425bc7cad87b3b835e5f0cac22f488","title":"DVQA: Understanding Data Visualizations via Question Answering"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"cb6be69c67b0b15ebbda89a126f4dd62a4d32958","title":"FigureQA: An Annotated Figure Dataset for Visual Reasoning"},{"paperId":"7cfa5c97164129ce3630511f639040d28db1d4b7","title":"FiLM: Visual Reasoning with a General Conditioning Layer"},{"paperId":"678fd7c48efe21434148b4b3482c2b8b3ee618fc","title":"Deep Neural Solver for Math Word Problems"},{"paperId":"81f466a535cdec4957989999f9ca381bc4fe14e9","title":"From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems"},{"paperId":"c20584e878efbc4d716710b9aa2f4954c4014638","title":"Learning Fine-Grained Expressions to Solve Math Word Problems"},{"paperId":"7220d7ec31f6dc6ad7030f601743b5392513a2d9","title":"Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"38bdca75587ed41bb7e1e6673fe5118aa25efe89","title":"A Survey of Question Answering for Math and Science Problem"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"f1318d15d7d8ab626b92d0c70dbdc5b5d37e223f","title":"HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving"},{"paperId":"e77e00286a63a32dafb9629cd79f6a77bddb1941","title":"Deep Network Guided Proof Search"},{"paperId":"8b7336f5dd13a45d4aab38428b4a88ce507ea310","title":"Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving"},{"paperId":"3d3b4ec7789f634e0752d50484dad7d2ea2460d5","title":"Dialogue Learning With Human-In-The-Loop"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"ba84b8f16efc032945cd8ed960c121a9bcd064d5","title":"Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems"},{"paperId":"2ab3c53b08d4180c8642207ee09738f8df193b92","title":"Holophrasm: a neural Automated Theorem Prover for higher-order logic"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"09900b36ca35af12301507cf992675808a709838","title":"How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"},{"paperId":"f8909e8b359c72bc0e3257dadcc48a8787bca74b","title":"DeepMath - Deep Sequence Models for Premise Selection"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"0da2fc91eb560f4245d8e7b6dff9edd0a5727dff","title":"DRAW: A Challenging and Diverse Algebra Word Problem Set"},{"paperId":"6a9533730bc2070b222b056df9bf0ee16ba7a509","title":"Four Decades of Mizar"},{"paperId":"acfe603d8a90ee3d4187e426b26c4d7bf394b4c8","title":"Automatically Solving Number Word Problems by Semantic Parsing and Reasoning"},{"paperId":"c87dccf7c21e67679389f23f86f039cd96720c3f","title":"Solving Geometry Problems: Combining Text and Diagram Interpretation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"2a441a46e228ed0ea2251a4e61be6c7025b45766","title":"The Lean Theorem Prover (System Description)"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"32de44f01a96d4473d21099d15e25bc2b9f08e2f","title":"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"9f9d16e6cf929c66be59fd56d9870196c19410c1","title":"Distributed Asynchronous Online Learning for Natural Language Processing"},{"paperId":"226966243877f186d346be01047cf71cee1b5ec4","title":"Online EM for Unsupervised Models"},{"paperId":"1bc38b2c99f58a9199771c124248eaad2ab82671","title":"MPTP 0.2: Design, Implementation, and Initial Experiments"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"8a61ce3920931123d08dce297350c50ecb292d66","title":"Automated generation of readable proofs with geometric invariants"},{"paperId":"7ee54cd7419f1f90912508fdbdc37fdd47dc0947","title":"Isabelle: A Generic Theorem Prover"},{"paperId":"38a5fc91e0366bbd5121154aeb498a1a83573dd5","title":"Basic principles of mechanical theorem proving in elementary geometries"},{"paperId":"b9124861e4e874bbc477b4b726adf94f7d2ecdc4","title":"Learning"},{"paperId":"9fc77941297522cc420ce9292193dd04ed2ed1af","title":"Natural Language Input for a Computer Problem Solving System"},{"paperId":"7b202896ca3151fabb4c4197c329b530491c2b3e","title":"Empirical explorations of the logic theory machine: a case study in heuristic"},{"paperId":"9b7856e0b9b3187085567212471eee412eef8131","title":"Empirical Expiorations of the Geometry Theorem Machine"},{"paperId":"ecb6cad427818163b27bff2241edd1c8a7eb5946","title":"An Augmented Benchmark Dataset for Geometric Question Answering through Dual Parallel Text Encoding"},{"paperId":"760561c57f68044e2f1d089088df1da6c627b09a","title":"On the Advance of Making Language Models Better Reasoners"},{"paperId":"52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"4aca69be58a271b1be45ec7ebb3586569cec50b0","title":"ArMATH: a Dataset for Solving Arabic Math Word Problems"},{"paperId":"4578717d5593b88e1c10555ce67a14be312b84b2","title":"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning"},{"paperId":null,"title":"2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales"},{"paperId":null,"title":"An emerging area of research aims to combine elements of informal and formal theorem proving"},{"paperId":null,"title":"language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever"},{"paperId":null,"title":"Enhancing selfconsistency and performance of pretrained language models with nli"},{"paperId":"ec15ff1fc5c9780fd91902f286a9e3fcd00b890d","title":"Math Word Problem Solving with Explicit Numerical Values"},{"paperId":"494a102164b790a63dc99385b0e56d1e80e0a93c","title":"An Edge-Enhanced Hierarchical Graph-to-Tree Network for Math Word Problem Solving"},{"paperId":"a2fff5686bdf8e25003097107516d6ac13bf8b8e","title":"Improving Math Word Problems with Pre-trained Knowledge and Hierarchical Reasoning"},{"paperId":"adc61e21eafecfbf6ebecc570f9f913659a2bfb2","title":"Deep Learning Based Text Classification: A Comprehensive Review"},{"paperId":null,"title":"2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond"},{"paperId":null,"title":"Lisa: Language models of isabelle proofs"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":null,"title":"2022b). To address this issue, new benchmarks are proposed from various aspects"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Meta-math: A Computer Language for Mathematical Proofs"},{"paperId":"998bcc21375a5639e1b65bb6eb42e8cf5c30907a","title":"Synthesis of Solutions for Shaded Area Geometry Problems"},{"paperId":"eb1179c1a8514fee677d9f23ab112367adef6321","title":"The coq proof assistant reference manual"},{"paperId":"162d958ff885f1462aeda91cd72582323fd6a1f4","title":"Gradient-based learning applied to document recognition"},{"paperId":"4a59af12923633416e5d79f4a85285f154375d26","title":"Isabelle"},{"paperId":"78fc3b741828124d8a79667466516a544143bf68","title":"Computers and Thought"},{"paperId":"a4eb8d6ee85af4d82ace0a1989f5af7baeecc747","title":"An Introduction to Java Geometry Expert"},{"paperId":null,"title":"Tianlong Ma, and Liang He. 2022a. A survey of human-in-the-loop for machine learning. Future Generation Computer Systems"},{"paperId":null,"title":"2022. Learning to understand plane geometry diagram"},{"paperId":null,"title":"2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)"},{"paperId":null,"title":"2022a. Holistic evaluation of language models"},{"paperId":null,"title":"2022. Estimating numbers without regression"}],"id":"2dbec38fe353ab0e495ad09263389dbc9260824d","summary":"This survey paper reviews the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade, and evaluates existing benchmarks and methods and discusses future research directions in this domain."},{"url":"https://www.semanticscholar.org/paper/ee7b871213e1deafadea4b7752467b5c5ab1b9fb","title":"GPT Can Solve Mathematical Problems Without a Calculator","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":5,"influentialCitationCount":0,"publicationDate":"06/09/2023","authors":"Z. Yang,Ming Ding,Qingsong Lv,Zhihuan Jiang,Zehai He,Yuyi Guo,Jinfeng Bai,Jie Tang","citations":[{"paperId":"5ee871537ae51e7e2e93d2a70fff5d100649a655","title":"Mathematical Language Models: A Survey"},{"paperId":"4726d1dc54851db99c29180127d840bd19f20afc","title":"Positional Description Matters for Transformers Arithmetic"},{"paperId":"f3fad1f742625a94f1ffe050987076111bb7b129","title":"How Far Have We Gone in Vulnerability Detection Using Large Language Models"},{"paperId":"a3ab8ff13f86beb677c315a1877b4be72a02d121","title":"Language Models as a Service: Overview of a New Paradigm and its Challenges"},{"paperId":"a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9","title":"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"}],"references":[{"paperId":"fc1ffc7df07cc9b665deca4a94b871732e1f0b4d","title":"Length Generalization in Arithmetic Transformers"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"e5adc219685c9941b9a3d029480af4a51c0ea05a","title":"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"7d645a3fd276918374fd9483fd675c28e46506d1","title":"Galactica: A Large Language Model for Science"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"88dd119dba5ee747851ade8f5d517b381614d918","title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"75bb9eda70751c63fc54dbe63377c673b7dbdb15","title":"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","title":"Training Compute-Optimal Large Language Models"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"50796b0f3edf9cb5ff1e447c298b33755378aa4f","title":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"44507bde6e9caf60b41c60d703e7972b520d48a6","title":"Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems"},{"paperId":"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","title":"Measuring Massive Multitask Language Understanding"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"Chinese-vicuna: A chinese instruction-following llama-based model"},{"paperId":null,"title":"Some cases generated by MathGLM on arithmetic tasks and math word problems"},{"paperId":null,"title":"Moss github"},{"paperId":null,"title":"mkai.org/chatgpt-optimizing-language-models-for-dialogue"},{"paperId":null,"title":"Baichuan-7b"}],"id":"ee7b871213e1deafadea4b7752467b5c5ab1b9fb","summary":"The MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set."},{"url":"https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe","title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":77,"citationCount":20,"influentialCitationCount":1,"publicationDate":"21/10/2022","authors":"Alessandro Stolfo,Zhijing Jin,K. Shridhar,B. Scholkopf,Mrinmaya Sachan","citations":[{"paperId":"f30b720e34d405f200270a6ef2d09e98585fb4d1","title":"CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models"},{"paperId":"44ffd3cd333bb41b900926556edb1c452759c398","title":"WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models"},{"paperId":"3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models"},{"paperId":"04dd492b506e48b7dd91ecb1e1fdb80c1ce30e34","title":"Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs"},{"paperId":"30ea884a5b8e4cffe18ebfef6c033e5c37b79eeb","title":"World Models for Math Story Problems"},{"paperId":"a563fa042b16533d23829d76e7e17bf19a05891c","title":"Large Language Models Are Not Strong Abstract Reasoners"},{"paperId":"5dc15ac1c92ab7492f121471823fb13a95d273ba","title":"A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis"},{"paperId":"3b0c49ca5ac0f441c302c9ca4def4804253552d5","title":"Robust Prompt Optimization for Large Language Models Against Distribution Shifts"},{"paperId":"4313b009cd30a469400d36b9fb20267620293d04","title":"A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers"},{"paperId":"bc2e8b613335259598ea5c49aea270469e9a35ed","title":"A PhD Student's Perspective on Research in NLP in the Era of Very Large Language Models"},{"paperId":"fad03a833345b38323d97ee4818d341d6ba4fbdf","title":"Measuring Consistency in Text-based Financial Forecasting Models"},{"paperId":"9a20f044fbb9c3dce0feebe0339eca56d68781dc","title":"Estimating the Causal Effects of Natural Logic Features in Neural NLI Models"},{"paperId":"b880c3d5c0116507e08ac33ff90d8371d75e333e","title":"Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility"},{"paperId":"7a6dc7071891cb3d658c93418801942a4c6ed373","title":"Autonomous GIS: the next-generation AI-powered GIS"},{"paperId":"3922365b7b40a447ecc57e027530cc90131e171e","title":"NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports"},{"paperId":"12d16f426edc6ab248fb476007bd1646282d4d68","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"8fd462f6248d5e3f1b6602697c09489086b5655f","title":"Distilling Reasoning Capabilities into Smaller Language Models"},{"paperId":"e6745fb621481ccb0ed53c267a37292e499c1b42","title":"Automatic Generation of Socratic Subquestions for Teaching Math Word Problems"},{"paperId":"8171511ceefbcb5b563432818c905d5c24ee7294","title":"Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution"},{"paperId":"72123a86eae2cb5c4eae8650f43524039d48875d","title":"Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions"}],"references":[{"paperId":"0a94fbb5e1c93513523f00e75d672ef4553861f9","title":"Can Large Language Models Infer Causation from Correlation?"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"c64c49571cea94efb31fab21ef255b2d4cecf707","title":"Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"39238a92de090c104936a4f78375b95600e42ce5","title":"NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"996445d847f06e99b0bd259345408a0cf1bce87e","title":"Locating and Editing Factual Associations in GPT"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"abdcbc579a9b63d19bda86569e220a54e4dad1ba","title":"Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"37588705a2af7d5b24d901dd33ade1ff293aabdd","title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"58c7a31bf47948d936de937b1cf7b49463608557","title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"},{"paperId":"130d432ccbc836380a212bea618f84ff094a6a52","title":"Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond"},{"paperId":"488de57dde884402d88ccecfd02347dd7e4d01a1","title":"Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models"},{"paperId":"57a1258571a21817d89197dc84c986861fb6e580","title":"Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning."},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"32999b20f890bcc5effe80197045d6c147226fe4","title":"Findings of the Association for Computational Linguistics: EMNLP 2020"},{"paperId":"4740ef8dc634864695b5b933f416980b9eb6555b","title":"An Empirical Investigation of Contextualized Number Prediction"},{"paperId":"f207387b23581aea152d167e1c8e573461e10cb8","title":"Adjusting for Confounding with Text Matching"},{"paperId":"f13e41d24e5d0a68ca662c1b49de398a6fb68251","title":"A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"4f0318290bc75294338fdeb450e4365929b3aa0c","title":"CausaLM: Causal Model Explanation Through Counterfactual Language Models"},{"paperId":"33ec7eb2168e37e3007d1059aa96b9a63254b4da","title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"},{"paperId":"131c19dc3d460e1973d4b015cc9f888bae4f200b","title":"Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"47f1eb0dc42189ba7cf21b76598c8217eb1b6e05","title":"Learning the Difference that Makes a Difference with Counterfactually-Augmented Data"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"09428af106c378616d7767a37c4f4070a2664e5a","title":"Adapting Text Embeddings for Causal Inference"},{"paperId":"f6e6c948a2074e38e0a4e9099c0f63773c6013dd","title":"Causality"},{"paperId":"e02ae07b45a2241f7ee1180b446ed7ba208c51e8","title":"Elements of Causal Inference: Foundations and Learning Algorithms"},{"paperId":"81f466a535cdec4957989999f9ca381bc4fe14e9","title":"From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems"},{"paperId":"7220d7ec31f6dc6ad7030f601743b5392513a2d9","title":"Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"c87dccf7c21e67679389f23f86f039cd96720c3f","title":"Solving Geometry Problems: Combining Text and Diagram Interpretation"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"3a395daf6c97c084cf9c3827384c53caf6502921","title":"Learning to Automatically Solve Algebra Word Problems"},{"paperId":"9162873c2beca61d97699c2a4491aa123a1c6c75","title":"The independence of language and mathematical reasoning."},{"paperId":"d8574a62874312b81347438b1566cdb1c6d5abe5","title":"Direct and Indirect Effects"},{"paperId":"81a15463a09b9555a78b755e43f9a1c278321ce3","title":"Causal diagrams for empirical research"},{"paperId":"d319aeb4c73bd0ff650a5c83630b3040b42759b5","title":"THE NUMBER π"},{"paperId":"9fc77941297522cc420ce9292193dd04ed2ed1af","title":"Natural Language Input for a Computer Problem Solving System"},{"paperId":null,"title":"Stanford alpaca: An instruction-following llama model"},{"paperId":"0b031700fd4fb450ee7fabbd2120900e73c95c54","title":"CausalNLP Tutorial: An Introduction to Causality for Natural Language Processing"},{"paperId":"52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"0266eac00547e099ccc654907ab5e7b9335d4293","title":"Mining the Cause of Political Decision-Making from Social Media: A Case Study of COVID-19 Policies across the US States"},{"paperId":"6806b0d1e425639a42da4763f170569b86448c87","title":"A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More"},{"paperId":"d08e4348d197e45871c133090ccd5ce76a4ae21b","title":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)"},{"paperId":null,"title":"2021c. Causal direction"},{"paperId":null,"title":"2021): removing the question from the MWP templates, we observe a sensitivityrobustness degradation to random guessing (i.e., TCE ≃ DCE)"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"78d08b8ab4132defffe98ec7f80a51452203f70d","title":"Investigating Gender Bias in Language Models Using Causal Mediation Analysis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"2795be658d617b651865d15c2f151b0eecf340f0","title":"States"},{"paperId":"1527aa0f0b4d095fd92b3e88bf7c178d7703f2cb","title":"Learning Pipelines with Limited Data and Domain Knowledge: A Study in Parsing Physics Problems"},{"paperId":"88c68cc460d0fb9a316a4a91ffee6af681b1c2df","title":"The Critique Of Pure Reason"},{"paperId":null,"title":"Thought beyond language: Neural dissociation of algebra and natural language"},{"paperId":null,"title":"Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?"},{"paperId":null,"title":"error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?"},{"paperId":null,"title":"the accuracy of the models considered in terms of accuracy at 1 and accuracy are displayed"},{"paperId":null,"title":"2023. Psychologically-inspired causal prompts"},{"paperId":null,"title":"1 How do the intervention"},{"paperId":null,"title":"B.3 What is the relation between and the RCC metric? We examine the relationship between and robustness,"}],"id":"9b45af10429681249fafb07c3b6012ea4ce63ffe","summary":"This work proposes a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution, and applies this framework on a test bed of math word problems."},{"url":"https://www.semanticscholar.org/paper/002cfed5d4d9bf2fdaddb11d32f14751f2250e0c","title":"Teaching Arithmetic to Small Transformers","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":15,"influentialCitationCount":1,"publicationDate":"07/07/2023","authors":"Nayoung Lee,Kartik K. Sreenivasan,Jason D. Lee,Kangwook Lee,Dimitris Papailiopoulos","citations":[{"paperId":"18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6","title":"Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception"},{"paperId":"4726d1dc54851db99c29180127d840bd19f20afc","title":"Positional Description Matters for Transformers Arithmetic"},{"paperId":"dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills"},{"paperId":"ed3ec3ba82f7ba7adf8aa47a73c685ac023d6efd","title":"Looped Transformers are Better at Learning Learning Algorithms"},{"paperId":"7d083d654f66f763302d8a5f0678beb753f6507b","title":"Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review"},{"paperId":"42b877f7423fc727bff5b6e173ad336da33079a9","title":"Uncovering hidden geometry in Transformers via disentangling position and context"},{"paperId":"1effda8ce21573ed864eadfdc7b233aac39b8fe6","title":"Amortizing intractable inference in large language models"},{"paperId":"45ee010607cad91728ae7fbad6cce3d805b93526","title":"Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation"},{"paperId":"e892a225c417fbac7545c3e31b45d1c42dc9c933","title":"Chain-of-Thought Reasoning is a Policy Improvement Operator"},{"paperId":"b1fe7bdcfef4e12febe7e8bed8826e66689d60ed","title":"Auto-Regressive Next-Token Predictors are Universal Learners"},{"paperId":"cfe6d6796e37ff6ff40d3ca30e59c3df4cd47ad6","title":"Can transformers learn the greatest common divisor?"},{"paperId":"edea0781ef29b06dc5d8aa7e9ddab3cffe87b80a","title":"It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models"},{"paperId":"d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","title":"Exposing Attention Glitches with Flip-Flop Language Modeling"},{"paperId":"69bfa665e507fcee4a8d003933998eb89f336c9f","title":"Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning"},{"paperId":"bba47079d385bab0facf4527d5da35b7ff0a5c3d","title":"Basic Arithmetic Properties in the Space of Language Model Prompts"}],"references":[{"paperId":"d40dbe668d5b68419e934dfa4c5851ffa1c24aa2","title":"Exposing Attention Glitches with Flip-Flop Language Modeling"},{"paperId":"be8db99310602d66bba64bcf41a572c45816fbfc","title":"Let's Verify Step by Step"},{"paperId":"aec826ff336ca442697d5f908ab1668f1ea18987","title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"},{"paperId":"f9570989919338079088270a9cf1a7afc8db8093","title":"DataComp: In search of the next generation of multimodal datasets"},{"paperId":"9e3c493fb09dcd61bb05e8c5659f23327b7b6340","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"574beee702be3856d60aa482ec725168fe64fc99","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"d4c60620570801a231a7756f931dda1740288fb9","title":"Looped Transformers as Programmable Computers"},{"paperId":"3cbffab9d7981da6662d474aaa056dcbd3c1701e","title":"Emergent analogical reasoning in large language models"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"379e42895f6d40ab9e9559609f505aba89145a5d","title":"Efficiently Scaling Transformer Inference"},{"paperId":"49e46e615747f258517248de5a736814fada17ee","title":"What is my math transformer doing? - Three results on interpretability and generalization"},{"paperId":"3fa70115248377c3d1517c9f978791a296fbc1dd","title":"Large Language Models Can Self-Improve"},{"paperId":"1bb6d5761903c7ac978188ae36e2648905e95dc5","title":"Transcending Scaling Laws with 0.1% Extra Compute"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"2a7ae3e98357569c41424dacd60c62d3df78a0db","title":"Limitations of Language Models in Arithmetic and Symbolic Induction"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"bd1331b233e84bab7eba503abc60b31ac08e7881","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"45ece6f3b0a319dba60c20b3013b5161dd49c58b","title":"Linear algebra with transformers"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"9f8451b9bf14bca1ca42a9d2131c22c443d9e827","title":"A Data-Centric Approach for Training Deep Neural Networks with Less Data"},{"paperId":"e2c4dabd962d32a8c439b4d9ef9fafdf6235ae45","title":"Data-Centric AI Requires Rethinking Data Notion"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"0e9ac2cfc5a3ecb66eeace720901390f7809ba0a","title":"Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"f56fc41eff96afbc64030ce5e97d444706e4997c","title":"Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"509b4661ed74a24c2ffdbf131f9e1c6a1783752d","title":"Are Transformers universal approximators of sequence-to-sequence functions?"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"874e9318c09c711ecd48a903b3824a3a03e2cd62","title":"Explain Yourself! Leveraging Language Models for Commonsense Reasoning"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"42dea2a24629bd4b1b56619536e263b078becddd","title":"Towards Synthesizing Complex Programs From Input-Output Examples"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"6b024162f81e8ff7aa34c3a43d601a912d012c78","title":"Making Neural Programming Architectures Generalize via Recursion"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"4ea80c206b8ad73a6d320c9d8ed0321d84fe6d85","title":"Recursive Neural Networks for Learning Logical Semantics"},{"paperId":"7d335f988990f20e0e2d0085e60be24ff3bd56d6","title":"Learning to Discover Efficient Mathematical Identities"},{"paperId":"08caf0d048cfbe528f8d9db7005d040f76b55c24","title":"Can recursive neural tensor networks learn logical reasoning?"},{"paperId":"121e71ddc1ba36ebfc0ed5001a4c8a5fece65b99","title":"The algebraic combinatorial approach for low-rank matrix completion"},{"paperId":"ee7778874a4ff13edc60ff37a7c3c0964173e0e8","title":"A Simpler Approach to Matrix Completion"},{"paperId":null,"title":"Total number 1-digit 2-digit 3-digit 0-carry-ons 1-carry-ons 2-carry"},{"paperId":null,"title":"Star: Self-taught reasoner bootstrapping reasoning with reasoning"},{"paperId":"c4cb90a67f45e7cbacb5286e934b309e89843922","title":"Attention is Turing-Complete"},{"paperId":"b59947541d2ac4211c4b17554b2e16c260299bed","title":"Have You Seen That Number? Investigating Extrapolation in Question Answering Models"},{"paperId":null,"title":"Open clone of openai’s unreleased webtext dataset scraper"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Introducing mpt-7b: A new standard for open source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b"},{"paperId":null,"title":"For each of the four formatting techniques, as applied to each arithmetic operation we provide the details below. (i) Plain refers to the simplest formatting where we simply create a sequence as the"},{"paperId":null,"title":"Andrej karpathy's lightweight implementation of medium-sized gpts. GitHub, 2022"}],"id":"002cfed5d4d9bf2fdaddb11d32f14751f2250e0c","summary":"This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective."},{"url":"https://www.semanticscholar.org/paper/5df55d94ab5026ff84ec01871592108fbadbddbe","title":"Measurement Extraction with Natural Language Processing: A Review","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":175,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jan Göpfert,Patrick Kuckertz,J. Weinand,Leander Kotzur,D. Stolten","citations":[{"paperId":"a6a757eb09b88ebcbd10c74abb3dab62911ba4a7","title":"Applications of Data Science and Artificial Intelligence"}],"references":[{"paperId":"251c3afbaafcc9b5178534be9109f644bfc5e912","title":"Enhancing Knowledge Bases with Quantity Facts"},{"paperId":"c11b6653a4af4ab030ff91d3183f090694115b07","title":"FiNER: Financial Numeric Entity Recognition for XBRL Tagging"},{"paperId":"24f09c1eb5d7b662bd53892f846600c5e0b66a6c","title":"Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"},{"paperId":"44a418cbd4116d050c6164ccc36933935057f0a7","title":"ChemDataExtractor 2.0: Autopopulated Ontologies for Materials Science"},{"paperId":"2f473817144c9adeaf6e10d75facc746a1118f95","title":"Addressing disorder in scholarly communication: Strategies from NISO Plus 2021"},{"paperId":"b894b9823f921217979bf7006f8cf1b3e5248ee1","title":"Geo-Quantities: A Framework for Automatic Extraction of Measurements and Spatial Context from Scientific Documents"},{"paperId":"40a1f266bb5ca853837355bfff272a55f0049c81","title":"Mining Numbers in Text: A Survey"},{"paperId":"1bd44ce763d246b44426a146fd5239b34b852c3d","title":"What's in a Measurement? Using GPT-3 on SemEval 2021 Task 8 - MeasEval"},{"paperId":"3e1c30939ef7a6576e3b2c7b9c243576632d73dc","title":"Analyzing Research Trends in Inorganic Materials Literature Using NLP"},{"paperId":"2781182a38b6303dc5b4cb224579e9a365156bc3","title":"LATEX-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes"},{"paperId":"825c5ee9dfb9bf729830452132db84872b5e691c","title":"UPB at SemEval-2021 Task 8: Extracting Semantic Information on Measurements as Multi-Turn Question Answering"},{"paperId":"1cd3be6144ecf29f4434727a7a577245baafb3e8","title":"Counts@IITK at SemEval-2021 Task 8: SciBERT Based Entity And Semantic Relation Extraction For Scientific Data"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"4d0ad7ab446fe5d9097ac19cd0965913f1a110b2","title":"AnaSearch: Extract, Retrieve and Visualize Structured Results from Unstructured Text for Analytical Queries"},{"paperId":"2cc3ab9fa41ba2804e301f7eae9598636e62422a","title":"Investigating the Limitations of Transformers with Simple Arithmetic Tasks"},{"paperId":"b964afe5b755022f1f1e6915d23df9a7f65c911c","title":"FLERT: Document-Level Features for Named Entity Recognition"},{"paperId":"0772212420238b48233256a59912f8e6a31cde3d","title":"NumClaim: Investor's Fine-grained Claim Detection"},{"paperId":"eedf2748a9a1ba2779cde95fd8bad9c2260d5317","title":"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention"},{"paperId":"4aa6e5d8e95810a21a81fd37321f5b2fe1c779bc","title":"Creating Hardware Component Knowledge Bases with Training Data Generation and Multi-task Learning"},{"paperId":"dc6c247b65ff296f89745cf37bcacd8e21a096ba","title":"Clinical quantitative information recognition and entity-quantity association from Chinese electronic medical records"},{"paperId":"2e44f39e9887e1cdd91d48ab18a0bae53ff7f81a","title":"AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types"},{"paperId":"595e215e2e96f52f4e617447b60cbee35ec8297f","title":"The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"2021afefda7a7e7721828893a757e3fdb92efb62","title":"Entities with Quantities: Extraction, Search, and Ranking"},{"paperId":"c01dcc5531f818a253f01c758f396de6783073f6","title":"LATTE: A knowledge-based method to normalize various expressions of laboratory test results in free text of Chinese electronic health records"},{"paperId":"718c48795c2ef0fccf5ab37809678119cae51078","title":"EXTraction of EMR numerical data: an efficient and generalizable tool to EXTEND clinical research"},{"paperId":"99f1898b9466fcf3f13061402a6ac70bb9f42c10","title":"Qsearch: Answering Quantity Queries from Text"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"4628e4ce1a897660243853a6dfdc2de4dbd10c31","title":"Ontology-based clinical information extraction from physician's free-text notes"},{"paperId":"ac0f9bd6d00644215515be42d203f59945a41107","title":"Text-mined dataset of inorganic materials synthesis recipes"},{"paperId":"e34ac69ea76bf45d8c1da6f639dced7c0d965222","title":"Automatic Identification and Normalisation of Physical Measurements in Scientific Literature"},{"paperId":"e9074a7a43c6e3b133c45960fcec0a9a34e59fdd","title":"Twenty-five years of information extraction"},{"paperId":"0427110f0e79f41e69a8eb00a3ec8868bac26a4f","title":"Do NLP Models Know Numbers? Probing Numeracy in Embeddings"},{"paperId":"76fd100c9776f9d42c61618f31df7a33a6ffda00","title":"A Novel Cascade Binary Tagging Framework for Relational Triple Extraction"},{"paperId":"52fa450740913a6cdcb4d9395b45e203f46cab79","title":"Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"2a0fc534ebf1b22b5851f5b1fb781dd707f7c530","title":"Numeral Attachment with Auxiliary Tasks"},{"paperId":"11bb73c6cdec9c5ce579194c26ad74c722e32f95","title":"Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"},{"paperId":"9fc8a8a9d54a8f598e98bbc52001af13a40007d9","title":"Automated Detection of Measurements and Their Descriptors in Radiology Reports Using a Hybrid Natural Language Processing Algorithm"},{"paperId":"0e5f4ce4c3ce2a208a5353825b1fd7223b89a50e","title":"ConTrOn: Continuously Trained Ontology based on Technical Data Sheets and Wikidata"},{"paperId":"2bbca6254a1866f7b7bff977aac504dd01852bf4","title":"Proposal for Automatic Extraction Framework of Superconductors Related Information from Scientific Literature"},{"paperId":"2c5ec74fb56fbfbceaa4cd5c8312ada4e2e19503","title":"Entity-Relation Extraction as Multi-Turn Question Answering"},{"paperId":"ed7f9ecb033ea3f139bc97096caf406a8f0d91a9","title":"CrowdPT: Summarizing Crowd Opinions as Professional Analyst"},{"paperId":"6b436ec854ca81cc04342a89585621d9f812e1c6","title":"A Machine Learning Approach to Zeolite Synthesis Enabled by Automatic Literature Data Extraction"},{"paperId":"5160254e9dd5d6c0006f113a5784467eedd449e0","title":"Complexities, variations, and errors of numbering within clinical notes: the potential impact on information extraction and cohort-identification"},{"paperId":"5170e27a034701d649ad51716893b022278e5977","title":"Accurate Product Attribute Extraction on the Field"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"acc2ad56a9c68c799747e08d978f9803997c1527","title":"Inorganic Materials Synthesis Planning with Literature-Trained Neural Networks"},{"paperId":"6c7b92b30efc44d2cde9697dbaa2e18720aa1c8b","title":"Last Words: What Can Be Accomplished with the State of the Art in Information Extraction? A Personal View"},{"paperId":"cee1f8cf54f2d8c278a66debf8d76a1957bd7012","title":"An Automated Approach for Clinical Quantitative Information Extraction from Chinese Electronic Medical Records"},{"paperId":"cabb0a468af8184e0e930841435b65679b580521","title":"Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting"},{"paperId":"2be63d6b7dc137aa202907a471243e91deab258d","title":"Textual Analogy Parsing: What’s Shared and What’s Compared among Analogous Facts"},{"paperId":"5b82372ed9fcd64e92ac449a4cb9c0c4d977c0c2","title":"By the numbers: The magic of numerical intelligence in text analytic systems"},{"paperId":"a810362ca5284e5894b07abb76b8f647802fe32c","title":"The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"},{"paperId":"45f146bdf91198ea93858d0619c2acc5761e540d","title":"Open Information Extraction from Conjunctive Sentences"},{"paperId":"9a7ceaaaf443122de6edd6be4ec209dafc4ff418","title":"Annotating Measurable Quantitative Informationin Language: for an ISO Standard"},{"paperId":"7a628a7e61904f7dca8757ca9205c08a325adf74","title":"Quantitative Information Extraction From Social Data"},{"paperId":"825546fd7a3c942b51174460e2549aba09ecaa8b","title":"Auto-generated materials database of Curie and Néel temperatures via semi-supervised relationship extraction"},{"paperId":"757fb787cb4fa914e9dc20512ce03a1728ebb230","title":"OpenTag: Open Attribute Value Extraction from Product Profiles"},{"paperId":"e4715a13f6364b1c81e64f247651c3d9e80b6808","title":"Link Prediction Based on Graph Neural Networks"},{"paperId":"3eea18aa966e605ffc71b2d476325c04c3d46b2f","title":"Combining pattern matching with word embeddings for the extraction of experimental variables from scientific literature"},{"paperId":"88ddea93a98bae235af462d2b93ad6e6a703f742","title":"EliIE: An open-source information extraction system for clinical trial eligibility criteria"},{"paperId":"e69c18f7e5c424e57adb0d93445997d348ae2cc9","title":"Materials Synthesis Insights from Scientific Literature via Text Extraction and Machine Learning"},{"paperId":"b3ceb1ed2f5396ca43254e6ce6ede440dbe9af7b","title":"A Robust Number Parser Based on Conditional Random Fields"},{"paperId":"e326d127fe781abd306367c9c9e99333c0f9a669","title":"Machine-learned and codified synthesis parameters of oxide materials"},{"paperId":"6b1dc96587ee83c19c0abe7dcbd4c419776a5a5b","title":"Measurement Context Extraction from Text: Discovering Opportunities and Gaps in Earth Science"},{"paperId":"f7630f2e3ddedc442071252ae42a6d82d521bcd4","title":"Correlating Lab Test Results in Clinical Notes with Structured Lab Data: A Case Study in HbA1c and Glucose"},{"paperId":"29324d2dbe98a97e0647193b39fe8f4aae4edc37","title":"ChemDataExtractor: A toolkit for automated extraction of chemical information from the scientific literature"},{"paperId":"d0d030ccebd669ed5faeca504091bbe5fff4aefb","title":"Bootstrapping for Numerical Open IE"},{"paperId":"4f4eb0381c9287e4a73624fccc231aacf875d95a","title":"Unlocking echocardiogram measurements for heart disease research through natural language processing"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a24412de657c0196f2a85c542aa137fc7f6e7f87","title":"Chaudron: Extending DBpedia with Measurement"},{"paperId":"3ceb067197503885bd45e8dd75f0263cc9664ce4","title":"Cardinal Virtues: Extracting Relation Cardinalities from Text"},{"paperId":"5ad176d3de3e06a0c0d9bf875bd9159af7546da8","title":"Congestive heart failure information extraction framework for automated treatment performance measures assessment"},{"paperId":"0e46803ac8fc715b72d7f935a3f383ade945487f","title":"Fonduer: Knowledge Base Construction from Richly Formatted Data"},{"paperId":"4d855ca307c6c0a6f9c9e647dd32c9c8a87b3490","title":"Extraction of left ventricular ejection fraction information from various types of clinical reports"},{"paperId":"2d7fcca1b0bde8e3f0450f4c8e67e6cbf519bff1","title":"Equation Parsing : Mapping Sentences to Grounded Equations"},{"paperId":"c825b92effcf400ab1679071cef8233a5a6a18ba","title":"How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions"},{"paperId":"4477c61d6350f4d700e5488e204d845ef2aacc6a","title":"Tumor information extraction in radiology reports for hepatocellular carcinoma patients"},{"paperId":"698d0035950d70651e258ae7506f2ac69c440110","title":"A PUT-Based Approach to Automatically Extracting Quantities and Generating Final Answers for Numerical Attributes"},{"paperId":"67c6251b678cb6a837d8a968e720ee807cf09df6","title":"Valx: A System for Extracting and Structuring Numeric Lab Test Comparison Statements from Text"},{"paperId":"6149eea0b589fddbbeda46a0a92da3c097d2ef39","title":"Semantic NLP-Based Information Extraction from Construction Regulatory Documents for Automated Compliance Checking"},{"paperId":"513167c08db5139162710aad9b2c217b344df2c4","title":"Numerical Relation Extraction with Minimal Supervision"},{"paperId":"e61872836a0b862ba020d7d4b42d933800c55d8d","title":"The development of models to predict melting and pyrolysis point data associated with several hundred thousand compounds mined from PATENTS"},{"paperId":"46604a57e942c45e1d49d0ddfdc3b4d4b084f826","title":"Natural Language Processing Techniques for Extracting and Categorizing Finding Measurements in Narrative Radiology Reports"},{"paperId":"c4ee4ca9259f0fb1d790e4b78025532a6b592d55","title":"Framework for automatic information extraction from research papers on nanocrystal devices"},{"paperId":"446f1eaec22a90574670491073cd5b03bfa1e273","title":"Identification and Verification of Simple Claims about Statistical Properties"},{"paperId":"31d41baf65e8608d89579a75c2ecfb92367d0f8e","title":"DEXTER: Large-Scale Discovery and Extraction of Product Specifications on the Web"},{"paperId":"af88ce6116c2cd2927a4198745e99e5465173783","title":"Bidirectional LSTM-CRF Models for Sequence Tagging"},{"paperId":"9653d5c2c7844347343d073bbedd96e05d52f69b","title":"Pointer Networks"},{"paperId":"b4ed5c9458da8b98b528aa64cbe60cb0794041a6","title":"Mining Measured Information from Text"},{"paperId":"1ada64844626a284a4d014293bd45775361d6d92","title":"A natural language processing pipeline for pairing measurements uniquely across free-text CT reports"},{"paperId":"0b3999372e8cb960e08680545fc688c294c6a861","title":"LeadMine: a grammar and dictionary driven approach to entity recognition"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"916625b47bb7b5ff558858e19a5393648eb86af2","title":"Processing of Quantitative Expressions with Measurement Units in the Nominative, Genitive, and Accusative Cases for Belarusian and Russian"},{"paperId":"bc51700cfae5ad693530331875b04ab4b62fc03e","title":"Open-domain quantity queries on web tables: annotation, response, and consensus models"},{"paperId":"d7e454698f64d551935f04a12900a0731522840a","title":"Automatic Annotation of Parameters from Nanodevice Development Research Papers"},{"paperId":"408ac938daaf273427d969190c3ac5958bf636b6","title":"Semantic Information Extraction on Domain Specific Data Sheets"},{"paperId":"4a6e392a0f98bfafaff1f19b245304937108db48","title":"Automatic Extraction of Nanoparticle Properties Using Natural Language Processing: NanoSifter an Application to Acquire PAMAM Dendrimer Properties"},{"paperId":"45a2d3d8f8ddd933d7a4d4111d0f057341987f08","title":"Information extraction from nanotoxicity related publications"},{"paperId":"fe264ce1bad4d176cae8387488fd64a536d0724c","title":"Automatically Pairing Measured Findings across Narrative Abdomen CT Reports"},{"paperId":"0692b9ad39145f57237199f3d4488667d5d9e5e7","title":"Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!"},{"paperId":"98446a959c059b0a3908e8c3504be236d70447e9","title":"Getting More Out of Biomedical Documents with GATE's Full Lifecycle Open Source Text Analytics"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"c05a6ed9bcb4adfc1f70805d8b30658a6235f75e","title":"Extracting laboratory test information from biomedical text"},{"paperId":"b4c143e581a0e44a1a73ef84bd02a86e8a9a33d7","title":"Automatic Information Extraction of Experiments from Nanodevices Development Papers"},{"paperId":"a8b54d0801d716033670c368e25967316fa54474","title":"Automated extraction of ejection fraction for quality measurement using regular expressions in Unstructured Information Management Architecture (UIMA) for heart failure"},{"paperId":"fbe358ce706371b93c10c4395cab9a78ad3aef67","title":"Multi-instance Multi-label Learning for Relation Extraction"},{"paperId":"abd0d4ed53a86f7af4674b838d590edff6edf9a4","title":"Natural language processing: an introduction"},{"paperId":"d48edf9e81653f4c3da716b037b0b50d54c5b034","title":"Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations"},{"paperId":"7cd91e3ea97ff7366628370e46a69aaa046f7146","title":"ChemicalTagger: A tool for semantic text-mining in chemistry"},{"paperId":"ff6a34e67bb78105ed599d375932e0f580bad384","title":"SCAD: collective discovery of attribute values"},{"paperId":"6c93b11ce414776553ed5cb9eb5d3cffa2a5fc6b","title":"Extraction and Approximation of Numerical Attributes from the Web"},{"paperId":"bd298c1bcefcd7feb108111cd72758c265d16ee6","title":"Learning 5000 Relational Extractors"},{"paperId":"51e6a2a301c47b4a889c79d2e5e54b560c71ab21","title":"Rule-based information extraction from patients' clinical data"},{"paperId":"d55c8fbb7f314b5599c69d89f5993999cf5553af","title":"GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications"},{"paperId":"d84b57362e2010f6f65357267df7e0157af30684","title":"Distant supervision for relation extraction without labeled data"},{"paperId":"047b4d76b86f3a9a55aba87923af1ed9e6562087","title":"Large-scale, parallel automatic patent annotation"},{"paperId":"4d1e1b15ba023d3a033c02fa34ca0e893b709696","title":"Numerical Data Integration for Cooperative Question-Answering"},{"paperId":"6fe1db3f50a528b60c2cfc8f60c33073e07e031d","title":"Fast Methods for Kernel-Based Text Analysis"},{"paperId":"f4ba954b0412773d047dc41231c733de0c1f4926","title":"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"},{"paperId":"6723dda58e5e09089ec78ba42827b65859f030e2","title":"Message Understanding Conference- 6: A Brief History"},{"paperId":"ac91d2df6a7e01b5f632822de09fa1aef40d7d05","title":"Design of the MUC-6 Evaluation"},{"paperId":"95cef0621d0702905842d69602630d61c2c60c42","title":"Research Papers"},{"paperId":"b672d2ec81c9395c312d57a27931864d07664592","title":"A Meta-framework for Spatiotemporal Quantity Extraction from Text"},{"paperId":null,"title":"BERT trained from scratch"},{"paperId":"dd97b0b041587fe73d4dbfc8a72e2cb615e85cb8","title":"Numerals in Financial Narratives"},{"paperId":"f9b611df731c00f54397e3a0a76e3536ea9079c0","title":"CONNER: A Cascade Count and Measurement Extraction Tool for Scientific Discourse"},{"paperId":"dc9db9aff2995428164ae9b99514323be5d2d7f5","title":"LIORI at SemEval-2021 Task 8: Ask Transformer for measurements"},{"paperId":"fd651b69b5683c79b368c43df4446917a8405946","title":"Extracting Material Property Measurement Data from Scientific Articles"},{"paperId":"958f9294a8607c6a9e65acd82dbb5eb30d502c9c","title":"DPR at SemEval-2021 Task 8: Dynamic Path Reasoning for Measurement Relation Extraction"},{"paperId":"8c0e163f4a66312ebea2d9407c4876277f57f9cd","title":"A Machine Learning Pipeline for Automatic Extraction of Statistic Reports and Experimental Conditions from Scientific Papers"},{"paperId":"900de875b80562ea93ae98e258944a6e33f60dec","title":"Leveraging Knowledge Graph and DeepNER to Improve UoM Handling in Search"},{"paperId":"8893567aa71140c22d61c4e5e8f387f04049c4f4","title":"KGP at SemEval-2021 Task 8: Leveraging Multi-Staged Language Models for Extracting Measurements, their Attributes and Relations"},{"paperId":"e04424dfbfea86b63884585298c39b8887740ea3","title":"CLaC-BP at SemEval-2021 Task 8: SciBERT Plus Rules for MeasEval"},{"paperId":"31699d03a49e38295298f1b1a53185644abba12e","title":"Numeracy enhances the Literacy of Language Models"},{"paperId":"f56110fb7607c30d490ec9be6d5c48e8f2fbcf9c","title":"CLaC-np at SemEval-2021 Task 8: Dependency DGCNN"},{"paperId":"ed8ee851f48f7363122bd54c0b7c717b4981ab99","title":"Stanford MLab at SemEval-2021 Task 8: 48 Hours Is All You Need"},{"paperId":null,"title":"2021b) Char.-level BiLSTM-CRF"},{"paperId":null,"title":"2021a)M Stanford MLab"},{"paperId":"923648ea014394ee8d3c2e04e35e4d790847a43d","title":"Numeral semantics"},{"paperId":"19cecb712b8774ecc09e251e87166b2b7c4d695f","title":"Entities with Quantities"},{"paperId":"3118c0633cb2d0f91b5ef88840343e47c3ca5623","title":"Do Language Embeddings capture Scales?"},{"paperId":"8df11ea11899a4ae24595285e359a3ae1eac1cbf","title":"Ontology-based NLP information extraction to enrich nanomaterial environmental exposure database"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"4a48ead398256ed298c11a883498652f7a6d70b9","title":"Parsing, Representing and Transforming Units of Measure"},{"paperId":"545322f1eef5a3e467a160f2d4037ea139165b4b","title":"Towards Learning from User Feedback for Ontology-based Information Extraction"},{"paperId":null,"title":"Dict.-matching of noun phrases"},{"paperId":"f84fc762a3229f436f2a472bd90ef5070b5d31fb","title":"Clinical information extraction applications: A literature review"},{"paperId":"31f17159a8f51449f7771160300bdea86ab3d38d","title":"QSRL : A Semantic Role-Labeling Schema for Quantitative Facts"},{"paperId":null,"title":"2018) Learned patterns & REGEX"},{"paperId":"e9a33f9582ce8759e69dfc394d5cdc40205ab75d","title":"The representation and extraction of qunatitative information"},{"paperId":null,"title":"2017b,a) NN, dict & DB matching & existing models"},{"paperId":null,"title":"2017c) TUCP, TUCP+Pred"},{"paperId":null,"title":"2021) approaches only the relation extraction subtask of MeasEval and is therefore not considered. Moreover, systems targeting other modalities than text are not considered"},{"paperId":"605fa6d94be36542eadef05a5e3b3cfa1ec37fec","title":"Diamonds in the Rough: Event Extraction from Imperfect Microblog Data"},{"paperId":"b410eda755e8ae42027b585ebddc0b2d5928ccd9","title":"Applications and Challenges of Text Mining with Patents"},{"paperId":"8677cdcf737dc90c037c73dc182968d9ea01d0dd","title":"Improving Heart Failure Information Extraction by Domain Adaptation"},{"paperId":null,"title":"OntoNotes Release 5.0"},{"paperId":null,"title":"Identification of Expressions with Units of Measurement in Scientific, Technical & Legal Texts in Belarusian and Russian"},{"paperId":"84c4b819e4e9422e8e7d8bfaa603e13716a792e0","title":"Barriers to Retrieving Patient Information from Electronic Health Record Data: Failure Analysis from the TREC Medical Records Track"},{"paperId":null,"title":"Quantities, Units, Dimensions and Types"},{"paperId":"dad67f5cab0624a97dc29409439209277bda1e1c","title":"Detecting measurement expressions using NooJ"},{"paperId":null,"title":"REGEX & lexical resources"},{"paperId":"d746cd387522d826b2ab402cb37c96059fa04261","title":"Corroborating Answers from Multiple Web Sources"},{"paperId":"55c38392fddee4107c71a3045f2cbd01ddd984b8","title":"Extraction and Visualization of Trend Information from Newspaper Articles and Blogs"},{"paperId":"1e24769f16f35eb127efcbe40a604892c8e526c4","title":"NTCIR workshop 6 meeting : proceedings of the 6th NTCIR workshop meeting on evaluation of information access technologies: information retrieval, question answering and cross-lingual information access"},{"paperId":"15c4745d23bafe5ffebd40f58b246c806d2e7dbe","title":"Capturing QP-relevant Information from Natural Language Text"},{"paperId":"0617dd6924df7a3491c299772b70e90507b195dc","title":"The Automatic Content Extraction (ACE) Program – Tasks, Data, and Evaluation"},{"paperId":"6bd1a5cb61cde554cbe52e747c94d56e983ecd9e","title":"Overview of MUC-7"},{"paperId":"51995d854ef966de332b93a8a765ce8c74f06255","title":"UvA-DARE (Digital Academic Repository) SemEval-2021 Task 8: MeasEval -- Extracting Counts and Measurements and their Related Contexts"},{"paperId":null,"title":"2022. Explosion/spaCy: New Span Ruler component, JSON (de)serialization of Doc, span analyzer and more"},{"paperId":null,"title":"of the 15th International Workshop on"}],"id":"5df55d94ab5026ff84ec01871592108fbadbddbe","summary":"In this review, an overview of prior work on measurement extraction is presented and different approaches to measurement extraction are described and the challenges posed by this task are outlined."},{"url":"https://www.semanticscholar.org/paper/587f22e4e04d77ba0750deea69192fbfb73d7435","title":"Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":4,"influentialCitationCount":0,"publicationDate":"04/06/2023","authors":"Beichen Zhang,Kun Zhou,Xilin Wei,Wayne Xin Zhao,Jing Sha,Shijin Wang,Ji-rong Wen","citations":[{"paperId":"da89cdeb0014666f4024f797d0c67cd45d92a7c9","title":"FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity"},{"paperId":"b272513916b45c8517d289d7abee4a53e6832187","title":"ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"},{"paperId":"3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models"},{"paperId":"888728745dbb769e29ed475d4f7661eebe1a71cf","title":"A Survey on Evaluation of Large Language Models"}],"references":[{"paperId":"95ca67ba607d7859ee8eec457f4b59b115d69bf5","title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models"},{"paperId":"bcdaf6c98ddbd6809cf6241aa77200d7394db163","title":"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"},{"paperId":"e0f27336698c84709bd60b6b7f4ce588cbae66bf","title":"StructGPT: A General Framework for Large Language Model to Reason over Structured Data"},{"paperId":"5c0f3b0e46e6125bf2f454bcd8565a6f3430a54c","title":"Learning to Plan with Natural Language"},{"paperId":"261549439aebdda72b648ecc462448fd24857ac1","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"57100e39d0413ee585b381ba9ab366e8a6cf2866","title":"Solving Math Word Problems by Combining Language Models With Symbolic Solvers"},{"paperId":"68c834c19cd126bbd6d25a3572d7205cfed76271","title":"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"},{"paperId":"c61d54644e9aedcfc756e5d6fe4cc8b78c87755d","title":"A Survey of Large Language Models"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"b626560f19f815808a289ef5c24a17c57320da70","title":"MathPrompter: Mathematical Reasoning using Large Language Models"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"e070ff286709db28312e08b52b05539debe88146","title":"Measuring and Narrowing the Compositionality Gap in Language Models"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"c88cafa3e980765a64febe369ceb7c2aa7261d2a","title":"Complexity-Based Prompting for Multi-Step Reasoning"},{"paperId":"2a7ae3e98357569c41424dacd60c62d3df78a0db","title":"Limitations of Language Models in Arithmetic and Symbolic Induction"},{"paperId":"cd31ce657fba5dcdd74c557170bbd9ef0636cb13","title":"JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"04b1ef8f8dcfc11fc3dd90af695eede941267998","title":"The Effectiveness of Mind Mapping Learning Models Based on Contextual Learning on Mathematical Problem Solving Ability"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":"3d1b34384499f77bf317b01594c8c60af212582b","title":"SymPy: Symbolic computing in Python"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"17230f5b3956188055a48c5f4f61d131cce0662f","title":"Parsing Algebraic Word Problems into Equations"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":"47ced790a563344efae66588b5fb7fe6cca29ed3","title":"The Probabilistic Relevance Framework: BM25 and Beyond"},{"paperId":"3cc310717f422592c3cc9a046943777468c14358","title":"A Survey of Web Information Extraction Systems"},{"paperId":null,"title":"Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought reasoning"},{"paperId":null,"title":"Shreya Havaldar"}],"id":"587f22e4e04d77ba0750deea69192fbfb73d7435","summary":"Experimental results on CARP and six other datasets show that the proposed DELI mostly outperforms competitive baselines, and can further boost the performance of existing CoT methods."},{"url":"https://www.semanticscholar.org/paper/13ac3b7f1ace63c76c1e081898369f8e0505411c","title":"Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/08/2023","authors":"Dingzirui Wang,Longxu Dou,Wenbin Zhang,Junyu Zeng,Wanxiang Che","citations":[{"paperId":"dd69049674f41d4ef5314b8f95bacfe59de31376","title":"Conditions for Length Generalization in Learning Reasoning Skills"}],"references":[{"paperId":"1e2eba005ccd8ab7a668a525c5b43245853bdaf1","title":"Reasoning in Large Language Models Through Symbolic Math Word Problems"},{"paperId":"8568d7bd9dfb5ba0b91940b938b44a88fafdf95b","title":"Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models"},{"paperId":"d75d11d2c89c01cd284383546ae057cb827dc272","title":"Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning"},{"paperId":"6289de84a02f0c27734f295ada565603ac958948","title":"Tab-CoT: Zero-shot Tabular Chain of Thought"},{"paperId":"8c7846c9805834dbe2fb0c8f48253b8d65b79d6a","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"},{"paperId":"a52dd1e900200e0733eea927edc7d6c27aeba187","title":"TheoremQA: A Theorem-driven Question Answering dataset"},{"paperId":"261549439aebdda72b648ecc462448fd24857ac1","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"57100e39d0413ee585b381ba9ab366e8a6cf2866","title":"Solving Math Word Problems by Combining Language Models With Symbolic Solvers"},{"paperId":"574beee702be3856d60aa482ec725168fe64fc99","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4"},{"paperId":"b626560f19f815808a289ef5c24a17c57320da70","title":"MathPrompter: Mathematical Reasoning using Large Language Models"},{"paperId":"2dbec38fe353ab0e495ad09263389dbc9260824d","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"e826ac71dad8c4ce36d82fb7add43e3d306bb7e1","title":"Making Language Models Better Reasoners with Step-Aware Verifier"},{"paperId":"5fa148e8bfcd5fa3df05311c0ec8abb13caf1c05","title":"GraphQ IR: Unifying the Semantic Parsing of Graph Query Languages with One Intermediate Representation"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"379e9d5bba8a617b3114bd5b562b14aa6abc5282","title":"Natural SQL: Making SQL Easier to Infer from Natural Language Specifications"},{"paperId":"99053e3a708fc27709c9dab33110dc98b187c158","title":"FinQA: A Dataset of Numerical Reasoning over Financial Data"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"28a5a53dafacebad8a7c47773079caeffb9a5baa","title":"Representing Numbers in NLP: a Survey and a Vision"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"34c8b5b816d3ae6d433ca5e5735f264754ff4c2c","title":"N-LTP: An Open-source Neural Language Technology Platform for Chinese"},{"paperId":"ac9173e4b8b34dc24c18980ee32be6bf15e7b38d","title":"Graph-to-Tree Learning for Solving Math Word Problems"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"b9372e972997c5056bb79c70526230baed2e372b","title":"Multi-hop Reading Comprehension through Question Decomposition and Rescoring"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"676a7e362e283cb953135f506eb0a07a8b0be0ed","title":"A Survey on Semantic Parsing"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"e889e2dd1647fdfc28edb9abe08a863b704f08be","title":"Using Intermediate Representations to Solve Math Word Problems"},{"paperId":"ad406dea8c9c616402c000f32261700b77f9ac3a","title":"Multi-hop Inference for Sentence-level TextGraphs: How Challenging is Meaningfully Combining Information for Science Question Answering?"},{"paperId":"3d1b34384499f77bf317b01594c8c60af212582b","title":"SymPy: Symbolic computing in Python"},{"paperId":"2d7fcca1b0bde8e3f0450f4c8e67e6cbf519bff1","title":"Equation Parsing : Mapping Sentences to Grounded Equations"},{"paperId":"4c6fe6179c408e1fbb3871af13d1a8e64f766e54","title":"Solving General Arithmetic Word Problems"},{"paperId":"2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9","title":"MAWPS: A Math Word Problem Repository"},{"paperId":"e72e5ee5de14fd463ab58ce830474157258e3578","title":"Abstract Meaning Representation for Sembanking"},{"paperId":"6eac5e325302826c6778ca7dfa92fb1ceb28f02e","title":"Word Problems"},{"paperId":"700bde5ae8a045bd8519ea45180906f04ebc8dea","title":"Exploring the Secrets Behind the Learning Difficulty of Meaning Representations for Semantic Parsing"},{"paperId":null,"title":"Abstract syntax tree implementation idioms"},{"paperId":null,"title":"Decomposition enhances reasoning via self-evaluation"},{"paperId":null,"title":"A theorem-driven question"}],"id":"13ac3b7f1ace63c76c1e081898369f8e0505411c","summary":"This paper presents a method called Boosting Numerical Reason\\textbfing by Decomposing the Generation of Equations (Bridge), which can improve the accuracy of LLMs in generating equations as IMRs by reducing the tendency of generating constant expressions and programs."}]}